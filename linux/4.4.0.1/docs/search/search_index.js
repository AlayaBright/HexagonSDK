const local_index = {"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Hexagon SDK overview The Qualcomm Hexagon\u2122 SDK provides a set of tools, software, and documentation that assist developers in running applications on Qualcomm devices and leveraging the Hexagon DSPs. This page provides a brief overview of the Qualcomm embedded compute devices that this Hexagon SDK supports, the process of writing applications running on the CPU and offloading tasks onto the DSP, and how the Hexagon SDK tools and documentation facilitate this process. Prerequisites The Hexagon SDK installer requires the following: 64-bit Windows 7, Windows 8, or Windows 10 64-bit Ubuntu 16.x or later Getting started For a quick start, browse through the base SDK examples and run some of the examples on simulator and on target. Each of these examples illustrate a few tools or software features as explained in the list of examples table. The calculator example is a good example to start with as it introduces a number of basic concepts. To gain a more solid understanding of the SDK and cDSP that it supports, review the rest of this section. Then browse through the Tools and Software tabs to learn about the development tools and software components available in the SDK. For detailed documentation on a specific topic, see the reference manuals and use the examples for a better understanding. SDK components The Hexagon SDK includes the following components, each covered in one of the documentation tabs and intended to help software developers as follows: Development Tools : Building, running, debugging, and profiling Hexagon-based applications Software Developing Hexagon-based applications for the run-time ecosystem on Qualcomm devices Code Examples : Source projects illustrating different aspects of developing Hexagon-based applications Reference Documentation: Additional resources with in-depth information Add-ons : Additional packages available for installation into this SDK Directory Structure The Hexagon SDK tree structure is as follows: /docs Hexagon SDK documentation /examples Code examples /utils Helper scripts to automate tasks commonly performed by developers /inc Public header files needed to access system-level software libraries /libs System-level software libraries and Hexagon libraries /ipc Inter-processor communication libraries providing FastRPC support /rtos Real-time DSP OS /tools Development tools (see Tools tab) /add-ons SDK add-ons (see Add-ons tab) Development process The process of developing a Hexagon-based feature for a Qualcomm device typically involves several of the following steps (not necessarily always in this order). Develop the core algorithm of interest in C/C++ (for example, in an x86 environment). Once the workload is functional and the top-level API's are defined, build it as a Hexagon shared library and an executable test driver that exports main(), as is done with the examples in this SDK. Build , execute , debug , and profile the test driver on the Hexagon ISS (Instruction Set Simulator) and/or target device. Prepare to integrate the shared Hexagon library into a run-time HLOS application or multimedia framework. To do this, build an inter-processor interface for the shared Hexagon library by using the SDK's FastRPC framework . This might require updating your shared library and test driver to use the RPC interface. Optimize and enhance the Hexagon shared library using the techniques documented in this SDK, such as: Multi-threading the Hexagon workload Utilizing the System Libraries and Hexagon DSP libraries of this SDK Optimizing the core loops by implementing cache pre-fetching, tiling, writing intrinsics or assembly, or in applicable cases, Halide, TVM, or other compiler-based frameworks. Iterating the optimizations with profiling and tracing to reduce cache misses and other stalls until satisfactory performance is reached. Architecture overview This section provides an overview of Hexagon processors supported by this SDK and used for embedded advanced computing applications. Qualcomm Snapdragon processors Qualcomm Technologies, Inc. (QTI) offers a large and increasing number of variants of the Snapdragon chipset solution. The Snapdragon mobile product family is organized into five product tiers. The highest tier includes the SM8xxx series (premium) and SM7xxx series (high tier). Lower tiers include the SM6xx, SM4xx and SM2xx series. These product tiers are differentiated by scalable computing resources for the CPU, GPU, and DSP processors. When moving from low to premium tiers, these processor resource changes are characterized by an increasing number of processors, increasing processor complexity, and increasing clock speeds. For a full list of products, visit the space on mobile processors on our website. The following diagram provides an overview of one representative premium tier product: the SM8150 chipset. The processing units include a Kryo CPU, an Adreno 640 GPU, and four Hexagon DSPs, each devoted to a specific application space: sensor (sDSP), modem (mDSP), audio (aDSP), and compute (cDSP). Chip and DSP variants Each Qualcomm chip includes multiple Hexagon DSPs such as the compute DSP (cDSP), audio DSP (aDSP), and sensor DSP (SLPI -- Sensor Low Power Island). Each of these DSPs implement a specific Instruction Set Architecture (ISA) version. The ISAs supported by this Hexagon SDK are the following: V65 V66 V68 The compute DSP, which is intended for compute-intensive tasks such as image processing, computer vision, and camera streaming, also includes an instruction set extension for fixed-point vector operations called Hexagon Vector eXtensions (HVX). Starting with Lahaina, the cDSP is being referred to as Qualcomm\u00ae Hexagon\u2122 Tensor Processor to reflect its ability to process neural network tensor data efficiently. In the Hexagon SDK, all references to cDSP apply to Hexagon Tensor Processor also. The reference documentation contains a section that lists the various reference manuals for each of these ISA versions, with or without HVX extensions. Compared to the host CPU, the DSP typically runs at a lower clock speed but provides more parallelism opportunities at the instruction level. This often makes the DSP a better alternative in terms of throughput and/or power consumption. As a result, it is preferable to offload as many large compute-intensive tasks as possible onto the DSP to reduce power consumption of the device and free up cycles on the CPU for additional features. RPC is the main mechanisms allowing to offload tasks onto the DSP. The following table provides a quick summary of some of the devices supported by the current Hexagon SDK and the key cDSP features available in these devices. Chip number Chip name cDSP Turbo L1 Turbo Nominal HVX HMX L2 VTCM SM8350 Lahaina V68 1.5 GHz 1.4 GHz 1.2 GHz 4 1 1 MB 4 MB SM8250 Kona V66 1.5 GHz 1.4 GHz 1.2 GHz 4 0 1 MB 256 KB SM8150 Hana V66 1.5 GHz 1.3 GHz 1.2 GHz 4 0 1 MB 256 KB SM7250 Saipan V66 1.5 GHz 1.4 GHz 1.2 GHz 4 0 1 MB 256 KB SM6250 Rennell V66 1.2 GHz 1.2 GHz .95 GHz 2 0 512 KB 256 KB SM4250 Kamorta V66 - 1.0 GHz .83 GHz 2 0 512 KB 256 KB Devices are grouped in families identified by the highest tier device in that family. For example, the Lahaina family includes the Lahaina, Cedros, and Bitra. Tier Hana family Kona family Lahaina family 8xxx Hana (SM8150) Kona (SM8250) Lahaina (SM8350) 7xxx Moorea (SM7150) Saipan (SM7250) Cedros (SM7xxx) 6xxx Rennell (SM6250) Bitra (SM6xxx) 4xxx Kamorta (SM4250) ### cDSP The following diagram provides an overview of the processing units within the cDSP and how they connect to the memory cache. Hexagon core The Hexagon core is made of several DSP hardware threads \u2014 four or six on most current cDSPs. Each DSP hardware thread has access to the Hexagon scalar instructions, which perform fixed-point and floating-point operations on single or pairs of 32-bit registers. Each data unit is capable of performing a load or store up to 64-bit wide, or 32-bit scalar ALU operation. Each execution unit is capable of 16/32/64-bit vectorized multiply, ALU, bit manipulation, shift or floating-point operations. Prior to V66, the scalar floating-point and multiplier resources were shared by all execution units. This meant that all combined hardware threads performed a maximum of one of each operation every processor cycle. Starting with V66, each cluster has its own floating-point and multiplier resources. As shown on the diagram above, a cluster refers to a pair of threads (Thread 0&1 and Thread 2&3). Within a cluster, the two threads typically commit instruction packets on alternating clock cycles because most instructions require at least two clock cycles to complete. In the best case, each cluster completes one instruction packet on every DSP clock cycle, yielding a total throughput of (2 * DSP Clock) instruction packets per second as long as stalls are avoided. (For guidelines on minimizing latencies, see the discussion on DSP optimization techniques .) Hexagon HVX unit HVX is a coprocessor that adds 128-byte vector processing capabilities to the cDSP. Scalar hardware threads use the HVX coprocessor by accessing an HVX register file, also referred as HVX context. As shown in the V66 block diagram above, instruction packets flow through the scalar Hexagon pipeline, where any included scalar instructions are processed. Instructions then continue onto the HVX Vector FIFO. As such, HVX instructions can be intermixed with scalar instructions, even within the same instruction packet. Hexagon HMX unit HMX is a matrix engine introduced with Lahaina. It provides very high throughput for convolution operations. HMX instructions are not directly exposed by this SDK. HMX acceleration is only made available through machine learning libraries and tools such as the QNN SDK, which allows Neural Networks to run on Lahaina and makes heavy use of the HMX engine. Memory subsystem The following diagram provides an overview of the DSP memory subsystem. The cDSP has a two-level cache memory subsystem. L1 is only accessible to the scalar unit, making L2 the second level memory for the scalar unit and the first level memory for the HVX coprocessor. L1 is write-through only. This allows the caches to be hardware coherent. To maintain coherency, if an HVX store hits in L1, the L1 line is invalidated. The vector units support a variety of load/store instructions, including support for unaligned vectors and per-byte conditional stores. A pipelined vector FIFO is in place for the HVX hardware to read L2 contents and hide L2 read latencies from the programmer. For a more in-depth discussion on memory latencies, see the memory section in the optimization guidelines. The cDSP also includes a Tightly Coupled Memory (TCM) called VTCM (Vector TCM). VTCM is a low-power memory that provides approximately twice the bandwidth of L2 and reduces store-to-load latencies. VTCM is required for performing scatter-gather lookup operations and for using the HMX engine. Development boards This section provides a high-level overview of board options that are available to developers working with the Hexagon compute DSP. For more details, see the websites of each provider. Name Provider Description Open-Q For example: SDM660 Qualcomm HDK made of two components: SOM powered by Snapdragon processor and including a 64-bit multicore CPU, Qualcomm Adreno GPU, Hexagon DSP along with Android OS. Carrier board to provide additional connectivity and display capabilities. DragonBoard\u2122 development kit For example: SM8150 Arrow Electronics Single small development board MTP For example: MTP8250, MTP8350 Qualcomm Earliest reference platforms to become available after new processors come out. Limited supply. Only provided through direct engagements. QRD For example: QRD8350 Qualcomm Qualcomm reference design in mobile phone form factor. IP Camera For example: SDA626, SA8155P, RB3 Thundercomm IP Camera Reference Design, Automotive Development Platform, Robotics Platform, VR development kit, and so on.","title":"Home"},{"location":"index.html#hexagon-sdk-overview","text":"The Qualcomm Hexagon\u2122 SDK provides a set of tools, software, and documentation that assist developers in running applications on Qualcomm devices and leveraging the Hexagon DSPs. This page provides a brief overview of the Qualcomm embedded compute devices that this Hexagon SDK supports, the process of writing applications running on the CPU and offloading tasks onto the DSP, and how the Hexagon SDK tools and documentation facilitate this process.","title":"Hexagon SDK overview"},{"location":"index.html#prerequisites","text":"The Hexagon SDK installer requires the following: 64-bit Windows 7, Windows 8, or Windows 10 64-bit Ubuntu 16.x or later","title":"Prerequisites"},{"location":"index.html#getting-started","text":"For a quick start, browse through the base SDK examples and run some of the examples on simulator and on target. Each of these examples illustrate a few tools or software features as explained in the list of examples table. The calculator example is a good example to start with as it introduces a number of basic concepts. To gain a more solid understanding of the SDK and cDSP that it supports, review the rest of this section. Then browse through the Tools and Software tabs to learn about the development tools and software components available in the SDK. For detailed documentation on a specific topic, see the reference manuals and use the examples for a better understanding.","title":"Getting started"},{"location":"index.html#sdk-components","text":"The Hexagon SDK includes the following components, each covered in one of the documentation tabs and intended to help software developers as follows: Development Tools : Building, running, debugging, and profiling Hexagon-based applications Software Developing Hexagon-based applications for the run-time ecosystem on Qualcomm devices Code Examples : Source projects illustrating different aspects of developing Hexagon-based applications Reference Documentation: Additional resources with in-depth information Add-ons : Additional packages available for installation into this SDK","title":"SDK components"},{"location":"index.html#directory-structure","text":"The Hexagon SDK tree structure is as follows: /docs Hexagon SDK documentation /examples Code examples /utils Helper scripts to automate tasks commonly performed by developers /inc Public header files needed to access system-level software libraries /libs System-level software libraries and Hexagon libraries /ipc Inter-processor communication libraries providing FastRPC support /rtos Real-time DSP OS /tools Development tools (see Tools tab) /add-ons SDK add-ons (see Add-ons tab)","title":"Directory Structure"},{"location":"index.html#development-process","text":"The process of developing a Hexagon-based feature for a Qualcomm device typically involves several of the following steps (not necessarily always in this order). Develop the core algorithm of interest in C/C++ (for example, in an x86 environment). Once the workload is functional and the top-level API's are defined, build it as a Hexagon shared library and an executable test driver that exports main(), as is done with the examples in this SDK. Build , execute , debug , and profile the test driver on the Hexagon ISS (Instruction Set Simulator) and/or target device. Prepare to integrate the shared Hexagon library into a run-time HLOS application or multimedia framework. To do this, build an inter-processor interface for the shared Hexagon library by using the SDK's FastRPC framework . This might require updating your shared library and test driver to use the RPC interface. Optimize and enhance the Hexagon shared library using the techniques documented in this SDK, such as: Multi-threading the Hexagon workload Utilizing the System Libraries and Hexagon DSP libraries of this SDK Optimizing the core loops by implementing cache pre-fetching, tiling, writing intrinsics or assembly, or in applicable cases, Halide, TVM, or other compiler-based frameworks. Iterating the optimizations with profiling and tracing to reduce cache misses and other stalls until satisfactory performance is reached.","title":"Development process"},{"location":"index.html#architecture-overview","text":"This section provides an overview of Hexagon processors supported by this SDK and used for embedded advanced computing applications.","title":"Architecture overview"},{"location":"index.html#qualcomm-snapdragon-processors","text":"Qualcomm Technologies, Inc. (QTI) offers a large and increasing number of variants of the Snapdragon chipset solution. The Snapdragon mobile product family is organized into five product tiers. The highest tier includes the SM8xxx series (premium) and SM7xxx series (high tier). Lower tiers include the SM6xx, SM4xx and SM2xx series. These product tiers are differentiated by scalable computing resources for the CPU, GPU, and DSP processors. When moving from low to premium tiers, these processor resource changes are characterized by an increasing number of processors, increasing processor complexity, and increasing clock speeds. For a full list of products, visit the space on mobile processors on our website. The following diagram provides an overview of one representative premium tier product: the SM8150 chipset. The processing units include a Kryo CPU, an Adreno 640 GPU, and four Hexagon DSPs, each devoted to a specific application space: sensor (sDSP), modem (mDSP), audio (aDSP), and compute (cDSP).","title":"Qualcomm Snapdragon processors"},{"location":"index.html#chip-and-dsp-variants","text":"Each Qualcomm chip includes multiple Hexagon DSPs such as the compute DSP (cDSP), audio DSP (aDSP), and sensor DSP (SLPI -- Sensor Low Power Island). Each of these DSPs implement a specific Instruction Set Architecture (ISA) version. The ISAs supported by this Hexagon SDK are the following: V65 V66 V68 The compute DSP, which is intended for compute-intensive tasks such as image processing, computer vision, and camera streaming, also includes an instruction set extension for fixed-point vector operations called Hexagon Vector eXtensions (HVX). Starting with Lahaina, the cDSP is being referred to as Qualcomm\u00ae Hexagon\u2122 Tensor Processor to reflect its ability to process neural network tensor data efficiently. In the Hexagon SDK, all references to cDSP apply to Hexagon Tensor Processor also. The reference documentation contains a section that lists the various reference manuals for each of these ISA versions, with or without HVX extensions. Compared to the host CPU, the DSP typically runs at a lower clock speed but provides more parallelism opportunities at the instruction level. This often makes the DSP a better alternative in terms of throughput and/or power consumption. As a result, it is preferable to offload as many large compute-intensive tasks as possible onto the DSP to reduce power consumption of the device and free up cycles on the CPU for additional features. RPC is the main mechanisms allowing to offload tasks onto the DSP. The following table provides a quick summary of some of the devices supported by the current Hexagon SDK and the key cDSP features available in these devices. Chip number Chip name cDSP Turbo L1 Turbo Nominal HVX HMX L2 VTCM SM8350 Lahaina V68 1.5 GHz 1.4 GHz 1.2 GHz 4 1 1 MB 4 MB SM8250 Kona V66 1.5 GHz 1.4 GHz 1.2 GHz 4 0 1 MB 256 KB SM8150 Hana V66 1.5 GHz 1.3 GHz 1.2 GHz 4 0 1 MB 256 KB SM7250 Saipan V66 1.5 GHz 1.4 GHz 1.2 GHz 4 0 1 MB 256 KB SM6250 Rennell V66 1.2 GHz 1.2 GHz .95 GHz 2 0 512 KB 256 KB SM4250 Kamorta V66 - 1.0 GHz .83 GHz 2 0 512 KB 256 KB Devices are grouped in families identified by the highest tier device in that family. For example, the Lahaina family includes the Lahaina, Cedros, and Bitra. Tier Hana family Kona family Lahaina family 8xxx Hana (SM8150) Kona (SM8250) Lahaina (SM8350) 7xxx Moorea (SM7150) Saipan (SM7250) Cedros (SM7xxx) 6xxx Rennell (SM6250) Bitra (SM6xxx) 4xxx Kamorta (SM4250) ### cDSP The following diagram provides an overview of the processing units within the cDSP and how they connect to the memory cache.","title":"Chip and DSP variants"},{"location":"index.html#hexagon-core","text":"The Hexagon core is made of several DSP hardware threads \u2014 four or six on most current cDSPs. Each DSP hardware thread has access to the Hexagon scalar instructions, which perform fixed-point and floating-point operations on single or pairs of 32-bit registers. Each data unit is capable of performing a load or store up to 64-bit wide, or 32-bit scalar ALU operation. Each execution unit is capable of 16/32/64-bit vectorized multiply, ALU, bit manipulation, shift or floating-point operations. Prior to V66, the scalar floating-point and multiplier resources were shared by all execution units. This meant that all combined hardware threads performed a maximum of one of each operation every processor cycle. Starting with V66, each cluster has its own floating-point and multiplier resources. As shown on the diagram above, a cluster refers to a pair of threads (Thread 0&1 and Thread 2&3). Within a cluster, the two threads typically commit instruction packets on alternating clock cycles because most instructions require at least two clock cycles to complete. In the best case, each cluster completes one instruction packet on every DSP clock cycle, yielding a total throughput of (2 * DSP Clock) instruction packets per second as long as stalls are avoided. (For guidelines on minimizing latencies, see the discussion on DSP optimization techniques .)","title":"Hexagon core"},{"location":"index.html#hexagon-hvx-unit","text":"HVX is a coprocessor that adds 128-byte vector processing capabilities to the cDSP. Scalar hardware threads use the HVX coprocessor by accessing an HVX register file, also referred as HVX context. As shown in the V66 block diagram above, instruction packets flow through the scalar Hexagon pipeline, where any included scalar instructions are processed. Instructions then continue onto the HVX Vector FIFO. As such, HVX instructions can be intermixed with scalar instructions, even within the same instruction packet.","title":"Hexagon HVX unit"},{"location":"index.html#hexagon-hmx-unit","text":"HMX is a matrix engine introduced with Lahaina. It provides very high throughput for convolution operations. HMX instructions are not directly exposed by this SDK. HMX acceleration is only made available through machine learning libraries and tools such as the QNN SDK, which allows Neural Networks to run on Lahaina and makes heavy use of the HMX engine.","title":"Hexagon HMX unit"},{"location":"index.html#memory-subsystem","text":"The following diagram provides an overview of the DSP memory subsystem. The cDSP has a two-level cache memory subsystem. L1 is only accessible to the scalar unit, making L2 the second level memory for the scalar unit and the first level memory for the HVX coprocessor. L1 is write-through only. This allows the caches to be hardware coherent. To maintain coherency, if an HVX store hits in L1, the L1 line is invalidated. The vector units support a variety of load/store instructions, including support for unaligned vectors and per-byte conditional stores. A pipelined vector FIFO is in place for the HVX hardware to read L2 contents and hide L2 read latencies from the programmer. For a more in-depth discussion on memory latencies, see the memory section in the optimization guidelines. The cDSP also includes a Tightly Coupled Memory (TCM) called VTCM (Vector TCM). VTCM is a low-power memory that provides approximately twice the bandwidth of L2 and reduces store-to-load latencies. VTCM is required for performing scatter-gather lookup operations and for using the HMX engine.","title":"Memory subsystem"},{"location":"index.html#development-boards","text":"This section provides a high-level overview of board options that are available to developers working with the Hexagon compute DSP. For more details, see the websites of each provider. Name Provider Description Open-Q For example: SDM660 Qualcomm HDK made of two components: SOM powered by Snapdragon processor and including a 64-bit multicore CPU, Qualcomm Adreno GPU, Hexagon DSP along with Android OS. Carrier board to provide additional connectivity and display capabilities. DragonBoard\u2122 development kit For example: SM8150 Arrow Electronics Single small development board MTP For example: MTP8250, MTP8350 Qualcomm Earliest reference platforms to become available after new processors come out. Limited supply. Only provided through direct engagements. QRD For example: QRD8350 Qualcomm Qualcomm reference design in mobile phone form factor. IP Camera For example: SDA626, SA8155P, RB3 Thundercomm IP Camera Reference Design, Automotive Development Platform, Robotics Platform, VR development kit, and so on.","title":"Development boards"},{"location":"support.html","text":"Support For Hexagon SDK questions or feedback, visit the Hexagon DSP developer network, http://developer.qualcomm.com/hexagon-forum . To register for updates and errata, visit https://developer.qualcomm.com/hexagon-updates .","title":"Support"},{"location":"support.html#support","text":"For Hexagon SDK questions or feedback, visit the Hexagon DSP developer network, http://developer.qualcomm.com/hexagon-forum . To register for updates and errata, visit https://developer.qualcomm.com/hexagon-updates .","title":"Support"},{"location":"add-ons/audio.html","text":"Audio add-on documentation The audio add-on provides information and tools to develop and integrate audio modules into the Qualcomm audio framework. The add-on includes an introduction to the audio and voice topologies, and procedures to develop, test, and integrate audio modules. It also describes the Common Audio Processor Interface version 2 (CAPIv2) and explains how to provide tuning support using the Qualcomm Audio Calibration Tool (QACT). Installation The audio add-on is currently not present in this Hexagon SDK. If you would like to install it, follow the steps below. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select Audio1.x and the version you wish to install Click \"Install\" to begin the installation process For Linux: Open a CLI Run qpm-cli --install audio1.x Follow the prompts to select the core installation you want for the Audio addon","title":"Audio"},{"location":"add-ons/audio.html#audio-add-on-documentation","text":"The audio add-on provides information and tools to develop and integrate audio modules into the Qualcomm audio framework. The add-on includes an introduction to the audio and voice topologies, and procedures to develop, test, and integrate audio modules. It also describes the Common Audio Processor Interface version 2 (CAPIv2) and explains how to provide tuning support using the Qualcomm Audio Calibration Tool (QACT).","title":"Audio add-on documentation"},{"location":"add-ons/audio.html#installation","text":"The audio add-on is currently not present in this Hexagon SDK. If you would like to install it, follow the steps below. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select Audio1.x and the version you wish to install Click \"Install\" to begin the installation process For Linux: Open a CLI Run qpm-cli --install audio1.x Follow the prompts to select the core installation you want for the Audio addon","title":"Installation"},{"location":"add-ons/audio_post_addon.html","text":"Audio add-on documentation The audio add-on is installed. Please click here to access the documentation on the audio add-on.","title":"Audio add-on documentation"},{"location":"add-ons/audio_post_addon.html#audio-add-on-documentation","text":"The audio add-on is installed. Please click here to access the documentation on the audio add-on.","title":"Audio add-on documentation"},{"location":"add-ons/compute.html","text":"Compute add-on documentation Overview The compute add-on contains libraries and tools to develop general compute intensive applications which are primarily focussed on data, image and video processing use cases. It has examples to demonstrate the usage of the standard compute libraries that are being embedded in this add-on and also provide end-to-end frameworks to plug and play an algorithm to process and enhance camera captures. The Compute add-on covers the following features: Camera streamer Qualcomm CV library FastCV UBWC DMA Camera CHI Image DSPQ example HVX benchmark example Installation The compute add-on is currently not present in the base Hexagon SDK. If you would like to install it, follow the steps below. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select Compute1.x and the version you wish to install Click \"Install\" to begin the installation process For Linux: Open a CLI Run qpm-cli --install compute1.x Follow the prompts to select the core installation you want for the Compute addon","title":"Compute"},{"location":"add-ons/compute.html#compute-add-on-documentation","text":"","title":"Compute add-on documentation"},{"location":"add-ons/compute.html#overview","text":"The compute add-on contains libraries and tools to develop general compute intensive applications which are primarily focussed on data, image and video processing use cases. It has examples to demonstrate the usage of the standard compute libraries that are being embedded in this add-on and also provide end-to-end frameworks to plug and play an algorithm to process and enhance camera captures. The Compute add-on covers the following features: Camera streamer Qualcomm CV library FastCV UBWC DMA Camera CHI Image DSPQ example HVX benchmark example","title":"Overview"},{"location":"add-ons/compute.html#installation","text":"The compute add-on is currently not present in the base Hexagon SDK. If you would like to install it, follow the steps below. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select Compute1.x and the version you wish to install Click \"Install\" to begin the installation process For Linux: Open a CLI Run qpm-cli --install compute1.x Follow the prompts to select the core installation you want for the Compute addon","title":"Installation"},{"location":"add-ons/compute_post_addon.html","text":"Compute add-On documentation The compute add-on is installed. Please click here to access the documentation on the compute add-on.","title":"Compute add-On documentation"},{"location":"add-ons/compute_post_addon.html#compute-add-on-documentation","text":"The compute add-on is installed. Please click here to access the documentation on the compute add-on.","title":"Compute add-On documentation"},{"location":"add-ons/eai.html","text":"eAI add-on documentation Overview The Qualcomm embedded Artificial Intelligence (eAI) framework provides a machine learning engine for deeply embedded environments. The eAI add-on provides information and tools to develop and integrate machine learning modules into the Qualcomm framework. For more detailed information regarding supported features and execution environments refer to the eAI Add-On documentation. Installation The eAI add-on is currently not present in the base Hexagon SDK. If you would like to install it follow the steps below. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select eAI1.x and the version you wish to install Click \"Install\" to begin the installation process For Linux: Open a CLI Run qpm-cli --install eAI1.x Follow the prompts to select the core installation you want for the eAI addon","title":"eAI"},{"location":"add-ons/eai.html#eai-add-on-documentation","text":"","title":"eAI add-on documentation"},{"location":"add-ons/eai.html#overview","text":"The Qualcomm embedded Artificial Intelligence (eAI) framework provides a machine learning engine for deeply embedded environments. The eAI add-on provides information and tools to develop and integrate machine learning modules into the Qualcomm framework. For more detailed information regarding supported features and execution environments refer to the eAI Add-On documentation.","title":"Overview"},{"location":"add-ons/eai.html#installation","text":"The eAI add-on is currently not present in the base Hexagon SDK. If you would like to install it follow the steps below. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select eAI1.x and the version you wish to install Click \"Install\" to begin the installation process For Linux: Open a CLI Run qpm-cli --install eAI1.x Follow the prompts to select the core installation you want for the eAI addon","title":"Installation"},{"location":"add-ons/eai_post_addon.html","text":"eAI add-on documentation The eAI add-on is installed. The add-on contains support for developing machine learning modules using either the eAI machine learning engine or via the GEMM utility which provides baremetal access to embedded Neural Processing Unit (eNPU). Click here to access the eAI Runtime documentation on the eAI add-on. Click here to access the GEMM Utility documentation on the eAI add-on.","title":"eAI add-on documentation"},{"location":"add-ons/eai_post_addon.html#eai-add-on-documentation","text":"The eAI add-on is installed. The add-on contains support for developing machine learning modules using either the eAI machine learning engine or via the GEMM utility which provides baremetal access to embedded Neural Processing Unit (eNPU). Click here to access the eAI Runtime documentation on the eAI add-on. Click here to access the GEMM Utility documentation on the eAI add-on.","title":"eAI add-on documentation"},{"location":"add-ons/qnx.html","text":"QNX add-on Documentation Overview QNX add-on enables SDK support for QNX OS running on the Application processor. This add-on provides QNX-specific build files, headers & libraries to enable building of Hexagon base SDK and Compute add-on examples with QNX v7.0 and QNX v7.1 toolchains on the CPU side and Hexagon v8.2.08 and v8.4.11 toolchains on the Hexagon side for Gen3(Hana-SA8155, Poipu-SA8195, Talos-SA6155) and Gen4(Makena-SA8540) Auto targets respectively. Installation The QNX add-on is not included in the base Hexagon SDK. It can be installed by the following steps. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select \"QNX_Addon_1.x\" and the version you wish to install Click \"Install\" to begin the installation process For Linux: Open a CLI Run qpm-cli --login <username> <password> to login to QPM with your username and password Run qpm-cli --catalog-refresh to refresh the catalog Run qpm-cli --license-activate qnx_addon_1.x Run qpm-cli --install qnx_addon_1.x to install QNX addon. Follow the prompts to select the core installation you want for the QNX addon","title":"QNX"},{"location":"add-ons/qnx.html#qnx-add-on-documentation","text":"","title":"QNX add-on Documentation"},{"location":"add-ons/qnx.html#overview","text":"QNX add-on enables SDK support for QNX OS running on the Application processor. This add-on provides QNX-specific build files, headers & libraries to enable building of Hexagon base SDK and Compute add-on examples with QNX v7.0 and QNX v7.1 toolchains on the CPU side and Hexagon v8.2.08 and v8.4.11 toolchains on the Hexagon side for Gen3(Hana-SA8155, Poipu-SA8195, Talos-SA6155) and Gen4(Makena-SA8540) Auto targets respectively.","title":"Overview"},{"location":"add-ons/qnx.html#installation","text":"The QNX add-on is not included in the base Hexagon SDK. It can be installed by the following steps. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select \"QNX_Addon_1.x\" and the version you wish to install Click \"Install\" to begin the installation process For Linux: Open a CLI Run qpm-cli --login <username> <password> to login to QPM with your username and password Run qpm-cli --catalog-refresh to refresh the catalog Run qpm-cli --license-activate qnx_addon_1.x Run qpm-cli --install qnx_addon_1.x to install QNX addon. Follow the prompts to select the core installation you want for the QNX addon","title":"Installation"},{"location":"add-ons/qnx_post_addon.html","text":"QNX add-on documentation QNX add-on is installed. Please click here to access the documentation on the QNX add-on.","title":"QNX add-on documentation"},{"location":"add-ons/qnx_post_addon.html#qnx-add-on-documentation","text":"QNX add-on is installed. Please click here to access the documentation on the QNX add-on.","title":"QNX add-on documentation"},{"location":"add-ons/wos.html","text":"WoS add-on documentation Overview The WoS add-on contains libraries and tools to develop applications for targets supporting Windows on Snapdragon (WoS). Upon installation, it enables to build the applications for ARM-based Windows HLOS and demonstrates running these applications on the target using the Hexagon SDK examples. The WoS add-on covers the following features: CMake based build system Windows toolchain file for cross-compiling x64-ARM64 based applications Examples Calculator Calculator_c++ HAP_example Multithreading Profiling Benchmark Libraries Rpcmem Remote Getopt_custom library for getopt support in windows Tools Getserial to retrieve the serial number from target SysMon application for profiling Installation The WoS add-on is currently not present in the base Hexagon SDK. If you would like to install it, follow the steps below. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select Wos1.x and the version you wish to install Click \"Install\" to begin the installation process Note: The WoS add-on is only supported on Windows Host machine.","title":"WoS add-on documentation"},{"location":"add-ons/wos.html#wos-add-on-documentation","text":"","title":"WoS add-on documentation"},{"location":"add-ons/wos.html#overview","text":"The WoS add-on contains libraries and tools to develop applications for targets supporting Windows on Snapdragon (WoS). Upon installation, it enables to build the applications for ARM-based Windows HLOS and demonstrates running these applications on the target using the Hexagon SDK examples. The WoS add-on covers the following features: CMake based build system Windows toolchain file for cross-compiling x64-ARM64 based applications Examples Calculator Calculator_c++ HAP_example Multithreading Profiling Benchmark Libraries Rpcmem Remote Getopt_custom library for getopt support in windows Tools Getserial to retrieve the serial number from target SysMon application for profiling","title":"Overview"},{"location":"add-ons/wos.html#installation","text":"The WoS add-on is currently not present in the base Hexagon SDK. If you would like to install it, follow the steps below. For Windows: Open QPM Under \"All\" expand \"Qualcomm Hexagon SDK Product Suite\" Select Wos1.x and the version you wish to install Click \"Install\" to begin the installation process Note: The WoS add-on is only supported on Windows Host machine.","title":"Installation"},{"location":"add-ons/wos_post_addon.html","text":"WoS add-on documentation The WoS add-on is installed. Please click here to access the documentation on the WoS add-on.","title":"WoS add-on documentation"},{"location":"add-ons/wos_post_addon.html#wos-add-on-documentation","text":"The WoS add-on is installed. Please click here to access the documentation on the WoS add-on.","title":"WoS add-on documentation"},{"location":"examples/index.html","text":"Hexagon SDK examples Each example illustrates one or more SDK features as summarized in the List of examples section. All examples are designed to run on both the simulator and target. Setup Follow the setup instructions before running any example. Remember to always run the setup_sdk_env script to set up the SDK local environment. Walkthrough scripts Most example projects come with a walkthrough python script that automates the following steps: Sign the device so that libraries can be loaded on it. Build the executable that runs on the application processor. Build the Hexagon DSP library, which implements the functions that the application processor will invoke. Run the example on the simulator. Push the executable, DSP library, and, if required, any test data on the target. Run the example on the target. Usage of the script is as follows, which is displayed by using the -h option: <walkthrough script> [-h] [-L] [-T TARGET] [-s SERIAL_NUM] [-d DOMAIN] [-M] [-N] [-32] [-dest HLOS_ROOT_DEST] [-libdest HLOS_LIB_DEST] Optional arguments: -h, --help Show the help message -L Build the executable for Linux Embedded HLOS (Default is Linux Android) -T TARGET Build the DSP library for TARGET: <sm8150, sm6150, qcs405, sm7150, sm6125, sm8250, lahaina, rennell, saipan, kamorta, bitra, agatti, qrb5165, qcs403, qcs610, cedros, mannar> -s SERIAL_NUM Specify the adb serial number of device. -d DOMAIN Use the DSP subsystem DOMAIN: Options=<0/1/2/3> (0: ADSP, 3: CDSP, 2: SDSP, 1: MDSP). Domain value is optional. Default is 0 for agatti and 3 for other targets. -M Do not recompile Hexagon and Android variants -N Skip test signature installation -32 Build executable for the 32-bit target -dest HLOS_ROOT_DEST Override the default HLOS root directory <vendor, usr, data>. Default is /vendor for LA and /usr for LE targets. -libdest HLOS_LIB_DEST Override the default directory for HLOS library <vendor/lib, usr/lib, data/lib> OR <vendor/lib64, usr/lib64, data/lib64> One of the most useful options is - N : it skips the step of signing the device, which only needs to be performed on a newly flashed device. Out of - T (TARGET) and - s (SERIAL_NUM), only one is mandatory. Default device paths used by example walkthrough scripts For LA targets:: HLOS exe destination: /vendor/bin HLOS lib destination: /vendor/lib (32 bit) OR /vendor/lib64 (64 bit) DSP lib destination: /vendor/lib/rfsa/dsp/sdk For LE targets:: HLOS exe destination: /usr/bin HLOS lib destination: /usr/lib (32 bit) OR /usr/lib64 (64 bit) DSP lib destination: /usr/lib/rfsa/dsp/sdk Usage examples python calculator_walkthrough.py -T sm8150 -32 The command above will: sign the SM8150 device build the calculator 32-bit executable and libraries for the LA HLOS(Android) build the calculator Hexagon libraries run the example on the simulator as specified in the hexagon.min rules push the built binaries and run the example on target The following configurations will be used: DSP used for offloading: cDSP HLOS exe destination: /vendor/bin HLOS lib destination: /vendor/lib cDSP lib destination: /vendor/lib/rfsa/dsp/sdk python calculator_walkthrough.py -T lahaina -d 0 -dest data The command above will: sign the Lahaina device build the calculator 64-bit executable and libraries for the LA HLOS(Android) build the calculator Hexagon libraries run the example on the simulator as specified in the hexagon.min rules push the built binaries and run the example on target The following configurations will be used: DSP used for offloading: aDSP HLOS exe destination: /data/bin HLOS lib destination: /data/lib64 aDSP lib destination: /data/lib/rfsa/dsp/sdk python calculator_walkthrough.py -T qcs405 -L -N -dest data -libdest data/lib The command above will: * skip the device signing * build the calculator 64-bit executable and libraries for the LE HLOS(UbuntuARM) * build the calculator Hexagon libraries * run the example on the simulator as specified in the hexagon.min rules * push the built binaries and run the example on target The following configurations will be used: DSP used for offloading: cDSP HLOS exe destination: /data/bin HLOS lib destination: /data/lib cDSP lib destination: /data/lib/rfsa/dsp/sdk NOTE: For building Hexagon libraries, the Hexagon architecture is determined based on the target_name or serial_number passed as arguments. NOTE: The option -T can be used to identify the required device if multiple devices are connected simultaneously. However, if multiple devices of the same type are connected, for example if there are two SM8250 devices connected simultaneously, then use the -s option to differentiate between devices based on the ADB serial numbers. The ADB serial number of a device is a string created by ADB to uniquely identify the device by its port number. adb devices -l e76a39c6 device product:kona model:Kona_for_arm64 device:kona f68b78d7 device product:kona model:Kona_for_arm64 device:kona python calculator_walkthrough.py -s e76a39c6 -N The command above will: skip the device signing build the calculator 64-bit executable and libraries for the LA HLOS(Android) build the calculator Hexagon libraries run the example on the simulator as specified in the hexagon.min rules push the built binaries and run the example on target List of examples The following table lists all the examples that are included in the base SDK and the various features that they illustrate. Example Features illustrated in the example asyncdspq_example Demonstrate the usage of the asynchronous message queue calculator FastRPC Building Simulating Target testing Multi-domain/multi-DSP support Unsigned PD calculator_c++ C++ on the DSP calculator_c++_apk C++ on the application processor dspqueue Asynchronous DSP Packet Queue gtest Demonstrate usage of the Google test framework HAP example HAP APIs Remote DSP capabilities APIs LPI example LPI mode multithreading Multithreading using QuRT threads, barriers and mutexes Debugging IDE profiling CPU and DSP timers and cycle counters Simulator profiling sysMon Hexagon Trace Analyzer qhl QHL (Qualcomm Hexagon Libraries) qhl_hvx QHL HVX (Qualcomm Hexagon Libraries for HVX) HVX qprintf qprintf (printf library extension for HVX and assembly)","title":"Overview"},{"location":"examples/index.html#hexagon-sdk-examples","text":"Each example illustrates one or more SDK features as summarized in the List of examples section. All examples are designed to run on both the simulator and target.","title":"Hexagon SDK examples"},{"location":"examples/index.html#setup","text":"Follow the setup instructions before running any example. Remember to always run the setup_sdk_env script to set up the SDK local environment.","title":"Setup"},{"location":"examples/index.html#walkthrough-scripts","text":"Most example projects come with a walkthrough python script that automates the following steps: Sign the device so that libraries can be loaded on it. Build the executable that runs on the application processor. Build the Hexagon DSP library, which implements the functions that the application processor will invoke. Run the example on the simulator. Push the executable, DSP library, and, if required, any test data on the target. Run the example on the target. Usage of the script is as follows, which is displayed by using the -h option: <walkthrough script> [-h] [-L] [-T TARGET] [-s SERIAL_NUM] [-d DOMAIN] [-M] [-N] [-32] [-dest HLOS_ROOT_DEST] [-libdest HLOS_LIB_DEST] Optional arguments: -h, --help Show the help message -L Build the executable for Linux Embedded HLOS (Default is Linux Android) -T TARGET Build the DSP library for TARGET: <sm8150, sm6150, qcs405, sm7150, sm6125, sm8250, lahaina, rennell, saipan, kamorta, bitra, agatti, qrb5165, qcs403, qcs610, cedros, mannar> -s SERIAL_NUM Specify the adb serial number of device. -d DOMAIN Use the DSP subsystem DOMAIN: Options=<0/1/2/3> (0: ADSP, 3: CDSP, 2: SDSP, 1: MDSP). Domain value is optional. Default is 0 for agatti and 3 for other targets. -M Do not recompile Hexagon and Android variants -N Skip test signature installation -32 Build executable for the 32-bit target -dest HLOS_ROOT_DEST Override the default HLOS root directory <vendor, usr, data>. Default is /vendor for LA and /usr for LE targets. -libdest HLOS_LIB_DEST Override the default directory for HLOS library <vendor/lib, usr/lib, data/lib> OR <vendor/lib64, usr/lib64, data/lib64> One of the most useful options is - N : it skips the step of signing the device, which only needs to be performed on a newly flashed device. Out of - T (TARGET) and - s (SERIAL_NUM), only one is mandatory.","title":"Walkthrough scripts"},{"location":"examples/index.html#default-device-paths-used-by-example-walkthrough-scripts","text":"For LA targets:: HLOS exe destination: /vendor/bin HLOS lib destination: /vendor/lib (32 bit) OR /vendor/lib64 (64 bit) DSP lib destination: /vendor/lib/rfsa/dsp/sdk For LE targets:: HLOS exe destination: /usr/bin HLOS lib destination: /usr/lib (32 bit) OR /usr/lib64 (64 bit) DSP lib destination: /usr/lib/rfsa/dsp/sdk","title":"Default device paths used by example walkthrough scripts"},{"location":"examples/index.html#usage-examples","text":"python calculator_walkthrough.py -T sm8150 -32 The command above will: sign the SM8150 device build the calculator 32-bit executable and libraries for the LA HLOS(Android) build the calculator Hexagon libraries run the example on the simulator as specified in the hexagon.min rules push the built binaries and run the example on target The following configurations will be used: DSP used for offloading: cDSP HLOS exe destination: /vendor/bin HLOS lib destination: /vendor/lib cDSP lib destination: /vendor/lib/rfsa/dsp/sdk python calculator_walkthrough.py -T lahaina -d 0 -dest data The command above will: sign the Lahaina device build the calculator 64-bit executable and libraries for the LA HLOS(Android) build the calculator Hexagon libraries run the example on the simulator as specified in the hexagon.min rules push the built binaries and run the example on target The following configurations will be used: DSP used for offloading: aDSP HLOS exe destination: /data/bin HLOS lib destination: /data/lib64 aDSP lib destination: /data/lib/rfsa/dsp/sdk python calculator_walkthrough.py -T qcs405 -L -N -dest data -libdest data/lib The command above will: * skip the device signing * build the calculator 64-bit executable and libraries for the LE HLOS(UbuntuARM) * build the calculator Hexagon libraries * run the example on the simulator as specified in the hexagon.min rules * push the built binaries and run the example on target The following configurations will be used: DSP used for offloading: cDSP HLOS exe destination: /data/bin HLOS lib destination: /data/lib cDSP lib destination: /data/lib/rfsa/dsp/sdk NOTE: For building Hexagon libraries, the Hexagon architecture is determined based on the target_name or serial_number passed as arguments. NOTE: The option -T can be used to identify the required device if multiple devices are connected simultaneously. However, if multiple devices of the same type are connected, for example if there are two SM8250 devices connected simultaneously, then use the -s option to differentiate between devices based on the ADB serial numbers. The ADB serial number of a device is a string created by ADB to uniquely identify the device by its port number. adb devices -l e76a39c6 device product:kona model:Kona_for_arm64 device:kona f68b78d7 device product:kona model:Kona_for_arm64 device:kona python calculator_walkthrough.py -s e76a39c6 -N The command above will: skip the device signing build the calculator 64-bit executable and libraries for the LA HLOS(Android) build the calculator Hexagon libraries run the example on the simulator as specified in the hexagon.min rules push the built binaries and run the example on target","title":"Usage examples"},{"location":"examples/index.html#list-of-examples","text":"The following table lists all the examples that are included in the base SDK and the various features that they illustrate. Example Features illustrated in the example asyncdspq_example Demonstrate the usage of the asynchronous message queue calculator FastRPC Building Simulating Target testing Multi-domain/multi-DSP support Unsigned PD calculator_c++ C++ on the DSP calculator_c++_apk C++ on the application processor dspqueue Asynchronous DSP Packet Queue gtest Demonstrate usage of the Google test framework HAP example HAP APIs Remote DSP capabilities APIs LPI example LPI mode multithreading Multithreading using QuRT threads, barriers and mutexes Debugging IDE profiling CPU and DSP timers and cycle counters Simulator profiling sysMon Hexagon Trace Analyzer qhl QHL (Qualcomm Hexagon Libraries) qhl_hvx QHL HVX (Qualcomm Hexagon Libraries for HVX) HVX qprintf qprintf (printf library extension for HVX and assembly)","title":"List of examples"},{"location":"examples/asyncdspq_example/index.html","text":"Asyncdspq example The asyncdspq (asynchronous message queue) is a utility library for asynchronous communication between the application processor and the DSP. This example contains the following three test cases to demonstrate the usage of asyncdspq. All these examples run on target and not on simulator. Basic messaging (queuetest) Image processing queue (fcvqueue) Queue performance (queueperf) Note: This example and the asyncdspq library itself will be deprecated when its replacement, the asynchronous packet queue , will be available in the next SDK release. Basic messaging The queuetest example application contains a number of basic message queue tests and examples. In all cases the application processor sends two integers to the DSP, the DSP adds them together, and delivers a result back. The following tests are located in the function test_asyncdspq() . Simple Messages The first two tests send a single message from the application processor to the DSP and vice versa. The basic sequence is as follows: The application processor creates a queue with asyncdspq_create() . For read tests the CPU writes a message to the queue with asyncdspq_write() . The CPU passes the queue attach handle from asyncdspq_create() to the DSP over a regular FastRPC call. The DSP attaches to the queue with asyncdspq_attach() . The DSP either reads the message from the queue or writes one. The DSP detaches from the queue with asyncdspq_detach() . The application processor verifies the result. The application processor destroys the queue with asyncdspq_destroy() . Note that both tests use a single queue and use other mechanisms to verify the response. This is a valid configuration and can be useful for clients that mostly only send messages in one direction. The third test simply sends multiple messages from the DSP to the application processor. Tests with multiple queues The following tests in test_asyncdspq() send messages between the CPU and the DSP using multiple queues, both using blocking read/write operations and using callbacks. Canceling operations The tests in test_asyncdspq_cancel() test canceling synchronous read/write operations under different circumstances. Many clients may not need to use asyncdspq_cancel() if they have another mechanism to ensure queues are empty before destroying them; typically this is done by sending an explicit \u201cclose\u201d message and waiting for a response. Threading test_asyncdspq_threads() tests different threading scenarios. The message queue library is designed to be thread-safe for read and write operations, but queues should be created and destroyed from the same thread. Most clients will use a single thread for reading and writing the queue, either from their own thread or callbacks from the queue library. Image processing queue The fcvqueue example implements a queue of image processing operations using FastCV, which is a available in the compute add-on. It illustrates a more complex message queue use case, and also shows how to handle buffer allocation, memory mapping, and cache management. The threading and callback model is typical for most message queue clients: fcvqueue uses blocking writes and non-blocking reads with message callbacks. The fcvqueue example is split between three main files: fcvqueue.c : CPU-side implementation. The rest of this section refers primarily to functions in this file. fcvqueue_dsp_imp.c : DSP-side implementation for the example. Contains FastRPC methods called from fcvqueue.c and the message queue client code. fcvqueuetest.c : Test application. Start here to follow the calling sequences. This section briefly describes the key parts of the example. For more details refer to the source code, starting with the test application. Note that fcvqueue is not thread-safe, and while the code checks for errors codes from all the functions it calls it does not attempt to recover from errors. The queue library generally does not return recoverable errors except where separately noted; errors result from either programming errors or running out of system resources such as memory. Initialization and uninitialization fcvqueue_create() allocates internal structures and creates two queues--one for requests and one for responses--and calls the DSP-side implementation to attach to them. fcvqueue_destroy() ensures the queues are empty, cancels any operations on the DSP side, and detaches and destroys queues. Memory allocation and de-allocation fcvqueue_alloc_buffer() allocates shareable ION buffers using the rpcmem library and sends them to the DSP to be mapped. FastRPC handles mapping the buffer to the DSP's SMMU and internal MMU; as long as the DSP code keeps a mapping open (with HAP_mmap() ) the buffers won't be unmapped. Subsequent operations on the buffer refer to it by its file descriptor (FD). The DSP-side code maintains a mapping of FDs to buffer addresses. This avoids having to map/unmap buffers for each operation, but also requires explicit cache maintenance operations. fcvqueue_free_buffer() is the reverse of allocation: It unmaps buffers from the DSP, releasing FastRPC MMU/SMMU mappings, and deallocates the memory. Synchronization Most fcvqueue operations apart from initialization and memory allocation are asynchronous. The client queues requests which are processed by the DSP asynchronously without immediate feedback. To determine that processing has reached a specific point the client queue a synchronization request. fcvqueue_enqueue_sync() sends a synchronization request to the DSP. The DSP simply responds to the request with the same token; when the application processor receives the response it calls a callback function. This informs the client that all operations through the synchronization request have been completed. fcvqueue_sync() is a synchronous variant of the same request. It simply calls fcvqueue_enqueue_sync() and waits for a callback before returning. Once fcvqueue_sync() returns both the request and response queues are empty. Cache maintenance In addition to memory mapping, FastRPC also handles cache maintenance on its clients' behalf. Message queue clients must handle this explicitly, which can be more complex but also gives the client an opportunity to determine when to manage the cache. The fcvqueue example exposes cache maintenance through two operations: fcvqueue_enqueue_buffer_in() and fcvqueue_enqueue_buffer_out() , corresponding to cache invalidate and flush. After a buffer is allocated it is considered to be in application processor ownership, and the CPU can place input data in the buffer. Before the DSP can use the buffer it must be transferred to DSP ownership with fcvqueue_enqueue_buffer_in() . This invalidates the buffer on the DSP side to ensure there is no stale data in DSP caches. While the buffer is in DSP ownership the application processor must not access it. To make a buffer and its contents available to the application processor again the client must call fcvqueue_enqueue_buffer_out() . This flushes DSP caches to ensure all data is written back to memory and is available to the CPU. Since this is an asynchronous operation the client must synchronize with the queue before accessing the buffer. Note that the fcvqueue implementation does not perform cache maintenance operations on the application processor side, but rather assumes buffers are cache coherent. SDM845 supports one-way cache coherency with the Compute DSP and buffers are allocated as coherent by default; to use non-coherent buffers the client must perform cache maintenance on the application processor too. Image processing operations fcvqueue exposes four simple image processing operations: Copy, 3x3 Dilate, 3x3 Gaussian Blur, and 3x3 Median Filter. The copy operation is implemented as a simple memcpy() , while the other operations use the FastCV library. fcvqueue_enqueue_op() enqueues an image processing operation. Both input and output buffers must be in DSP ownership, i.e. the client must first request cache invalidation with fcvqueue_enqueue_buffer_in() . Since fcvqueue does not automatically perform cache maintenance the output will not be available to the application processor before the client calls fcvqueue_enqueue_buffer_out() and waits for its completion. There is no need to perform cache maintenance operations on temporary buffers - if the output of one operation is just used as the input of another one on the DSP the buffers can stay local to the DSP. This is a common pattern for many use cases including chains of image processing filters or per-layer neural network processing. Basic Usage Sequence A basic fcvqueue sequence consists of the following steps: Initialize: fcvqueue_create() . See Initialization and Uninitialization Allocate and map buffers: fcvqueue_alloc_buffer() , fcvqueue_buffer_ptr() . See Memory Allocation and Deallocation Place input data in input buffers Cache maintenance; move buffers to DSP ownership: fcvqueue_enqueue_buffer_in() . See Cache Maintenance Use the buffers for image processing operations: fcvqueue_enqueue_op() . See Image Processing Operations Cache maintenance to get output data back to the CPU: fcvqueue_enqueue_buffer_out() . See Cache Maintenance Use output data De-initialization fcvqueuetest.c illustrates this approach. Performance fcvqueuetest.c contains a couple of simple performance benchmarks. They help illustrate when using the asynchronous message queue is beneficial over simple FastRPC calls: A single synchronous operation (dilate) is as fast as a FastRPC call, but the queue shows benefit when the client can enqueue multiple operations before synchronizing. Queue performance The queueperf example illustrates how message queue performance is affected by different queue depths and message sizes. Communication overhead grows when the queue can hold fewer messages; at the worst case when the queue only fits one message communication becomes synchronous and there is no benefit over using regular FastRPC calls.","title":"Asyncdspq"},{"location":"examples/asyncdspq_example/index.html#asyncdspq-example","text":"The asyncdspq (asynchronous message queue) is a utility library for asynchronous communication between the application processor and the DSP. This example contains the following three test cases to demonstrate the usage of asyncdspq. All these examples run on target and not on simulator. Basic messaging (queuetest) Image processing queue (fcvqueue) Queue performance (queueperf) Note: This example and the asyncdspq library itself will be deprecated when its replacement, the asynchronous packet queue , will be available in the next SDK release.","title":"Asyncdspq example"},{"location":"examples/asyncdspq_example/index.html#basic-messaging","text":"The queuetest example application contains a number of basic message queue tests and examples. In all cases the application processor sends two integers to the DSP, the DSP adds them together, and delivers a result back. The following tests are located in the function test_asyncdspq() .","title":"Basic messaging"},{"location":"examples/asyncdspq_example/index.html#simple-messages","text":"The first two tests send a single message from the application processor to the DSP and vice versa. The basic sequence is as follows: The application processor creates a queue with asyncdspq_create() . For read tests the CPU writes a message to the queue with asyncdspq_write() . The CPU passes the queue attach handle from asyncdspq_create() to the DSP over a regular FastRPC call. The DSP attaches to the queue with asyncdspq_attach() . The DSP either reads the message from the queue or writes one. The DSP detaches from the queue with asyncdspq_detach() . The application processor verifies the result. The application processor destroys the queue with asyncdspq_destroy() . Note that both tests use a single queue and use other mechanisms to verify the response. This is a valid configuration and can be useful for clients that mostly only send messages in one direction. The third test simply sends multiple messages from the DSP to the application processor.","title":"Simple Messages"},{"location":"examples/asyncdspq_example/index.html#tests-with-multiple-queues","text":"The following tests in test_asyncdspq() send messages between the CPU and the DSP using multiple queues, both using blocking read/write operations and using callbacks.","title":"Tests with multiple queues"},{"location":"examples/asyncdspq_example/index.html#canceling-operations","text":"The tests in test_asyncdspq_cancel() test canceling synchronous read/write operations under different circumstances. Many clients may not need to use asyncdspq_cancel() if they have another mechanism to ensure queues are empty before destroying them; typically this is done by sending an explicit \u201cclose\u201d message and waiting for a response.","title":"Canceling operations"},{"location":"examples/asyncdspq_example/index.html#threading","text":"test_asyncdspq_threads() tests different threading scenarios. The message queue library is designed to be thread-safe for read and write operations, but queues should be created and destroyed from the same thread. Most clients will use a single thread for reading and writing the queue, either from their own thread or callbacks from the queue library.","title":"Threading"},{"location":"examples/asyncdspq_example/index.html#image-processing-queue","text":"The fcvqueue example implements a queue of image processing operations using FastCV, which is a available in the compute add-on. It illustrates a more complex message queue use case, and also shows how to handle buffer allocation, memory mapping, and cache management. The threading and callback model is typical for most message queue clients: fcvqueue uses blocking writes and non-blocking reads with message callbacks. The fcvqueue example is split between three main files: fcvqueue.c : CPU-side implementation. The rest of this section refers primarily to functions in this file. fcvqueue_dsp_imp.c : DSP-side implementation for the example. Contains FastRPC methods called from fcvqueue.c and the message queue client code. fcvqueuetest.c : Test application. Start here to follow the calling sequences. This section briefly describes the key parts of the example. For more details refer to the source code, starting with the test application. Note that fcvqueue is not thread-safe, and while the code checks for errors codes from all the functions it calls it does not attempt to recover from errors. The queue library generally does not return recoverable errors except where separately noted; errors result from either programming errors or running out of system resources such as memory.","title":"Image processing queue"},{"location":"examples/asyncdspq_example/index.html#initialization-and-uninitialization","text":"fcvqueue_create() allocates internal structures and creates two queues--one for requests and one for responses--and calls the DSP-side implementation to attach to them. fcvqueue_destroy() ensures the queues are empty, cancels any operations on the DSP side, and detaches and destroys queues.","title":"Initialization and uninitialization"},{"location":"examples/asyncdspq_example/index.html#memory-allocation-and-de-allocation","text":"fcvqueue_alloc_buffer() allocates shareable ION buffers using the rpcmem library and sends them to the DSP to be mapped. FastRPC handles mapping the buffer to the DSP's SMMU and internal MMU; as long as the DSP code keeps a mapping open (with HAP_mmap() ) the buffers won't be unmapped. Subsequent operations on the buffer refer to it by its file descriptor (FD). The DSP-side code maintains a mapping of FDs to buffer addresses. This avoids having to map/unmap buffers for each operation, but also requires explicit cache maintenance operations. fcvqueue_free_buffer() is the reverse of allocation: It unmaps buffers from the DSP, releasing FastRPC MMU/SMMU mappings, and deallocates the memory.","title":"Memory allocation and de-allocation"},{"location":"examples/asyncdspq_example/index.html#synchronization","text":"Most fcvqueue operations apart from initialization and memory allocation are asynchronous. The client queues requests which are processed by the DSP asynchronously without immediate feedback. To determine that processing has reached a specific point the client queue a synchronization request. fcvqueue_enqueue_sync() sends a synchronization request to the DSP. The DSP simply responds to the request with the same token; when the application processor receives the response it calls a callback function. This informs the client that all operations through the synchronization request have been completed. fcvqueue_sync() is a synchronous variant of the same request. It simply calls fcvqueue_enqueue_sync() and waits for a callback before returning. Once fcvqueue_sync() returns both the request and response queues are empty.","title":"Synchronization"},{"location":"examples/asyncdspq_example/index.html#cache-maintenance","text":"In addition to memory mapping, FastRPC also handles cache maintenance on its clients' behalf. Message queue clients must handle this explicitly, which can be more complex but also gives the client an opportunity to determine when to manage the cache. The fcvqueue example exposes cache maintenance through two operations: fcvqueue_enqueue_buffer_in() and fcvqueue_enqueue_buffer_out() , corresponding to cache invalidate and flush. After a buffer is allocated it is considered to be in application processor ownership, and the CPU can place input data in the buffer. Before the DSP can use the buffer it must be transferred to DSP ownership with fcvqueue_enqueue_buffer_in() . This invalidates the buffer on the DSP side to ensure there is no stale data in DSP caches. While the buffer is in DSP ownership the application processor must not access it. To make a buffer and its contents available to the application processor again the client must call fcvqueue_enqueue_buffer_out() . This flushes DSP caches to ensure all data is written back to memory and is available to the CPU. Since this is an asynchronous operation the client must synchronize with the queue before accessing the buffer. Note that the fcvqueue implementation does not perform cache maintenance operations on the application processor side, but rather assumes buffers are cache coherent. SDM845 supports one-way cache coherency with the Compute DSP and buffers are allocated as coherent by default; to use non-coherent buffers the client must perform cache maintenance on the application processor too.","title":"Cache maintenance"},{"location":"examples/asyncdspq_example/index.html#image-processing-operations","text":"fcvqueue exposes four simple image processing operations: Copy, 3x3 Dilate, 3x3 Gaussian Blur, and 3x3 Median Filter. The copy operation is implemented as a simple memcpy() , while the other operations use the FastCV library. fcvqueue_enqueue_op() enqueues an image processing operation. Both input and output buffers must be in DSP ownership, i.e. the client must first request cache invalidation with fcvqueue_enqueue_buffer_in() . Since fcvqueue does not automatically perform cache maintenance the output will not be available to the application processor before the client calls fcvqueue_enqueue_buffer_out() and waits for its completion. There is no need to perform cache maintenance operations on temporary buffers - if the output of one operation is just used as the input of another one on the DSP the buffers can stay local to the DSP. This is a common pattern for many use cases including chains of image processing filters or per-layer neural network processing.","title":"Image processing operations"},{"location":"examples/asyncdspq_example/index.html#basic-usage-sequence","text":"A basic fcvqueue sequence consists of the following steps: Initialize: fcvqueue_create() . See Initialization and Uninitialization Allocate and map buffers: fcvqueue_alloc_buffer() , fcvqueue_buffer_ptr() . See Memory Allocation and Deallocation Place input data in input buffers Cache maintenance; move buffers to DSP ownership: fcvqueue_enqueue_buffer_in() . See Cache Maintenance Use the buffers for image processing operations: fcvqueue_enqueue_op() . See Image Processing Operations Cache maintenance to get output data back to the CPU: fcvqueue_enqueue_buffer_out() . See Cache Maintenance Use output data De-initialization fcvqueuetest.c illustrates this approach.","title":"Basic Usage Sequence"},{"location":"examples/asyncdspq_example/index.html#performance","text":"fcvqueuetest.c contains a couple of simple performance benchmarks. They help illustrate when using the asynchronous message queue is beneficial over simple FastRPC calls: A single synchronous operation (dilate) is as fast as a FastRPC call, but the queue shows benefit when the client can enqueue multiple operations before synchronizing.","title":"Performance"},{"location":"examples/asyncdspq_example/index.html#queue-performance","text":"The queueperf example illustrates how message queue performance is affected by different queue depths and message sizes. Communication overhead grows when the queue can hold fewer messages; at the worst case when the queue only fits one message communication becomes synchronous and there is no benefit over using regular FastRPC calls.","title":"Queue performance"},{"location":"examples/calculator/index.html","text":"Calculator example Overview The calculator example illustrates the following features: Using the command line , perform the following steps Building Simulator testing On-target testing Using the IDE , perform following steps Import project Building Simulator testing Modifying the example Project structure Customizing the calculator example Please note that in this document we are discussing procedures for Android targets only. For LE targets refer OS support CPU . The calculator example consumes an array of integers and returns both the sum and the maximum value of that array. Using command line Building The example comes with a walkthrough script called calculator_walkthrough.py . Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts. Walkthrough script automates building, running and signing the device steps mentioned in this section. You can run walkthrough script if you are suck at any step in this section and examine the output of script and/or script itself for better understanding. Without the walkthrough script, you will need to build both the Android and Hexagon modules. This is accomplished by running the following make commands (assuming your desired Hexagon architecture version is v65): make android BUILD=Debug make hexagon BUILD=Debug DSP_ARCH=v65 For more information on the build syntax, please refer to the building reference instructions . Alternatively, you can build the same Android and Hexagon modules with CMake. For Windows: cmake_build.cmd hexagon BUILD=Debug cmake_build.cmd android BUILD=Debug For Linux: source cmake_build.bash hexagon BUILD=Debug source cmake_build.bash android BUILD=Debug To target a different DSP architecture than the default one (V65), simply use the DSP_ARCH option when using the hexagon target. For more information on CMake usage, see the CMake documentation . Simulator testing Now that we have built the code, we will discuss how to run the code on the simulator. The Hexagon Simulator, hexagon-sim , is found in %DEFAULT_HEXAGON_TOOLS_ROOT% \\Tools\\bin\\ . For reference documentation on the Hexagon simulator, please refer to hexagon simulator document. For general instructions on the process of running code on the simulator and on device, please refer to the run instructions. The following command will build the Debug Hexagon variant of the calculator example targeting the v65 instruction set: make hexagon BUILD=Debug DSP_ARCH=v65 This command will result in creating a binary ELF calculator_q . The simulator command generated by the build system to run calculator_q can be found in the the last section Command line used to invoke simulator of the generated file hexagon_Debug_toolv84_v65\\pmu_stats.txt . You can reuse this command directly and/or modify it as desired for running additional simulations. The main simulator command line options are as follows: Simulator option Description --mv* Simulate for a particular architecture version of Hexagon. E.g. -mv65 . --simulated_returnval Cause the simulator to return a value to its caller indicating the final execution status of the target application. --usefs <path> Cause the simulator to search for required files in the directory with the specified path. --pmu_statsfile <filename> Generate a PMU statistics file with the specified name. See the SysMon Profiler for more information about PMU events. --help Print available simulator options. As an example, here is how the calculator_q ELF file might be run on the simulator: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-sim -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt hexagon_Debug_toolv84_v65/calculator_q -- This command should generate the following output, which highlights that two tests were executed and completed successfully. hexagon-sim INFO: The rev_id used in the simulation is 0x00004065 (v65a_512) - allocate 1024 bytes from ION heap - creating sequence of numbers from 0 to 255 - compute sum on domain 0 - call calculator_sum on the DSP - sum = 32640 - call calculator_max on the DSP =============== DSP: maximum result 255 ==============: HIGH:0x0:55:calculator_imp.c - max value = 255 - allocate 1024 bytes from ION heap - creating sequence of numbers from 0 to 255 - compute sum locally =============== DSP: local sum result 32640 =============== - find max locally =============== DSP: local max result 255 =============== ############################################################ Summary Report ############################################################ Pass: 2 Undetermined: 0 Fail: 0 Did not run: 0 On-target testing Now that we have run the code on the simulator, let's discuss the process of running the code on target. If you want to run your code on target without using the walkthrough script, please use the following steps: Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push the HLOS side calculator test executable and supporting calculator stub library onto the device's file system adb shell mkdir -p /vendor/bin/ adb push android_Debug_aarch64/ship/calculator /vendor/bin/ adb shell chmod 777 /vendor/bin/calculator adb push android_Debug_aarch64/ship/libcalculator.so /vendor/lib64/ Push the Hexagon Shared Object to the device's file system adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_Debug_toolv84_v65/ship/libcalculator_skel.so /vendor/lib/rfsa/dsp/sdk/ Generate and push a device-specific test signature based on the device's serial number. Follow the steps listed in the Use signer.py section of the signing documentation. Note: This step only needs to be done once as the same test signature will enable loading any module. Redirect DSP FARF messages to ADB logcat by creating a farf file. For more information on logcat please refer to the logcat section of messaging . adb shell echo \"0x1f > /vendor/lib/rfsa/dsp/sdk/calculator.farf\" Launch a new CLI shell to view the DSP's diagnostic messages via logcat Open a new shell or command window and type: adb logcat -s adsprpc NOTE: This adsprpc filter captures messages from any of the DSPs (e.g aDSP, cDSP, or sDSP). Execute the example Please note that following two environment variables are to be passed along with example binary and its arguments LD_LIBRARY_PATH : Location of HLOS-side stub library on device. ADSP_LIBRARY_PATH (or DSP_LIBRARY_PATH for SM8250 and later devices): Location of DSP libraries and TestSig on device. For example: # run example with 1000 array size on CPU adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 1 3 1000 # run example with 1000 array size on aDSP adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 0 0 1000 # run example with 1000 array size on sDSP adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 0 2 1000 # run example with 1000 array size on cDSP adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 0 3 1000 # run example with 1000 array size on cDSP Unsigned PD adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 0 3 1000 1 NOTE: The above commands will run successfully only on a signed device. The last command executes the example using an unsigned PD , and as such, will work properly even if the TestSig is removed from the DSP search path . These commands should generate the following output. Every block starts with --- Run Calculator Example on ... and should end with success . ---- Run Calculator Example Locally on Android ---- adb wait-for-device shell export LD_LIBRARY_PATH=\"/vendor/lib64/;\" /vendor/bin//calculator 1 0 1000 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum locally =============== DSP: local sum result 499500 =============== - find max locally =============== DSP: local max result 999 =============== - success ---- Run Calculator Example on aDSP ---- adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" /vendor/bin//calculator 0 0 1000 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum on domain 0 - call calculator_sum on the DSP - sum = 499500 - call calculator_max on the DSP - max value = 999 - success ---- Run Calculator Example on sDSP ---- adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" /vendor/bin//calculator 0 2 1000 0 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum on domain 2 - call calculator_sum on the DSP - sum = 499500 - call calculator_max on the DSP - max value = 999 - success ---- Run Calculator Example on cDSP ---- adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" /vendor/bin//calculator 0 3 1000 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum on domain 3 - call calculator_sum on the DSP - sum = 499500 - call calculator_max on the DSP - max value = 999 - success ---- Run Calculator Example on cDSP Unsigned PD ---- adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" /vendor/bin//calculator 0 3 1000 1 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum on domain 3 - requested signature-free dynamic module offload - call calculator_sum on the DSP - sum = 499500 - call calculator_max on the DSP - max value = 999 - success Using the IDE The previous sections described how to build and run the code on simulator or on target using the command line. This section explains how to perform the same steps using the IDE. Before following these instructions, please ensure that you have already created a workspace for your IDE. Importing project Once in the IDE, import calculator into your IDE workspace by right-clicking in Project Explorer and selecting Import -> Hexagon C/C++ -> Import Hexagon Project > Next . In the dialog box, specify the following project properties: Project type \u2013 Makefile Project Project name \u2013 calculator Project location \u2013 %HEXAGON_SDK_ROOT%\\examples\\calculator Click on the Finish button to finish importing the project Building To build the imported project, right-click on the project , select properties and then C/C++ Build . Under Builder Settings , enter the build command as make hexagon BUILD=Debug DSP_ARCH=v65 VERBOSE=1 (or enter your target-appropriate build flavor). While debugging, Debug build flavors are recommended when possible over ReleaseG. When ReleaseG is selected, the compiler will apply optimizations that will result in the disassembly interleaving various C instructions, which makes the code harder to understand while stepping through it. Click on Apply and Close to apply setting and close the window. Build the project by right-clicking on the project and select Build Project . Once building is done, you should see 0 errors, 0 warnings in the console window. Simulator testing To run the project on simulator, right-click on the project in Project Explorer and choose Run As -> Run Configuration . This command displays the Run Configurations dialog box, which enables you to configure the simulator, program arguments, and runtime environment. In order to debug on the simulator, the user must select the target-specific QuRT image runelf.pbn file for the desired architecture version by specifying the QuRT image path in the C/C++ Application field. This approach is illustrated below and will result in the debugger loading the user ELF, calculator_q, and calling main() . The dialog box displays tabs for configuring the simulator, program arguments, and runtime environment. (Note that the left-side pane in the dialog box includes a newly-created runtime configuration named calculator Default , which appears under the item Hexagon C/C++ Application .) To specify the simulator arguments, click on the Simulator tab in the dialog box. For this example please give following options in the miscellaneous flags section. The brief explanations of these flags is given above . -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt hexagon_Debug_toolv84_v65/calculator_q -- To execute the program, click on the Run button at the bottom of the dialog box. The dialog box closes and the following output is displayed in the console of the main IDE window. Modifying the example Project structure Now that you are able to build and run the code, let's understand how the code is organized. The Hexagon SDK's calculator example demonstrates the ability to offload computation on the DSP by calling a function on the HLOS and have it executed on the DSP. This is done via FastRPC in which the complexity of the remote procedure call is made transparent to the caller by only requiring the client to call a library function on the HLOS. That library on the HLOS is referred to as a stub and its corresponding implementation on the DSP is referred to as a skel. Makefile Root makefile that invokes variant-specific .min files to either build the application processor source code or the Hexagon DSP source code. hexagon.min , android.min , UbuntuARM.min Contains the make.d directives used to build the application processor and Hexagon DSP source code. inc/calculator.idl IDL interface that defines the calculator API. This IDL is compiled by the QAIC IDL compiler into the following files: calculator.h : C/C++ header file calculator_stub.c : Stub source that needs to be built for the HLOS (Android, Embedded Linux etc...) calculator_skel.c : Skel source that needs to be built for the Hexagon DSP src/calculator_main.c Source for the Android executable that calls the calculator stub on the HLOS side to offload the compute task onto the DSP. src/calculator_test.c Source for the HLOS-side test function. src/calculator_imp.c Source for the Hexagon-side implementation of the calculator interface and is compiled into a shared object. src/calculator_test_main.c Source for simulator executable which calls DSP side compute task on simulator. Customizing the calculator example The following steps will show you how you can declare and implement a new method on the DSP and invoke it from the application processor: Add a new method to the calculator interface in inc/calculator.idl long diff(in sequence<long> vec, rout long long res); Add a new implementation of that function in src/calculator_imp.c int calculator_diff(remote_handle64 h, const int* vec, int vecLen, int64* res) { int ii = 0; *res = vec[0]; for(ii = 1; ii < vecLen; ++ii) { *res = *res - vec[ii]; } FARF(HIGH, \"=============== DSP: diff result %lld ===============\", *res); return 0; } Call the new function from the executable ( src/calculator_main.c ). assert(0 == calculator_diff(h, test, num, &result)); Both android and hexagon binaries are to be rebuilt once modification is done.","title":"Calculator"},{"location":"examples/calculator/index.html#calculator-example","text":"","title":"Calculator example"},{"location":"examples/calculator/index.html#overview","text":"The calculator example illustrates the following features: Using the command line , perform the following steps Building Simulator testing On-target testing Using the IDE , perform following steps Import project Building Simulator testing Modifying the example Project structure Customizing the calculator example Please note that in this document we are discussing procedures for Android targets only. For LE targets refer OS support CPU . The calculator example consumes an array of integers and returns both the sum and the maximum value of that array.","title":"Overview"},{"location":"examples/calculator/index.html#using-command-line","text":"","title":"Using command line"},{"location":"examples/calculator/index.html#building","text":"The example comes with a walkthrough script called calculator_walkthrough.py . Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts. Walkthrough script automates building, running and signing the device steps mentioned in this section. You can run walkthrough script if you are suck at any step in this section and examine the output of script and/or script itself for better understanding. Without the walkthrough script, you will need to build both the Android and Hexagon modules. This is accomplished by running the following make commands (assuming your desired Hexagon architecture version is v65): make android BUILD=Debug make hexagon BUILD=Debug DSP_ARCH=v65 For more information on the build syntax, please refer to the building reference instructions . Alternatively, you can build the same Android and Hexagon modules with CMake. For Windows: cmake_build.cmd hexagon BUILD=Debug cmake_build.cmd android BUILD=Debug For Linux: source cmake_build.bash hexagon BUILD=Debug source cmake_build.bash android BUILD=Debug To target a different DSP architecture than the default one (V65), simply use the DSP_ARCH option when using the hexagon target. For more information on CMake usage, see the CMake documentation .","title":"Building"},{"location":"examples/calculator/index.html#simulator-testing","text":"Now that we have built the code, we will discuss how to run the code on the simulator. The Hexagon Simulator, hexagon-sim , is found in %DEFAULT_HEXAGON_TOOLS_ROOT% \\Tools\\bin\\ . For reference documentation on the Hexagon simulator, please refer to hexagon simulator document. For general instructions on the process of running code on the simulator and on device, please refer to the run instructions. The following command will build the Debug Hexagon variant of the calculator example targeting the v65 instruction set: make hexagon BUILD=Debug DSP_ARCH=v65 This command will result in creating a binary ELF calculator_q . The simulator command generated by the build system to run calculator_q can be found in the the last section Command line used to invoke simulator of the generated file hexagon_Debug_toolv84_v65\\pmu_stats.txt . You can reuse this command directly and/or modify it as desired for running additional simulations. The main simulator command line options are as follows: Simulator option Description --mv* Simulate for a particular architecture version of Hexagon. E.g. -mv65 . --simulated_returnval Cause the simulator to return a value to its caller indicating the final execution status of the target application. --usefs <path> Cause the simulator to search for required files in the directory with the specified path. --pmu_statsfile <filename> Generate a PMU statistics file with the specified name. See the SysMon Profiler for more information about PMU events. --help Print available simulator options. As an example, here is how the calculator_q ELF file might be run on the simulator: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-sim -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt hexagon_Debug_toolv84_v65/calculator_q -- This command should generate the following output, which highlights that two tests were executed and completed successfully. hexagon-sim INFO: The rev_id used in the simulation is 0x00004065 (v65a_512) - allocate 1024 bytes from ION heap - creating sequence of numbers from 0 to 255 - compute sum on domain 0 - call calculator_sum on the DSP - sum = 32640 - call calculator_max on the DSP =============== DSP: maximum result 255 ==============: HIGH:0x0:55:calculator_imp.c - max value = 255 - allocate 1024 bytes from ION heap - creating sequence of numbers from 0 to 255 - compute sum locally =============== DSP: local sum result 32640 =============== - find max locally =============== DSP: local max result 255 =============== ############################################################ Summary Report ############################################################ Pass: 2 Undetermined: 0 Fail: 0 Did not run: 0","title":"Simulator testing"},{"location":"examples/calculator/index.html#on-target-testing","text":"Now that we have run the code on the simulator, let's discuss the process of running the code on target. If you want to run your code on target without using the walkthrough script, please use the following steps: Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push the HLOS side calculator test executable and supporting calculator stub library onto the device's file system adb shell mkdir -p /vendor/bin/ adb push android_Debug_aarch64/ship/calculator /vendor/bin/ adb shell chmod 777 /vendor/bin/calculator adb push android_Debug_aarch64/ship/libcalculator.so /vendor/lib64/ Push the Hexagon Shared Object to the device's file system adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_Debug_toolv84_v65/ship/libcalculator_skel.so /vendor/lib/rfsa/dsp/sdk/ Generate and push a device-specific test signature based on the device's serial number. Follow the steps listed in the Use signer.py section of the signing documentation. Note: This step only needs to be done once as the same test signature will enable loading any module. Redirect DSP FARF messages to ADB logcat by creating a farf file. For more information on logcat please refer to the logcat section of messaging . adb shell echo \"0x1f > /vendor/lib/rfsa/dsp/sdk/calculator.farf\" Launch a new CLI shell to view the DSP's diagnostic messages via logcat Open a new shell or command window and type: adb logcat -s adsprpc NOTE: This adsprpc filter captures messages from any of the DSPs (e.g aDSP, cDSP, or sDSP). Execute the example Please note that following two environment variables are to be passed along with example binary and its arguments LD_LIBRARY_PATH : Location of HLOS-side stub library on device. ADSP_LIBRARY_PATH (or DSP_LIBRARY_PATH for SM8250 and later devices): Location of DSP libraries and TestSig on device. For example: # run example with 1000 array size on CPU adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 1 3 1000 # run example with 1000 array size on aDSP adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 0 0 1000 # run example with 1000 array size on sDSP adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 0 2 1000 # run example with 1000 array size on cDSP adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 0 3 1000 # run example with 1000 array size on cDSP Unsigned PD adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" calculator 0 3 1000 1 NOTE: The above commands will run successfully only on a signed device. The last command executes the example using an unsigned PD , and as such, will work properly even if the TestSig is removed from the DSP search path . These commands should generate the following output. Every block starts with --- Run Calculator Example on ... and should end with success . ---- Run Calculator Example Locally on Android ---- adb wait-for-device shell export LD_LIBRARY_PATH=\"/vendor/lib64/;\" /vendor/bin//calculator 1 0 1000 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum locally =============== DSP: local sum result 499500 =============== - find max locally =============== DSP: local max result 999 =============== - success ---- Run Calculator Example on aDSP ---- adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" /vendor/bin//calculator 0 0 1000 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum on domain 0 - call calculator_sum on the DSP - sum = 499500 - call calculator_max on the DSP - max value = 999 - success ---- Run Calculator Example on sDSP ---- adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" /vendor/bin//calculator 0 2 1000 0 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum on domain 2 - call calculator_sum on the DSP - sum = 499500 - call calculator_max on the DSP - max value = 999 - success ---- Run Calculator Example on cDSP ---- adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" /vendor/bin//calculator 0 3 1000 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum on domain 3 - call calculator_sum on the DSP - sum = 499500 - call calculator_max on the DSP - max value = 999 - success ---- Run Calculator Example on cDSP Unsigned PD ---- adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk;\" /vendor/bin//calculator 0 3 1000 1 - starting calculator multi domains test - allocate 4000 bytes from ION heap - creating sequence of numbers from 0 to 999 - compute sum on domain 3 - requested signature-free dynamic module offload - call calculator_sum on the DSP - sum = 499500 - call calculator_max on the DSP - max value = 999 - success","title":"On-target testing"},{"location":"examples/calculator/index.html#using-the-ide","text":"The previous sections described how to build and run the code on simulator or on target using the command line. This section explains how to perform the same steps using the IDE. Before following these instructions, please ensure that you have already created a workspace for your IDE.","title":"Using the IDE"},{"location":"examples/calculator/index.html#importing-project","text":"Once in the IDE, import calculator into your IDE workspace by right-clicking in Project Explorer and selecting Import -> Hexagon C/C++ -> Import Hexagon Project > Next . In the dialog box, specify the following project properties: Project type \u2013 Makefile Project Project name \u2013 calculator Project location \u2013 %HEXAGON_SDK_ROOT%\\examples\\calculator Click on the Finish button to finish importing the project","title":"Importing project"},{"location":"examples/calculator/index.html#building_1","text":"To build the imported project, right-click on the project , select properties and then C/C++ Build . Under Builder Settings , enter the build command as make hexagon BUILD=Debug DSP_ARCH=v65 VERBOSE=1 (or enter your target-appropriate build flavor). While debugging, Debug build flavors are recommended when possible over ReleaseG. When ReleaseG is selected, the compiler will apply optimizations that will result in the disassembly interleaving various C instructions, which makes the code harder to understand while stepping through it. Click on Apply and Close to apply setting and close the window. Build the project by right-clicking on the project and select Build Project . Once building is done, you should see 0 errors, 0 warnings in the console window.","title":"Building"},{"location":"examples/calculator/index.html#simulator-testing_1","text":"To run the project on simulator, right-click on the project in Project Explorer and choose Run As -> Run Configuration . This command displays the Run Configurations dialog box, which enables you to configure the simulator, program arguments, and runtime environment. In order to debug on the simulator, the user must select the target-specific QuRT image runelf.pbn file for the desired architecture version by specifying the QuRT image path in the C/C++ Application field. This approach is illustrated below and will result in the debugger loading the user ELF, calculator_q, and calling main() . The dialog box displays tabs for configuring the simulator, program arguments, and runtime environment. (Note that the left-side pane in the dialog box includes a newly-created runtime configuration named calculator Default , which appears under the item Hexagon C/C++ Application .) To specify the simulator arguments, click on the Simulator tab in the dialog box. For this example please give following options in the miscellaneous flags section. The brief explanations of these flags is given above . -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt hexagon_Debug_toolv84_v65/calculator_q -- To execute the program, click on the Run button at the bottom of the dialog box. The dialog box closes and the following output is displayed in the console of the main IDE window.","title":"Simulator testing"},{"location":"examples/calculator/index.html#modifying-the-example","text":"","title":"Modifying the example"},{"location":"examples/calculator/index.html#project-structure","text":"Now that you are able to build and run the code, let's understand how the code is organized. The Hexagon SDK's calculator example demonstrates the ability to offload computation on the DSP by calling a function on the HLOS and have it executed on the DSP. This is done via FastRPC in which the complexity of the remote procedure call is made transparent to the caller by only requiring the client to call a library function on the HLOS. That library on the HLOS is referred to as a stub and its corresponding implementation on the DSP is referred to as a skel. Makefile Root makefile that invokes variant-specific .min files to either build the application processor source code or the Hexagon DSP source code. hexagon.min , android.min , UbuntuARM.min Contains the make.d directives used to build the application processor and Hexagon DSP source code. inc/calculator.idl IDL interface that defines the calculator API. This IDL is compiled by the QAIC IDL compiler into the following files: calculator.h : C/C++ header file calculator_stub.c : Stub source that needs to be built for the HLOS (Android, Embedded Linux etc...) calculator_skel.c : Skel source that needs to be built for the Hexagon DSP src/calculator_main.c Source for the Android executable that calls the calculator stub on the HLOS side to offload the compute task onto the DSP. src/calculator_test.c Source for the HLOS-side test function. src/calculator_imp.c Source for the Hexagon-side implementation of the calculator interface and is compiled into a shared object. src/calculator_test_main.c Source for simulator executable which calls DSP side compute task on simulator.","title":"Project structure"},{"location":"examples/calculator/index.html#customizing-the-calculator-example","text":"The following steps will show you how you can declare and implement a new method on the DSP and invoke it from the application processor: Add a new method to the calculator interface in inc/calculator.idl long diff(in sequence<long> vec, rout long long res); Add a new implementation of that function in src/calculator_imp.c int calculator_diff(remote_handle64 h, const int* vec, int vecLen, int64* res) { int ii = 0; *res = vec[0]; for(ii = 1; ii < vecLen; ++ii) { *res = *res - vec[ii]; } FARF(HIGH, \"=============== DSP: diff result %lld ===============\", *res); return 0; } Call the new function from the executable ( src/calculator_main.c ). assert(0 == calculator_diff(h, test, num, &result)); Both android and hexagon binaries are to be rebuilt once modification is done.","title":"Customizing the calculator example"},{"location":"examples/calculator_c%2B%2B/index.html","text":"Calculator_C++ example Overview calculator_c++ is an example for using standard C++ in shared objects. It creates an HLOS application that remotely invokes C++ functions on the DSP using FastRPC. This example is derived from the calculator example. For more information on the setup and how to compile, please refer to the calculator example. The remaining of this page focuses only on how to use C++ code for the DSP. The calculator_c++ example includes the following five functions to showcase some C++ features calculator_plus_sum - calculates sum of elements of vector of integers. calculator_plus_static_sum - calculates sum of elements of vector of integers of statically initialized object. calculator_plus_test_tls - tests thread-local storage feature of C++11. calculator_plus_iostream_sum - calculates sum of elements of stream of data passed as file using standard input/output streams library(iostream). calculator_plus_uppercase_count - counts uppercase letters in passed string. For more information on C++(eg. supported standards, tools), please check the C/C++ section of the reference manuals. Please note that in this document we are discussing only the command-line procedure for running calculator_c++ example. For any IDE-related steps, refer to the IDE section of calculator example. Simulator testing The Hexagon simulator hexagon-sim is located under $DEFAULT_HEXAGON_TOOLS\\Tools\\bin\\ . While building hexagon variant (with a command like make hexagon BUILD=Debug DSP_ARCH=v65 ), the calculator example will create a binary ELF calculator_q.so . The simulator command generated by the make file to run calculator_q.so can be found in last section( Command line used to invoke simulator: ) of the generated file hexagon_Debug_toolv84_v65/pmu_stats.txt . You can use this command directly and/or modify it as desired for running additional simulations. Some of the available simulator command line options are discussed in simulator testing section of calculator . For more information on available options, please refer hexagon simulator document. Once the dynamic object has been created, you can execute it on the simulator as follows: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-sim -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0/rtos/qurt//computev65/sdksim_bin/runelf.pbn -- C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0/libs/run_main_on_hexagon/ship/hexagon_toolv84_v65/run_main_on_hexagon_sim -- calculator_q.so The test should complete indicating that \"1 test passed' in the Summary report displayed on the command line. On-target testing To run the calculator_plus test on target, simply execute the following instructions. For more information, please refer to the calculator example. Use ADB as root and remount system read/write adb root adb wait-for-device adb remount The HLOS side calculator_plus test executable and supporting calculator_c++ stub library must be pushed onto the device as follows: adb push android_Debug_aarch64/ship/calculator_plus /vendor/bin adb shell chmod 777 /vendor/bin/calculator_plus The Hexagon shared object must be pushed on to the device's file system as follows: adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_Debug_toolv84_v65/ship/libcalculator_plus_skel.so /vendor/lib/rfsa/dsp/sdk/ adb push hexagon_Debug_toolv84_v65/libc++.so.1 /vendor/lib/rfsa/dsp/sdk/ adb push hexagon_Debug_toolv84_v65/libc++abi.so.1 /vendor/lib/rfsa/dsp/sdk/ adb push calculator.input /vendor/lib/rfsa/dsp/sdk/ Note: The Hexagon Tools version 8.4.x introduces new symbols for c++17 compliance into the c++ libraries. The definitions of these symbols are not present on targets whose DSP image is built with a version of the Hexagon Tools older than 8.4.x. To overcome this problem, when using Hexagon tools older than 8.4.x, link the static weak_refs.a library as a whole archive to your custom skel library to forcefully include the weak definitions of these new symbols in the resulting shared library. (If the library is not linked as a whole archive, the symbols will be garbage-collected by the linker and not included in the resulting library.) For example, to do so with the calculator_c++ example, add the following rule to the hexagon.min file: libcalculator_plus_skel_LD_FLAGS +=--start-group --whole-archive $(HEXAGON_SDK_ROOT)/libs/weak_refs/ship/hexagon_toolv84/weak_refs.a --no-whole-archive --end-group Note: Refer to the queries section for more information on the libraries libc++.so.1 and libc++abi.so.1 . Generate a device specific test signature based on the device's serial number Follow the steps listed in the Use signer.py section of the signing documentation. Note: This step only needs to be done once as the same test signature will enable loading any module. Redirect DSP FARF messages to ADB logcat by creating a farf file adb shell echo \"0x1f > /vendor/lib/rfsa/dsp/sdk/calculator.farf\" Execute the example as follows: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/ DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/calculator_plus sum 10 The command line above shows how to execute calculator_plus_sum. You can replace the last two arguments sum 10 with the following to run other flavors of the benchmark: Arguments Description static_sum <Input> calculates sum of elements of vector of integers of statically initialized object. test_tls tests thread-local storage feature of C++11 iostream_sum <Input file location> calculates sum of elements of stream of data passed as file using standard input/output streams library(iostream). The output file for the iostream test will be written to /vendor/lib/rfsa/dsp/sdk/calculator.output uppercase_count <Input string> counts uppercase letters in passed string All tests return 0 on success. Refer to the DSP logs for more details on the execution of each of these tests. Note: test_tls is not supported on unsigned PD due to the limitation of POSIX APIs on the unsigned cDSP. Common queries How to compile binaries for alternate C++ standards? You can pass -std flag along with standard name in CXX_FLAGS . For example: CXX_FLAGS += -std=c++14 Alternate available C++ standards are C++03 and C++14. For more information on C++ library support, please refer to the Hexagon LLVM C/C++ Compiler . Which library to use: libstdc++ or libc++ for C++ code? libstdc++ should only be used when running a library compiled for the C++03 standard. For any newer standard, the libc++ and libc++abi libraries should be used instead as shown in hexagon.min . For more information on C++ library support, see Hexagon LLVM C/C++ Compiler How to find the libraries libc++.so.1 and libc++abi.so.1 ? When the example is run on the target or in the hexagon simulator, the loader tries to load the dependent c++ libraries and looks for their symbolic links during this process. The symbolic links for the libraries libc++.so and libc++abi.so have to be present in the current directory to be available to the loader. These C++ libraries have symbolic links as follows: libc++.so -> libc++.so.1 -> libc++.so.1.0 libc++abi.so -> libc++abi.so.1 -> libc++abi.so.1.0 These symbolic links are copied to the current directory by adding copy rules as follows in the hexagon.min file: TARGET_DIR = $(HEXAGON_LIB_DIR)/$(V_ARCH)/G0/pic # Copy needed libraries to local build directory $(V)/libcalculator_plus_skel.so: $(V)/libc++.so.1 $(V)/libc++abi.so.1 # Copy both versions of the library to the local build directory # The loader will select one version at link time $(V)/libc++.so.1: $(V)/libc++abi.so.1 $(call cp_af,$(TARGET_DIR)/libc++.so.1 $(V)) $(call cp_af,$(TARGET_DIR)/libc++.so.1.0 $(V)) $(V)/libc++abi.so.1: $(call cp_af,$(TARGET_DIR)/libc++abi.so.1 $(V)) $(call cp_af,$(TARGET_DIR)/libc++abi.so.1.0 $(V))","title":"Calculator C++"},{"location":"examples/calculator_c%2B%2B/index.html#calculator_c-example","text":"","title":"Calculator_C++ example"},{"location":"examples/calculator_c%2B%2B/index.html#overview","text":"calculator_c++ is an example for using standard C++ in shared objects. It creates an HLOS application that remotely invokes C++ functions on the DSP using FastRPC. This example is derived from the calculator example. For more information on the setup and how to compile, please refer to the calculator example. The remaining of this page focuses only on how to use C++ code for the DSP. The calculator_c++ example includes the following five functions to showcase some C++ features calculator_plus_sum - calculates sum of elements of vector of integers. calculator_plus_static_sum - calculates sum of elements of vector of integers of statically initialized object. calculator_plus_test_tls - tests thread-local storage feature of C++11. calculator_plus_iostream_sum - calculates sum of elements of stream of data passed as file using standard input/output streams library(iostream). calculator_plus_uppercase_count - counts uppercase letters in passed string. For more information on C++(eg. supported standards, tools), please check the C/C++ section of the reference manuals. Please note that in this document we are discussing only the command-line procedure for running calculator_c++ example. For any IDE-related steps, refer to the IDE section of calculator example.","title":"Overview"},{"location":"examples/calculator_c%2B%2B/index.html#simulator-testing","text":"The Hexagon simulator hexagon-sim is located under $DEFAULT_HEXAGON_TOOLS\\Tools\\bin\\ . While building hexagon variant (with a command like make hexagon BUILD=Debug DSP_ARCH=v65 ), the calculator example will create a binary ELF calculator_q.so . The simulator command generated by the make file to run calculator_q.so can be found in last section( Command line used to invoke simulator: ) of the generated file hexagon_Debug_toolv84_v65/pmu_stats.txt . You can use this command directly and/or modify it as desired for running additional simulations. Some of the available simulator command line options are discussed in simulator testing section of calculator . For more information on available options, please refer hexagon simulator document. Once the dynamic object has been created, you can execute it on the simulator as follows: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-sim -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0/rtos/qurt//computev65/sdksim_bin/runelf.pbn -- C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0/libs/run_main_on_hexagon/ship/hexagon_toolv84_v65/run_main_on_hexagon_sim -- calculator_q.so The test should complete indicating that \"1 test passed' in the Summary report displayed on the command line.","title":"Simulator testing"},{"location":"examples/calculator_c%2B%2B/index.html#on-target-testing","text":"To run the calculator_plus test on target, simply execute the following instructions. For more information, please refer to the calculator example. Use ADB as root and remount system read/write adb root adb wait-for-device adb remount The HLOS side calculator_plus test executable and supporting calculator_c++ stub library must be pushed onto the device as follows: adb push android_Debug_aarch64/ship/calculator_plus /vendor/bin adb shell chmod 777 /vendor/bin/calculator_plus The Hexagon shared object must be pushed on to the device's file system as follows: adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_Debug_toolv84_v65/ship/libcalculator_plus_skel.so /vendor/lib/rfsa/dsp/sdk/ adb push hexagon_Debug_toolv84_v65/libc++.so.1 /vendor/lib/rfsa/dsp/sdk/ adb push hexagon_Debug_toolv84_v65/libc++abi.so.1 /vendor/lib/rfsa/dsp/sdk/ adb push calculator.input /vendor/lib/rfsa/dsp/sdk/ Note: The Hexagon Tools version 8.4.x introduces new symbols for c++17 compliance into the c++ libraries. The definitions of these symbols are not present on targets whose DSP image is built with a version of the Hexagon Tools older than 8.4.x. To overcome this problem, when using Hexagon tools older than 8.4.x, link the static weak_refs.a library as a whole archive to your custom skel library to forcefully include the weak definitions of these new symbols in the resulting shared library. (If the library is not linked as a whole archive, the symbols will be garbage-collected by the linker and not included in the resulting library.) For example, to do so with the calculator_c++ example, add the following rule to the hexagon.min file: libcalculator_plus_skel_LD_FLAGS +=--start-group --whole-archive $(HEXAGON_SDK_ROOT)/libs/weak_refs/ship/hexagon_toolv84/weak_refs.a --no-whole-archive --end-group Note: Refer to the queries section for more information on the libraries libc++.so.1 and libc++abi.so.1 . Generate a device specific test signature based on the device's serial number Follow the steps listed in the Use signer.py section of the signing documentation. Note: This step only needs to be done once as the same test signature will enable loading any module. Redirect DSP FARF messages to ADB logcat by creating a farf file adb shell echo \"0x1f > /vendor/lib/rfsa/dsp/sdk/calculator.farf\" Execute the example as follows: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/ DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/calculator_plus sum 10 The command line above shows how to execute calculator_plus_sum. You can replace the last two arguments sum 10 with the following to run other flavors of the benchmark: Arguments Description static_sum <Input> calculates sum of elements of vector of integers of statically initialized object. test_tls tests thread-local storage feature of C++11 iostream_sum <Input file location> calculates sum of elements of stream of data passed as file using standard input/output streams library(iostream). The output file for the iostream test will be written to /vendor/lib/rfsa/dsp/sdk/calculator.output uppercase_count <Input string> counts uppercase letters in passed string All tests return 0 on success. Refer to the DSP logs for more details on the execution of each of these tests. Note: test_tls is not supported on unsigned PD due to the limitation of POSIX APIs on the unsigned cDSP.","title":"On-target testing"},{"location":"examples/calculator_c%2B%2B/index.html#common-queries","text":"How to compile binaries for alternate C++ standards? You can pass -std flag along with standard name in CXX_FLAGS . For example: CXX_FLAGS += -std=c++14 Alternate available C++ standards are C++03 and C++14. For more information on C++ library support, please refer to the Hexagon LLVM C/C++ Compiler . Which library to use: libstdc++ or libc++ for C++ code? libstdc++ should only be used when running a library compiled for the C++03 standard. For any newer standard, the libc++ and libc++abi libraries should be used instead as shown in hexagon.min . For more information on C++ library support, see Hexagon LLVM C/C++ Compiler How to find the libraries libc++.so.1 and libc++abi.so.1 ? When the example is run on the target or in the hexagon simulator, the loader tries to load the dependent c++ libraries and looks for their symbolic links during this process. The symbolic links for the libraries libc++.so and libc++abi.so have to be present in the current directory to be available to the loader. These C++ libraries have symbolic links as follows: libc++.so -> libc++.so.1 -> libc++.so.1.0 libc++abi.so -> libc++abi.so.1 -> libc++abi.so.1.0 These symbolic links are copied to the current directory by adding copy rules as follows in the hexagon.min file: TARGET_DIR = $(HEXAGON_LIB_DIR)/$(V_ARCH)/G0/pic # Copy needed libraries to local build directory $(V)/libcalculator_plus_skel.so: $(V)/libc++.so.1 $(V)/libc++abi.so.1 # Copy both versions of the library to the local build directory # The loader will select one version at link time $(V)/libc++.so.1: $(V)/libc++abi.so.1 $(call cp_af,$(TARGET_DIR)/libc++.so.1 $(V)) $(call cp_af,$(TARGET_DIR)/libc++.so.1.0 $(V)) $(V)/libc++abi.so.1: $(call cp_af,$(TARGET_DIR)/libc++abi.so.1 $(V)) $(call cp_af,$(TARGET_DIR)/libc++abi.so.1.0 $(V))","title":"Common queries"},{"location":"examples/calculator_c%2B%2B_apk/index.html","text":"Calculator C++ APK example This example illustrates how to build an end-to-end Android APK that uses C++ APIs in the Android application layer and offloads computation to the DSP. Starting with Android-P, access to /system libraries from the /vendor partition and vice-versa are prohibited with the addition of Treble restrictions. Hence, vendor applications do not have access to the libc++_shared.so library located in /system . However APKs have access to the libray located in system . Thus using C++ in an Android vendor application requires constructing an APK that dynamically links to the C++ libraries. APKs are untrusted applications. Starting with Android-R untrusted applications cannot create signed PDs on the DSPs for offloading. They can only create unsigned PDs on the compute DSP for offloading. Hence this example uses unsigned PD. On-target testing This section describes installation and use of the existing pre-compiled APK calculator_c++.apk on the device. To run the APK on device, you will need to install the APK and push the Hexagon shared object onto your device. This APK can run directly on all LA devices supported in the Hexagon SDK. Install the APK The example comes with a pre-existing APK that can be installed on device directly. For instructions on how to modify the implementation and rebuild the APK, see how to customize the application . To install the existing APK on device, execute the following command: adb install calculator_c++.apk Generate and push the DSP shared object The DSP source code of the calculator C++ APK example is under $HEXAGON_SDK_ROOT/examples/calculator_c++_apk/dsp . You can build the DSP shared object with the following instructions: cd dsp make hexagon BUILD=Debug DSP_ARCH=v65 Refer to the build instructions of the calculator example for more details on building shared objects. When the shared object is ready, push it to /vendor/lib/rfsa/dsp on your Android device. For example: adb root adb remount adb push hexagon_Debug_toolv84_v65\\ship\\libcalculator_skel.so /vendor/lib/rfsa/dsp Application customization The steps below explain how to recreate the APK introduced in the previous section . We first create a new Android Studio project that uses Java as the Android app development language and the Java Native Interface (JNI) for enabling native C/C++ support and thus interfacing the DSP. We then customize both the project setup and the source code to build the customized APK with the tools used with the Hexagon SDK. Create a new Android Studio project Use this link to download and install the Android SDK. If you are new to Android Studio, familiarize yourself with existing online tutorials to learn how to create, build, and run projects Create a new Android Native C++ project. Native development enables the Java application to interface C methods, which can then make FastRPC calls to offload workload on the Hexagon DSP. Select the following options Name your project calculator Select Java as the language Default toolchain (on the next configuration page) Customize the project compilation tools This section explains how to configure your Android Studio project to use external compilation tools provided with the Hexagon SDK instead of using the Android Studio SDK manager to download and bundle these tools as part of Android Studio. Android NDK Under File --> Project Structure --> SDK Location, set the Android NDK location to where the Android NDK is installed with the Hexagon SDK. For example, assuming the Hexagon SDK is installed under C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0 : Android NDK location: C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0\\tools\\android-ndk-r19c cmake Edit the local.properties file of the calculator project to set cmake.dir to the path where the cmake version installed with the Hexagon SDK is installed. Make sure to precede each special character : and \\ with \\ . For example, using the same assumption as above for the Hexagon SDK install location: cmake.dir=C\\:\\\\Qualcomm\\\\Hexagon_SDK\\\\4.4.0.0\\\\tools\\\\utils\\\\cmake-3.17.0-win64-x64 Note: The Android SDK relies on the build system Ninja for the CMake backend. In order to use the external version of CMake, you need to download , put the ninja.exe executable on your system path, and then restart Android Studio. When creating a new project, Android Studio sets the cmake version to a default value in the build.gradle file of the app module: externalNativeBuild { cmake { path \"src/main/cpp/CMakeLists.txt\" version \"3.10.2\" } } This version may differ from the one provided in the Hexagon SDK, which will cause a failure. Remove the cmake version number specified as follows: externalNativeBuild { cmake { path \"src/main/cpp/CMakeLists.txt\" } } Customize the project source files The calculator_c++_apk example folder contains all the source files you need to customize the native C++ project you just created following the instructions above . These source files demonstrate how to offload a workload onto the DSP. We recommend that you compare side by side the files present in your newly created Android Studio calculator project with those included under the app folder of the Hexagon SDK calculator_c++_apk example to understand how to modify the project. Alternatively, you may add all the provided source files to your project with a simple copy command as follows: cp -fr app\\src\\main\\* C:\\Users\\<your_user_name>\\AndroidStudioProjects\\calculator\\app\\src\\main\\ Note: We do not guarantee that this approach will always work as Android Studio evolves rapidly and modifying files when Android Studio synchronizes a project may generate some errors. This command adds or replaces the following source files to your project: File name Description AndroidManifest.xml General information about the application to the Android build tools, the Android operating system, and Google Play. The modified manifest file adds a declaration of DisplayResults.java as an activity. MainActivity.java Functionality of the main screen when the application opens DisplayResults.java Functionality of the screen used to display the results of the calculator activity_main.xml Layout for the main screen of the application activity_display_result.xml Layout for the screen used to display the results of the calculator strings.xml String values to be displayed on screen calculator-jni.cpp JNI file contains the function implemented to make a remote call calculator_sum to the DSP CMakeLists.txt CMake configuration file containing the rules to compile the IDL and JNI cpp source files, and link the required Hexagon SDK libraries needed to offload a workload onto the DSP Customize the project build.gradle file The build.gradle of the app module file maintained in the calculator project configures the build for the calculator project. Two changes are needed: Add a cmake argument to externalNativeBuild to dynamically link libc++_shared.so to the application: externalNativeBuild { cmake { cppFlags \"\" arguments \"-DANDROID_STL=c++_shared\" } } Specify the ABI filter for the NDK Since different Android handsets use different CPUs, it is required to specify a compatible Android Binary Interface(ABI) for each CPU architecture an application works with. Below we are specifying a default variant as 64-bit arm V8a instruction set. ndk { moduleName \"calculator\" abiFilter \"arm64-v8a\" ldLibs \"log\" } The build.gradle provided under the app folder illustrates how to set both of these properties. Testing the customized APK After following all the customization steps above and building the project in Android Studio, you should obtain a new APK file under the app\\build\\outputs\\apk\\debug folder of your Android Studio project. Install this new APK on your device as explained earlier to confirm that your calculator application still works as expected. Debugging Gradle sync issues Please note that if you are facing synchronization issues with the Gradle build system on the Android SDK, use the documentation provided in the link to update your IDE and Android SDK tools. Debugging on-target errors Analyze the application-processor and DSP logs to debug on-target functionality of the application. For more information on debugging and logging, please refer to debug and messaging documentation, respectively. libcdsprpc.so packaged in APK If you are seeing issues when running the APK on the device because APK is picking the stubbed version of rpcmem APIs, you should check if the compiled APK is packaging the stubbed version of remote library libcdsprpc.so into the APK. The following logs in the logcat would mean that the stubbed library is packaged into the APK. remote.c:110:rpcmem alloc stubbed routine - Return failure remote.c:100:Invoking stubbed routine - Return failure You can view the APK in Android Studio or simply unzip the APK in Windows explorer to find out if the library libcdsprpc.so is packaged. In Android Studio, an APK looks like this. Ideally when the library is not packaged, the APK looks like this. To fix this issue, you can add the following packaging rule in build.gradle of the app module. This prevents the inclusion of the stubbed library into the APK. android { packagingOptions { exclude \"lib/arm64-v8a/libcdsprpc.so\" } }","title":"Calculator C++ APK"},{"location":"examples/calculator_c%2B%2B_apk/index.html#calculator-c-apk-example","text":"This example illustrates how to build an end-to-end Android APK that uses C++ APIs in the Android application layer and offloads computation to the DSP. Starting with Android-P, access to /system libraries from the /vendor partition and vice-versa are prohibited with the addition of Treble restrictions. Hence, vendor applications do not have access to the libc++_shared.so library located in /system . However APKs have access to the libray located in system . Thus using C++ in an Android vendor application requires constructing an APK that dynamically links to the C++ libraries. APKs are untrusted applications. Starting with Android-R untrusted applications cannot create signed PDs on the DSPs for offloading. They can only create unsigned PDs on the compute DSP for offloading. Hence this example uses unsigned PD.","title":"Calculator C++ APK example"},{"location":"examples/calculator_c%2B%2B_apk/index.html#on-target-testing","text":"This section describes installation and use of the existing pre-compiled APK calculator_c++.apk on the device. To run the APK on device, you will need to install the APK and push the Hexagon shared object onto your device. This APK can run directly on all LA devices supported in the Hexagon SDK.","title":"On-target testing"},{"location":"examples/calculator_c%2B%2B_apk/index.html#install-the-apk","text":"The example comes with a pre-existing APK that can be installed on device directly. For instructions on how to modify the implementation and rebuild the APK, see how to customize the application . To install the existing APK on device, execute the following command: adb install calculator_c++.apk","title":"Install the APK"},{"location":"examples/calculator_c%2B%2B_apk/index.html#generate-and-push-the-dsp-shared-object","text":"The DSP source code of the calculator C++ APK example is under $HEXAGON_SDK_ROOT/examples/calculator_c++_apk/dsp . You can build the DSP shared object with the following instructions: cd dsp make hexagon BUILD=Debug DSP_ARCH=v65 Refer to the build instructions of the calculator example for more details on building shared objects. When the shared object is ready, push it to /vendor/lib/rfsa/dsp on your Android device. For example: adb root adb remount adb push hexagon_Debug_toolv84_v65\\ship\\libcalculator_skel.so /vendor/lib/rfsa/dsp","title":"Generate and push the DSP shared object"},{"location":"examples/calculator_c%2B%2B_apk/index.html#application-customization","text":"The steps below explain how to recreate the APK introduced in the previous section . We first create a new Android Studio project that uses Java as the Android app development language and the Java Native Interface (JNI) for enabling native C/C++ support and thus interfacing the DSP. We then customize both the project setup and the source code to build the customized APK with the tools used with the Hexagon SDK.","title":"Application customization"},{"location":"examples/calculator_c%2B%2B_apk/index.html#create-a-new-android-studio-project","text":"Use this link to download and install the Android SDK. If you are new to Android Studio, familiarize yourself with existing online tutorials to learn how to create, build, and run projects Create a new Android Native C++ project. Native development enables the Java application to interface C methods, which can then make FastRPC calls to offload workload on the Hexagon DSP. Select the following options Name your project calculator Select Java as the language Default toolchain (on the next configuration page)","title":"Create a new Android Studio project"},{"location":"examples/calculator_c%2B%2B_apk/index.html#customize-the-project-compilation-tools","text":"This section explains how to configure your Android Studio project to use external compilation tools provided with the Hexagon SDK instead of using the Android Studio SDK manager to download and bundle these tools as part of Android Studio.","title":"Customize the project compilation tools"},{"location":"examples/calculator_c%2B%2B_apk/index.html#android-ndk","text":"Under File --> Project Structure --> SDK Location, set the Android NDK location to where the Android NDK is installed with the Hexagon SDK. For example, assuming the Hexagon SDK is installed under C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0 : Android NDK location: C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0\\tools\\android-ndk-r19c","title":"Android NDK"},{"location":"examples/calculator_c%2B%2B_apk/index.html#cmake","text":"Edit the local.properties file of the calculator project to set cmake.dir to the path where the cmake version installed with the Hexagon SDK is installed. Make sure to precede each special character : and \\ with \\ . For example, using the same assumption as above for the Hexagon SDK install location: cmake.dir=C\\:\\\\Qualcomm\\\\Hexagon_SDK\\\\4.4.0.0\\\\tools\\\\utils\\\\cmake-3.17.0-win64-x64 Note: The Android SDK relies on the build system Ninja for the CMake backend. In order to use the external version of CMake, you need to download , put the ninja.exe executable on your system path, and then restart Android Studio. When creating a new project, Android Studio sets the cmake version to a default value in the build.gradle file of the app module: externalNativeBuild { cmake { path \"src/main/cpp/CMakeLists.txt\" version \"3.10.2\" } } This version may differ from the one provided in the Hexagon SDK, which will cause a failure. Remove the cmake version number specified as follows: externalNativeBuild { cmake { path \"src/main/cpp/CMakeLists.txt\" } }","title":"cmake"},{"location":"examples/calculator_c%2B%2B_apk/index.html#customize-the-project-source-files","text":"The calculator_c++_apk example folder contains all the source files you need to customize the native C++ project you just created following the instructions above . These source files demonstrate how to offload a workload onto the DSP. We recommend that you compare side by side the files present in your newly created Android Studio calculator project with those included under the app folder of the Hexagon SDK calculator_c++_apk example to understand how to modify the project. Alternatively, you may add all the provided source files to your project with a simple copy command as follows: cp -fr app\\src\\main\\* C:\\Users\\<your_user_name>\\AndroidStudioProjects\\calculator\\app\\src\\main\\ Note: We do not guarantee that this approach will always work as Android Studio evolves rapidly and modifying files when Android Studio synchronizes a project may generate some errors. This command adds or replaces the following source files to your project: File name Description AndroidManifest.xml General information about the application to the Android build tools, the Android operating system, and Google Play. The modified manifest file adds a declaration of DisplayResults.java as an activity. MainActivity.java Functionality of the main screen when the application opens DisplayResults.java Functionality of the screen used to display the results of the calculator activity_main.xml Layout for the main screen of the application activity_display_result.xml Layout for the screen used to display the results of the calculator strings.xml String values to be displayed on screen calculator-jni.cpp JNI file contains the function implemented to make a remote call calculator_sum to the DSP CMakeLists.txt CMake configuration file containing the rules to compile the IDL and JNI cpp source files, and link the required Hexagon SDK libraries needed to offload a workload onto the DSP","title":"Customize the project source files"},{"location":"examples/calculator_c%2B%2B_apk/index.html#customize-the-project-buildgradle-file","text":"The build.gradle of the app module file maintained in the calculator project configures the build for the calculator project. Two changes are needed: Add a cmake argument to externalNativeBuild to dynamically link libc++_shared.so to the application: externalNativeBuild { cmake { cppFlags \"\" arguments \"-DANDROID_STL=c++_shared\" } } Specify the ABI filter for the NDK Since different Android handsets use different CPUs, it is required to specify a compatible Android Binary Interface(ABI) for each CPU architecture an application works with. Below we are specifying a default variant as 64-bit arm V8a instruction set. ndk { moduleName \"calculator\" abiFilter \"arm64-v8a\" ldLibs \"log\" } The build.gradle provided under the app folder illustrates how to set both of these properties.","title":"Customize the project build.gradle file"},{"location":"examples/calculator_c%2B%2B_apk/index.html#testing-the-customized-apk","text":"After following all the customization steps above and building the project in Android Studio, you should obtain a new APK file under the app\\build\\outputs\\apk\\debug folder of your Android Studio project. Install this new APK on your device as explained earlier to confirm that your calculator application still works as expected.","title":"Testing the customized APK"},{"location":"examples/calculator_c%2B%2B_apk/index.html#debugging","text":"","title":"Debugging"},{"location":"examples/calculator_c%2B%2B_apk/index.html#gradle-sync-issues","text":"Please note that if you are facing synchronization issues with the Gradle build system on the Android SDK, use the documentation provided in the link to update your IDE and Android SDK tools.","title":"Gradle sync issues"},{"location":"examples/calculator_c%2B%2B_apk/index.html#debugging-on-target-errors","text":"Analyze the application-processor and DSP logs to debug on-target functionality of the application. For more information on debugging and logging, please refer to debug and messaging documentation, respectively.","title":"Debugging on-target errors"},{"location":"examples/calculator_c%2B%2B_apk/index.html#libcdsprpcso-packaged-in-apk","text":"If you are seeing issues when running the APK on the device because APK is picking the stubbed version of rpcmem APIs, you should check if the compiled APK is packaging the stubbed version of remote library libcdsprpc.so into the APK. The following logs in the logcat would mean that the stubbed library is packaged into the APK. remote.c:110:rpcmem alloc stubbed routine - Return failure remote.c:100:Invoking stubbed routine - Return failure You can view the APK in Android Studio or simply unzip the APK in Windows explorer to find out if the library libcdsprpc.so is packaged. In Android Studio, an APK looks like this. Ideally when the library is not packaged, the APK looks like this. To fix this issue, you can add the following packaging rule in build.gradle of the app module. This prevents the inclusion of the stubbed library into the APK. android { packagingOptions { exclude \"lib/arm64-v8a/libcdsprpc.so\" } }","title":"libcdsprpc.so packaged in APK"},{"location":"examples/dspqueue/index.html","text":"Asynchronous DSP packet queue example Overview The dspqueue example illustrates using the Asynchronous DSP packet queue for communication between the host CPU and the cDSP. The API and the example are supported on Android target only, for Lahaina and later products. See the feature matrix for details on feature support. Building and running the example The dspqueue example includes a walkthrough script called dspqueue_walkthrough.py . Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts. Walkthrough script automates building and the example as discussed in this section. Without the walkthrough script, the example can be built like other SDK examples, using make commands such as these: make android BUILD=Debug make hexagon BUILD=Debug DSP_ARCH=v68 For more information on the build syntax, please refer to the building reference instructions . The packet queue API is only supported on target, so the example cannot be run on the simulator. To run the example on a target, push the generated binaries to the device: adb push android_Debug_aarch64/ship/dspqueue_sample /vendor/bin/ adb shell chmod 777 /vendor/bin/dspqueue_sample adb push hexagon_Debug_toolv84_v68/ship/libdspqueue_sample_skel.so /vendor/lib/rfsa/adsp/ After this simply execute dspqueue_sample on the target: adb shell dspqueue_sample Note that by default the example runs in an unsigned PD . Example application details The example application contains two separate test scenarios. The following sections discuss these and any common elements in detail. Echo Test The echo test illustrates a basic dspqueue scenario: A host CPU application sends requests to the DSP, which responds back by echoing the same message. The test is driven by echo_test() in src/dspqueue_sample.c . This section discusses the key steps in detail. dspqueue_create ( DSPQUEUE_CDSP , 0 , // Flags 256 , // Request queue size 256 , // Response queue size packet_callback , error_callback_fatal , ( void * ) c , // Callback context & queue ); This call creates a new queue. This example uses a relatively small queue (256 bytes for both requests and responses); for use cases with larger messages a larger queue can yield better performance. The two callback functions are used for handling responses from the DSP ( packet_callback() ) and error handling ( error_callback_fatal() ). In this example the error callback simply terminates the process with an error. dspqueue_export ( queue , & dsp_queue_id ); dspqueue_sample_start ( sample_handle , dsp_queue_id ); These calls export the queue for use on the DSP, and pass the queue ID to the DSP side of the test application. The DSP side is implemented in src/dspqueue_sample_imp.c , with the IDL interface defined in inc/dspqueue_sample.idl as is typical for Hexagon SDK applications. The example uses a regular synchronous FastRPC call to pass the queue ID to the DSP and start the use case - this is common for packet queue applications. The DSP side implementation uses the queue ID to create a local handle to the same queue: dspqueue_import ( dsp_queue_id , // Queue ID from dspqueue_export sample_packet_callback , // Packet callback sample_error_callback , // Error callback; no errors expected on the DSP ( void * ) c , // Callback context & c -> queue ); The DSP implementation has its own set of callback functions. All requests are processed in the packet callback ( sample_packet_fallback() ). The current implementation does not use a DSP-side error callback, but it is included for completeness. After the queue has been created and connected, the host CPU sends a sequence of requests to the DSP: dspqueue_write ( queue , 0 , // Flags 0 , NULL , // No buffer references in this packet 2 * sizeof ( uint32_t ), ( const uint8_t * ) msg , // Message 1000000 ); // Timeout The packets consist of two 32-bit words, with no buffer references. The call uses a one-second timeout to catch error situations where the DSP may have stopped responding; alternatively clients can use DSPQUEUE_TIMEOUT_NONE . On the DSP side incoming request packets are handled in the packet callback. The callback function structure is typical of most dspqueue clients: while ( 1 ) { //... err = dspqueue_read_noblock ( queue , & flags , SAMPLE_MAX_PACKET_BUFFERS , // Maximum number of buffer references & num_bufs , // Number of buffer references bufs , // Buffer references sizeof ( msg ), // Max message length & msg_length , // Message length ( uint8_t * ) msg ); // Message if ( err == AEE_EWOULDBLOCK ) { return ; } switch ( msg [ 0 ] ) { case SAMPLE_MSG_ECHO : resp_msg [ 0 ] = SAMPLE_MSG_ECHO_RESP ; resp_msg [ 1 ] = msg [ 1 ]; dspqueue_write ( queue , 0 , // Flags 0 , NULL , // No buffers 8 , ( const uint8_t * ) resp_msg , // Message DSPQUEUE_TIMEOUT_NONE ); break ; //... } } The callback repeatedly reads packets from the queue until it is empty ( AEE_EWOULDBLOCK ). This ensures all packets are consumed - the client does not necessarily get a separate callback for each packet, and multiple new packets can arrive while a previous callback is being handled. In this example each received packet is handled directly in the callback function; for the echo test the DSP simply sends the same payload word back to the CPU. Real-world clients would likely start processing in background worker threads, and eventually send a response back to the CPU once all work is complete. The host CPU receives response packets in a packet callback function virtually identical to the DSP one discussed above. Finally at the end of the test the queue is closed first on the DSP followed by the host CPU: dspqueue_sample_stop ( sample_handle ); dspqueue_close ( queue ); Data Processing Test The second test in the dspqueue example illustrates using a packet queue to send data processing requests to the DSP. Its structure is very similar to the echo test , so this section only discusses the differences between the two. The test uses a set of shared memory buffers to send input data and receive output from the DSP. The buffers are allocated and mapped to the DSP before processing can start: buffers [ i ] = rpcmem_alloc ( RPCMEM_HEAP_ID_SYSTEM , RPCMEM_DEFAULT_FLAGS , BUFFER_SIZE ); fds [ i ] = rpcmem_to_fd ( buffers [ i ]); fastrpc_mmap ( CDSP_DOMAIN_ID , fds [ i ], buffers [ i ], 0 , BUFFER_SIZE , FASTRPC_MAP_FD ); rpcmem_alloc() allocates a shareable ION buffer using the RPCMEM library , rpcmem_to_fd() retrieves its corresponding File Descriptor number, and finally fastrpc_mmap() maps it to the DSP. In all calls BUFFER_SIZE is the size of the buffer in bytes. Queue creation is similar to the echo test, except the processing test uses a larger queue to account for larger packets with buffer references: dspqueue_create ( DSPQUEUE_CDSP , 0 , // Flags 4096 , // Request queue size 4096 , // Response queue size packet_callback , error_callback_fatal , ( void * ) c , // Callback context & queue ); Each request packet sent to the DSP includes two buffer references: One for input data, and one to hold output from the DSP: struct dspqueue_buffer bufs [ 2 ]; memset ( bufs , 0 , sizeof ( bufs )); bufs [ 0 ]. fd = fds [ input_buf ]; bufs [ 0 ]. flags = ( DSPQUEUE_BUFFER_FLAG_REF | // Take a reference DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER | // Flush CPU DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT ); // Invalidate DSP bufs [ 1 ]. fd = fds [ output_buf ]; bufs [ 1 ]. flags = ( DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER ); dspqueue_write ( queue , 0 , // Flags - the framework will update this 2 , bufs , // Buffer references sizeof ( msg ), ( const uint8_t * ) msg , // Message 1000000 ); // Timeout On the host CPU side the example requests the framework to take references to both buffers, flush the buffers on the CPU side to ensure all input data is visible to other devices in the system and no dirty data remains in the caches for the output buffer, and invalidate it on the DSP to ensure it sees an up to date version in memory. The DSP will trigger cache maintenance operations on the output buffer as a part of its response packet. On the DSP side the packet callback function ( sample_packet_callback() ) retrieves buffer information from the incoming packet, runs the processing algorithm, and constructs a response: case SAMPLE_MSG_BYTE_SQUARE : len = bufs [ 0 ]. size ; byte_square ( bufs [ 0 ]. ptr , bufs [ 1 ]. ptr , len ); resp_msg [ 0 ] = SAMPLE_MSG_BYTE_SQUARE_RESP ; memset ( resp_bufs , 0 , sizeof ( resp_bufs )); resp_bufs [ 0 ]. fd = bufs [ 0 ]. fd ; resp_bufs [ 0 ]. flags = DSPQUEUE_BUFFER_FLAG_DEREF ; // Release reference // (If we had written to the input buffer, we'd also need to flush it) resp_bufs [ 1 ]. fd = bufs [ 1 ]. fd ; resp_bufs [ 1 ]. flags = ( DSPQUEUE_BUFFER_FLAG_DEREF | // Release reference DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER | // Flush DSP DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT ); // Invalidate CPU dspqueue_write ( queue , 0 , // Flags 2 , resp_bufs , // Buffer references 4 , ( const uint8_t * ) resp_msg , // Message DSPQUEUE_TIMEOUT_NONE ); Note that the response packet also includes a reference to the input buffer to release the reference taken in the request packet. The response also triggers cache maintenance operations for the output buffer to ensure the results are visible to the host CPU. At the end of the test the processing test unmaps and frees buffers. Finally the processing test also illustrates how to use early wakeup packets to reduce latency between the DSP and host CPU. See Early Wakeup for more discussion on how and when to use early wakeup packets. Performance Measurements The dspqueue example application performs some performance measurements to illustrate when using the packet queue can be more efficient than regular synchronous FastRPC calls. For each scenario the application measures and prints three sets of numbers for each configuration: Total elapsed time DSP processing time: The time taken on the DSP to run the processing algorithm Overhead: The difference between DSP processing time and total elapsed time, divided by the number of operations. This accounts for inter-processor communication, cache maintenance operations, and other overhead from the framework. The exact values seen will vary on depending on the target device used for testing, but generally the following results should be visible: For individual operations without early wakeup the packet queue is not more efficient than synchronous FastRPC calls Early wakeup can significantly reduce the latency for single-shot operations The packet queue can have significantly lower overhead than synchronous FastRPC calls when the application can queue a larger number of requests. Additionally, the example also illustrates using persistently mapped buffers to reduce overhead for regular synchronous FastRPC calls.","title":"DSP Packet Queue"},{"location":"examples/dspqueue/index.html#asynchronous-dsp-packet-queue-example","text":"","title":"Asynchronous DSP packet queue example"},{"location":"examples/dspqueue/index.html#overview","text":"The dspqueue example illustrates using the Asynchronous DSP packet queue for communication between the host CPU and the cDSP. The API and the example are supported on Android target only, for Lahaina and later products. See the feature matrix for details on feature support.","title":"Overview"},{"location":"examples/dspqueue/index.html#building-and-running-the-example","text":"The dspqueue example includes a walkthrough script called dspqueue_walkthrough.py . Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts. Walkthrough script automates building and the example as discussed in this section. Without the walkthrough script, the example can be built like other SDK examples, using make commands such as these: make android BUILD=Debug make hexagon BUILD=Debug DSP_ARCH=v68 For more information on the build syntax, please refer to the building reference instructions . The packet queue API is only supported on target, so the example cannot be run on the simulator. To run the example on a target, push the generated binaries to the device: adb push android_Debug_aarch64/ship/dspqueue_sample /vendor/bin/ adb shell chmod 777 /vendor/bin/dspqueue_sample adb push hexagon_Debug_toolv84_v68/ship/libdspqueue_sample_skel.so /vendor/lib/rfsa/adsp/ After this simply execute dspqueue_sample on the target: adb shell dspqueue_sample Note that by default the example runs in an unsigned PD .","title":"Building and running the example"},{"location":"examples/dspqueue/index.html#example-application-details","text":"The example application contains two separate test scenarios. The following sections discuss these and any common elements in detail.","title":"Example application details"},{"location":"examples/dspqueue/index.html#example-echo","text":"The echo test illustrates a basic dspqueue scenario: A host CPU application sends requests to the DSP, which responds back by echoing the same message. The test is driven by echo_test() in src/dspqueue_sample.c . This section discusses the key steps in detail. dspqueue_create ( DSPQUEUE_CDSP , 0 , // Flags 256 , // Request queue size 256 , // Response queue size packet_callback , error_callback_fatal , ( void * ) c , // Callback context & queue ); This call creates a new queue. This example uses a relatively small queue (256 bytes for both requests and responses); for use cases with larger messages a larger queue can yield better performance. The two callback functions are used for handling responses from the DSP ( packet_callback() ) and error handling ( error_callback_fatal() ). In this example the error callback simply terminates the process with an error. dspqueue_export ( queue , & dsp_queue_id ); dspqueue_sample_start ( sample_handle , dsp_queue_id ); These calls export the queue for use on the DSP, and pass the queue ID to the DSP side of the test application. The DSP side is implemented in src/dspqueue_sample_imp.c , with the IDL interface defined in inc/dspqueue_sample.idl as is typical for Hexagon SDK applications. The example uses a regular synchronous FastRPC call to pass the queue ID to the DSP and start the use case - this is common for packet queue applications. The DSP side implementation uses the queue ID to create a local handle to the same queue: dspqueue_import ( dsp_queue_id , // Queue ID from dspqueue_export sample_packet_callback , // Packet callback sample_error_callback , // Error callback; no errors expected on the DSP ( void * ) c , // Callback context & c -> queue ); The DSP implementation has its own set of callback functions. All requests are processed in the packet callback ( sample_packet_fallback() ). The current implementation does not use a DSP-side error callback, but it is included for completeness. After the queue has been created and connected, the host CPU sends a sequence of requests to the DSP: dspqueue_write ( queue , 0 , // Flags 0 , NULL , // No buffer references in this packet 2 * sizeof ( uint32_t ), ( const uint8_t * ) msg , // Message 1000000 ); // Timeout The packets consist of two 32-bit words, with no buffer references. The call uses a one-second timeout to catch error situations where the DSP may have stopped responding; alternatively clients can use DSPQUEUE_TIMEOUT_NONE . On the DSP side incoming request packets are handled in the packet callback. The callback function structure is typical of most dspqueue clients: while ( 1 ) { //... err = dspqueue_read_noblock ( queue , & flags , SAMPLE_MAX_PACKET_BUFFERS , // Maximum number of buffer references & num_bufs , // Number of buffer references bufs , // Buffer references sizeof ( msg ), // Max message length & msg_length , // Message length ( uint8_t * ) msg ); // Message if ( err == AEE_EWOULDBLOCK ) { return ; } switch ( msg [ 0 ] ) { case SAMPLE_MSG_ECHO : resp_msg [ 0 ] = SAMPLE_MSG_ECHO_RESP ; resp_msg [ 1 ] = msg [ 1 ]; dspqueue_write ( queue , 0 , // Flags 0 , NULL , // No buffers 8 , ( const uint8_t * ) resp_msg , // Message DSPQUEUE_TIMEOUT_NONE ); break ; //... } } The callback repeatedly reads packets from the queue until it is empty ( AEE_EWOULDBLOCK ). This ensures all packets are consumed - the client does not necessarily get a separate callback for each packet, and multiple new packets can arrive while a previous callback is being handled. In this example each received packet is handled directly in the callback function; for the echo test the DSP simply sends the same payload word back to the CPU. Real-world clients would likely start processing in background worker threads, and eventually send a response back to the CPU once all work is complete. The host CPU receives response packets in a packet callback function virtually identical to the DSP one discussed above. Finally at the end of the test the queue is closed first on the DSP followed by the host CPU: dspqueue_sample_stop ( sample_handle ); dspqueue_close ( queue );","title":"Echo Test"},{"location":"examples/dspqueue/index.html#example-processing","text":"The second test in the dspqueue example illustrates using a packet queue to send data processing requests to the DSP. Its structure is very similar to the echo test , so this section only discusses the differences between the two. The test uses a set of shared memory buffers to send input data and receive output from the DSP. The buffers are allocated and mapped to the DSP before processing can start: buffers [ i ] = rpcmem_alloc ( RPCMEM_HEAP_ID_SYSTEM , RPCMEM_DEFAULT_FLAGS , BUFFER_SIZE ); fds [ i ] = rpcmem_to_fd ( buffers [ i ]); fastrpc_mmap ( CDSP_DOMAIN_ID , fds [ i ], buffers [ i ], 0 , BUFFER_SIZE , FASTRPC_MAP_FD ); rpcmem_alloc() allocates a shareable ION buffer using the RPCMEM library , rpcmem_to_fd() retrieves its corresponding File Descriptor number, and finally fastrpc_mmap() maps it to the DSP. In all calls BUFFER_SIZE is the size of the buffer in bytes. Queue creation is similar to the echo test, except the processing test uses a larger queue to account for larger packets with buffer references: dspqueue_create ( DSPQUEUE_CDSP , 0 , // Flags 4096 , // Request queue size 4096 , // Response queue size packet_callback , error_callback_fatal , ( void * ) c , // Callback context & queue ); Each request packet sent to the DSP includes two buffer references: One for input data, and one to hold output from the DSP: struct dspqueue_buffer bufs [ 2 ]; memset ( bufs , 0 , sizeof ( bufs )); bufs [ 0 ]. fd = fds [ input_buf ]; bufs [ 0 ]. flags = ( DSPQUEUE_BUFFER_FLAG_REF | // Take a reference DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER | // Flush CPU DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT ); // Invalidate DSP bufs [ 1 ]. fd = fds [ output_buf ]; bufs [ 1 ]. flags = ( DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER ); dspqueue_write ( queue , 0 , // Flags - the framework will update this 2 , bufs , // Buffer references sizeof ( msg ), ( const uint8_t * ) msg , // Message 1000000 ); // Timeout On the host CPU side the example requests the framework to take references to both buffers, flush the buffers on the CPU side to ensure all input data is visible to other devices in the system and no dirty data remains in the caches for the output buffer, and invalidate it on the DSP to ensure it sees an up to date version in memory. The DSP will trigger cache maintenance operations on the output buffer as a part of its response packet. On the DSP side the packet callback function ( sample_packet_callback() ) retrieves buffer information from the incoming packet, runs the processing algorithm, and constructs a response: case SAMPLE_MSG_BYTE_SQUARE : len = bufs [ 0 ]. size ; byte_square ( bufs [ 0 ]. ptr , bufs [ 1 ]. ptr , len ); resp_msg [ 0 ] = SAMPLE_MSG_BYTE_SQUARE_RESP ; memset ( resp_bufs , 0 , sizeof ( resp_bufs )); resp_bufs [ 0 ]. fd = bufs [ 0 ]. fd ; resp_bufs [ 0 ]. flags = DSPQUEUE_BUFFER_FLAG_DEREF ; // Release reference // (If we had written to the input buffer, we'd also need to flush it) resp_bufs [ 1 ]. fd = bufs [ 1 ]. fd ; resp_bufs [ 1 ]. flags = ( DSPQUEUE_BUFFER_FLAG_DEREF | // Release reference DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER | // Flush DSP DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT ); // Invalidate CPU dspqueue_write ( queue , 0 , // Flags 2 , resp_bufs , // Buffer references 4 , ( const uint8_t * ) resp_msg , // Message DSPQUEUE_TIMEOUT_NONE ); Note that the response packet also includes a reference to the input buffer to release the reference taken in the request packet. The response also triggers cache maintenance operations for the output buffer to ensure the results are visible to the host CPU. At the end of the test the processing test unmaps and frees buffers. Finally the processing test also illustrates how to use early wakeup packets to reduce latency between the DSP and host CPU. See Early Wakeup for more discussion on how and when to use early wakeup packets.","title":"Data Processing Test"},{"location":"examples/dspqueue/index.html#performance-measurements","text":"The dspqueue example application performs some performance measurements to illustrate when using the packet queue can be more efficient than regular synchronous FastRPC calls. For each scenario the application measures and prints three sets of numbers for each configuration: Total elapsed time DSP processing time: The time taken on the DSP to run the processing algorithm Overhead: The difference between DSP processing time and total elapsed time, divided by the number of operations. This accounts for inter-processor communication, cache maintenance operations, and other overhead from the framework. The exact values seen will vary on depending on the target device used for testing, but generally the following results should be visible: For individual operations without early wakeup the packet queue is not more efficient than synchronous FastRPC calls Early wakeup can significantly reduce the latency for single-shot operations The packet queue can have significantly lower overhead than synchronous FastRPC calls when the application can queue a larger number of requests. Additionally, the example also illustrates using persistently mapped buffers to reduce overhead for regular synchronous FastRPC calls.","title":"Performance Measurements"},{"location":"examples/gtest/index.html","text":"Googletest Mocking (gMock) Framework Overview Google's framework for writing and using C++ mock classes. It can help you derive better designs of your system and write better tests. It is inspired by: jMock , EasyMock , and Hamcrest , and designed with C++'s specifics in mind. gMock: provides a declarative syntax for defining mocks, can define partial (hybrid) mocks, which are a cross of real and mock objects, handles functions of arbitrary types and overloaded functions, comes with a rich set of matchers for validating function arguments, uses an intuitive syntax for controlling the behavior of a mock, does automatic verification of expectations (no record-and-replay needed), allows arbitrary (partial) ordering constraints on function calls to be expressed, lets a user extend it by defining new matchers and actions. does not use exceptions, and is easy to learn and use. Details and examples can be found here: gMock for Dummies Legacy gMock FAQ gMock Cookbook gMock Cheat Sheet Please note that code under scripts/generator/ is from the cppclean project and under the Apache License, which is different from Google Mock's license. Google Mock is a part of Google Test C++ testing framework and a subject to the same requirements. More information about building GoogleTest can be found at $HEXAGON_SDK_ROOT/utils/googletest/gtest/README.md","title":"GTEST"},{"location":"examples/gtest/index.html#googletest-mocking-gmock-framework","text":"","title":"Googletest Mocking (gMock) Framework"},{"location":"examples/gtest/index.html#overview","text":"Google's framework for writing and using C++ mock classes. It can help you derive better designs of your system and write better tests. It is inspired by: jMock , EasyMock , and Hamcrest , and designed with C++'s specifics in mind. gMock: provides a declarative syntax for defining mocks, can define partial (hybrid) mocks, which are a cross of real and mock objects, handles functions of arbitrary types and overloaded functions, comes with a rich set of matchers for validating function arguments, uses an intuitive syntax for controlling the behavior of a mock, does automatic verification of expectations (no record-and-replay needed), allows arbitrary (partial) ordering constraints on function calls to be expressed, lets a user extend it by defining new matchers and actions. does not use exceptions, and is easy to learn and use. Details and examples can be found here: gMock for Dummies Legacy gMock FAQ gMock Cookbook gMock Cheat Sheet Please note that code under scripts/generator/ is from the cppclean project and under the Apache License, which is different from Google Mock's license. Google Mock is a part of Google Test C++ testing framework and a subject to the same requirements. More information about building GoogleTest can be found at $HEXAGON_SDK_ROOT/utils/googletest/gtest/README.md","title":"Overview"},{"location":"examples/gtest/docs/Pkgconfig.html","text":"Using GoogleTest from various build systems GoogleTest comes with pkg-config files that can be used to determine all necessary flags for compiling and linking to GoogleTest (and GoogleMock). Pkg-config is a standardised plain-text format containing the includedir (-I) path necessary macro (-D) definitions further required flags (-pthread) the library (-L) path the library (-l) to link to All current build systems support pkg-config in one way or another. For all examples here we assume you want to compile the sample samples/sample3_unittest.cc . CMake Using pkg-config in CMake is fairly easy: cmake_minimum_required ( VERSION 3.0 ) cmake_policy ( SET CMP0048 NEW ) project ( my_gtest_pkgconfig VERSION 0.0.1 LANGUAGES CXX ) find_package ( PkgConfig ) pkg_search_module ( GTEST REQUIRED gtest_main ) add_executable ( testapp samples/sample3_unittest.cc ) target_link_libraries ( testapp ${ GTEST_LDFLAGS } ) target_compile_options ( testapp PUBLIC ${ GTEST_CFLAGS } ) include ( CTest ) add_test ( first_and_only_test testapp ) It is generally recommended that you use target_compile_options + _CFLAGS over target_include_directories + _INCLUDE_DIRS as the former includes not just -I flags (GoogleTest might require a macro indicating to internal headers that all libraries have been compiled with threading enabled. In addition, GoogleTest might also require -pthread in the compiling step, and as such splitting the pkg-config Cflags variable into include dirs and macros for target_compile_definitions() might still miss this). The same recommendation goes for using _LDFLAGS over the more commonplace _LIBRARIES , which happens to discard -L flags and -pthread . Autotools Finding GoogleTest in Autoconf and using it from Automake is also fairly easy: In your configure.ac : AC_PREREQ([2.69]) AC_INIT([my_gtest_pkgconfig], [0.0.1]) AC_CONFIG_SRCDIR([samples/sample3_unittest.cc]) AC_PROG_CXX PKG_CHECK_MODULES([GTEST], [gtest_main]) AM_INIT_AUTOMAKE([foreign subdir-objects]) AC_CONFIG_FILES([Makefile]) AC_OUTPUT and in your Makefile.am : check_PROGRAMS = testapp TESTS = $(check_PROGRAMS) testapp_SOURCES = samples/sample3_unittest.cc testapp_CXXFLAGS = $(GTEST_CFLAGS) testapp_LDADD = $(GTEST_LIBS) Meson Meson natively uses pkgconfig to query dependencies: project('my_gtest_pkgconfig', 'cpp', version : '0.0.1') gtest_dep = dependency('gtest_main') testapp = executable( 'testapp', files(['samples/sample3_unittest.cc']), dependencies : gtest_dep, install : false) test('first_and_only_test', testapp) Plain Makefiles Since pkg-config is a small Unix command-line utility, it can be used in handwritten Makefile s too: GTEST_CFLAGS = ` pkg-config --cflags gtest_main ` GTEST_LIBS = ` pkg-config --libs gtest_main ` .PHONY : tests all tests : all ./testapp all : testapp testapp : testapp . o $( CXX ) $( CXXFLAGS ) $( LDFLAGS ) $< -o $@ $( GTEST_LIBS ) testapp.o : samples / sample 3 _unittest . cc $( CXX ) $( CPPFLAGS ) $( CXXFLAGS ) $< -c -o $@ $( GTEST_CFLAGS ) Help! pkg-config can't find GoogleTest! Let's say you have a CMakeLists.txt along the lines of the one in this tutorial and you try to run cmake . It is very possible that you get a failure along the lines of: -- Checking for one of the modules 'gtest_main' CMake Error at /usr/share/cmake/Modules/FindPkgConfig.cmake:640 (message): None of the required 'gtest_main' found These failures are common if you installed GoogleTest yourself and have not sourced it from a distro or other package manager. If so, you need to tell pkg-config where it can find the .pc files containing the information. Say you installed GoogleTest to /usr/local , then it might be that the .pc files are installed under /usr/local/lib64/pkgconfig . If you set export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig pkg-config will also try to look in PKG_CONFIG_PATH to find gtest_main.pc .","title":"Pkgconfig"},{"location":"examples/gtest/docs/Pkgconfig.html#using-googletest-from-various-build-systems","text":"GoogleTest comes with pkg-config files that can be used to determine all necessary flags for compiling and linking to GoogleTest (and GoogleMock). Pkg-config is a standardised plain-text format containing the includedir (-I) path necessary macro (-D) definitions further required flags (-pthread) the library (-L) path the library (-l) to link to All current build systems support pkg-config in one way or another. For all examples here we assume you want to compile the sample samples/sample3_unittest.cc .","title":"Using GoogleTest from various build systems"},{"location":"examples/gtest/docs/Pkgconfig.html#cmake","text":"Using pkg-config in CMake is fairly easy: cmake_minimum_required ( VERSION 3.0 ) cmake_policy ( SET CMP0048 NEW ) project ( my_gtest_pkgconfig VERSION 0.0.1 LANGUAGES CXX ) find_package ( PkgConfig ) pkg_search_module ( GTEST REQUIRED gtest_main ) add_executable ( testapp samples/sample3_unittest.cc ) target_link_libraries ( testapp ${ GTEST_LDFLAGS } ) target_compile_options ( testapp PUBLIC ${ GTEST_CFLAGS } ) include ( CTest ) add_test ( first_and_only_test testapp ) It is generally recommended that you use target_compile_options + _CFLAGS over target_include_directories + _INCLUDE_DIRS as the former includes not just -I flags (GoogleTest might require a macro indicating to internal headers that all libraries have been compiled with threading enabled. In addition, GoogleTest might also require -pthread in the compiling step, and as such splitting the pkg-config Cflags variable into include dirs and macros for target_compile_definitions() might still miss this). The same recommendation goes for using _LDFLAGS over the more commonplace _LIBRARIES , which happens to discard -L flags and -pthread .","title":"CMake"},{"location":"examples/gtest/docs/Pkgconfig.html#autotools","text":"Finding GoogleTest in Autoconf and using it from Automake is also fairly easy: In your configure.ac : AC_PREREQ([2.69]) AC_INIT([my_gtest_pkgconfig], [0.0.1]) AC_CONFIG_SRCDIR([samples/sample3_unittest.cc]) AC_PROG_CXX PKG_CHECK_MODULES([GTEST], [gtest_main]) AM_INIT_AUTOMAKE([foreign subdir-objects]) AC_CONFIG_FILES([Makefile]) AC_OUTPUT and in your Makefile.am : check_PROGRAMS = testapp TESTS = $(check_PROGRAMS) testapp_SOURCES = samples/sample3_unittest.cc testapp_CXXFLAGS = $(GTEST_CFLAGS) testapp_LDADD = $(GTEST_LIBS)","title":"Autotools"},{"location":"examples/gtest/docs/Pkgconfig.html#meson","text":"Meson natively uses pkgconfig to query dependencies: project('my_gtest_pkgconfig', 'cpp', version : '0.0.1') gtest_dep = dependency('gtest_main') testapp = executable( 'testapp', files(['samples/sample3_unittest.cc']), dependencies : gtest_dep, install : false) test('first_and_only_test', testapp)","title":"Meson"},{"location":"examples/gtest/docs/Pkgconfig.html#plain-makefiles","text":"Since pkg-config is a small Unix command-line utility, it can be used in handwritten Makefile s too: GTEST_CFLAGS = ` pkg-config --cflags gtest_main ` GTEST_LIBS = ` pkg-config --libs gtest_main ` .PHONY : tests all tests : all ./testapp all : testapp testapp : testapp . o $( CXX ) $( CXXFLAGS ) $( LDFLAGS ) $< -o $@ $( GTEST_LIBS ) testapp.o : samples / sample 3 _unittest . cc $( CXX ) $( CPPFLAGS ) $( CXXFLAGS ) $< -c -o $@ $( GTEST_CFLAGS )","title":"Plain Makefiles"},{"location":"examples/gtest/docs/Pkgconfig.html#help-pkg-config-cant-find-googletest","text":"Let's say you have a CMakeLists.txt along the lines of the one in this tutorial and you try to run cmake . It is very possible that you get a failure along the lines of: -- Checking for one of the modules 'gtest_main' CMake Error at /usr/share/cmake/Modules/FindPkgConfig.cmake:640 (message): None of the required 'gtest_main' found These failures are common if you installed GoogleTest yourself and have not sourced it from a distro or other package manager. If so, you need to tell pkg-config where it can find the .pc files containing the information. Say you installed GoogleTest to /usr/local , then it might be that the .pc files are installed under /usr/local/lib64/pkgconfig . If you set export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig pkg-config will also try to look in PKG_CONFIG_PATH to find gtest_main.pc .","title":"Help! pkg-config can't find GoogleTest!"},{"location":"examples/gtest/docs/advanced.html","text":"Advanced googletest Topics Introduction Now that you have read the googletest Primer and learned how to write tests using googletest, it's time to learn some new tricks. This document will show you more assertions as well as how to construct complex failure messages, propagate fatal failures, reuse and speed up your test fixtures, and use various flags with your tests. More Assertions This section covers some less frequently used, but still significant, assertions. Explicit Success and Failure These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into them. SUCCEED (); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. NOTE: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to googletest's output in the future. FAIL (); ADD_FAILURE (); ADD_FAILURE_AT ( \"file_path\" , line_number ); FAIL() generates a fatal failure, while ADD_FAILURE() and ADD_FAILURE_AT() generate a nonfatal failure. These are useful when control flow, rather than a Boolean expression, determines the test's success or failure. For example, you might want to write something like: switch ( expression ) { case 1 : ... some checks ... case 2 : ... some other checks ... default : FAIL () << \"We shouldn't get here.\" ; } NOTE: you can only use FAIL() in functions that return void . See the Assertion Placement section for more information. Exception Assertions These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW(statement, exception_type); EXPECT_THROW(statement, exception_type); statement throws an exception of the given type ASSERT_ANY_THROW(statement); EXPECT_ANY_THROW(statement); statement throws an exception of any type ASSERT_NO_THROW(statement); EXPECT_NO_THROW(statement); statement doesn't throw any exception Examples: ASSERT_THROW ( Foo ( 5 ), bar_exception ); EXPECT_NO_THROW ({ int n = 5 ; Bar ( & n ); }); Availability : requires exceptions to be enabled in the build environment Predicate Assertions for Better Error Messages Even though googletest has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. googletest gives you three different options to solve this problem: Using an Existing Boolean Function If you already have a function or functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1(pred1, val1) EXPECT_PRED1(pred1, val1) pred1(val1) is true ASSERT_PRED2(pred2, val1, val2) EXPECT_PRED2(pred2, val1, val2) pred1(val1, val2) is true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true if m and n have no common divisors except 1. bool MutuallyPrime ( int m , int n ) { ... } const int a = 3 ; const int b = 4 ; const int c = 10 ; the assertion EXPECT_PRED2 ( MutuallyPrime , a , b ); will succeed, while the assertion EXPECT_PRED2 ( MutuallyPrime , b , c ); will fail with the message MutuallyPrime(b, c) is false, where b is 4 c is 10 NOTE: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this for how to resolve it. Using a Function That Returns an AssertionResult While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess (); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure (); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: :: testing :: AssertionResult IsEven ( int n ) { if (( n % 2 ) == 0 ) return :: testing :: AssertionSuccess (); else return :: testing :: AssertionFailure () << n << \" is odd\" ; } instead of: bool IsEven ( int n ) { return ( n % 2 ) == 0 ; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: IsEven(Fib(4)) Actual: false (3 is odd) Expected: true instead of a more opaque Value of: IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well (one third of Boolean assertions in the Google code base are negative ones), and are fine with making the predicate slower in the success case, you can supply a success message: :: testing :: AssertionResult IsEven ( int n ) { if (( n % 2 ) == 0 ) return :: testing :: AssertionSuccess () << n << \" is even\" ; else return :: testing :: AssertionFailure () << n << \" is odd\" ; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: IsEven(Fib(6)) Actual: true (8 is even) Expected: false Using a Predicate-Formatter If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1(pred_format1, val1); EXPECT_PRED_FORMAT1(pred_format1, val1); pred_format1(val1) is successful ASSERT_PRED_FORMAT2(pred_format2, val1, val2); EXPECT_PRED_FORMAT2(pred_format2, val1, val2); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous group of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: :: testing :: AssertionResult PredicateFormattern ( const char * expr1 , const char * expr2 , ... const char * exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. As an example, let's improve the failure message in MutuallyPrime() , which was used with EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor ( int m , int n ) { ... } // A predicate-formatter for asserting that two integers are mutually prime. :: testing :: AssertionResult AssertMutuallyPrime ( const char * m_expr , const char * n_expr , int m , int n ) { if ( MutuallyPrime ( m , n )) return :: testing :: AssertionSuccess (); return :: testing :: AssertionFailure () << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor ( m , n ); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2 ( AssertMutuallyPrime , b , c ); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the built-in assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* . Floating-Point Comparison Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and googletest provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see here . Floating-Point Macros Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ(val1, val2); EXPECT_FLOAT_EQ(val1, val2); the two float values are almost equal ASSERT_DOUBLE_EQ(val1, val2); EXPECT_DOUBLE_EQ(val1, val2); the two double values are almost equal By \"almost equal\" we mean the values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR(val1, val2, abs_error); EXPECT_NEAR(val1, val2, abs_error); the difference between val1 and val2 doesn't exceed the given absolute error Floating-Point Predicate-Format Functions Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2 ( :: testing :: FloatLE , val1 , val2 ); EXPECT_PRED_FORMAT2 ( :: testing :: DoubleLE , val1 , val2 ); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 . Asserting Using gMock Matchers gMock comes with a library of matchers for validating arguments passed to mock objects. A gMock matcher is basically a predicate that knows how to describe itself. It can be used in these assertion macros: Fatal assertion Nonfatal assertion Verifies ASSERT_THAT(value, matcher); EXPECT_THAT(value, matcher); value matches matcher For example, StartsWith(prefix) is a matcher that matches a string starting with prefix , and you can write: using :: testing :: StartsWith ; ... // Verifies that Foo() returns a string starting with \"Hello\". EXPECT_THAT ( Foo (), StartsWith ( \"Hello\" )); Read this recipe in the gMock Cookbook for more details. gMock has a rich set of matchers. You can do many things googletest cannot do alone with them. For a list of matchers gMock provides, read this . It's easy to write your own matchers too. gMock is bundled with googletest, so you don't need to add any build dependency in order to take advantage of this. Just include \"testing/base/public/gmock.h\" and you're ready to go. More String Assertions (Please read the previous section first if you haven't.) You can use the gMock string matchers with EXPECT_THAT() or ASSERT_THAT() to do more string comparison tricks (sub-string, prefix, suffix, regular expression, and etc). For example, using :: testing :: HasSubstr ; using :: testing :: MatchesRegex ; ... ASSERT_THAT ( foo_string , HasSubstr ( \"needle\" )); EXPECT_THAT ( bar_string , MatchesRegex ( \" \\\\ w* \\\\ d+\" )); If the string contains a well-formed HTML or XML document, you can check whether its DOM tree matches an XPath expression : // Currently still in //template/prototemplate/testing:xpath_matcher #include \"template/prototemplate/testing/xpath_matcher.h\" using prototemplate :: testing :: MatchesXPath ; EXPECT_THAT ( html_string , MatchesXPath ( \"//a[text()='click here']\" )); Windows HRESULT assertions These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED(expression) EXPECT_HRESULT_SUCCEEDED(expression) expression is a success HRESULT ASSERT_HRESULT_FAILED(expression) EXPECT_HRESULT_FAILED(expression) expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr < IShellDispatch2 > shell ; ASSERT_HRESULT_SUCCEEDED ( shell . CoCreateInstance ( L \"Shell.Application\" )); CComVariant empty ; ASSERT_HRESULT_SUCCEEDED ( shell -> ShellExecute ( CComBSTR ( url ), empty , empty , empty , empty )); Type Assertions You can call the function :: testing :: StaticAssertTypeEq < T1 , T2 > (); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, the compiler error message will say that type1 and type2 are not the same type and most likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat : When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template < typename T > class Foo { public : void Bar () { :: testing :: StaticAssertTypeEq < int , T > (); } }; the code: void Test1 () { Foo < bool > foo ; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2 () { Foo < bool > foo ; foo . Bar (); } to cause a compiler error. Assertion Placement You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google's not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" or \"cannot initialize return object of type 'bool' with an rvalue of type 'void'\" or \"error: no viable conversion from 'void' to 'string'\" . If you need to use fatal assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . NOTE: Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them; you'll get a compilation error if you try. Instead, either call abort and crash the entire test executable, or put the fatal assertion in a SetUp / TearDown function; see constructor/destructor vs. SetUp / TearDown WARNING: A fatal assertion in a helper function (private void-returning method) called from a constructor or destructor does not does not terminate the current test, as your intuition might suggest: it merely returns from the constructor or destructor early, possibly leaving your object in a partially-constructed or partially-destructed state! You almost certainly want to abort or use SetUp / TearDown instead. Teaching googletest How to Print Your Values When a test assertion such as EXPECT_EQ fails, googletest prints the argument values to help you debug. It does this using a user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. As mentioned earlier, the printer is extensible . That means you can teach it to do a better job at printing your particular type than to dump the bytes. To do that, define << for your type: #include <ostream> namespace foo { class Bar { // We want googletest to be able to print instances of this. ... // Create a free inline friend function. friend std :: ostream & operator << ( std :: ostream & os , const Bar & bar ) { return os << bar . DebugString (); // whatever needed to print bar to os } }; // If you can't declare the function in the class it's important that the // << operator is defined in the SAME namespace that defines Bar. C++'s look-up // rules rely on that. std :: ostream & operator << ( std :: ostream & os , const Bar & bar ) { return os << bar . DebugString (); // whatever needed to print bar to os } } // namespace foo Sometimes, this might not be an option: your team may consider it bad style to have a << operator for Bar , or Bar may already have a << operator that doesn't do what you want (and you cannot change it). If so, you can instead define a PrintTo() function like this: #include <ostream> namespace foo { class Bar { ... friend void PrintTo ( const Bar & bar , std :: ostream * os ) { * os << bar . DebugString (); // whatever needed to print bar to os } }; // If you can't declare the function in the class it's important that PrintTo() // is defined in the SAME namespace that defines Bar. C++'s look-up rules rely // on that. void PrintTo ( const Bar & bar , std :: ostream * os ) { * os << bar . DebugString (); // whatever needed to print bar to os } } // namespace foo If you have defined both << and PrintTo() , the latter will be used when googletest is concerned. This allows you to customize how the value appears in googletest's output without affecting code that relies on the behavior of its << operator. If you want to print a value x using googletest's value printer yourself, just call ::testing::PrintToString(x) , which returns an std::string : vector < pair < Bar , int > > bar_ints = GetBarIntVector (); EXPECT_TRUE ( IsCorrectBarIntVector ( bar_ints )) << \"bar_ints = \" << :: testing :: PrintToString ( bar_ints ); Death Tests In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates (except by throwing an exception) in an expected fashion is also a death test. Note that if a piece of code throws an exception, we don't consider it \"death\" for the purpose of death tests, as the caller of the code could catch the exception and avoid the crash. If you want to verify exceptions thrown by your code, see Exception Assertions . If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures How to Write a Death Test googletest has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH(statement, matcher); EXPECT_DEATH(statement, matcher); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED(statement, matcher); EXPECT_DEATH_IF_SUPPORTED(statement, matcher); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT(statement, predicate, matcher); EXPECT_EXIT(statement, predicate, matcher); statement exits with the given error, and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and matcher is either a GMock matcher matching a const std::string& or a (Perl) regular expression - either of which is matched against the stderr output of statement . For legacy reasons, a bare string (i.e. with no matcher) is interpreted as ContainsRegex(str) , not Eq(str) . Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. NOTE: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if *statement* terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . googletest defines a few predicates that handle the most common cases: :: testing :: ExitedWithCode ( exit_code ) This expression is true if the program exited normally with the given exit code. :: testing :: KilledBySignal ( signal_number ) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as googletest assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST ( MyDeathTest , Foo ) { // This death test uses a compound statement. ASSERT_DEATH ({ int n = 5 ; Foo ( & n ); }, \"Error on line .* of Foo()\" ); } TEST ( MyDeathTest , NormalExit ) { EXPECT_EXIT ( NormalExit (), :: testing :: ExitedWithCode ( 0 ), \"Success\" ); } TEST ( MyDeathTest , KillMyself ) { EXPECT_EXIT ( KillMyself (), :: testing :: KilledBySignal ( SIGKILL ), \"Sending myself unblockable signal\" ); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary. Death Test Naming IMPORTANT: We strongly recommend you to follow the convention of naming your test suite (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use using or typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public :: testing :: Test { ... }; using FooDeathTest = FooTest ; TEST_F ( FooTest , DoesThis ) { // normal test } TEST_F ( FooDeathTest , DoesThat ) { // death test } Regular Expression Syntax On POSIX systems (e.g. Linux, Cygwin, and Mac), googletest uses the POSIX extended regular expression syntax. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, googletest uses its own simple regular expression implementation. It lacks many features. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support ( A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): Expression Meaning c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, googletest defines macros to govern which regular expression it is using. The macros are: GTEST_USES_SIMPLE_RE=1 or GTEST_USES_POSIX_RE=1 . If you want your death tests to work in all cases, you can either #if on these macros or use the more limited syntax only. How It Works Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails. Death Tests And Threads The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. googletest has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test suites with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent. Death Test Styles The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. The automated testing framework does not set the style flag. You can choose a particular style of death tests by setting the flag programmatically: testing :: FLAGS_gtest_death_test_style = \"threadsafe\" You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: int main ( int argc , char ** argv ) { InitGoogle ( argv [ 0 ], & argc , & argv , true ); :: testing :: FLAGS_gtest_death_test_style = \"fast\" ; return RUN_ALL_TESTS (); } TEST ( MyDeathTest , TestOne ) { :: testing :: FLAGS_gtest_death_test_style = \"threadsafe\" ; // This test is run in the \"threadsafe\" style: ASSERT_DEATH ( ThisShouldDie (), \"\" ); } TEST ( MyDeathTest , TestTwo ) { // This test is run in the \"fast\" style: ASSERT_DEATH ( ThisShouldDie (), \"\" ); } Caveats The statement argument of ASSERT_EXIT() can be any valid C++ statement. If it leaves the current function via a return statement or by throwing an exception, the death test is considered to have failed. Some googletest macros may return from the current function (e.g. ASSERT_TRUE() ), so be sure to avoid them in statement . Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) . Using Assertions in Sub-routines Adding Traces to Assertions If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro or the ScopedTrace utility: SCOPED_TRACE ( message ); ScopedTrace trace ( \"file_path\" , line_number , message ); where message can be anything streamable to std::ostream . SCOPED_TRACE macro will cause the current file name, line number, and the given message to be added in every failure message. ScopedTrace accepts explicit file name and line number in arguments, which is useful for writing test helpers. The effect will be undone when the control leaves the current lexical scope. For example, 10 : void Sub1 ( int n ) { 11 : EXPECT_EQ ( Bar ( n ), 1 ); 12 : EXPECT_EQ ( Bar ( n + 1 ), 2 ); 13 : } 14 : 15 : TEST ( FooTest , Bar ) { 16 : { 17 : SCOPED_TRACE ( \"A\" ); // This trace point will be included in 18 : // every failure in this scope. 19 : Sub1 ( 1 ); 20 : } 21 : // Now it won't. 22 : Sub1 ( 9 ); 23 : } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs - hit return on a line number and you'll be taken to that line in the source file! Propagating Fatal Failures A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine () { // Generates a fatal failure and aborts the current function. ASSERT_EQ ( 1 , 2 ); // The following won't be executed. ... } TEST ( FooTest , Bar ) { Subroutine (); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int * p = NULL ; * p = 3 ; // Segfault! } To alleviate this, googletest provides three different solutions. You could use either exceptions, the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections. Asserting on Subroutines with an exception The following code can turn ASSERT-failure into an exception: class ThrowListener : public testing :: EmptyTestEventListener { void OnTestPartResult ( const testing :: TestPartResult & result ) override { if ( result . type () == testing :: TestPartResult :: kFatalFailure ) { throw testing :: AssertionException ( result ); } } }; int main ( int argc , char ** argv ) { ... testing :: UnitTest :: GetInstance () -> listeners (). Append ( new ThrowListener ); return RUN_ALL_TESTS (); } This listener should be added after other listeners if you have any, otherwise they won't see failed OnTestPartResult . Asserting on Subroutines As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that googletest offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE(statement); EXPECT_NO_FATAL_FAILURE(statement); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE ( Foo ()); int i ; EXPECT_NO_FATAL_FAILURE ({ i = Bar (); }); Assertions from multiple threads are currently not supported on Windows. Checking for Failures in the Current Test HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public : ... static bool HasFatalFailure (); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST ( FooTest , Bar ) { Subroutine (); // Aborts if Subroutine() had a fatal failure. if ( HasFatalFailure ()) return ; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if ( :: testing :: Test :: HasFatalFailure ()) return ; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind. Logging Additional Information In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a string or an int . The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F ( WidgetUsageTest , MinAndMaxWidgets ) { RecordProperty ( \"MaximumWidgets\" , ComputeMaxUsage ()); RecordProperty ( \"MinimumWidgets\" , ComputeMinUsage ()); } will output XML like this: ... <testcase name= \"MinAndMaxWidgets\" status= \"run\" time= \"0.006\" classname= \"WidgetUsageTest\" MaximumWidgets= \"12\" MinimumWidgets= \"9\" /> ... NOTE: RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. *key* must be a valid XML attribute name, and cannot conflict with the ones already used by googletest ( name , status , time , classname , type_param , and value_param ). Calling RecordProperty() outside of the lifespan of a test is allowed. If it's called outside of a test but between a test suite's SetUpTestSuite() and TearDownTestSuite() methods, it will be attributed to the XML element for the test suite. If it's called outside of all test suites (e.g. in a test environment), it will be attributed to the top-level XML element. Sharing Resources Between Tests in the Same Test Suite googletest creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in their sharing a single resource copy. So, in addition to per-test set-up/tear-down, googletest also supports per-test-suite set-up/tear-down. To use it: In your test fixture class (say FooTest ), declare as static some member variables to hold the shared resources. Outside your test fixture class (typically just below it), define those member variables, optionally giving them initial values. In the same test fixture class, define a static void SetUpTestSuite() function (remember not to spell it as SetupTestSuite with a small u !) to set up the shared resources and a static void TearDownTestSuite() function to tear them down. That's it! googletest automatically calls SetUpTestSuite() before running the first test in the FooTest test suite (i.e. before creating the first FooTest object), and calls TearDownTestSuite() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-suite set-up and tear-down: class FooTest : public :: testing :: Test { protected : // Per-test-suite set-up. // Called before the first test in this test suite. // Can be omitted if not needed. static void SetUpTestSuite () { shared_resource_ = new ...; } // Per-test-suite tear-down. // Called after the last test in this test suite. // Can be omitted if not needed. static void TearDownTestSuite () { delete shared_resource_ ; shared_resource_ = NULL ; } // You can define per-test set-up logic as usual. virtual void SetUp () { ... } // You can define per-test tear-down logic as usual. virtual void TearDown () { ... } // Some expensive resource shared by all tests. static T * shared_resource_ ; }; T * FooTest :: shared_resource_ = NULL ; TEST_F ( FooTest , Test1 ) { ... you can refer to shared_resource_ here ... } TEST_F ( FooTest , Test2 ) { ... you can refer to shared_resource_ here ... } NOTE: Though the above code declares SetUpTestSuite() protected, it may sometimes be necessary to declare it public, such as when using it with TEST_P . Global Set-Up and Tear-Down Just as you can do set-up and tear-down at the test level and the test suite level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment : public :: testing :: Environment { public : virtual ~ Environment () {} // Override this to define how to set up the environment. void SetUp () override {} // Override this to define how to tear down the environment. void TearDown () override {} }; Then, you register an instance of your environment class with googletest by calling the ::testing::AddGlobalTestEnvironment() function: Environment * AddGlobalTestEnvironment ( Environment * env ); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of each environment object, then runs the tests if none of the environments reported fatal failures and GTEST_SKIP() was not called. RUN_ALL_TESTS() always calls TearDown() with each environment object, regardless of whether or not the tests were run. It's OK to register multiple environment objects. In this suite, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that googletest takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: :: testing :: Environment * const foo_env = :: testing :: AddGlobalTestEnvironment ( new FooEnvironment ); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized). Value-Parameterized Tests Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. This is useful in a number of situations, for example: You have a piece of code whose behavior is affected by one or more command-line flags. You want to make sure your code performs correctly for various values of those flags. You want to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it! How to Write Value-Parameterized Tests To write value-parameterized tests, first you should define a fixture class. It must be derived from both testing::Test and testing::WithParamInterface<T> (the latter is a pure interface), where T is the type of your parameter values. For convenience, you can just derive the fixture class from testing::TestWithParam<T> , which itself is derived from both testing::Test and testing::WithParamInterface<T> . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. NOTE: If your test fixture defines SetUpTestSuite() or TearDownTestSuite() they must be declared public rather than protected in order to use TEST_P . class FooTest : public testing :: TestWithParam < const char *> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; // Or, when you want to add parameters to a pre-existing fixture class: class BaseTest : public testing :: Test { ... }; class BarTest : public BaseTest , public testing :: WithParamInterface < const char *> { ... }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P ( FooTest , DoesBlah ) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE ( foo . Blah ( GetParam ())); ... } TEST_P ( FooTest , HasBlahBlah ) { ... } Finally, you can use INSTANTIATE_TEST_SUITE_P to instantiate the test suite with any set of parameters you want. googletest defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Parameter Generator Behavior Range(begin, end [, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin,end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (Cartesian product) as std\\:\\:tuples of the values generated by the N generators. For more details, see the comments at the definitions of these functions. The following statement will instantiate tests from the FooTest test suite each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_SUITE_P ( InstantiationName , FooTest , testing :: Values ( \"meeny\" , \"miny\" , \"moe\" )); NOTE: The code above must be placed at global or namespace scope, not at function scope. NOTE: Don't forget this step! If you do your test will silently pass, but none of its suites will ever run! To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_SUITE_P is a prefix that will be added to the actual test suite name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest_filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char * pets [] = { \"cat\" , \"dog\" }; INSTANTIATE_TEST_SUITE_P ( AnotherInstantiationName , FooTest , testing :: ValuesIn ( pets )); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_SUITE_P will instantiate all tests in the given test suite, whether their definitions come before or after the INSTANTIATE_TEST_SUITE_P statement. You can see $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample7_unittest.cc and $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample8_unittest.cc for more examples. $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample7_unittest.cc for \"Parameterized Test example\" $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample8_unittest.cc for \"Parameterized Test example with multiple parameters\" Creating Value-Parameterized Abstract Tests In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, they can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_SUITE_P() , and depending on the library target that contains foo_param_test.cc . You can instantiate the same abstract test suite multiple times, possibly in different source files. Specifying Names for Value-Parameterized Test Parameters The optional last argument to INSTANTIATE_TEST_SUITE_P() allows the user to specify a function or functor that generates custom test name suffixes based on the test parameters. The function should accept one argument of type testing::TestParamInfo<class ParamType> , and return std::string . testing::PrintToStringParamName is a builtin test suffix generator that returns the value of testing::PrintToString(GetParam()) . It does not work for std::string or C strings. NOTE: test names must be non-empty, unique, and may only contain ASCII alphanumeric characters. In particular, they should not contain underscores class MyTestSuite : public testing :: TestWithParam < int > {}; TEST_P ( MyTestSuite , MyTest ) { std :: cout << \"Example Test Param: \" << GetParam () << std :: endl ; } INSTANTIATE_TEST_SUITE_P ( MyGroup , MyTestSuite , testing :: Range ( 0 , 10 ), testing :: PrintToStringParamName ()); Providing a custom functor allows for more control over test parameter name generation, especially for types where the automatic conversion does not generate helpful parameter names (e.g. strings as demonstrated above). The following example illustrates this for multiple parameters, an enumeration type and a string, and also demonstrates how to combine generators. It uses a lambda for conciseness: enum class MyType { MY_FOO = 0 , MY_BAR = 1 }; class MyTestSuite : public testing :: TestWithParam < std :: tuple < MyType , string >> { }; INSTANTIATE_TEST_SUITE_P ( MyGroup , MyTestSuite , testing :: Combine ( testing :: Values ( MyType :: VALUE_0 , MyType :: VALUE_1 ), testing :: ValuesIn ( \"\" , \"\" )), []( const testing :: TestParamInfo < MyTestSuite :: ParamType >& info ) { string name = absl :: StrCat ( std :: get < 0 > ( info . param ) == MY_FOO ? \"Foo\" : \"Bar\" , \"_\" , std :: get < 1 > ( info . param )); absl :: c_replace_if ( name , []( char c ) { return ! std :: isalnum ( c ); }, '_' ); return name ; }); Typed Tests Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template < typename T > class FooTest : public :: testing :: Test { public : ... typedef std :: list < T > List ; static T shared_ ; T value_ ; }; Next, associate a list of types with the test suite, which will be repeated for each type in the list: using MyTypes = :: testing :: Types < char , int , unsigned int > ; TYPED_TEST_SUITE ( FooTest , MyTypes ); The type alias ( using or typedef ) is necessary for the TYPED_TEST_SUITE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test suite. You can repeat this as many times as you want: TYPED_TEST ( FooTest , DoesBlah ) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this -> value_ ; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture :: shared_ ; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture :: List values ; values . push_back ( n ); ... } TYPED_TEST ( FooTest , HasPropertyA ) { ... } You can see $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample6_unittest.cc for a complete \"Typed Test example\" Type-Parameterized Tests Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with their type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template < typename T > class FooTest : public :: testing :: Test { ... }; Next, declare that you will define a type-parameterized test suite: TYPED_TEST_SUITE_P ( FooTest ); Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P ( FooTest , DoesBlah ) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0 ; ... } TYPED_TEST_P ( FooTest , HasPropertyA ) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_SUITE_P macro before you can instantiate them. The first argument of the macro is the test suite name; the rest are the names of the tests in this test suite: REGISTER_TYPED_TEST_SUITE_P ( FooTest , DoesBlah , HasPropertyA ); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef :: testing :: Types < char , int , unsigned int > MyTypes ; INSTANTIATE_TYPED_TEST_SUITE_P ( My , FooTest , MyTypes ); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_SUITE_P macro is a prefix that will be added to the actual test suite name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_SUITE_P ( My , FooTest , int ); You can see $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample6_unittest.cc for a complete example. Testing Private Code If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle, most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design. The desire to test internal implementation is often a sign that the class is doing too much. Consider extracting an implementation class, and testing it. Then use that implementation class in the original class. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members To test them, we use the following special techniques: Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. (#including .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients. Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST ( TestSuiteName , TestName ); For example, // foo.h class Foo { ... private : FRIEND_TEST ( FooTest , BarReturnsZeroOnNull ); int Bar ( void * x ); }; // foo_test.cc ... TEST ( FooTest , BarReturnsZeroOnNull ) { Foo foo ; EXPECT_EQ ( foo . Bar ( NULL ), 0 ); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest ; FRIEND_TEST ( FooTest , Bar ); FRIEND_TEST ( FooTest , Baz ); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public :: testing :: Test { protected : ... }; TEST_F ( FooTest , Bar ) { ... } TEST_F ( FooTest , Baz ) { ... } } // namespace my_namespace \"Catching\" Failures If you are building a testing utility on top of googletest, you'll want to test your utility. What framework would you use to test it? googletest, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But googletest doesn't use exceptions, so how do we test that a piece of code generates an expected failure? gunit-spi.h contains some constructs to do this. After #including this header, you can use EXPECT_FATAL_FAILURE ( statement , substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure in the current thread whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE ( statement , substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. Only failures in the current thread are checked to determine the result of this type of expectations. If statement creates new threads, failures in these threads are also ignored. If you want to catch failures in other threads as well, use one of the following macros instead: EXPECT_FATAL_FAILURE_ON_ALL_THREADS ( statement , substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS ( statement , substring ); NOTE: Assertions from multiple threads are currently not supported on Windows. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE{_ON_ALL_THREADS}() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE{_ON_ALL_THREADS}() cannot return a value. Registering tests programmatically The TEST macros handle the vast majority of all use cases, but there are few were runtime registration logic is required. For those cases, the framework provides the ::testing::RegisterTest that allows callers to register arbitrary tests dynamically. This is an advanced API only to be used when the TEST macros are insufficient. The macros should be preferred when possible, as they avoid most of the complexity of calling this function. It provides the following signature: template < typename Factory > TestInfo * RegisterTest ( const char * test_suite_name , const char * test_name , const char * type_param , const char * value_param , const char * file , int line , Factory factory ); The factory argument is a factory callable (move-constructible) object or function pointer that creates a new instance of the Test object. It handles ownership to the caller. The signature of the callable is Fixture*() , where Fixture is the test fixture class for the test. All tests registered with the same test_suite_name must return the same fixture type. This is checked at runtime. The framework will infer the fixture class from the factory and will call the SetUpTestSuite and TearDownTestSuite for it. Must be called before RUN_ALL_TESTS() is invoked, otherwise behavior is undefined. Use case example: class MyFixture : public :: testing :: Test { public : // All of these optional, just like in regular macro usage. static void SetUpTestSuite () { ... } static void TearDownTestSuite () { ... } void SetUp () override { ... } void TearDown () override { ... } }; class MyTest : public MyFixture { public : explicit MyTest ( int data ) : data_ ( data ) {} void TestBody () override { ... } private : int data_ ; }; void RegisterMyTests ( const std :: vector < int >& values ) { for ( int v : values ) { :: testing :: RegisterTest ( \"MyFixture\" , ( \"Test\" + std :: to_string ( v )). c_str (), nullptr , std :: to_string ( v ). c_str (), __FILE__ , __LINE__ , // Important to use the fixture type as the return type here. [ = ]() -> MyFixture * { return new MyTest ( v ); }); } } ... int main ( int argc , char ** argv ) { std :: vector < int > values_to_test = LoadValuesFromConfig (); RegisterMyTests ( values_to_test ); ... return RUN_ALL_TESTS (); } Getting the Current Test's Name Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public : // Returns the test suite name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char * test_suite_name () const ; const char * name () const ; }; } To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const :: testing :: TestInfo * const test_info = :: testing :: UnitTest :: GetInstance () -> current_test_info (); printf ( \"We are in test %s of test suite %s. \\n \" , test_info -> name (), test_info -> test_suite_name ()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test suite name in TestSuiteSetUp() , TestSuiteTearDown() (where you know the test suite name implicitly), or functions called from them. Extending googletest by Handling Test Events googletest provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test suite, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example. Defining Event Listeners To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: UnitTest reflects the state of the entire test program, TestSuite has information about a test suite, which can contain one or more tests, TestInfo contains the state of a test, and TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public :: testing :: EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart ( const :: testing :: TestInfo & test_info ) { printf ( \"*** Test %s.%s starting. \\n \" , test_info . test_suite_name (), test_info . name ()); } // Called after a failed assertion or a SUCCESS(). virtual void OnTestPartResult ( const :: testing :: TestPartResult & test_part_result ) { printf ( \"%s in %s:%d \\n %s \\n \" , test_part_result . failed () ? \"*** Failure\" : \"Success\" , test_part_result . file_name (), test_part_result . line_number (), test_part_result . summary ()); } // Called after a test ends. virtual void OnTestEnd ( const :: testing :: TestInfo & test_info ) { printf ( \"*** Test %s.%s ending. \\n \" , test_info . test_suite_name (), test_info . name ()); } }; Using Event Listeners To use the event listener you have defined, add an instance of it to the googletest event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main ( int argc , char ** argv ) { :: testing :: InitGoogleTest ( & argc , argv ); // Gets hold of the event listener list. :: testing :: TestEventListeners & listeners = :: testing :: UnitTest :: GetInstance () -> listeners (); // Adds a listener to the end. googletest takes the ownership. listeners . Append ( new MinimalistPrinter ); return RUN_ALL_TESTS (); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners . Release ( listeners . default_result_printer ()); listeners . Append ( new MinimalistPrinter ); return RUN_ALL_TESTS (); Now, sit back and enjoy a completely different output from your tests. For more details, see $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample9_unittest.cc . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier. Generating Failures in Listeners You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. See $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample10_unittest.cc for an example of a failure-raising listener. Running Test Programs: Advanced Options googletest test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. If an option is specified both by an environment variable and by a flag, the latter takes precedence. Selecting Tests Listing Test Names Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestSuite1. TestName1 TestName2 TestSuite2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag. Running a Subset of the Tests By default, a googletest program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, googletest will only run the tests whose full names (in the form of TestSuiteName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns ) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns ). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test suite FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test suite FooTest except FooTest.Bar . ./foo_test --gtest_filter=FooTest.*:BarTest.*-FooTest.Bar:BarTest.Foo Runs everything in test suite FooTest except FooTest.Bar and everything in test suite BarTest except BarTest.Foo . Temporarily Disabling Tests If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test suite, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test suite name. For example, the following tests won't be run by googletest, even though they will still be compiled: // Tests that Foo does Abc. TEST ( FooTest , DISABLED_DoesAbc ) { ... } class DISABLED_BarTest : public :: testing :: Test { ... }; // Tests that Bar does Xyz. TEST_F ( DISABLED_BarTest , DoesXyz ) { ... } NOTE: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, googletest will print a banner warning you if a test program contains any disabled tests. TIP: You can easily count the number of disabled tests you have using gsearch and/or grep . This number can be used as a metric for improving your test quality. Temporarily Enabling Disabled Tests To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest_filter flag to further select which disabled tests to run. Repeating the Tests Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the test fails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar.* Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code, it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable. Shuffling the Tests You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, googletest uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer in the range [0, 99999]. The seed value 0 is special: it tells googletest to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , googletest will pick a different random seed and re-shuffle the tests in each iteration. Controlling Test Output Colored Terminal Output googletest can use colors in its terminal output to make it easier to spot the important information: ... [----------] 1 test from FooTest [ RUN ] FooTest.DoesAbc [ OK ] FooTest.DoesAbc [----------] 2 tests from BarTest [ RUN ] BarTest.HasXyzProperty [ OK ] BarTest.HasXyzProperty [ RUN ] BarTest.ReturnsTrueOnSuccess ... some error messages ... [ FAILED ] BarTest.ReturnsTrueOnSuccess ... [==========] 30 tests from 14 test suites ran. [ PASSED ] 28 tests. [ FAILED ] 2 tests, listed below: [ FAILED ] BarTest.ReturnsTrueOnSuccess [ FAILED ] AnotherTest.DoesXyz 2 FAILED TESTS You can set the GTEST_COLOR environment variable or the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let googletest decide. When the value is auto , googletest will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color . Suppressing the Elapsed Time By default, googletest prints the time it takes to run each test. To disable that, run the test program with the --gtest_print_time=0 command line flag, or set the GTEST_PRINT_TIME environment variable to 0 . Suppressing UTF-8 Text Output In case of assertion failures, googletest prints expected and actual values of type string both as hex-encoded strings as well as in readable UTF-8 text if they contain valid non-ASCII UTF-8 characters. If you want to suppress the UTF-8 text because, for example, you don't have an UTF-8 compatible output medium, run the test program with --gtest_print_utf8=0 or set the GTEST_PRINT_UTF8 environment variable to 0 . Generating an XML Report googletest can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. The report is also used by the http://unittest dashboard to show per-test-method error messages. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:path_to_output_file\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), googletest will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), googletest will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report is based on the junitreport Ant task. Since that format was originally intended for Java, a little interpretation is required to make it apply to googletest tests, as shown here: <testsuites name= \"AllTests\" ... > <testsuite name= \"test_case_name\" ... > <testcase name= \"test_name\" ... > <failure message= \"...\" /> <failure message= \"...\" /> <failure message= \"...\" /> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to googletest test suites. <testcase> elements correspond to googletest test functions. For instance, the following program TEST ( MathTest , Addition ) { ... } TEST ( MathTest , Subtraction ) { ... } TEST ( LogicTest , NonContradiction ) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests= \"3\" failures= \"1\" errors= \"0\" time= \"0.035\" timestamp= \"2011-10-31T18:52:42\" name= \"AllTests\" > <testsuite name= \"MathTest\" tests= \"2\" failures= \"1\" errors= \"0\" time= \"0.015\" > <testcase name= \"Addition\" status= \"run\" time= \"0.007\" classname= \"\" > <failure message= \"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type= \"\" > ... </failure> <failure message= \"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type= \"\" > ... </failure> </testcase> <testcase name= \"Subtraction\" status= \"run\" time= \"0.005\" classname= \"\" > </testcase> </testsuite> <testsuite name= \"LogicTest\" tests= \"1\" failures= \"0\" errors= \"0\" time= \"0.005\" > <testcase name= \"NonContradiction\" status= \"run\" time= \"0.005\" classname= \"\" > </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the googletest program or test suite contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test suite, or entire test program in seconds. The timestamp attribute records the local date and time of the test execution. Each <failure> element corresponds to a single failed googletest assertion. Generating a JSON Report googletest can also emit a JSON report as an alternative format to XML. To generate the JSON report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"json:path_to_output_file\" , which will create the file at the given location. You can also just use the string \"json\" , in which case the output can be found in the test_detail.json file in the current directory. The report format conforms to the following JSON Schema: { \"$schema\" : \"http://json-schema.org/schema#\" , \"type\" : \"object\" , \"definitions\" : { \"TestCase\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"tests\" : { \"type\" : \"integer\" }, \"failures\" : { \"type\" : \"integer\" }, \"disabled\" : { \"type\" : \"integer\" }, \"time\" : { \"type\" : \"string\" }, \"testsuite\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/TestInfo\" } } } }, \"TestInfo\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"status\" : { \"type\" : \"string\" , \"enum\" : [ \"RUN\" , \"NOTRUN\" ] }, \"time\" : { \"type\" : \"string\" }, \"classname\" : { \"type\" : \"string\" }, \"failures\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/Failure\" } } } }, \"Failure\" : { \"type\" : \"object\" , \"properties\" : { \"failures\" : { \"type\" : \"string\" }, \"type\" : { \"type\" : \"string\" } } } }, \"properties\" : { \"tests\" : { \"type\" : \"integer\" }, \"failures\" : { \"type\" : \"integer\" }, \"disabled\" : { \"type\" : \"integer\" }, \"errors\" : { \"type\" : \"integer\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" }, \"time\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"testsuites\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/TestCase\" } } } } The report uses the format that conforms to the following Proto3 using the JSON encoding : syntax = \"proto3\" ; package googletest ; import \"google/protobuf/timestamp.proto\" ; import \"google/protobuf/duration.proto\" ; message UnitTest { int32 tests = 1 ; int32 failures = 2 ; int32 disabled = 3 ; int32 errors = 4 ; google.protobuf.Timestamp timestamp = 5 ; google.protobuf.Duration time = 6 ; string name = 7 ; repeated TestCase testsuites = 8 ; } message TestCase { string name = 1 ; int32 tests = 2 ; int32 failures = 3 ; int32 disabled = 4 ; int32 errors = 5 ; google.protobuf.Duration time = 6 ; repeated TestInfo testsuite = 7 ; } message TestInfo { string name = 1 ; enum Status { RUN = 0 ; NOTRUN = 1 ; } Status status = 2 ; google.protobuf.Duration time = 3 ; string classname = 4 ; message Failure { string failures = 1 ; string type = 2 ; } repeated Failure failures = 5 ; } For instance, the following program TEST ( MathTest , Addition ) { ... } TEST ( MathTest , Subtraction ) { ... } TEST ( LogicTest , NonContradiction ) { ... } could generate this report: { \"tests\" : 3 , \"failures\" : 1 , \"errors\" : 0 , \"time\" : \"0.035s\" , \"timestamp\" : \"2011-10-31T18:52:42Z\" , \"name\" : \"AllTests\" , \"testsuites\" : [ { \"name\" : \"MathTest\" , \"tests\" : 2 , \"failures\" : 1 , \"errors\" : 0 , \"time\" : \"0.015s\" , \"testsuite\" : [ { \"name\" : \"Addition\" , \"status\" : \"RUN\" , \"time\" : \"0.007s\" , \"classname\" : \"\" , \"failures\" : [ { \"message\" : \"Value of: add(1, 1)\\n Actual: 3\\nExpected: 2\" , \"type\" : \"\" }, { \"message\" : \"Value of: add(1, -1)\\n Actual: 1\\nExpected: 0\" , \"type\" : \"\" } ] }, { \"name\" : \"Subtraction\" , \"status\" : \"RUN\" , \"time\" : \"0.005s\" , \"classname\" : \"\" } ] }, { \"name\" : \"LogicTest\" , \"tests\" : 1 , \"failures\" : 0 , \"errors\" : 0 , \"time\" : \"0.005s\" , \"testsuite\" : [ { \"name\" : \"NonContradiction\" , \"status\" : \"RUN\" , \"time\" : \"0.005s\" , \"classname\" : \"\" } ] } ] } IMPORTANT: The exact format of the JSON document is subject to change. Controlling How Failures Are Reported Turning Assertion Failures into Break-Points When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. googletest's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag. Disabling Catching Test-Thrown Exceptions googletest can be used either with or without exceptions enabled. If a test throws a C++ exception or (on Windows) a structured exception (SEH), by default googletest catches it, reports it as a test failure, and continues with the next test method. This maximizes the coverage of a test run. Also, on Windows an uncaught exception will cause a pop-up window, so catching the exceptions allows you to run the tests automatically. When debugging the test failures, however, you may instead want the exceptions to be handled by the debugger, such that you can examine the call stack when an exception is thrown. To achieve that, set the GTEST_CATCH_EXCEPTIONS environment variable to 0 , or use the --gtest_catch_exceptions=0 flag when running the tests.","title":"Advanced googletest Topics"},{"location":"examples/gtest/docs/advanced.html#advanced-googletest-topics","text":"","title":"Advanced googletest Topics"},{"location":"examples/gtest/docs/advanced.html#introduction","text":"Now that you have read the googletest Primer and learned how to write tests using googletest, it's time to learn some new tricks. This document will show you more assertions as well as how to construct complex failure messages, propagate fatal failures, reuse and speed up your test fixtures, and use various flags with your tests.","title":"Introduction"},{"location":"examples/gtest/docs/advanced.html#more-assertions","text":"This section covers some less frequently used, but still significant, assertions.","title":"More Assertions"},{"location":"examples/gtest/docs/advanced.html#explicit-success-and-failure","text":"These three assertions do not actually test a value or expression. Instead, they generate a success or failure directly. Like the macros that actually perform a test, you may stream a custom failure message into them. SUCCEED (); Generates a success. This does NOT make the overall test succeed. A test is considered successful only if none of its assertions fail during its execution. NOTE: SUCCEED() is purely documentary and currently doesn't generate any user-visible output. However, we may add SUCCEED() messages to googletest's output in the future. FAIL (); ADD_FAILURE (); ADD_FAILURE_AT ( \"file_path\" , line_number ); FAIL() generates a fatal failure, while ADD_FAILURE() and ADD_FAILURE_AT() generate a nonfatal failure. These are useful when control flow, rather than a Boolean expression, determines the test's success or failure. For example, you might want to write something like: switch ( expression ) { case 1 : ... some checks ... case 2 : ... some other checks ... default : FAIL () << \"We shouldn't get here.\" ; } NOTE: you can only use FAIL() in functions that return void . See the Assertion Placement section for more information.","title":"Explicit Success and Failure"},{"location":"examples/gtest/docs/advanced.html#exception-assertions","text":"These are for verifying that a piece of code throws (or does not throw) an exception of the given type: Fatal assertion Nonfatal assertion Verifies ASSERT_THROW(statement, exception_type); EXPECT_THROW(statement, exception_type); statement throws an exception of the given type ASSERT_ANY_THROW(statement); EXPECT_ANY_THROW(statement); statement throws an exception of any type ASSERT_NO_THROW(statement); EXPECT_NO_THROW(statement); statement doesn't throw any exception Examples: ASSERT_THROW ( Foo ( 5 ), bar_exception ); EXPECT_NO_THROW ({ int n = 5 ; Bar ( & n ); }); Availability : requires exceptions to be enabled in the build environment","title":"Exception Assertions"},{"location":"examples/gtest/docs/advanced.html#predicate-assertions-for-better-error-messages","text":"Even though googletest has a rich set of assertions, they can never be complete, as it's impossible (nor a good idea) to anticipate all scenarios a user might run into. Therefore, sometimes a user has to use EXPECT_TRUE() to check a complex expression, for lack of a better macro. This has the problem of not showing you the values of the parts of the expression, making it hard to understand what went wrong. As a workaround, some users choose to construct the failure message by themselves, streaming it into EXPECT_TRUE() . However, this is awkward especially when the expression has side-effects or is expensive to evaluate. googletest gives you three different options to solve this problem:","title":"Predicate Assertions for Better Error Messages"},{"location":"examples/gtest/docs/advanced.html#using-an-existing-boolean-function","text":"If you already have a function or functor that returns bool (or a type that can be implicitly converted to bool ), you can use it in a predicate assertion to get the function arguments printed for free: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED1(pred1, val1) EXPECT_PRED1(pred1, val1) pred1(val1) is true ASSERT_PRED2(pred2, val1, val2) EXPECT_PRED2(pred2, val1, val2) pred1(val1, val2) is true ... ... ... In the above, predn is an n -ary predicate function or functor, where val1 , val2 , ..., and valn are its arguments. The assertion succeeds if the predicate returns true when applied to the given arguments, and fails otherwise. When the assertion fails, it prints the value of each argument. In either case, the arguments are evaluated exactly once. Here's an example. Given // Returns true if m and n have no common divisors except 1. bool MutuallyPrime ( int m , int n ) { ... } const int a = 3 ; const int b = 4 ; const int c = 10 ; the assertion EXPECT_PRED2 ( MutuallyPrime , a , b ); will succeed, while the assertion EXPECT_PRED2 ( MutuallyPrime , b , c ); will fail with the message MutuallyPrime(b, c) is false, where b is 4 c is 10 NOTE: If you see a compiler error \"no matching function to call\" when using ASSERT_PRED* or EXPECT_PRED* , please see this for how to resolve it.","title":"Using an Existing Boolean Function"},{"location":"examples/gtest/docs/advanced.html#using-a-function-that-returns-an-assertionresult","text":"While EXPECT_PRED*() and friends are handy for a quick job, the syntax is not satisfactory: you have to use different macros for different arities, and it feels more like Lisp than C++. The ::testing::AssertionResult class solves this problem. An AssertionResult object represents the result of an assertion (whether it's a success or a failure, and an associated message). You can create an AssertionResult using one of these factory functions: namespace testing { // Returns an AssertionResult object to indicate that an assertion has // succeeded. AssertionResult AssertionSuccess (); // Returns an AssertionResult object to indicate that an assertion has // failed. AssertionResult AssertionFailure (); } You can then use the << operator to stream messages to the AssertionResult object. To provide more readable messages in Boolean assertions (e.g. EXPECT_TRUE() ), write a predicate function that returns AssertionResult instead of bool . For example, if you define IsEven() as: :: testing :: AssertionResult IsEven ( int n ) { if (( n % 2 ) == 0 ) return :: testing :: AssertionSuccess (); else return :: testing :: AssertionFailure () << n << \" is odd\" ; } instead of: bool IsEven ( int n ) { return ( n % 2 ) == 0 ; } the failed assertion EXPECT_TRUE(IsEven(Fib(4))) will print: Value of: IsEven(Fib(4)) Actual: false (3 is odd) Expected: true instead of a more opaque Value of: IsEven(Fib(4)) Actual: false Expected: true If you want informative messages in EXPECT_FALSE and ASSERT_FALSE as well (one third of Boolean assertions in the Google code base are negative ones), and are fine with making the predicate slower in the success case, you can supply a success message: :: testing :: AssertionResult IsEven ( int n ) { if (( n % 2 ) == 0 ) return :: testing :: AssertionSuccess () << n << \" is even\" ; else return :: testing :: AssertionFailure () << n << \" is odd\" ; } Then the statement EXPECT_FALSE(IsEven(Fib(6))) will print Value of: IsEven(Fib(6)) Actual: true (8 is even) Expected: false","title":"Using a Function That Returns an AssertionResult"},{"location":"examples/gtest/docs/advanced.html#using-a-predicate-formatter","text":"If you find the default message generated by (ASSERT|EXPECT)_PRED* and (ASSERT|EXPECT)_(TRUE|FALSE) unsatisfactory, or some arguments to your predicate do not support streaming to ostream , you can instead use the following predicate-formatter assertions to fully customize how the message is formatted: Fatal assertion Nonfatal assertion Verifies ASSERT_PRED_FORMAT1(pred_format1, val1); EXPECT_PRED_FORMAT1(pred_format1, val1); pred_format1(val1) is successful ASSERT_PRED_FORMAT2(pred_format2, val1, val2); EXPECT_PRED_FORMAT2(pred_format2, val1, val2); pred_format2(val1, val2) is successful ... ... ... The difference between this and the previous group of macros is that instead of a predicate, (ASSERT|EXPECT)_PRED_FORMAT* take a predicate-formatter ( pred_formatn ), which is a function or functor with the signature: :: testing :: AssertionResult PredicateFormattern ( const char * expr1 , const char * expr2 , ... const char * exprn , T1 val1 , T2 val2 , ... Tn valn ); where val1 , val2 , ..., and valn are the values of the predicate arguments, and expr1 , expr2 , ..., and exprn are the corresponding expressions as they appear in the source code. The types T1 , T2 , ..., and Tn can be either value types or reference types. For example, if an argument has type Foo , you can declare it as either Foo or const Foo& , whichever is appropriate. As an example, let's improve the failure message in MutuallyPrime() , which was used with EXPECT_PRED2() : // Returns the smallest prime common divisor of m and n, // or 1 when m and n are mutually prime. int SmallestPrimeCommonDivisor ( int m , int n ) { ... } // A predicate-formatter for asserting that two integers are mutually prime. :: testing :: AssertionResult AssertMutuallyPrime ( const char * m_expr , const char * n_expr , int m , int n ) { if ( MutuallyPrime ( m , n )) return :: testing :: AssertionSuccess (); return :: testing :: AssertionFailure () << m_expr << \" and \" << n_expr << \" (\" << m << \" and \" << n << \") are not mutually prime, \" << \"as they have a common divisor \" << SmallestPrimeCommonDivisor ( m , n ); } With this predicate-formatter, we can use EXPECT_PRED_FORMAT2 ( AssertMutuallyPrime , b , c ); to generate the message b and c (4 and 10) are not mutually prime, as they have a common divisor 2. As you may have realized, many of the built-in assertions we introduced earlier are special cases of (EXPECT|ASSERT)_PRED_FORMAT* . In fact, most of them are indeed defined using (EXPECT|ASSERT)_PRED_FORMAT* .","title":"Using a Predicate-Formatter"},{"location":"examples/gtest/docs/advanced.html#floating-point-comparison","text":"Comparing floating-point numbers is tricky. Due to round-off errors, it is very unlikely that two floating-points will match exactly. Therefore, ASSERT_EQ 's naive comparison usually doesn't work. And since floating-points can have a wide value range, no single fixed error bound works. It's better to compare by a fixed relative error bound, except for values close to 0 due to the loss of precision there. In general, for floating-point comparison to make sense, the user needs to carefully choose the error bound. If they don't want or care to, comparing in terms of Units in the Last Place (ULPs) is a good default, and googletest provides assertions to do this. Full details about ULPs are quite long; if you want to learn more, see here .","title":"Floating-Point Comparison"},{"location":"examples/gtest/docs/advanced.html#floating-point-macros","text":"Fatal assertion Nonfatal assertion Verifies ASSERT_FLOAT_EQ(val1, val2); EXPECT_FLOAT_EQ(val1, val2); the two float values are almost equal ASSERT_DOUBLE_EQ(val1, val2); EXPECT_DOUBLE_EQ(val1, val2); the two double values are almost equal By \"almost equal\" we mean the values are within 4 ULP's from each other. The following assertions allow you to choose the acceptable error bound: Fatal assertion Nonfatal assertion Verifies ASSERT_NEAR(val1, val2, abs_error); EXPECT_NEAR(val1, val2, abs_error); the difference between val1 and val2 doesn't exceed the given absolute error","title":"Floating-Point Macros"},{"location":"examples/gtest/docs/advanced.html#floating-point-predicate-format-functions","text":"Some floating-point operations are useful, but not that often used. In order to avoid an explosion of new macros, we provide them as predicate-format functions that can be used in predicate assertion macros (e.g. EXPECT_PRED_FORMAT2 , etc). EXPECT_PRED_FORMAT2 ( :: testing :: FloatLE , val1 , val2 ); EXPECT_PRED_FORMAT2 ( :: testing :: DoubleLE , val1 , val2 ); Verifies that val1 is less than, or almost equal to, val2 . You can replace EXPECT_PRED_FORMAT2 in the above table with ASSERT_PRED_FORMAT2 .","title":"Floating-Point Predicate-Format Functions"},{"location":"examples/gtest/docs/advanced.html#asserting-using-gmock-matchers","text":"gMock comes with a library of matchers for validating arguments passed to mock objects. A gMock matcher is basically a predicate that knows how to describe itself. It can be used in these assertion macros: Fatal assertion Nonfatal assertion Verifies ASSERT_THAT(value, matcher); EXPECT_THAT(value, matcher); value matches matcher For example, StartsWith(prefix) is a matcher that matches a string starting with prefix , and you can write: using :: testing :: StartsWith ; ... // Verifies that Foo() returns a string starting with \"Hello\". EXPECT_THAT ( Foo (), StartsWith ( \"Hello\" )); Read this recipe in the gMock Cookbook for more details. gMock has a rich set of matchers. You can do many things googletest cannot do alone with them. For a list of matchers gMock provides, read this . It's easy to write your own matchers too. gMock is bundled with googletest, so you don't need to add any build dependency in order to take advantage of this. Just include \"testing/base/public/gmock.h\" and you're ready to go.","title":"Asserting Using gMock Matchers"},{"location":"examples/gtest/docs/advanced.html#more-string-assertions","text":"(Please read the previous section first if you haven't.) You can use the gMock string matchers with EXPECT_THAT() or ASSERT_THAT() to do more string comparison tricks (sub-string, prefix, suffix, regular expression, and etc). For example, using :: testing :: HasSubstr ; using :: testing :: MatchesRegex ; ... ASSERT_THAT ( foo_string , HasSubstr ( \"needle\" )); EXPECT_THAT ( bar_string , MatchesRegex ( \" \\\\ w* \\\\ d+\" )); If the string contains a well-formed HTML or XML document, you can check whether its DOM tree matches an XPath expression : // Currently still in //template/prototemplate/testing:xpath_matcher #include \"template/prototemplate/testing/xpath_matcher.h\" using prototemplate :: testing :: MatchesXPath ; EXPECT_THAT ( html_string , MatchesXPath ( \"//a[text()='click here']\" ));","title":"More String Assertions"},{"location":"examples/gtest/docs/advanced.html#windows-hresult-assertions","text":"These assertions test for HRESULT success or failure. Fatal assertion Nonfatal assertion Verifies ASSERT_HRESULT_SUCCEEDED(expression) EXPECT_HRESULT_SUCCEEDED(expression) expression is a success HRESULT ASSERT_HRESULT_FAILED(expression) EXPECT_HRESULT_FAILED(expression) expression is a failure HRESULT The generated output contains the human-readable error message associated with the HRESULT code returned by expression . You might use them like this: CComPtr < IShellDispatch2 > shell ; ASSERT_HRESULT_SUCCEEDED ( shell . CoCreateInstance ( L \"Shell.Application\" )); CComVariant empty ; ASSERT_HRESULT_SUCCEEDED ( shell -> ShellExecute ( CComBSTR ( url ), empty , empty , empty , empty ));","title":"Windows HRESULT assertions"},{"location":"examples/gtest/docs/advanced.html#type-assertions","text":"You can call the function :: testing :: StaticAssertTypeEq < T1 , T2 > (); to assert that types T1 and T2 are the same. The function does nothing if the assertion is satisfied. If the types are different, the function call will fail to compile, the compiler error message will say that type1 and type2 are not the same type and most likely (depending on the compiler) show you the actual values of T1 and T2 . This is mainly useful inside template code. Caveat : When used inside a member function of a class template or a function template, StaticAssertTypeEq<T1, T2>() is effective only if the function is instantiated. For example, given: template < typename T > class Foo { public : void Bar () { :: testing :: StaticAssertTypeEq < int , T > (); } }; the code: void Test1 () { Foo < bool > foo ; } will not generate a compiler error, as Foo<bool>::Bar() is never actually instantiated. Instead, you need: void Test2 () { Foo < bool > foo ; foo . Bar (); } to cause a compiler error.","title":"Type Assertions"},{"location":"examples/gtest/docs/advanced.html#assertion-placement","text":"You can use assertions in any C++ function. In particular, it doesn't have to be a method of the test fixture class. The one constraint is that assertions that generate a fatal failure ( FAIL* and ASSERT_* ) can only be used in void-returning functions. This is a consequence of Google's not using exceptions. By placing it in a non-void function you'll get a confusing compile error like \"error: void value not ignored as it ought to be\" or \"cannot initialize return object of type 'bool' with an rvalue of type 'void'\" or \"error: no viable conversion from 'void' to 'string'\" . If you need to use fatal assertions in a function that returns non-void, one option is to make the function return the value in an out parameter instead. For example, you can rewrite T2 Foo(T1 x) to void Foo(T1 x, T2* result) . You need to make sure that *result contains some sensible value even when the function returns prematurely. As the function now returns void , you can use any assertion inside of it. If changing the function's type is not an option, you should just use assertions that generate non-fatal failures, such as ADD_FAILURE* and EXPECT_* . NOTE: Constructors and destructors are not considered void-returning functions, according to the C++ language specification, and so you may not use fatal assertions in them; you'll get a compilation error if you try. Instead, either call abort and crash the entire test executable, or put the fatal assertion in a SetUp / TearDown function; see constructor/destructor vs. SetUp / TearDown WARNING: A fatal assertion in a helper function (private void-returning method) called from a constructor or destructor does not does not terminate the current test, as your intuition might suggest: it merely returns from the constructor or destructor early, possibly leaving your object in a partially-constructed or partially-destructed state! You almost certainly want to abort or use SetUp / TearDown instead.","title":"Assertion Placement"},{"location":"examples/gtest/docs/advanced.html#teaching-googletest-how-to-print-your-values","text":"When a test assertion such as EXPECT_EQ fails, googletest prints the argument values to help you debug. It does this using a user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. As mentioned earlier, the printer is extensible . That means you can teach it to do a better job at printing your particular type than to dump the bytes. To do that, define << for your type: #include <ostream> namespace foo { class Bar { // We want googletest to be able to print instances of this. ... // Create a free inline friend function. friend std :: ostream & operator << ( std :: ostream & os , const Bar & bar ) { return os << bar . DebugString (); // whatever needed to print bar to os } }; // If you can't declare the function in the class it's important that the // << operator is defined in the SAME namespace that defines Bar. C++'s look-up // rules rely on that. std :: ostream & operator << ( std :: ostream & os , const Bar & bar ) { return os << bar . DebugString (); // whatever needed to print bar to os } } // namespace foo Sometimes, this might not be an option: your team may consider it bad style to have a << operator for Bar , or Bar may already have a << operator that doesn't do what you want (and you cannot change it). If so, you can instead define a PrintTo() function like this: #include <ostream> namespace foo { class Bar { ... friend void PrintTo ( const Bar & bar , std :: ostream * os ) { * os << bar . DebugString (); // whatever needed to print bar to os } }; // If you can't declare the function in the class it's important that PrintTo() // is defined in the SAME namespace that defines Bar. C++'s look-up rules rely // on that. void PrintTo ( const Bar & bar , std :: ostream * os ) { * os << bar . DebugString (); // whatever needed to print bar to os } } // namespace foo If you have defined both << and PrintTo() , the latter will be used when googletest is concerned. This allows you to customize how the value appears in googletest's output without affecting code that relies on the behavior of its << operator. If you want to print a value x using googletest's value printer yourself, just call ::testing::PrintToString(x) , which returns an std::string : vector < pair < Bar , int > > bar_ints = GetBarIntVector (); EXPECT_TRUE ( IsCorrectBarIntVector ( bar_ints )) << \"bar_ints = \" << :: testing :: PrintToString ( bar_ints );","title":"Teaching googletest How to Print Your Values"},{"location":"examples/gtest/docs/advanced.html#death-tests","text":"In many applications, there are assertions that can cause application failure if a condition is not met. These sanity checks, which ensure that the program is in a known good state, are there to fail at the earliest possible time after some program state is corrupted. If the assertion checks the wrong condition, then the program may proceed in an erroneous state, which could lead to memory corruption, security holes, or worse. Hence it is vitally important to test that such assertion statements work as expected. Since these precondition checks cause the processes to die, we call such tests death tests . More generally, any test that checks that a program terminates (except by throwing an exception) in an expected fashion is also a death test. Note that if a piece of code throws an exception, we don't consider it \"death\" for the purpose of death tests, as the caller of the code could catch the exception and avoid the crash. If you want to verify exceptions thrown by your code, see Exception Assertions . If you want to test EXPECT_*()/ASSERT_*() failures in your test code, see Catching Failures","title":"Death Tests"},{"location":"examples/gtest/docs/advanced.html#how-to-write-a-death-test","text":"googletest has the following macros to support death tests: Fatal assertion Nonfatal assertion Verifies ASSERT_DEATH(statement, matcher); EXPECT_DEATH(statement, matcher); statement crashes with the given error ASSERT_DEATH_IF_SUPPORTED(statement, matcher); EXPECT_DEATH_IF_SUPPORTED(statement, matcher); if death tests are supported, verifies that statement crashes with the given error; otherwise verifies nothing ASSERT_EXIT(statement, predicate, matcher); EXPECT_EXIT(statement, predicate, matcher); statement exits with the given error, and its exit code matches predicate where statement is a statement that is expected to cause the process to die, predicate is a function or function object that evaluates an integer exit status, and matcher is either a GMock matcher matching a const std::string& or a (Perl) regular expression - either of which is matched against the stderr output of statement . For legacy reasons, a bare string (i.e. with no matcher) is interpreted as ContainsRegex(str) , not Eq(str) . Note that statement can be any valid statement (including compound statement ) and doesn't have to be an expression. As usual, the ASSERT variants abort the current test function, while the EXPECT variants do not. NOTE: We use the word \"crash\" here to mean that the process terminates with a non-zero exit status code. There are two possibilities: either the process has called exit() or _exit() with a non-zero value, or it may be killed by a signal. This means that if *statement* terminates the process with a 0 exit code, it is not considered a crash by EXPECT_DEATH . Use EXPECT_EXIT instead if this is the case, or if you want to restrict the exit code more precisely. A predicate here must accept an int and return a bool . The death test succeeds only if the predicate returns true . googletest defines a few predicates that handle the most common cases: :: testing :: ExitedWithCode ( exit_code ) This expression is true if the program exited normally with the given exit code. :: testing :: KilledBySignal ( signal_number ) // Not available on Windows. This expression is true if the program was killed by the given signal. The *_DEATH macros are convenient wrappers for *_EXIT that use a predicate that verifies the process' exit code is non-zero. Note that a death test only cares about three things: does statement abort or exit the process? (in the case of ASSERT_EXIT and EXPECT_EXIT ) does the exit status satisfy predicate ? Or (in the case of ASSERT_DEATH and EXPECT_DEATH ) is the exit status non-zero? And does the stderr output match regex ? In particular, if statement generates an ASSERT_* or EXPECT_* failure, it will not cause the death test to fail, as googletest assertions don't abort the process. To write a death test, simply use one of the above macros inside your test function. For example, TEST ( MyDeathTest , Foo ) { // This death test uses a compound statement. ASSERT_DEATH ({ int n = 5 ; Foo ( & n ); }, \"Error on line .* of Foo()\" ); } TEST ( MyDeathTest , NormalExit ) { EXPECT_EXIT ( NormalExit (), :: testing :: ExitedWithCode ( 0 ), \"Success\" ); } TEST ( MyDeathTest , KillMyself ) { EXPECT_EXIT ( KillMyself (), :: testing :: KilledBySignal ( SIGKILL ), \"Sending myself unblockable signal\" ); } verifies that: calling Foo(5) causes the process to die with the given error message, calling NormalExit() causes the process to print \"Success\" to stderr and exit with exit code 0, and calling KillMyself() kills the process with signal SIGKILL . The test function body may contain other assertions and statements as well, if necessary.","title":"How to Write a Death Test"},{"location":"examples/gtest/docs/advanced.html#death-test-naming","text":"IMPORTANT: We strongly recommend you to follow the convention of naming your test suite (not test) *DeathTest when it contains a death test, as demonstrated in the above example. The Death Tests And Threads section below explains why. If a test fixture class is shared by normal tests and death tests, you can use using or typedef to introduce an alias for the fixture class and avoid duplicating its code: class FooTest : public :: testing :: Test { ... }; using FooDeathTest = FooTest ; TEST_F ( FooTest , DoesThis ) { // normal test } TEST_F ( FooDeathTest , DoesThat ) { // death test }","title":"Death Test Naming"},{"location":"examples/gtest/docs/advanced.html#regular-expression-syntax","text":"On POSIX systems (e.g. Linux, Cygwin, and Mac), googletest uses the POSIX extended regular expression syntax. To learn about this syntax, you may want to read this Wikipedia entry . On Windows, googletest uses its own simple regular expression implementation. It lacks many features. For example, we don't support union ( \"x|y\" ), grouping ( \"(xy)\" ), brackets ( \"[xy]\" ), and repetition count ( \"x{5,7}\" ), among others. Below is what we do support ( A denotes a literal character, period ( . ), or a single \\\\ escape sequence; x and y denote regular expressions.): Expression Meaning c matches any literal character c \\\\d matches any decimal digit \\\\D matches any character that's not a decimal digit \\\\f matches \\f \\\\n matches \\n \\\\r matches \\r \\\\s matches any ASCII whitespace, including \\n \\\\S matches any character that's not a whitespace \\\\t matches \\t \\\\v matches \\v \\\\w matches any letter, _ , or decimal digit \\\\W matches any character that \\\\w doesn't match \\\\c matches any literal character c , which must be a punctuation . matches any single character except \\n A? matches 0 or 1 occurrences of A A* matches 0 or many occurrences of A A+ matches 1 or many occurrences of A ^ matches the beginning of a string (not that of each line) $ matches the end of a string (not that of each line) xy matches x followed by y To help you determine which capability is available on your system, googletest defines macros to govern which regular expression it is using. The macros are: GTEST_USES_SIMPLE_RE=1 or GTEST_USES_POSIX_RE=1 . If you want your death tests to work in all cases, you can either #if on these macros or use the more limited syntax only.","title":"Regular Expression Syntax"},{"location":"examples/gtest/docs/advanced.html#how-it-works","text":"Under the hood, ASSERT_EXIT() spawns a new process and executes the death test statement in that process. The details of how precisely that happens depend on the platform and the variable ::testing::GTEST_FLAG(death_test_style) (which is initialized from the command-line flag --gtest_death_test_style ). On POSIX systems, fork() (or clone() on Linux) is used to spawn the child, after which: If the variable's value is \"fast\" , the death test statement is immediately executed. If the variable's value is \"threadsafe\" , the child process re-executes the unit test binary just as it was originally invoked, but with some extra flags to cause just the single death test under consideration to be run. On Windows, the child is spawned using the CreateProcess() API, and re-executes the binary to cause just the single death test under consideration to be run - much like the threadsafe mode on POSIX. Other values for the variable are illegal and will cause the death test to fail. Currently, the flag's default value is \"fast\" the child's exit status satisfies the predicate, and the child's stderr matches the regular expression. If the death test statement runs to completion without dying, the child process will nonetheless terminate, and the assertion fails.","title":"How It Works"},{"location":"examples/gtest/docs/advanced.html#death-tests-and-threads","text":"The reason for the two death test styles has to do with thread safety. Due to well-known problems with forking in the presence of threads, death tests should be run in a single-threaded context. Sometimes, however, it isn't feasible to arrange that kind of environment. For example, statically-initialized modules may start threads before main is ever reached. Once threads have been created, it may be difficult or impossible to clean them up. googletest has three features intended to raise awareness of threading issues. A warning is emitted if multiple threads are running when a death test is encountered. Test suites with a name ending in \"DeathTest\" are run before all other tests. It uses clone() instead of fork() to spawn the child process on Linux ( clone() is not available on Cygwin and Mac), as fork() is more likely to cause the child to hang when the parent process has multiple threads. It's perfectly fine to create threads inside a death test statement; they are executed in a separate process and cannot affect the parent.","title":"Death Tests And Threads"},{"location":"examples/gtest/docs/advanced.html#death-test-styles","text":"The \"threadsafe\" death test style was introduced in order to help mitigate the risks of testing in a possibly multithreaded environment. It trades increased test execution time (potentially dramatically so) for improved thread safety. The automated testing framework does not set the style flag. You can choose a particular style of death tests by setting the flag programmatically: testing :: FLAGS_gtest_death_test_style = \"threadsafe\" You can do this in main() to set the style for all death tests in the binary, or in individual tests. Recall that flags are saved before running each test and restored afterwards, so you need not do that yourself. For example: int main ( int argc , char ** argv ) { InitGoogle ( argv [ 0 ], & argc , & argv , true ); :: testing :: FLAGS_gtest_death_test_style = \"fast\" ; return RUN_ALL_TESTS (); } TEST ( MyDeathTest , TestOne ) { :: testing :: FLAGS_gtest_death_test_style = \"threadsafe\" ; // This test is run in the \"threadsafe\" style: ASSERT_DEATH ( ThisShouldDie (), \"\" ); } TEST ( MyDeathTest , TestTwo ) { // This test is run in the \"fast\" style: ASSERT_DEATH ( ThisShouldDie (), \"\" ); }","title":"Death Test Styles"},{"location":"examples/gtest/docs/advanced.html#caveats","text":"The statement argument of ASSERT_EXIT() can be any valid C++ statement. If it leaves the current function via a return statement or by throwing an exception, the death test is considered to have failed. Some googletest macros may return from the current function (e.g. ASSERT_TRUE() ), so be sure to avoid them in statement . Since statement runs in the child process, any in-memory side effect (e.g. modifying a variable, releasing memory, etc) it causes will not be observable in the parent process. In particular, if you release memory in a death test, your program will fail the heap check as the parent process will never see the memory reclaimed. To solve this problem, you can try not to free memory in a death test; free the memory again in the parent process; or do not use the heap checker in your program. Due to an implementation detail, you cannot place multiple death test assertions on the same line; otherwise, compilation will fail with an unobvious error message. Despite the improved thread safety afforded by the \"threadsafe\" style of death test, thread problems such as deadlock are still possible in the presence of handlers registered with pthread_atfork(3) .","title":"Caveats"},{"location":"examples/gtest/docs/advanced.html#using-assertions-in-sub-routines","text":"","title":"Using Assertions in Sub-routines"},{"location":"examples/gtest/docs/advanced.html#adding-traces-to-assertions","text":"If a test sub-routine is called from several places, when an assertion inside it fails, it can be hard to tell which invocation of the sub-routine the failure is from. You can alleviate this problem using extra logging or custom failure messages, but that usually clutters up your tests. A better solution is to use the SCOPED_TRACE macro or the ScopedTrace utility: SCOPED_TRACE ( message ); ScopedTrace trace ( \"file_path\" , line_number , message ); where message can be anything streamable to std::ostream . SCOPED_TRACE macro will cause the current file name, line number, and the given message to be added in every failure message. ScopedTrace accepts explicit file name and line number in arguments, which is useful for writing test helpers. The effect will be undone when the control leaves the current lexical scope. For example, 10 : void Sub1 ( int n ) { 11 : EXPECT_EQ ( Bar ( n ), 1 ); 12 : EXPECT_EQ ( Bar ( n + 1 ), 2 ); 13 : } 14 : 15 : TEST ( FooTest , Bar ) { 16 : { 17 : SCOPED_TRACE ( \"A\" ); // This trace point will be included in 18 : // every failure in this scope. 19 : Sub1 ( 1 ); 20 : } 21 : // Now it won't. 22 : Sub1 ( 9 ); 23 : } could result in messages like these: path/to/foo_test.cc:11: Failure Value of: Bar(n) Expected: 1 Actual: 2 Trace: path/to/foo_test.cc:17: A path/to/foo_test.cc:12: Failure Value of: Bar(n + 1) Expected: 2 Actual: 3 Without the trace, it would've been difficult to know which invocation of Sub1() the two failures come from respectively. (You could add an extra message to each assertion in Sub1() to indicate the value of n , but that's tedious.) Some tips on using SCOPED_TRACE : With a suitable message, it's often enough to use SCOPED_TRACE at the beginning of a sub-routine, instead of at each call site. When calling sub-routines inside a loop, make the loop iterator part of the message in SCOPED_TRACE such that you can know which iteration the failure is from. Sometimes the line number of the trace point is enough for identifying the particular invocation of a sub-routine. In this case, you don't have to choose a unique message for SCOPED_TRACE . You can simply use \"\" . You can use SCOPED_TRACE in an inner scope when there is one in the outer scope. In this case, all active trace points will be included in the failure messages, in reverse order they are encountered. The trace dump is clickable in Emacs - hit return on a line number and you'll be taken to that line in the source file!","title":"Adding Traces to Assertions"},{"location":"examples/gtest/docs/advanced.html#propagating-fatal-failures","text":"A common pitfall when using ASSERT_* and FAIL* is not understanding that when they fail they only abort the current function , not the entire test. For example, the following test will segfault: void Subroutine () { // Generates a fatal failure and aborts the current function. ASSERT_EQ ( 1 , 2 ); // The following won't be executed. ... } TEST ( FooTest , Bar ) { Subroutine (); // The intended behavior is for the fatal failure // in Subroutine() to abort the entire test. // The actual behavior: the function goes on after Subroutine() returns. int * p = NULL ; * p = 3 ; // Segfault! } To alleviate this, googletest provides three different solutions. You could use either exceptions, the (ASSERT|EXPECT)_NO_FATAL_FAILURE assertions or the HasFatalFailure() function. They are described in the following two subsections.","title":"Propagating Fatal Failures"},{"location":"examples/gtest/docs/advanced.html#asserting-on-subroutines-with-an-exception","text":"The following code can turn ASSERT-failure into an exception: class ThrowListener : public testing :: EmptyTestEventListener { void OnTestPartResult ( const testing :: TestPartResult & result ) override { if ( result . type () == testing :: TestPartResult :: kFatalFailure ) { throw testing :: AssertionException ( result ); } } }; int main ( int argc , char ** argv ) { ... testing :: UnitTest :: GetInstance () -> listeners (). Append ( new ThrowListener ); return RUN_ALL_TESTS (); } This listener should be added after other listeners if you have any, otherwise they won't see failed OnTestPartResult .","title":"Asserting on Subroutines with an exception"},{"location":"examples/gtest/docs/advanced.html#asserting-on-subroutines","text":"As shown above, if your test calls a subroutine that has an ASSERT_* failure in it, the test will continue after the subroutine returns. This may not be what you want. Often people want fatal failures to propagate like exceptions. For that googletest offers the following macros: Fatal assertion Nonfatal assertion Verifies ASSERT_NO_FATAL_FAILURE(statement); EXPECT_NO_FATAL_FAILURE(statement); statement doesn't generate any new fatal failures in the current thread. Only failures in the thread that executes the assertion are checked to determine the result of this type of assertions. If statement creates new threads, failures in these threads are ignored. Examples: ASSERT_NO_FATAL_FAILURE ( Foo ()); int i ; EXPECT_NO_FATAL_FAILURE ({ i = Bar (); }); Assertions from multiple threads are currently not supported on Windows.","title":"Asserting on Subroutines"},{"location":"examples/gtest/docs/advanced.html#checking-for-failures-in-the-current-test","text":"HasFatalFailure() in the ::testing::Test class returns true if an assertion in the current test has suffered a fatal failure. This allows functions to catch fatal failures in a sub-routine and return early. class Test { public : ... static bool HasFatalFailure (); }; The typical usage, which basically simulates the behavior of a thrown exception, is: TEST ( FooTest , Bar ) { Subroutine (); // Aborts if Subroutine() had a fatal failure. if ( HasFatalFailure ()) return ; // The following won't be executed. ... } If HasFatalFailure() is used outside of TEST() , TEST_F() , or a test fixture, you must add the ::testing::Test:: prefix, as in: if ( :: testing :: Test :: HasFatalFailure ()) return ; Similarly, HasNonfatalFailure() returns true if the current test has at least one non-fatal failure, and HasFailure() returns true if the current test has at least one failure of either kind.","title":"Checking for Failures in the Current Test"},{"location":"examples/gtest/docs/advanced.html#logging-additional-information","text":"In your test code, you can call RecordProperty(\"key\", value) to log additional information, where value can be either a string or an int . The last value recorded for a key will be emitted to the XML output if you specify one. For example, the test TEST_F ( WidgetUsageTest , MinAndMaxWidgets ) { RecordProperty ( \"MaximumWidgets\" , ComputeMaxUsage ()); RecordProperty ( \"MinimumWidgets\" , ComputeMinUsage ()); } will output XML like this: ... <testcase name= \"MinAndMaxWidgets\" status= \"run\" time= \"0.006\" classname= \"WidgetUsageTest\" MaximumWidgets= \"12\" MinimumWidgets= \"9\" /> ... NOTE: RecordProperty() is a static member of the Test class. Therefore it needs to be prefixed with ::testing::Test:: if used outside of the TEST body and the test fixture class. *key* must be a valid XML attribute name, and cannot conflict with the ones already used by googletest ( name , status , time , classname , type_param , and value_param ). Calling RecordProperty() outside of the lifespan of a test is allowed. If it's called outside of a test but between a test suite's SetUpTestSuite() and TearDownTestSuite() methods, it will be attributed to the XML element for the test suite. If it's called outside of all test suites (e.g. in a test environment), it will be attributed to the top-level XML element.","title":"Logging Additional Information"},{"location":"examples/gtest/docs/advanced.html#sharing-resources-between-tests-in-the-same-test-suite","text":"googletest creates a new test fixture object for each test in order to make tests independent and easier to debug. However, sometimes tests use resources that are expensive to set up, making the one-copy-per-test model prohibitively expensive. If the tests don't change the resource, there's no harm in their sharing a single resource copy. So, in addition to per-test set-up/tear-down, googletest also supports per-test-suite set-up/tear-down. To use it: In your test fixture class (say FooTest ), declare as static some member variables to hold the shared resources. Outside your test fixture class (typically just below it), define those member variables, optionally giving them initial values. In the same test fixture class, define a static void SetUpTestSuite() function (remember not to spell it as SetupTestSuite with a small u !) to set up the shared resources and a static void TearDownTestSuite() function to tear them down. That's it! googletest automatically calls SetUpTestSuite() before running the first test in the FooTest test suite (i.e. before creating the first FooTest object), and calls TearDownTestSuite() after running the last test in it (i.e. after deleting the last FooTest object). In between, the tests can use the shared resources. Remember that the test order is undefined, so your code can't depend on a test preceding or following another. Also, the tests must either not modify the state of any shared resource, or, if they do modify the state, they must restore the state to its original value before passing control to the next test. Here's an example of per-test-suite set-up and tear-down: class FooTest : public :: testing :: Test { protected : // Per-test-suite set-up. // Called before the first test in this test suite. // Can be omitted if not needed. static void SetUpTestSuite () { shared_resource_ = new ...; } // Per-test-suite tear-down. // Called after the last test in this test suite. // Can be omitted if not needed. static void TearDownTestSuite () { delete shared_resource_ ; shared_resource_ = NULL ; } // You can define per-test set-up logic as usual. virtual void SetUp () { ... } // You can define per-test tear-down logic as usual. virtual void TearDown () { ... } // Some expensive resource shared by all tests. static T * shared_resource_ ; }; T * FooTest :: shared_resource_ = NULL ; TEST_F ( FooTest , Test1 ) { ... you can refer to shared_resource_ here ... } TEST_F ( FooTest , Test2 ) { ... you can refer to shared_resource_ here ... } NOTE: Though the above code declares SetUpTestSuite() protected, it may sometimes be necessary to declare it public, such as when using it with TEST_P .","title":"Sharing Resources Between Tests in the Same Test Suite"},{"location":"examples/gtest/docs/advanced.html#global-set-up-and-tear-down","text":"Just as you can do set-up and tear-down at the test level and the test suite level, you can also do it at the test program level. Here's how. First, you subclass the ::testing::Environment class to define a test environment, which knows how to set-up and tear-down: class Environment : public :: testing :: Environment { public : virtual ~ Environment () {} // Override this to define how to set up the environment. void SetUp () override {} // Override this to define how to tear down the environment. void TearDown () override {} }; Then, you register an instance of your environment class with googletest by calling the ::testing::AddGlobalTestEnvironment() function: Environment * AddGlobalTestEnvironment ( Environment * env ); Now, when RUN_ALL_TESTS() is called, it first calls the SetUp() method of each environment object, then runs the tests if none of the environments reported fatal failures and GTEST_SKIP() was not called. RUN_ALL_TESTS() always calls TearDown() with each environment object, regardless of whether or not the tests were run. It's OK to register multiple environment objects. In this suite, their SetUp() will be called in the order they are registered, and their TearDown() will be called in the reverse order. Note that googletest takes ownership of the registered environment objects. Therefore do not delete them by yourself. You should call AddGlobalTestEnvironment() before RUN_ALL_TESTS() is called, probably in main() . If you use gtest_main , you need to call this before main() starts for it to take effect. One way to do this is to define a global variable like this: :: testing :: Environment * const foo_env = :: testing :: AddGlobalTestEnvironment ( new FooEnvironment ); However, we strongly recommend you to write your own main() and call AddGlobalTestEnvironment() there, as relying on initialization of global variables makes the code harder to read and may cause problems when you register multiple environments from different translation units and the environments have dependencies among them (remember that the compiler doesn't guarantee the order in which global variables from different translation units are initialized).","title":"Global Set-Up and Tear-Down"},{"location":"examples/gtest/docs/advanced.html#value-parameterized-tests","text":"Value-parameterized tests allow you to test your code with different parameters without writing multiple copies of the same test. This is useful in a number of situations, for example: You have a piece of code whose behavior is affected by one or more command-line flags. You want to make sure your code performs correctly for various values of those flags. You want to test different implementations of an OO interface. You want to test your code over various inputs (a.k.a. data-driven testing). This feature is easy to abuse, so please exercise your good sense when doing it!","title":"Value-Parameterized Tests"},{"location":"examples/gtest/docs/advanced.html#how-to-write-value-parameterized-tests","text":"To write value-parameterized tests, first you should define a fixture class. It must be derived from both testing::Test and testing::WithParamInterface<T> (the latter is a pure interface), where T is the type of your parameter values. For convenience, you can just derive the fixture class from testing::TestWithParam<T> , which itself is derived from both testing::Test and testing::WithParamInterface<T> . T can be any copyable type. If it's a raw pointer, you are responsible for managing the lifespan of the pointed values. NOTE: If your test fixture defines SetUpTestSuite() or TearDownTestSuite() they must be declared public rather than protected in order to use TEST_P . class FooTest : public testing :: TestWithParam < const char *> { // You can implement all the usual fixture class members here. // To access the test parameter, call GetParam() from class // TestWithParam<T>. }; // Or, when you want to add parameters to a pre-existing fixture class: class BaseTest : public testing :: Test { ... }; class BarTest : public BaseTest , public testing :: WithParamInterface < const char *> { ... }; Then, use the TEST_P macro to define as many test patterns using this fixture as you want. The _P suffix is for \"parameterized\" or \"pattern\", whichever you prefer to think. TEST_P ( FooTest , DoesBlah ) { // Inside a test, access the test parameter with the GetParam() method // of the TestWithParam<T> class: EXPECT_TRUE ( foo . Blah ( GetParam ())); ... } TEST_P ( FooTest , HasBlahBlah ) { ... } Finally, you can use INSTANTIATE_TEST_SUITE_P to instantiate the test suite with any set of parameters you want. googletest defines a number of functions for generating test parameters. They return what we call (surprise!) parameter generators . Here is a summary of them, which are all in the testing namespace: Parameter Generator Behavior Range(begin, end [, step]) Yields values {begin, begin+step, begin+step+step, ...} . The values do not include end . step defaults to 1. Values(v1, v2, ..., vN) Yields values {v1, v2, ..., vN} . ValuesIn(container) and ValuesIn(begin,end) Yields values from a C-style array, an STL-style container, or an iterator range [begin, end) Bool() Yields sequence {false, true} . Combine(g1, g2, ..., gN) Yields all combinations (Cartesian product) as std\\:\\:tuples of the values generated by the N generators. For more details, see the comments at the definitions of these functions. The following statement will instantiate tests from the FooTest test suite each with parameter values \"meeny\" , \"miny\" , and \"moe\" . INSTANTIATE_TEST_SUITE_P ( InstantiationName , FooTest , testing :: Values ( \"meeny\" , \"miny\" , \"moe\" )); NOTE: The code above must be placed at global or namespace scope, not at function scope. NOTE: Don't forget this step! If you do your test will silently pass, but none of its suites will ever run! To distinguish different instances of the pattern (yes, you can instantiate it more than once), the first argument to INSTANTIATE_TEST_SUITE_P is a prefix that will be added to the actual test suite name. Remember to pick unique prefixes for different instantiations. The tests from the instantiation above will have these names: InstantiationName/FooTest.DoesBlah/0 for \"meeny\" InstantiationName/FooTest.DoesBlah/1 for \"miny\" InstantiationName/FooTest.DoesBlah/2 for \"moe\" InstantiationName/FooTest.HasBlahBlah/0 for \"meeny\" InstantiationName/FooTest.HasBlahBlah/1 for \"miny\" InstantiationName/FooTest.HasBlahBlah/2 for \"moe\" You can use these names in --gtest_filter . This statement will instantiate all tests from FooTest again, each with parameter values \"cat\" and \"dog\" : const char * pets [] = { \"cat\" , \"dog\" }; INSTANTIATE_TEST_SUITE_P ( AnotherInstantiationName , FooTest , testing :: ValuesIn ( pets )); The tests from the instantiation above will have these names: AnotherInstantiationName/FooTest.DoesBlah/0 for \"cat\" AnotherInstantiationName/FooTest.DoesBlah/1 for \"dog\" AnotherInstantiationName/FooTest.HasBlahBlah/0 for \"cat\" AnotherInstantiationName/FooTest.HasBlahBlah/1 for \"dog\" Please note that INSTANTIATE_TEST_SUITE_P will instantiate all tests in the given test suite, whether their definitions come before or after the INSTANTIATE_TEST_SUITE_P statement. You can see $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample7_unittest.cc and $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample8_unittest.cc for more examples. $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample7_unittest.cc for \"Parameterized Test example\" $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample8_unittest.cc for \"Parameterized Test example with multiple parameters\"","title":"How to Write Value-Parameterized Tests"},{"location":"examples/gtest/docs/advanced.html#creating-value-parameterized-abstract-tests","text":"In the above, we define and instantiate FooTest in the same source file. Sometimes you may want to define value-parameterized tests in a library and let other people instantiate them later. This pattern is known as abstract tests . As an example of its application, when you are designing an interface you can write a standard suite of abstract tests (perhaps using a factory function as the test parameter) that all implementations of the interface are expected to pass. When someone implements the interface, they can instantiate your suite to get all the interface-conformance tests for free. To define abstract tests, you should organize your code like this: Put the definition of the parameterized test fixture class (e.g. FooTest ) in a header file, say foo_param_test.h . Think of this as declaring your abstract tests. Put the TEST_P definitions in foo_param_test.cc , which includes foo_param_test.h . Think of this as implementing your abstract tests. Once they are defined, you can instantiate them by including foo_param_test.h , invoking INSTANTIATE_TEST_SUITE_P() , and depending on the library target that contains foo_param_test.cc . You can instantiate the same abstract test suite multiple times, possibly in different source files.","title":"Creating Value-Parameterized Abstract Tests"},{"location":"examples/gtest/docs/advanced.html#specifying-names-for-value-parameterized-test-parameters","text":"The optional last argument to INSTANTIATE_TEST_SUITE_P() allows the user to specify a function or functor that generates custom test name suffixes based on the test parameters. The function should accept one argument of type testing::TestParamInfo<class ParamType> , and return std::string . testing::PrintToStringParamName is a builtin test suffix generator that returns the value of testing::PrintToString(GetParam()) . It does not work for std::string or C strings. NOTE: test names must be non-empty, unique, and may only contain ASCII alphanumeric characters. In particular, they should not contain underscores class MyTestSuite : public testing :: TestWithParam < int > {}; TEST_P ( MyTestSuite , MyTest ) { std :: cout << \"Example Test Param: \" << GetParam () << std :: endl ; } INSTANTIATE_TEST_SUITE_P ( MyGroup , MyTestSuite , testing :: Range ( 0 , 10 ), testing :: PrintToStringParamName ()); Providing a custom functor allows for more control over test parameter name generation, especially for types where the automatic conversion does not generate helpful parameter names (e.g. strings as demonstrated above). The following example illustrates this for multiple parameters, an enumeration type and a string, and also demonstrates how to combine generators. It uses a lambda for conciseness: enum class MyType { MY_FOO = 0 , MY_BAR = 1 }; class MyTestSuite : public testing :: TestWithParam < std :: tuple < MyType , string >> { }; INSTANTIATE_TEST_SUITE_P ( MyGroup , MyTestSuite , testing :: Combine ( testing :: Values ( MyType :: VALUE_0 , MyType :: VALUE_1 ), testing :: ValuesIn ( \"\" , \"\" )), []( const testing :: TestParamInfo < MyTestSuite :: ParamType >& info ) { string name = absl :: StrCat ( std :: get < 0 > ( info . param ) == MY_FOO ? \"Foo\" : \"Bar\" , \"_\" , std :: get < 1 > ( info . param )); absl :: c_replace_if ( name , []( char c ) { return ! std :: isalnum ( c ); }, '_' ); return name ; });","title":"Specifying Names for Value-Parameterized Test Parameters"},{"location":"examples/gtest/docs/advanced.html#typed-tests","text":"Suppose you have multiple implementations of the same interface and want to make sure that all of them satisfy some common requirements. Or, you may have defined several types that are supposed to conform to the same \"concept\" and you want to verify it. In both cases, you want the same test logic repeated for different types. While you can write one TEST or TEST_F for each type you want to test (and you may even factor the test logic into a function template that you invoke from the TEST ), it's tedious and doesn't scale: if you want m tests over n types, you'll end up writing m*n TEST s. Typed tests allow you to repeat the same test logic over a list of types. You only need to write the test logic once, although you must know the type list when writing typed tests. Here's how you do it: First, define a fixture class template. It should be parameterized by a type. Remember to derive it from ::testing::Test : template < typename T > class FooTest : public :: testing :: Test { public : ... typedef std :: list < T > List ; static T shared_ ; T value_ ; }; Next, associate a list of types with the test suite, which will be repeated for each type in the list: using MyTypes = :: testing :: Types < char , int , unsigned int > ; TYPED_TEST_SUITE ( FooTest , MyTypes ); The type alias ( using or typedef ) is necessary for the TYPED_TEST_SUITE macro to parse correctly. Otherwise the compiler will think that each comma in the type list introduces a new macro argument. Then, use TYPED_TEST() instead of TEST_F() to define a typed test for this test suite. You can repeat this as many times as you want: TYPED_TEST ( FooTest , DoesBlah ) { // Inside a test, refer to the special name TypeParam to get the type // parameter. Since we are inside a derived class template, C++ requires // us to visit the members of FooTest via 'this'. TypeParam n = this -> value_ ; // To visit static members of the fixture, add the 'TestFixture::' // prefix. n += TestFixture :: shared_ ; // To refer to typedefs in the fixture, add the 'typename TestFixture::' // prefix. The 'typename' is required to satisfy the compiler. typename TestFixture :: List values ; values . push_back ( n ); ... } TYPED_TEST ( FooTest , HasPropertyA ) { ... } You can see $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample6_unittest.cc for a complete \"Typed Test example\"","title":"Typed Tests"},{"location":"examples/gtest/docs/advanced.html#type-parameterized-tests","text":"Type-parameterized tests are like typed tests, except that they don't require you to know the list of types ahead of time. Instead, you can define the test logic first and instantiate it with different type lists later. You can even instantiate it more than once in the same program. If you are designing an interface or concept, you can define a suite of type-parameterized tests to verify properties that any valid implementation of the interface/concept should have. Then, the author of each implementation can just instantiate the test suite with their type to verify that it conforms to the requirements, without having to write similar tests repeatedly. Here's an example: First, define a fixture class template, as we did with typed tests: template < typename T > class FooTest : public :: testing :: Test { ... }; Next, declare that you will define a type-parameterized test suite: TYPED_TEST_SUITE_P ( FooTest ); Then, use TYPED_TEST_P() to define a type-parameterized test. You can repeat this as many times as you want: TYPED_TEST_P ( FooTest , DoesBlah ) { // Inside a test, refer to TypeParam to get the type parameter. TypeParam n = 0 ; ... } TYPED_TEST_P ( FooTest , HasPropertyA ) { ... } Now the tricky part: you need to register all test patterns using the REGISTER_TYPED_TEST_SUITE_P macro before you can instantiate them. The first argument of the macro is the test suite name; the rest are the names of the tests in this test suite: REGISTER_TYPED_TEST_SUITE_P ( FooTest , DoesBlah , HasPropertyA ); Finally, you are free to instantiate the pattern with the types you want. If you put the above code in a header file, you can #include it in multiple C++ source files and instantiate it multiple times. typedef :: testing :: Types < char , int , unsigned int > MyTypes ; INSTANTIATE_TYPED_TEST_SUITE_P ( My , FooTest , MyTypes ); To distinguish different instances of the pattern, the first argument to the INSTANTIATE_TYPED_TEST_SUITE_P macro is a prefix that will be added to the actual test suite name. Remember to pick unique prefixes for different instances. In the special case where the type list contains only one type, you can write that type directly without ::testing::Types<...> , like this: INSTANTIATE_TYPED_TEST_SUITE_P ( My , FooTest , int ); You can see $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample6_unittest.cc for a complete example.","title":"Type-Parameterized Tests"},{"location":"examples/gtest/docs/advanced.html#testing-private-code","text":"If you change your software's internal implementation, your tests should not break as long as the change is not observable by users. Therefore, per the black-box testing principle, most of the time you should test your code through its public interfaces. If you still find yourself needing to test internal implementation code, consider if there's a better design. The desire to test internal implementation is often a sign that the class is doing too much. Consider extracting an implementation class, and testing it. Then use that implementation class in the original class. If you absolutely have to test non-public interface code though, you can. There are two cases to consider: Static functions ( not the same as static member functions!) or unnamed namespaces, and Private or protected class members To test them, we use the following special techniques: Both static functions and definitions/declarations in an unnamed namespace are only visible within the same translation unit. To test them, you can #include the entire .cc file being tested in your *_test.cc file. (#including .cc files is not a good way to reuse code - you should not do this in production code!) However, a better approach is to move the private code into the foo::internal namespace, where foo is the namespace your project normally uses, and put the private declarations in a *-internal.h file. Your production .cc files and your tests are allowed to include this internal header, but your clients are not. This way, you can fully test your internal implementation without leaking it to your clients. Private class members are only accessible from within the class or by friends. To access a class' private members, you can declare your test fixture as a friend to the class and define accessors in your fixture. Tests using the fixture can then access the private members of your production class via the accessors in the fixture. Note that even though your fixture is a friend to your production class, your tests are not automatically friends to it, as they are technically defined in sub-classes of the fixture. Another way to test private members is to refactor them into an implementation class, which is then declared in a *-internal.h file. Your clients aren't allowed to include this header but your tests can. Such is called the Pimpl (Private Implementation) idiom. Or, you can declare an individual test as a friend of your class by adding this line in the class body: FRIEND_TEST ( TestSuiteName , TestName ); For example, // foo.h class Foo { ... private : FRIEND_TEST ( FooTest , BarReturnsZeroOnNull ); int Bar ( void * x ); }; // foo_test.cc ... TEST ( FooTest , BarReturnsZeroOnNull ) { Foo foo ; EXPECT_EQ ( foo . Bar ( NULL ), 0 ); // Uses Foo's private member Bar(). } Pay special attention when your class is defined in a namespace, as you should define your test fixtures and tests in the same namespace if you want them to be friends of your class. For example, if the code to be tested looks like: namespace my_namespace { class Foo { friend class FooTest ; FRIEND_TEST ( FooTest , Bar ); FRIEND_TEST ( FooTest , Baz ); ... definition of the class Foo ... }; } // namespace my_namespace Your test code should be something like: namespace my_namespace { class FooTest : public :: testing :: Test { protected : ... }; TEST_F ( FooTest , Bar ) { ... } TEST_F ( FooTest , Baz ) { ... } } // namespace my_namespace","title":"Testing Private Code"},{"location":"examples/gtest/docs/advanced.html#catching-failures","text":"If you are building a testing utility on top of googletest, you'll want to test your utility. What framework would you use to test it? googletest, of course. The challenge is to verify that your testing utility reports failures correctly. In frameworks that report a failure by throwing an exception, you could catch the exception and assert on it. But googletest doesn't use exceptions, so how do we test that a piece of code generates an expected failure? gunit-spi.h contains some constructs to do this. After #including this header, you can use EXPECT_FATAL_FAILURE ( statement , substring ); to assert that statement generates a fatal (e.g. ASSERT_* ) failure in the current thread whose message contains the given substring , or use EXPECT_NONFATAL_FAILURE ( statement , substring ); if you are expecting a non-fatal (e.g. EXPECT_* ) failure. Only failures in the current thread are checked to determine the result of this type of expectations. If statement creates new threads, failures in these threads are also ignored. If you want to catch failures in other threads as well, use one of the following macros instead: EXPECT_FATAL_FAILURE_ON_ALL_THREADS ( statement , substring ); EXPECT_NONFATAL_FAILURE_ON_ALL_THREADS ( statement , substring ); NOTE: Assertions from multiple threads are currently not supported on Windows. For technical reasons, there are some caveats: You cannot stream a failure message to either macro. statement in EXPECT_FATAL_FAILURE{_ON_ALL_THREADS}() cannot reference local non-static variables or non-static members of this object. statement in EXPECT_FATAL_FAILURE{_ON_ALL_THREADS}() cannot return a value.","title":"\"Catching\" Failures"},{"location":"examples/gtest/docs/advanced.html#registering-tests-programmatically","text":"The TEST macros handle the vast majority of all use cases, but there are few were runtime registration logic is required. For those cases, the framework provides the ::testing::RegisterTest that allows callers to register arbitrary tests dynamically. This is an advanced API only to be used when the TEST macros are insufficient. The macros should be preferred when possible, as they avoid most of the complexity of calling this function. It provides the following signature: template < typename Factory > TestInfo * RegisterTest ( const char * test_suite_name , const char * test_name , const char * type_param , const char * value_param , const char * file , int line , Factory factory ); The factory argument is a factory callable (move-constructible) object or function pointer that creates a new instance of the Test object. It handles ownership to the caller. The signature of the callable is Fixture*() , where Fixture is the test fixture class for the test. All tests registered with the same test_suite_name must return the same fixture type. This is checked at runtime. The framework will infer the fixture class from the factory and will call the SetUpTestSuite and TearDownTestSuite for it. Must be called before RUN_ALL_TESTS() is invoked, otherwise behavior is undefined. Use case example: class MyFixture : public :: testing :: Test { public : // All of these optional, just like in regular macro usage. static void SetUpTestSuite () { ... } static void TearDownTestSuite () { ... } void SetUp () override { ... } void TearDown () override { ... } }; class MyTest : public MyFixture { public : explicit MyTest ( int data ) : data_ ( data ) {} void TestBody () override { ... } private : int data_ ; }; void RegisterMyTests ( const std :: vector < int >& values ) { for ( int v : values ) { :: testing :: RegisterTest ( \"MyFixture\" , ( \"Test\" + std :: to_string ( v )). c_str (), nullptr , std :: to_string ( v ). c_str (), __FILE__ , __LINE__ , // Important to use the fixture type as the return type here. [ = ]() -> MyFixture * { return new MyTest ( v ); }); } } ... int main ( int argc , char ** argv ) { std :: vector < int > values_to_test = LoadValuesFromConfig (); RegisterMyTests ( values_to_test ); ... return RUN_ALL_TESTS (); }","title":"Registering tests programmatically"},{"location":"examples/gtest/docs/advanced.html#getting-the-current-tests-name","text":"Sometimes a function may need to know the name of the currently running test. For example, you may be using the SetUp() method of your test fixture to set the golden file name based on which test is running. The ::testing::TestInfo class has this information: namespace testing { class TestInfo { public : // Returns the test suite name and the test name, respectively. // // Do NOT delete or free the return value - it's managed by the // TestInfo class. const char * test_suite_name () const ; const char * name () const ; }; } To obtain a TestInfo object for the currently running test, call current_test_info() on the UnitTest singleton object: // Gets information about the currently running test. // Do NOT delete the returned object - it's managed by the UnitTest class. const :: testing :: TestInfo * const test_info = :: testing :: UnitTest :: GetInstance () -> current_test_info (); printf ( \"We are in test %s of test suite %s. \\n \" , test_info -> name (), test_info -> test_suite_name ()); current_test_info() returns a null pointer if no test is running. In particular, you cannot find the test suite name in TestSuiteSetUp() , TestSuiteTearDown() (where you know the test suite name implicitly), or functions called from them.","title":"Getting the Current Test's Name"},{"location":"examples/gtest/docs/advanced.html#extending-googletest-by-handling-test-events","text":"googletest provides an event listener API to let you receive notifications about the progress of a test program and test failures. The events you can listen to include the start and end of the test program, a test suite, or a test method, among others. You may use this API to augment or replace the standard console output, replace the XML output, or provide a completely different form of output, such as a GUI or a database. You can also use test events as checkpoints to implement a resource leak checker, for example.","title":"Extending googletest by Handling Test Events"},{"location":"examples/gtest/docs/advanced.html#defining-event-listeners","text":"To define a event listener, you subclass either testing::TestEventListener or testing::EmptyTestEventListener The former is an (abstract) interface, where each pure virtual method can be overridden to handle a test event (For example, when a test starts, the OnTestStart() method will be called.). The latter provides an empty implementation of all methods in the interface, such that a subclass only needs to override the methods it cares about. When an event is fired, its context is passed to the handler function as an argument. The following argument types are used: UnitTest reflects the state of the entire test program, TestSuite has information about a test suite, which can contain one or more tests, TestInfo contains the state of a test, and TestPartResult represents the result of a test assertion. An event handler function can examine the argument it receives to find out interesting information about the event and the test program's state. Here's an example: class MinimalistPrinter : public :: testing :: EmptyTestEventListener { // Called before a test starts. virtual void OnTestStart ( const :: testing :: TestInfo & test_info ) { printf ( \"*** Test %s.%s starting. \\n \" , test_info . test_suite_name (), test_info . name ()); } // Called after a failed assertion or a SUCCESS(). virtual void OnTestPartResult ( const :: testing :: TestPartResult & test_part_result ) { printf ( \"%s in %s:%d \\n %s \\n \" , test_part_result . failed () ? \"*** Failure\" : \"Success\" , test_part_result . file_name (), test_part_result . line_number (), test_part_result . summary ()); } // Called after a test ends. virtual void OnTestEnd ( const :: testing :: TestInfo & test_info ) { printf ( \"*** Test %s.%s ending. \\n \" , test_info . test_suite_name (), test_info . name ()); } };","title":"Defining Event Listeners"},{"location":"examples/gtest/docs/advanced.html#using-event-listeners","text":"To use the event listener you have defined, add an instance of it to the googletest event listener list (represented by class TestEventListeners - note the \"s\" at the end of the name) in your main() function, before calling RUN_ALL_TESTS() : int main ( int argc , char ** argv ) { :: testing :: InitGoogleTest ( & argc , argv ); // Gets hold of the event listener list. :: testing :: TestEventListeners & listeners = :: testing :: UnitTest :: GetInstance () -> listeners (); // Adds a listener to the end. googletest takes the ownership. listeners . Append ( new MinimalistPrinter ); return RUN_ALL_TESTS (); } There's only one problem: the default test result printer is still in effect, so its output will mingle with the output from your minimalist printer. To suppress the default printer, just release it from the event listener list and delete it. You can do so by adding one line: ... delete listeners . Release ( listeners . default_result_printer ()); listeners . Append ( new MinimalistPrinter ); return RUN_ALL_TESTS (); Now, sit back and enjoy a completely different output from your tests. For more details, see $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample9_unittest.cc . You may append more than one listener to the list. When an On*Start() or OnTestPartResult() event is fired, the listeners will receive it in the order they appear in the list (since new listeners are added to the end of the list, the default text printer and the default XML generator will receive the event first). An On*End() event will be received by the listeners in the reverse order. This allows output by listeners added later to be framed by output from listeners added earlier.","title":"Using Event Listeners"},{"location":"examples/gtest/docs/advanced.html#generating-failures-in-listeners","text":"You may use failure-raising macros ( EXPECT_*() , ASSERT_*() , FAIL() , etc) when processing an event. There are some restrictions: You cannot generate any failure in OnTestPartResult() (otherwise it will cause OnTestPartResult() to be called recursively). A listener that handles OnTestPartResult() is not allowed to generate any failure. When you add listeners to the listener list, you should put listeners that handle OnTestPartResult() before listeners that can generate failures. This ensures that failures generated by the latter are attributed to the right test by the former. See $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample10_unittest.cc for an example of a failure-raising listener.","title":"Generating Failures in Listeners"},{"location":"examples/gtest/docs/advanced.html#running-test-programs-advanced-options","text":"googletest test programs are ordinary executables. Once built, you can run them directly and affect their behavior via the following environment variables and/or command line flags. For the flags to work, your programs must call ::testing::InitGoogleTest() before calling RUN_ALL_TESTS() . To see a list of supported flags and their usage, please run your test program with the --help flag. You can also use -h , -? , or /? for short. If an option is specified both by an environment variable and by a flag, the latter takes precedence.","title":"Running Test Programs: Advanced Options"},{"location":"examples/gtest/docs/advanced.html#selecting-tests","text":"","title":"Selecting Tests"},{"location":"examples/gtest/docs/advanced.html#listing-test-names","text":"Sometimes it is necessary to list the available tests in a program before running them so that a filter may be applied if needed. Including the flag --gtest_list_tests overrides all other flags and lists tests in the following format: TestSuite1. TestName1 TestName2 TestSuite2. TestName None of the tests listed are actually run if the flag is provided. There is no corresponding environment variable for this flag.","title":"Listing Test Names"},{"location":"examples/gtest/docs/advanced.html#running-a-subset-of-the-tests","text":"By default, a googletest program runs all tests the user has defined. Sometimes, you want to run only a subset of the tests (e.g. for debugging or quickly verifying a change). If you set the GTEST_FILTER environment variable or the --gtest_filter flag to a filter string, googletest will only run the tests whose full names (in the form of TestSuiteName.TestName ) match the filter. The format of a filter is a ' : '-separated list of wildcard patterns (called the positive patterns ) optionally followed by a ' - ' and another ' : '-separated pattern list (called the negative patterns ). A test matches the filter if and only if it matches any of the positive patterns but does not match any of the negative patterns. A pattern may contain '*' (matches any string) or '?' (matches any single character). For convenience, the filter '*-NegativePatterns' can be also written as '-NegativePatterns' . For example: ./foo_test Has no flag, and thus runs all its tests. ./foo_test --gtest_filter=* Also runs everything, due to the single match-everything * value. ./foo_test --gtest_filter=FooTest.* Runs everything in test suite FooTest . ./foo_test --gtest_filter=*Null*:*Constructor* Runs any test whose full name contains either \"Null\" or \"Constructor\" . ./foo_test --gtest_filter=-*DeathTest.* Runs all non-death tests. ./foo_test --gtest_filter=FooTest.*-FooTest.Bar Runs everything in test suite FooTest except FooTest.Bar . ./foo_test --gtest_filter=FooTest.*:BarTest.*-FooTest.Bar:BarTest.Foo Runs everything in test suite FooTest except FooTest.Bar and everything in test suite BarTest except BarTest.Foo .","title":"Running a Subset of the Tests"},{"location":"examples/gtest/docs/advanced.html#temporarily-disabling-tests","text":"If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0 , as disabled tests are still compiled (and thus won't rot). If you need to disable all tests in a test suite, you can either add DISABLED_ to the front of the name of each test, or alternatively add it to the front of the test suite name. For example, the following tests won't be run by googletest, even though they will still be compiled: // Tests that Foo does Abc. TEST ( FooTest , DISABLED_DoesAbc ) { ... } class DISABLED_BarTest : public :: testing :: Test { ... }; // Tests that Bar does Xyz. TEST_F ( DISABLED_BarTest , DoesXyz ) { ... } NOTE: This feature should only be used for temporary pain-relief. You still have to fix the disabled tests at a later date. As a reminder, googletest will print a banner warning you if a test program contains any disabled tests. TIP: You can easily count the number of disabled tests you have using gsearch and/or grep . This number can be used as a metric for improving your test quality.","title":"Temporarily Disabling Tests"},{"location":"examples/gtest/docs/advanced.html#temporarily-enabling-disabled-tests","text":"To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag or set the GTEST_ALSO_RUN_DISABLED_TESTS environment variable to a value other than 0 . You can combine this with the --gtest_filter flag to further select which disabled tests to run.","title":"Temporarily Enabling Disabled Tests"},{"location":"examples/gtest/docs/advanced.html#repeating-the-tests","text":"Once in a while you'll run into a test whose result is hit-or-miss. Perhaps it will fail only 1% of the time, making it rather hard to reproduce the bug under a debugger. This can be a major source of frustration. The --gtest_repeat flag allows you to repeat all (or selected) test methods in a program many times. Hopefully, a flaky test will eventually fail and give you a chance to debug. Here's how to use it: $ foo_test --gtest_repeat=1000 Repeat foo_test 1000 times and don't stop at failures. $ foo_test --gtest_repeat=-1 A negative count means repeating forever. $ foo_test --gtest_repeat=1000 --gtest_break_on_failure Repeat foo_test 1000 times, stopping at the first failure. This is especially useful when running under a debugger: when the test fails, it will drop into the debugger and you can then inspect variables and stacks. $ foo_test --gtest_repeat=1000 --gtest_filter=FooBar.* Repeat the tests whose name matches the filter 1000 times. If your test program contains global set-up/tear-down code, it will be repeated in each iteration as well, as the flakiness may be in it. You can also specify the repeat count by setting the GTEST_REPEAT environment variable.","title":"Repeating the Tests"},{"location":"examples/gtest/docs/advanced.html#shuffling-the-tests","text":"You can specify the --gtest_shuffle flag (or set the GTEST_SHUFFLE environment variable to 1 ) to run the tests in a program in a random order. This helps to reveal bad dependencies between tests. By default, googletest uses a random seed calculated from the current time. Therefore you'll get a different order every time. The console output includes the random seed value, such that you can reproduce an order-related test failure later. To specify the random seed explicitly, use the --gtest_random_seed=SEED flag (or set the GTEST_RANDOM_SEED environment variable), where SEED is an integer in the range [0, 99999]. The seed value 0 is special: it tells googletest to do the default behavior of calculating the seed from the current time. If you combine this with --gtest_repeat=N , googletest will pick a different random seed and re-shuffle the tests in each iteration.","title":"Shuffling the Tests"},{"location":"examples/gtest/docs/advanced.html#controlling-test-output","text":"","title":"Controlling Test Output"},{"location":"examples/gtest/docs/advanced.html#colored-terminal-output","text":"googletest can use colors in its terminal output to make it easier to spot the important information: ... [----------] 1 test from FooTest [ RUN ] FooTest.DoesAbc [ OK ] FooTest.DoesAbc [----------] 2 tests from BarTest [ RUN ] BarTest.HasXyzProperty [ OK ] BarTest.HasXyzProperty [ RUN ] BarTest.ReturnsTrueOnSuccess ... some error messages ... [ FAILED ] BarTest.ReturnsTrueOnSuccess ... [==========] 30 tests from 14 test suites ran. [ PASSED ] 28 tests. [ FAILED ] 2 tests, listed below: [ FAILED ] BarTest.ReturnsTrueOnSuccess [ FAILED ] AnotherTest.DoesXyz 2 FAILED TESTS You can set the GTEST_COLOR environment variable or the --gtest_color command line flag to yes , no , or auto (the default) to enable colors, disable colors, or let googletest decide. When the value is auto , googletest will use colors if and only if the output goes to a terminal and (on non-Windows platforms) the TERM environment variable is set to xterm or xterm-color .","title":"Colored Terminal Output"},{"location":"examples/gtest/docs/advanced.html#suppressing-the-elapsed-time","text":"By default, googletest prints the time it takes to run each test. To disable that, run the test program with the --gtest_print_time=0 command line flag, or set the GTEST_PRINT_TIME environment variable to 0 .","title":"Suppressing the Elapsed Time"},{"location":"examples/gtest/docs/advanced.html#suppressing-utf-8-text-output","text":"In case of assertion failures, googletest prints expected and actual values of type string both as hex-encoded strings as well as in readable UTF-8 text if they contain valid non-ASCII UTF-8 characters. If you want to suppress the UTF-8 text because, for example, you don't have an UTF-8 compatible output medium, run the test program with --gtest_print_utf8=0 or set the GTEST_PRINT_UTF8 environment variable to 0 .","title":"Suppressing UTF-8 Text Output"},{"location":"examples/gtest/docs/advanced.html#generating-an-xml-report","text":"googletest can emit a detailed XML report to a file in addition to its normal textual output. The report contains the duration of each test, and thus can help you identify slow tests. The report is also used by the http://unittest dashboard to show per-test-method error messages. To generate the XML report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"xml:path_to_output_file\" , which will create the file at the given location. You can also just use the string \"xml\" , in which case the output can be found in the test_detail.xml file in the current directory. If you specify a directory (for example, \"xml:output/directory/\" on Linux or \"xml:output\\directory\\\" on Windows), googletest will create the XML file in that directory, named after the test executable (e.g. foo_test.xml for test program foo_test or foo_test.exe ). If the file already exists (perhaps left over from a previous run), googletest will pick a different name (e.g. foo_test_1.xml ) to avoid overwriting it. The report is based on the junitreport Ant task. Since that format was originally intended for Java, a little interpretation is required to make it apply to googletest tests, as shown here: <testsuites name= \"AllTests\" ... > <testsuite name= \"test_case_name\" ... > <testcase name= \"test_name\" ... > <failure message= \"...\" /> <failure message= \"...\" /> <failure message= \"...\" /> </testcase> </testsuite> </testsuites> The root <testsuites> element corresponds to the entire test program. <testsuite> elements correspond to googletest test suites. <testcase> elements correspond to googletest test functions. For instance, the following program TEST ( MathTest , Addition ) { ... } TEST ( MathTest , Subtraction ) { ... } TEST ( LogicTest , NonContradiction ) { ... } could generate this report: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <testsuites tests= \"3\" failures= \"1\" errors= \"0\" time= \"0.035\" timestamp= \"2011-10-31T18:52:42\" name= \"AllTests\" > <testsuite name= \"MathTest\" tests= \"2\" failures= \"1\" errors= \"0\" time= \"0.015\" > <testcase name= \"Addition\" status= \"run\" time= \"0.007\" classname= \"\" > <failure message= \"Value of: add(1, 1)&#x0A; Actual: 3&#x0A;Expected: 2\" type= \"\" > ... </failure> <failure message= \"Value of: add(1, -1)&#x0A; Actual: 1&#x0A;Expected: 0\" type= \"\" > ... </failure> </testcase> <testcase name= \"Subtraction\" status= \"run\" time= \"0.005\" classname= \"\" > </testcase> </testsuite> <testsuite name= \"LogicTest\" tests= \"1\" failures= \"0\" errors= \"0\" time= \"0.005\" > <testcase name= \"NonContradiction\" status= \"run\" time= \"0.005\" classname= \"\" > </testcase> </testsuite> </testsuites> Things to note: The tests attribute of a <testsuites> or <testsuite> element tells how many test functions the googletest program or test suite contains, while the failures attribute tells how many of them failed. The time attribute expresses the duration of the test, test suite, or entire test program in seconds. The timestamp attribute records the local date and time of the test execution. Each <failure> element corresponds to a single failed googletest assertion.","title":"Generating an XML Report"},{"location":"examples/gtest/docs/advanced.html#generating-a-json-report","text":"googletest can also emit a JSON report as an alternative format to XML. To generate the JSON report, set the GTEST_OUTPUT environment variable or the --gtest_output flag to the string \"json:path_to_output_file\" , which will create the file at the given location. You can also just use the string \"json\" , in which case the output can be found in the test_detail.json file in the current directory. The report format conforms to the following JSON Schema: { \"$schema\" : \"http://json-schema.org/schema#\" , \"type\" : \"object\" , \"definitions\" : { \"TestCase\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"tests\" : { \"type\" : \"integer\" }, \"failures\" : { \"type\" : \"integer\" }, \"disabled\" : { \"type\" : \"integer\" }, \"time\" : { \"type\" : \"string\" }, \"testsuite\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/TestInfo\" } } } }, \"TestInfo\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"status\" : { \"type\" : \"string\" , \"enum\" : [ \"RUN\" , \"NOTRUN\" ] }, \"time\" : { \"type\" : \"string\" }, \"classname\" : { \"type\" : \"string\" }, \"failures\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/Failure\" } } } }, \"Failure\" : { \"type\" : \"object\" , \"properties\" : { \"failures\" : { \"type\" : \"string\" }, \"type\" : { \"type\" : \"string\" } } } }, \"properties\" : { \"tests\" : { \"type\" : \"integer\" }, \"failures\" : { \"type\" : \"integer\" }, \"disabled\" : { \"type\" : \"integer\" }, \"errors\" : { \"type\" : \"integer\" }, \"timestamp\" : { \"type\" : \"string\" , \"format\" : \"date-time\" }, \"time\" : { \"type\" : \"string\" }, \"name\" : { \"type\" : \"string\" }, \"testsuites\" : { \"type\" : \"array\" , \"items\" : { \"$ref\" : \"#/definitions/TestCase\" } } } } The report uses the format that conforms to the following Proto3 using the JSON encoding : syntax = \"proto3\" ; package googletest ; import \"google/protobuf/timestamp.proto\" ; import \"google/protobuf/duration.proto\" ; message UnitTest { int32 tests = 1 ; int32 failures = 2 ; int32 disabled = 3 ; int32 errors = 4 ; google.protobuf.Timestamp timestamp = 5 ; google.protobuf.Duration time = 6 ; string name = 7 ; repeated TestCase testsuites = 8 ; } message TestCase { string name = 1 ; int32 tests = 2 ; int32 failures = 3 ; int32 disabled = 4 ; int32 errors = 5 ; google.protobuf.Duration time = 6 ; repeated TestInfo testsuite = 7 ; } message TestInfo { string name = 1 ; enum Status { RUN = 0 ; NOTRUN = 1 ; } Status status = 2 ; google.protobuf.Duration time = 3 ; string classname = 4 ; message Failure { string failures = 1 ; string type = 2 ; } repeated Failure failures = 5 ; } For instance, the following program TEST ( MathTest , Addition ) { ... } TEST ( MathTest , Subtraction ) { ... } TEST ( LogicTest , NonContradiction ) { ... } could generate this report: { \"tests\" : 3 , \"failures\" : 1 , \"errors\" : 0 , \"time\" : \"0.035s\" , \"timestamp\" : \"2011-10-31T18:52:42Z\" , \"name\" : \"AllTests\" , \"testsuites\" : [ { \"name\" : \"MathTest\" , \"tests\" : 2 , \"failures\" : 1 , \"errors\" : 0 , \"time\" : \"0.015s\" , \"testsuite\" : [ { \"name\" : \"Addition\" , \"status\" : \"RUN\" , \"time\" : \"0.007s\" , \"classname\" : \"\" , \"failures\" : [ { \"message\" : \"Value of: add(1, 1)\\n Actual: 3\\nExpected: 2\" , \"type\" : \"\" }, { \"message\" : \"Value of: add(1, -1)\\n Actual: 1\\nExpected: 0\" , \"type\" : \"\" } ] }, { \"name\" : \"Subtraction\" , \"status\" : \"RUN\" , \"time\" : \"0.005s\" , \"classname\" : \"\" } ] }, { \"name\" : \"LogicTest\" , \"tests\" : 1 , \"failures\" : 0 , \"errors\" : 0 , \"time\" : \"0.005s\" , \"testsuite\" : [ { \"name\" : \"NonContradiction\" , \"status\" : \"RUN\" , \"time\" : \"0.005s\" , \"classname\" : \"\" } ] } ] } IMPORTANT: The exact format of the JSON document is subject to change.","title":"Generating a JSON Report"},{"location":"examples/gtest/docs/advanced.html#controlling-how-failures-are-reported","text":"","title":"Controlling How Failures Are Reported"},{"location":"examples/gtest/docs/advanced.html#turning-assertion-failures-into-break-points","text":"When running test programs under a debugger, it's very convenient if the debugger can catch an assertion failure and automatically drop into interactive mode. googletest's break-on-failure mode supports this behavior. To enable it, set the GTEST_BREAK_ON_FAILURE environment variable to a value other than 0 . Alternatively, you can use the --gtest_break_on_failure command line flag.","title":"Turning Assertion Failures into Break-Points"},{"location":"examples/gtest/docs/advanced.html#disabling-catching-test-thrown-exceptions","text":"googletest can be used either with or without exceptions enabled. If a test throws a C++ exception or (on Windows) a structured exception (SEH), by default googletest catches it, reports it as a test failure, and continues with the next test method. This maximizes the coverage of a test run. Also, on Windows an uncaught exception will cause a pop-up window, so catching the exceptions allows you to run the tests automatically. When debugging the test failures, however, you may instead want the exceptions to be handled by the debugger, such that you can examine the call stack when an exception is thrown. To achieve that, set the GTEST_CATCH_EXCEPTIONS environment variable to 0 , or use the --gtest_catch_exceptions=0 flag when running the tests.","title":"Disabling Catching Test-Thrown Exceptions"},{"location":"examples/gtest/docs/cheat_sheet.html","text":"gMock Cheat Sheet Defining a Mock Class Mocking a Normal Class Given class Foo { ... virtual ~ Foo (); virtual int GetSize () const = 0 ; virtual string Describe ( const char * name ) = 0 ; virtual string Describe ( int type ) = 0 ; virtual bool Process ( Bar elem , int count ) = 0 ; }; (note that ~Foo() must be virtual) we can define its mock as #include \"gmock/gmock.h\" class MockFoo : public Foo { ... MOCK_METHOD ( int , GetSize , (), ( const , override )); MOCK_METHOD ( string , Describe , ( const char * name ), ( override )); MOCK_METHOD ( string , Describe , ( int type ), ( override )); MOCK_METHOD ( bool , Process , ( Bar elem , int count ), ( override )); }; To create a \"nice\" mock, which ignores all uninteresting calls, a \"naggy\" mock, which warns on all uninteresting calls, or a \"strict\" mock, which treats them as failures: using :: testing :: NiceMock ; using :: testing :: NaggyMock ; using :: testing :: StrictMock ; NiceMock < MockFoo > nice_foo ; // The type is a subclass of MockFoo. NaggyMock < MockFoo > naggy_foo ; // The type is a subclass of MockFoo. StrictMock < MockFoo > strict_foo ; // The type is a subclass of MockFoo. Note: A mock object is currently naggy by default. We may make it nice by default in the future. Mocking a Class Template Class templates can be mocked just like any class. To mock template < typename Elem > class StackInterface { ... virtual ~ StackInterface (); virtual int GetSize () const = 0 ; virtual void Push ( const Elem & x ) = 0 ; }; (note that all member functions that are mocked, including ~StackInterface() must be virtual). template < typename Elem > class MockStack : public StackInterface < Elem > { ... MOCK_METHOD ( int , GetSize , (), ( const , override )); MOCK_METHOD ( void , Push , ( const Elem & x ), ( override )); }; Specifying Calling Conventions for Mock Functions If your mock function doesn't use the default calling convention, you can specify it by adding Calltype(convention) to MOCK_METHOD 's 4th parameter. For example, MOCK_METHOD ( bool , Foo , ( int n ), ( Calltype ( STDMETHODCALLTYPE ))); MOCK_METHOD ( int , Bar , ( double x , double y ), ( const , Calltype ( STDMETHODCALLTYPE ))); where STDMETHODCALLTYPE is defined by <objbase.h> on Windows. Using Mocks in Tests The typical work flow is: Import the gMock names you need to use. All gMock symbols are in the testing namespace unless they are macros or otherwise noted. Create the mock objects. Optionally, set the default actions of the mock objects. Set your expectations on the mock objects (How will they be called? What will they do?). Exercise code that uses the mock objects; if necessary, check the result using googletest assertions. When a mock object is destructed, gMock automatically verifies that all expectations on it have been satisfied. Here's an example: using :: testing :: Return ; // #1 TEST ( BarTest , DoesThis ) { MockFoo foo ; // #2 ON_CALL ( foo , GetSize ()) // #3 . WillByDefault ( Return ( 1 )); // ... other default actions ... EXPECT_CALL ( foo , Describe ( 5 )) // #4 . Times ( 3 ) . WillRepeatedly ( Return ( \"Category 5\" )); // ... other expectations ... EXPECT_EQ ( \"good\" , MyProductionFunction ( & foo )); // #5 } // #6 Setting Default Actions gMock has a built-in default action for any function that returns void , bool , a numeric value, or a pointer. In C++11, it will additionally returns the default-constructed value, if one exists for the given type. To customize the default action for functions with return type T : using :: testing :: DefaultValue ; // Sets the default value to be returned. T must be CopyConstructible. DefaultValue < T >:: Set ( value ); // Sets a factory. Will be invoked on demand. T must be MoveConstructible. // T MakeT(); DefaultValue < T >:: SetFactory ( & MakeT ); // ... use the mocks ... // Resets the default value. DefaultValue < T >:: Clear (); Example usage: // Sets the default action for return type std::unique_ptr<Buzz> to // creating a new Buzz every time. DefaultValue < std :: unique_ptr < Buzz >>:: SetFactory ( [] { return MakeUnique < Buzz > ( AccessLevel :: kInternal ); }); // When this fires, the default action of MakeBuzz() will run, which // will return a new Buzz object. EXPECT_CALL ( mock_buzzer_ , MakeBuzz ( \"hello\" )). Times ( AnyNumber ()); auto buzz1 = mock_buzzer_ . MakeBuzz ( \"hello\" ); auto buzz2 = mock_buzzer_ . MakeBuzz ( \"hello\" ); EXPECT_NE ( nullptr , buzz1 ); EXPECT_NE ( nullptr , buzz2 ); EXPECT_NE ( buzz1 , buzz2 ); // Resets the default action for return type std::unique_ptr<Buzz>, // to avoid interfere with other tests. DefaultValue < std :: unique_ptr < Buzz >>:: Clear (); To customize the default action for a particular method of a specific mock object, use ON_CALL() . ON_CALL() has a similar syntax to EXPECT_CALL() , but it is used for setting default behaviors (when you do not require that the mock method is called). See here for a more detailed discussion. ON_CALL ( mock - object , method ( matchers )) . With ( multi - argument - matcher ) ? . WillByDefault ( action ); Setting Expectations EXPECT_CALL() sets expectations on a mock method (How will it be called? What will it do?): EXPECT_CALL ( mock - object , method ( matchers ) ? ) . With ( multi - argument - matcher ) ? . Times ( cardinality ) ? . InSequence ( sequences ) * . After ( expectations ) * . WillOnce ( action ) * . WillRepeatedly ( action ) ? . RetiresOnSaturation (); ? For each item above, ? means it can be used at most once, while * means it can be used any number of times. In order to pass, EXPECT_CALL must be used before the calls are actually made. The (matchers) is a comma-separated list of matchers that correspond to each of the arguments of method , and sets the expectation only for calls of method that matches all of the matchers. If (matchers) is omitted, the expectation is the same as if the matchers were set to anything matchers (for example, (_, _, _, _) for a four-arg method). If Times() is omitted, the cardinality is assumed to be: Times(1) when there is neither WillOnce() nor WillRepeatedly() ; Times(n) when there are n WillOnce() s but no WillRepeatedly() , where n >= 1; or Times(AtLeast(n)) when there are n WillOnce() s and a WillRepeatedly() , where n >= 0. A method with no EXPECT_CALL() is free to be invoked any number of times , and the default action will be taken each time. Matchers A matcher matches a single argument. You can use it inside ON_CALL() or EXPECT_CALL() , or use it to validate a value directly using two macros: Macro Description EXPECT_THAT(actual_value, matcher) Asserts that actual_value matches matcher . ASSERT_THAT(actual_value, matcher) The same as EXPECT_THAT(actual_value, matcher) , except that it generates a fatal failure. Built-in matchers (where argument is the function argument, e.g. actual_value in the example above, or when used in the context of EXPECT_CALL(mock_object, method(matchers)) , the arguments of method ) are divided into several categories: Wildcard Matcher Description _ argument can be any value of the correct type. A<type>() or An<type>() argument can be any value of type type . Generic Comparison Matcher Description Eq(value) or value argument == value Ge(value) argument >= value Gt(value) argument > value Le(value) argument <= value Lt(value) argument < value Ne(value) argument != value IsFalse() argument evaluates to false in a Boolean context. IsTrue() argument evaluates to true in a Boolean context. IsNull() argument is a NULL pointer (raw or smart). NotNull() argument is a non-null pointer (raw or smart). Optional(m) argument is optional<> that contains a value matching m . VariantWith<T>(m) argument is variant<> that holds the alternative of type T with a value matching m . Ref(variable) argument is a reference to variable . TypedEq<type>(value) argument has type type and is equal to value . You may need to use this instead of Eq(value) when the mock function is overloaded. Except Ref() , these matchers make a copy of value in case it's modified or destructed later. If the compiler complains that value doesn't have a public copy constructor, try wrap it in ByRef() , e.g. Eq(ByRef(non_copyable_value)) . If you do that, make sure non_copyable_value is not changed afterwards, or the meaning of your matcher will be changed. Floating-Point Matchers Matcher Description DoubleEq(a_double) argument is a double value approximately equal to a_double , treating two NaNs as unequal. FloatEq(a_float) argument is a float value approximately equal to a_float , treating two NaNs as unequal. NanSensitiveDoubleEq(a_double) argument is a double value approximately equal to a_double , treating two NaNs as equal. NanSensitiveFloatEq(a_float) argument is a float value approximately equal to a_float , treating two NaNs as equal. The above matchers use ULP-based comparison (the same as used in googletest). They automatically pick a reasonable error bound based on the absolute value of the expected value. DoubleEq() and FloatEq() conform to the IEEE standard, which requires comparing two NaNs for equality to return false. The NanSensitive* version instead treats two NaNs as equal, which is often what a user wants. Matcher Description DoubleNear(a_double, max_abs_error) argument is a double value close to a_double (absolute error <= max_abs_error ), treating two NaNs as unequal. FloatNear(a_float, max_abs_error) argument is a float value close to a_float (absolute error <= max_abs_error ), treating two NaNs as unequal. NanSensitiveDoubleNear(a_double, max_abs_error) argument is a double value close to a_double (absolute error <= max_abs_error ), treating two NaNs as equal. NanSensitiveFloatNear(a_float, max_abs_error) argument is a float value close to a_float (absolute error <= max_abs_error ), treating two NaNs as equal. String Matchers The argument can be either a C string or a C++ string object: Matcher Description ContainsRegex(string) argument matches the given regular expression. EndsWith(suffix) argument ends with string suffix . HasSubstr(string) argument contains string as a sub-string. MatchesRegex(string) argument matches the given regular expression with the match starting at the first character and ending at the last character. StartsWith(prefix) argument starts with string prefix . StrCaseEq(string) argument is equal to string , ignoring case. StrCaseNe(string) argument is not equal to string , ignoring case. StrEq(string) argument is equal to string . StrNe(string) argument is not equal to string . ContainsRegex() and MatchesRegex() take ownership of the RE object. They use the regular expression syntax defined here . StrCaseEq() , StrCaseNe() , StrEq() , and StrNe() work for wide strings as well. Container Matchers Most STL-style containers support == , so you can use Eq(expected_container) or simply expected_container to match a container exactly. If you want to write the elements in-line, match them more flexibly, or get more informative messages, you can use: Matcher Description BeginEndDistanceIs(m) argument is a container whose begin() and end() iterators are separated by a number of increments matching m . E.g. BeginEndDistanceIs(2) or BeginEndDistanceIs(Lt(2)) . For containers that define a size() method, SizeIs(m) may be more efficient. ContainerEq(container) The same as Eq(container) except that the failure message also includes which elements are in one container but not the other. Contains(e) argument contains an element that matches e , which can be either a value or a matcher. Each(e) argument is a container where every element matches e , which can be either a value or a matcher. ElementsAre(e0, e1, ..., en) argument has n + 1 elements, where the i -th element matches ei , which can be a value or a matcher. ElementsAreArray({e0, e1, ..., en}) , ElementsAreArray(a_container) , ElementsAreArray(begin, end) , ElementsAreArray(array) , or ElementsAreArray(array, count) The same as ElementsAre() except that the expected element values/matchers come from an initializer list, STL-style container, iterator range, or C-style array. IsEmpty() argument is an empty container ( container.empty() ). IsSubsetOf({e0, e1, ..., en}) , IsSubsetOf(a_container) , IsSubsetOf(begin, end) , IsSubsetOf(array) , or IsSubsetOf(array, count) argument matches UnorderedElementsAre(x0, x1, ..., xk) for some subset {x0, x1, ..., xk} of the expected matchers. IsSupersetOf({e0, e1, ..., en}) , IsSupersetOf(a_container) , IsSupersetOf(begin, end) , IsSupersetOf(array) , or IsSupersetOf(array, count) Some subset of argument matches UnorderedElementsAre( expected matchers ) . Pointwise(m, container) , Pointwise(m, {e0, e1, ..., en}) argument contains the same number of elements as in container , and for all i, (the i-th element in argument , the i-th element in container ) match m , which is a matcher on 2-tuples. E.g. Pointwise(Le(), upper_bounds) verifies that each element in argument doesn't exceed the corresponding element in upper_bounds . See more detail below. SizeIs(m) argument is a container whose size matches m . E.g. SizeIs(2) or SizeIs(Lt(2)) . UnorderedElementsAre(e0, e1, ..., en) argument has n + 1 elements, and under some permutation of the elements, each element matches an ei (for a different i ), which can be a value or a matcher. UnorderedElementsAreArray({e0, e1, ..., en}) , UnorderedElementsAreArray(a_container) , UnorderedElementsAreArray(begin, end) , UnorderedElementsAreArray(array) , or UnorderedElementsAreArray(array, count) The same as UnorderedElementsAre() except that the expected element values/matchers come from an initializer list, STL-style container, iterator range, or C-style array. UnorderedPointwise(m, container) , UnorderedPointwise(m, {e0, e1, ..., en}) Like Pointwise(m, container) , but ignores the order of elements. WhenSorted(m) When argument is sorted using the < operator, it matches container matcher m . E.g. WhenSorted(ElementsAre(1, 2, 3)) verifies that argument contains elements 1, 2, and 3, ignoring order. WhenSortedBy(comparator, m) The same as WhenSorted(m) , except that the given comparator instead of < is used to sort argument . E.g. WhenSortedBy(std::greater(), ElementsAre(3, 2, 1)) . Notes: These matchers can also match: a native array passed by reference (e.g. in Foo(const int (&a)[5]) ), and an array passed as a pointer and a count (e.g. in Bar(const T* buffer, int len) -- see Multi-argument Matchers ). The array being matched may be multi-dimensional (i.e. its elements can be arrays). m in Pointwise(m, ...) should be a matcher for ::std::tuple<T, U> where T and U are the element type of the actual container and the expected container, respectively. For example, to compare two Foo containers where Foo doesn't support operator== , one might write: using :: std :: get ; MATCHER ( FooEq , \"\" ) { return std :: get < 0 > ( arg ). Equals ( std :: get < 1 > ( arg )); } ... EXPECT_THAT ( actual_foos , Pointwise ( FooEq (), expected_foos )); Member Matchers Matcher Description Field(&class::field, m) argument.field (or argument->field when argument is a plain pointer) matches matcher m , where argument is an object of type class . Key(e) argument.first matches e , which can be either a value or a matcher. E.g. Contains(Key(Le(5))) can verify that a map contains a key <= 5 . Pair(m1, m2) argument is an std::pair whose first field matches m1 and second field matches m2 . Property(&class::property, m) argument.property() (or argument->property() when argument is a plain pointer) matches matcher m , where argument is an object of type class . Matching the Result of a Function, Functor, or Callback Matcher Description ResultOf(f, m) f(argument) matches matcher m , where f is a function or functor. Pointer Matchers Matcher Description Pointee(m) argument (either a smart pointer or a raw pointer) points to a value that matches matcher m . WhenDynamicCastTo<T>(m) when argument is passed through dynamic_cast<T>() , it matches matcher m . Multi-argument Matchers Technically, all matchers match a single value. A \"multi-argument\" matcher is just one that matches a tuple . The following matchers can be used to match a tuple (x, y) : Matcher Description Eq() x == y Ge() x >= y Gt() x > y Le() x <= y Lt() x < y Ne() x != y You can use the following selectors to pick a subset of the arguments (or reorder them) to participate in the matching: Matcher Description AllArgs(m) Equivalent to m . Useful as syntactic sugar in .With(AllArgs(m)) . Args<N1, N2, ..., Nk>(m) The tuple of the k selected (using 0-based indices) arguments matches m , e.g. Args<1, 2>(Eq()) . Composite Matchers You can make a matcher from one or more other matchers: Matcher Description AllOf(m1, m2, ..., mn) argument matches all of the matchers m1 to mn . AllOfArray({m0, m1, ..., mn}) , AllOfArray(a_container) , AllOfArray(begin, end) , AllOfArray(array) , or AllOfArray(array, count) The same as AllOf() except that the matchers come from an initializer list, STL-style container, iterator range, or C-style array. AnyOf(m1, m2, ..., mn) argument matches at least one of the matchers m1 to mn . AnyOfArray({m0, m1, ..., mn}) , AnyOfArray(a_container) , AnyOfArray(begin, end) , AnyOfArray(array) , or AnyOfArray(array, count) The same as AnyOf() except that the matchers come from an initializer list, STL-style container, iterator range, or C-style array. Not(m) argument doesn't match matcher m . Adapters for Matchers Matcher Description MatcherCast<T>(m) casts matcher m to type Matcher<T> . SafeMatcherCast<T>(m) safely casts matcher m to type Matcher<T> . Truly(predicate) predicate(argument) returns something considered by C++ to be true, where predicate is a function or functor. AddressSatisfies(callback) and Truly(callback) take ownership of callback , which must be a permanent callback. Using Matchers as Predicates Matcher Description Matches(m)(value) evaluates to true if value matches m . You can use Matches(m) alone as a unary functor. ExplainMatchResult(m, value, result_listener) evaluates to true if value matches m , explaining the result to result_listener . Value(value, m) evaluates to true if value matches m . Defining Matchers Matcher Description MATCHER(IsEven, \"\") { return (arg % 2) == 0; } Defines a matcher IsEven() to match an even number. MATCHER_P(IsDivisibleBy, n, \"\") { *result_listener << \"where the remainder is \" << (arg % n); return (arg % n) == 0; } Defines a macher IsDivisibleBy(n) to match a number divisible by n . MATCHER_P2(IsBetween, a, b, std::string(negation ? \"isn't\" : \"is\") + \" between \" + PrintToString(a) + \" and \" + PrintToString(b)) { return a <= arg && arg <= b; } Defines a matcher IsBetween(a, b) to match a value in the range [ a , b ]. Notes: The MATCHER* macros cannot be used inside a function or class. The matcher body must be purely functional (i.e. it cannot have any side effect, and the result must not depend on anything other than the value being matched and the matcher parameters). You can use PrintToString(x) to convert a value x of any type to a string. Actions Actions specify what a mock function should do when invoked. Returning a Value Return() Return from a void mock function. Return(value) Return value . If the type of value is different to the mock function's return type, value is converted to the latter type at the time the expectation is set , not when the action is executed. ReturnArg<N>() Return the N -th (0-based) argument. ReturnNew<T>(a1, ..., ak) Return new T(a1, ..., ak) ; a different object is created each time. ReturnNull() Return a null pointer. ReturnPointee(ptr) Return the value pointed to by ptr . ReturnRef(variable) Return a reference to variable . ReturnRefOfCopy(value) Return a reference to a copy of value ; the copy lives as long as the action. Side Effects Assign(&variable, value) Assign value to variable. DeleteArg<N>() Delete the N -th (0-based) argument, which must be a pointer. SaveArg<N>(pointer) Save the N -th (0-based) argument to *pointer . SaveArgPointee<N>(pointer) Save the value pointed to by the N -th (0-based) argument to *pointer . SetArgReferee<N>(value) Assign value to the variable referenced by the N -th (0-based) argument. SetArgPointee<N>(value) Assign value to the variable pointed by the N -th (0-based) argument. SetArgumentPointee<N>(value) Same as SetArgPointee<N>(value) . Deprecated. Will be removed in v1.7.0. SetArrayArgument<N>(first, last) Copies the elements in source range [ first , last ) to the array pointed to by the N -th (0-based) argument, which can be either a pointer or an iterator. The action does not take ownership of the elements in the source range. SetErrnoAndReturn(error, value) Set errno to error and return value . Throw(exception) Throws the given exception, which can be any copyable value. Available since v1.1.0. Using a Function, Functor, or Lambda as an Action In the following, by \"callable\" we mean a free function, std::function , functor, or lambda. f Invoke f with the arguments passed to the mock function, where f is a callable. Invoke(f) Invoke f with the arguments passed to the mock function, where f can be a global/static function or a functor. Invoke(object_pointer, &class::method) Invoke the method on the object with the arguments passed to the mock function. InvokeWithoutArgs(f) Invoke f , which can be a global/static function or a functor. f must take no arguments. InvokeWithoutArgs(object_pointer, &class::method) Invoke the method on the object, which takes no arguments. InvokeArgument<N>(arg1, arg2, ..., argk) Invoke the mock function's N -th (0-based) argument, which must be a function or a functor, with the k arguments. The return value of the invoked function is used as the return value of the action. When defining a callable to be used with Invoke*() , you can declare any unused parameters as Unused : using :: testing :: Invoke ; double Distance ( Unused , double x , double y ) { return sqrt ( x * x + y * y ); } ... EXPECT_CALL ( mock , Foo ( \"Hi\" , _ , _ )). WillOnce ( Invoke ( Distance )); Invoke(callback) and InvokeWithoutArgs(callback) take ownership of callback , which must be permanent. The type of callback must be a base callback type instead of a derived one, e.g. BlockingClosure * done = new BlockingClosure ; ... Invoke ( done ) ...; // This won't compile! Closure * done2 = new BlockingClosure ; ... Invoke ( done2 ) ...; // This works. In InvokeArgument<N>(...) , if an argument needs to be passed by reference, wrap it inside ByRef() . For example, using :: testing :: ByRef ; using :: testing :: InvokeArgument ; ... InvokeArgument < 2 > ( 5 , string ( \"Hi\" ), ByRef ( foo )) calls the mock function's #2 argument, passing to it 5 and string(\"Hi\") by value, and foo by reference. Default Action Matcher Description DoDefault() Do the default action (specified by ON_CALL() or the built-in one). Note: due to technical reasons, DoDefault() cannot be used inside a composite action - trying to do so will result in a run-time error. Composite Actions DoAll(a1, a2, ..., an) Do all actions a1 to an and return the result of an in each invocation. The first n - 1 sub-actions must return void. IgnoreResult(a) Perform action a and ignore its result. a must not return void. WithArg<N>(a) Pass the N -th (0-based) argument of the mock function to action a and perform it. WithArgs<N1, N2, ..., Nk>(a) Pass the selected (0-based) arguments of the mock function to action a and perform it. WithoutArgs(a) Perform action a without any arguments. Defining Actions `struct SumAction {` \u2003`template ` \u2003`T operator()(T x, Ty) { return x + y; }` `};` Defines a generic functor that can be used as an action summing its arguments. ACTION(Sum) { return arg0 + arg1; } Defines an action Sum() to return the sum of the mock function's argument #0 and #1. ACTION_P(Plus, n) { return arg0 + n; } Defines an action Plus(n) to return the sum of the mock function's argument #0 and n . ACTION_Pk(Foo, p1, ..., pk) { statements; } Defines a parameterized action Foo(p1, ..., pk) to execute the given statements . The ACTION* macros cannot be used inside a function or class. Cardinalities These are used in Times() to specify how many times a mock function will be called: AnyNumber() The function can be called any number of times. AtLeast(n) The call is expected at least n times. AtMost(n) The call is expected at most n times. Between(m, n) The call is expected between m and n (inclusive) times. Exactly(n) or n The call is expected exactly n times. In particular, the call should never happen when n is 0. Expectation Order By default, the expectations can be matched in any order. If some or all expectations must be matched in a given order, there are two ways to specify it. They can be used either independently or together. The After Clause using :: testing :: Expectation ; ... Expectation init_x = EXPECT_CALL ( foo , InitX ()); Expectation init_y = EXPECT_CALL ( foo , InitY ()); EXPECT_CALL ( foo , Bar ()) . After ( init_x , init_y ); says that Bar() can be called only after both InitX() and InitY() have been called. If you don't know how many pre-requisites an expectation has when you write it, you can use an ExpectationSet to collect them: using :: testing :: ExpectationSet ; ... ExpectationSet all_inits ; for ( int i = 0 ; i < element_count ; i ++ ) { all_inits += EXPECT_CALL ( foo , InitElement ( i )); } EXPECT_CALL ( foo , Bar ()) . After ( all_inits ); says that Bar() can be called only after all elements have been initialized (but we don't care about which elements get initialized before the others). Modifying an ExpectationSet after using it in an .After() doesn't affect the meaning of the .After() . Sequences When you have a long chain of sequential expectations, it's easier to specify the order using sequences , which don't require you to given each expectation in the chain a different name. All expected calls in the same sequence must occur in the order they are specified. using :: testing :: Return ; using :: testing :: Sequence ; Sequence s1 , s2 ; ... EXPECT_CALL ( foo , Reset ()) . InSequence ( s1 , s2 ) . WillOnce ( Return ( true )); EXPECT_CALL ( foo , GetSize ()) . InSequence ( s1 ) . WillOnce ( Return ( 1 )); EXPECT_CALL ( foo , Describe ( A < const char *> ())) . InSequence ( s2 ) . WillOnce ( Return ( \"dummy\" )); says that Reset() must be called before both GetSize() and Describe() , and the latter two can occur in any order. To put many expectations in a sequence conveniently: using :: testing :: InSequence ; { InSequence seq ; EXPECT_CALL (...)...; EXPECT_CALL (...)...; ... EXPECT_CALL (...)...; } says that all expected calls in the scope of seq must occur in strict order. The name seq is irrelevant. Verifying and Resetting a Mock gMock will verify the expectations on a mock object when it is destructed, or you can do it earlier: using :: testing :: Mock ; ... // Verifies and removes the expectations on mock_obj; // returns true if and only if successful. Mock :: VerifyAndClearExpectations ( & mock_obj ); ... // Verifies and removes the expectations on mock_obj; // also removes the default actions set by ON_CALL(); // returns true if and only if successful. Mock :: VerifyAndClear ( & mock_obj ); You can also tell gMock that a mock object can be leaked and doesn't need to be verified: Mock :: AllowLeak ( & mock_obj ); Mock Classes gMock defines a convenient mock class template class MockFunction < R ( A1 , ..., An ) > { public : MOCK_METHOD ( R , Call , ( A1 , ..., An )); }; See this recipe for one application of it. Flags Flag Description --gmock_catch_leaked_mocks=0 Don't report leaked mock objects as failures. --gmock_verbose=LEVEL Sets the default verbosity level ( info , warning , or error ) of Google Mock messages.","title":"Cheat sheet"},{"location":"examples/gtest/docs/cheat_sheet.html#gmock-cheat-sheet","text":"","title":"gMock Cheat Sheet"},{"location":"examples/gtest/docs/cheat_sheet.html#defining-a-mock-class","text":"","title":"Defining a Mock Class"},{"location":"examples/gtest/docs/cheat_sheet.html#MockClass","text":"Given class Foo { ... virtual ~ Foo (); virtual int GetSize () const = 0 ; virtual string Describe ( const char * name ) = 0 ; virtual string Describe ( int type ) = 0 ; virtual bool Process ( Bar elem , int count ) = 0 ; }; (note that ~Foo() must be virtual) we can define its mock as #include \"gmock/gmock.h\" class MockFoo : public Foo { ... MOCK_METHOD ( int , GetSize , (), ( const , override )); MOCK_METHOD ( string , Describe , ( const char * name ), ( override )); MOCK_METHOD ( string , Describe , ( int type ), ( override )); MOCK_METHOD ( bool , Process , ( Bar elem , int count ), ( override )); }; To create a \"nice\" mock, which ignores all uninteresting calls, a \"naggy\" mock, which warns on all uninteresting calls, or a \"strict\" mock, which treats them as failures: using :: testing :: NiceMock ; using :: testing :: NaggyMock ; using :: testing :: StrictMock ; NiceMock < MockFoo > nice_foo ; // The type is a subclass of MockFoo. NaggyMock < MockFoo > naggy_foo ; // The type is a subclass of MockFoo. StrictMock < MockFoo > strict_foo ; // The type is a subclass of MockFoo. Note: A mock object is currently naggy by default. We may make it nice by default in the future.","title":"Mocking a Normal Class"},{"location":"examples/gtest/docs/cheat_sheet.html#MockTemplate","text":"Class templates can be mocked just like any class. To mock template < typename Elem > class StackInterface { ... virtual ~ StackInterface (); virtual int GetSize () const = 0 ; virtual void Push ( const Elem & x ) = 0 ; }; (note that all member functions that are mocked, including ~StackInterface() must be virtual). template < typename Elem > class MockStack : public StackInterface < Elem > { ... MOCK_METHOD ( int , GetSize , (), ( const , override )); MOCK_METHOD ( void , Push , ( const Elem & x ), ( override )); };","title":"Mocking a Class Template"},{"location":"examples/gtest/docs/cheat_sheet.html#specifying-calling-conventions-for-mock-functions","text":"If your mock function doesn't use the default calling convention, you can specify it by adding Calltype(convention) to MOCK_METHOD 's 4th parameter. For example, MOCK_METHOD ( bool , Foo , ( int n ), ( Calltype ( STDMETHODCALLTYPE ))); MOCK_METHOD ( int , Bar , ( double x , double y ), ( const , Calltype ( STDMETHODCALLTYPE ))); where STDMETHODCALLTYPE is defined by <objbase.h> on Windows.","title":"Specifying Calling Conventions for Mock Functions"},{"location":"examples/gtest/docs/cheat_sheet.html#UsingMocks","text":"The typical work flow is: Import the gMock names you need to use. All gMock symbols are in the testing namespace unless they are macros or otherwise noted. Create the mock objects. Optionally, set the default actions of the mock objects. Set your expectations on the mock objects (How will they be called? What will they do?). Exercise code that uses the mock objects; if necessary, check the result using googletest assertions. When a mock object is destructed, gMock automatically verifies that all expectations on it have been satisfied. Here's an example: using :: testing :: Return ; // #1 TEST ( BarTest , DoesThis ) { MockFoo foo ; // #2 ON_CALL ( foo , GetSize ()) // #3 . WillByDefault ( Return ( 1 )); // ... other default actions ... EXPECT_CALL ( foo , Describe ( 5 )) // #4 . Times ( 3 ) . WillRepeatedly ( Return ( \"Category 5\" )); // ... other expectations ... EXPECT_EQ ( \"good\" , MyProductionFunction ( & foo )); // #5 } // #6","title":"Using Mocks in Tests"},{"location":"examples/gtest/docs/cheat_sheet.html#OnCall","text":"gMock has a built-in default action for any function that returns void , bool , a numeric value, or a pointer. In C++11, it will additionally returns the default-constructed value, if one exists for the given type. To customize the default action for functions with return type T : using :: testing :: DefaultValue ; // Sets the default value to be returned. T must be CopyConstructible. DefaultValue < T >:: Set ( value ); // Sets a factory. Will be invoked on demand. T must be MoveConstructible. // T MakeT(); DefaultValue < T >:: SetFactory ( & MakeT ); // ... use the mocks ... // Resets the default value. DefaultValue < T >:: Clear (); Example usage: // Sets the default action for return type std::unique_ptr<Buzz> to // creating a new Buzz every time. DefaultValue < std :: unique_ptr < Buzz >>:: SetFactory ( [] { return MakeUnique < Buzz > ( AccessLevel :: kInternal ); }); // When this fires, the default action of MakeBuzz() will run, which // will return a new Buzz object. EXPECT_CALL ( mock_buzzer_ , MakeBuzz ( \"hello\" )). Times ( AnyNumber ()); auto buzz1 = mock_buzzer_ . MakeBuzz ( \"hello\" ); auto buzz2 = mock_buzzer_ . MakeBuzz ( \"hello\" ); EXPECT_NE ( nullptr , buzz1 ); EXPECT_NE ( nullptr , buzz2 ); EXPECT_NE ( buzz1 , buzz2 ); // Resets the default action for return type std::unique_ptr<Buzz>, // to avoid interfere with other tests. DefaultValue < std :: unique_ptr < Buzz >>:: Clear (); To customize the default action for a particular method of a specific mock object, use ON_CALL() . ON_CALL() has a similar syntax to EXPECT_CALL() , but it is used for setting default behaviors (when you do not require that the mock method is called). See here for a more detailed discussion. ON_CALL ( mock - object , method ( matchers )) . With ( multi - argument - matcher ) ? . WillByDefault ( action );","title":"Setting Default Actions"},{"location":"examples/gtest/docs/cheat_sheet.html#ExpectCall","text":"EXPECT_CALL() sets expectations on a mock method (How will it be called? What will it do?): EXPECT_CALL ( mock - object , method ( matchers ) ? ) . With ( multi - argument - matcher ) ? . Times ( cardinality ) ? . InSequence ( sequences ) * . After ( expectations ) * . WillOnce ( action ) * . WillRepeatedly ( action ) ? . RetiresOnSaturation (); ? For each item above, ? means it can be used at most once, while * means it can be used any number of times. In order to pass, EXPECT_CALL must be used before the calls are actually made. The (matchers) is a comma-separated list of matchers that correspond to each of the arguments of method , and sets the expectation only for calls of method that matches all of the matchers. If (matchers) is omitted, the expectation is the same as if the matchers were set to anything matchers (for example, (_, _, _, _) for a four-arg method). If Times() is omitted, the cardinality is assumed to be: Times(1) when there is neither WillOnce() nor WillRepeatedly() ; Times(n) when there are n WillOnce() s but no WillRepeatedly() , where n >= 1; or Times(AtLeast(n)) when there are n WillOnce() s and a WillRepeatedly() , where n >= 0. A method with no EXPECT_CALL() is free to be invoked any number of times , and the default action will be taken each time.","title":"Setting Expectations"},{"location":"examples/gtest/docs/cheat_sheet.html#MatcherList","text":"A matcher matches a single argument. You can use it inside ON_CALL() or EXPECT_CALL() , or use it to validate a value directly using two macros: Macro Description EXPECT_THAT(actual_value, matcher) Asserts that actual_value matches matcher . ASSERT_THAT(actual_value, matcher) The same as EXPECT_THAT(actual_value, matcher) , except that it generates a fatal failure. Built-in matchers (where argument is the function argument, e.g. actual_value in the example above, or when used in the context of EXPECT_CALL(mock_object, method(matchers)) , the arguments of method ) are divided into several categories:","title":"Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#wildcard","text":"Matcher Description _ argument can be any value of the correct type. A<type>() or An<type>() argument can be any value of type type .","title":"Wildcard"},{"location":"examples/gtest/docs/cheat_sheet.html#generic-comparison","text":"Matcher Description Eq(value) or value argument == value Ge(value) argument >= value Gt(value) argument > value Le(value) argument <= value Lt(value) argument < value Ne(value) argument != value IsFalse() argument evaluates to false in a Boolean context. IsTrue() argument evaluates to true in a Boolean context. IsNull() argument is a NULL pointer (raw or smart). NotNull() argument is a non-null pointer (raw or smart). Optional(m) argument is optional<> that contains a value matching m . VariantWith<T>(m) argument is variant<> that holds the alternative of type T with a value matching m . Ref(variable) argument is a reference to variable . TypedEq<type>(value) argument has type type and is equal to value . You may need to use this instead of Eq(value) when the mock function is overloaded. Except Ref() , these matchers make a copy of value in case it's modified or destructed later. If the compiler complains that value doesn't have a public copy constructor, try wrap it in ByRef() , e.g. Eq(ByRef(non_copyable_value)) . If you do that, make sure non_copyable_value is not changed afterwards, or the meaning of your matcher will be changed.","title":"Generic Comparison"},{"location":"examples/gtest/docs/cheat_sheet.html#FpMatchers","text":"Matcher Description DoubleEq(a_double) argument is a double value approximately equal to a_double , treating two NaNs as unequal. FloatEq(a_float) argument is a float value approximately equal to a_float , treating two NaNs as unequal. NanSensitiveDoubleEq(a_double) argument is a double value approximately equal to a_double , treating two NaNs as equal. NanSensitiveFloatEq(a_float) argument is a float value approximately equal to a_float , treating two NaNs as equal. The above matchers use ULP-based comparison (the same as used in googletest). They automatically pick a reasonable error bound based on the absolute value of the expected value. DoubleEq() and FloatEq() conform to the IEEE standard, which requires comparing two NaNs for equality to return false. The NanSensitive* version instead treats two NaNs as equal, which is often what a user wants. Matcher Description DoubleNear(a_double, max_abs_error) argument is a double value close to a_double (absolute error <= max_abs_error ), treating two NaNs as unequal. FloatNear(a_float, max_abs_error) argument is a float value close to a_float (absolute error <= max_abs_error ), treating two NaNs as unequal. NanSensitiveDoubleNear(a_double, max_abs_error) argument is a double value close to a_double (absolute error <= max_abs_error ), treating two NaNs as equal. NanSensitiveFloatNear(a_float, max_abs_error) argument is a float value close to a_float (absolute error <= max_abs_error ), treating two NaNs as equal.","title":"Floating-Point Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#string-matchers","text":"The argument can be either a C string or a C++ string object: Matcher Description ContainsRegex(string) argument matches the given regular expression. EndsWith(suffix) argument ends with string suffix . HasSubstr(string) argument contains string as a sub-string. MatchesRegex(string) argument matches the given regular expression with the match starting at the first character and ending at the last character. StartsWith(prefix) argument starts with string prefix . StrCaseEq(string) argument is equal to string , ignoring case. StrCaseNe(string) argument is not equal to string , ignoring case. StrEq(string) argument is equal to string . StrNe(string) argument is not equal to string . ContainsRegex() and MatchesRegex() take ownership of the RE object. They use the regular expression syntax defined here . StrCaseEq() , StrCaseNe() , StrEq() , and StrNe() work for wide strings as well.","title":"String Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#container-matchers","text":"Most STL-style containers support == , so you can use Eq(expected_container) or simply expected_container to match a container exactly. If you want to write the elements in-line, match them more flexibly, or get more informative messages, you can use: Matcher Description BeginEndDistanceIs(m) argument is a container whose begin() and end() iterators are separated by a number of increments matching m . E.g. BeginEndDistanceIs(2) or BeginEndDistanceIs(Lt(2)) . For containers that define a size() method, SizeIs(m) may be more efficient. ContainerEq(container) The same as Eq(container) except that the failure message also includes which elements are in one container but not the other. Contains(e) argument contains an element that matches e , which can be either a value or a matcher. Each(e) argument is a container where every element matches e , which can be either a value or a matcher. ElementsAre(e0, e1, ..., en) argument has n + 1 elements, where the i -th element matches ei , which can be a value or a matcher. ElementsAreArray({e0, e1, ..., en}) , ElementsAreArray(a_container) , ElementsAreArray(begin, end) , ElementsAreArray(array) , or ElementsAreArray(array, count) The same as ElementsAre() except that the expected element values/matchers come from an initializer list, STL-style container, iterator range, or C-style array. IsEmpty() argument is an empty container ( container.empty() ). IsSubsetOf({e0, e1, ..., en}) , IsSubsetOf(a_container) , IsSubsetOf(begin, end) , IsSubsetOf(array) , or IsSubsetOf(array, count) argument matches UnorderedElementsAre(x0, x1, ..., xk) for some subset {x0, x1, ..., xk} of the expected matchers. IsSupersetOf({e0, e1, ..., en}) , IsSupersetOf(a_container) , IsSupersetOf(begin, end) , IsSupersetOf(array) , or IsSupersetOf(array, count) Some subset of argument matches UnorderedElementsAre( expected matchers ) . Pointwise(m, container) , Pointwise(m, {e0, e1, ..., en}) argument contains the same number of elements as in container , and for all i, (the i-th element in argument , the i-th element in container ) match m , which is a matcher on 2-tuples. E.g. Pointwise(Le(), upper_bounds) verifies that each element in argument doesn't exceed the corresponding element in upper_bounds . See more detail below. SizeIs(m) argument is a container whose size matches m . E.g. SizeIs(2) or SizeIs(Lt(2)) . UnorderedElementsAre(e0, e1, ..., en) argument has n + 1 elements, and under some permutation of the elements, each element matches an ei (for a different i ), which can be a value or a matcher. UnorderedElementsAreArray({e0, e1, ..., en}) , UnorderedElementsAreArray(a_container) , UnorderedElementsAreArray(begin, end) , UnorderedElementsAreArray(array) , or UnorderedElementsAreArray(array, count) The same as UnorderedElementsAre() except that the expected element values/matchers come from an initializer list, STL-style container, iterator range, or C-style array. UnorderedPointwise(m, container) , UnorderedPointwise(m, {e0, e1, ..., en}) Like Pointwise(m, container) , but ignores the order of elements. WhenSorted(m) When argument is sorted using the < operator, it matches container matcher m . E.g. WhenSorted(ElementsAre(1, 2, 3)) verifies that argument contains elements 1, 2, and 3, ignoring order. WhenSortedBy(comparator, m) The same as WhenSorted(m) , except that the given comparator instead of < is used to sort argument . E.g. WhenSortedBy(std::greater(), ElementsAre(3, 2, 1)) . Notes: These matchers can also match: a native array passed by reference (e.g. in Foo(const int (&a)[5]) ), and an array passed as a pointer and a count (e.g. in Bar(const T* buffer, int len) -- see Multi-argument Matchers ). The array being matched may be multi-dimensional (i.e. its elements can be arrays). m in Pointwise(m, ...) should be a matcher for ::std::tuple<T, U> where T and U are the element type of the actual container and the expected container, respectively. For example, to compare two Foo containers where Foo doesn't support operator== , one might write: using :: std :: get ; MATCHER ( FooEq , \"\" ) { return std :: get < 0 > ( arg ). Equals ( std :: get < 1 > ( arg )); } ... EXPECT_THAT ( actual_foos , Pointwise ( FooEq (), expected_foos ));","title":"Container Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#member-matchers","text":"Matcher Description Field(&class::field, m) argument.field (or argument->field when argument is a plain pointer) matches matcher m , where argument is an object of type class . Key(e) argument.first matches e , which can be either a value or a matcher. E.g. Contains(Key(Le(5))) can verify that a map contains a key <= 5 . Pair(m1, m2) argument is an std::pair whose first field matches m1 and second field matches m2 . Property(&class::property, m) argument.property() (or argument->property() when argument is a plain pointer) matches matcher m , where argument is an object of type class .","title":"Member Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#matching-the-result-of-a-function-functor-or-callback","text":"Matcher Description ResultOf(f, m) f(argument) matches matcher m , where f is a function or functor.","title":"Matching the Result of a Function, Functor, or Callback"},{"location":"examples/gtest/docs/cheat_sheet.html#pointer-matchers","text":"Matcher Description Pointee(m) argument (either a smart pointer or a raw pointer) points to a value that matches matcher m . WhenDynamicCastTo<T>(m) when argument is passed through dynamic_cast<T>() , it matches matcher m .","title":"Pointer Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#MultiArgMatchers","text":"Technically, all matchers match a single value. A \"multi-argument\" matcher is just one that matches a tuple . The following matchers can be used to match a tuple (x, y) : Matcher Description Eq() x == y Ge() x >= y Gt() x > y Le() x <= y Lt() x < y Ne() x != y You can use the following selectors to pick a subset of the arguments (or reorder them) to participate in the matching: Matcher Description AllArgs(m) Equivalent to m . Useful as syntactic sugar in .With(AllArgs(m)) . Args<N1, N2, ..., Nk>(m) The tuple of the k selected (using 0-based indices) arguments matches m , e.g. Args<1, 2>(Eq()) .","title":"Multi-argument Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#composite-matchers","text":"You can make a matcher from one or more other matchers: Matcher Description AllOf(m1, m2, ..., mn) argument matches all of the matchers m1 to mn . AllOfArray({m0, m1, ..., mn}) , AllOfArray(a_container) , AllOfArray(begin, end) , AllOfArray(array) , or AllOfArray(array, count) The same as AllOf() except that the matchers come from an initializer list, STL-style container, iterator range, or C-style array. AnyOf(m1, m2, ..., mn) argument matches at least one of the matchers m1 to mn . AnyOfArray({m0, m1, ..., mn}) , AnyOfArray(a_container) , AnyOfArray(begin, end) , AnyOfArray(array) , or AnyOfArray(array, count) The same as AnyOf() except that the matchers come from an initializer list, STL-style container, iterator range, or C-style array. Not(m) argument doesn't match matcher m .","title":"Composite Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#adapters-for-matchers","text":"Matcher Description MatcherCast<T>(m) casts matcher m to type Matcher<T> . SafeMatcherCast<T>(m) safely casts matcher m to type Matcher<T> . Truly(predicate) predicate(argument) returns something considered by C++ to be true, where predicate is a function or functor. AddressSatisfies(callback) and Truly(callback) take ownership of callback , which must be a permanent callback.","title":"Adapters for Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#MatchersAsPredicatesCheat","text":"Matcher Description Matches(m)(value) evaluates to true if value matches m . You can use Matches(m) alone as a unary functor. ExplainMatchResult(m, value, result_listener) evaluates to true if value matches m , explaining the result to result_listener . Value(value, m) evaluates to true if value matches m .","title":"Using Matchers as Predicates"},{"location":"examples/gtest/docs/cheat_sheet.html#defining-matchers","text":"Matcher Description MATCHER(IsEven, \"\") { return (arg % 2) == 0; } Defines a matcher IsEven() to match an even number. MATCHER_P(IsDivisibleBy, n, \"\") { *result_listener << \"where the remainder is \" << (arg % n); return (arg % n) == 0; } Defines a macher IsDivisibleBy(n) to match a number divisible by n . MATCHER_P2(IsBetween, a, b, std::string(negation ? \"isn't\" : \"is\") + \" between \" + PrintToString(a) + \" and \" + PrintToString(b)) { return a <= arg && arg <= b; } Defines a matcher IsBetween(a, b) to match a value in the range [ a , b ]. Notes: The MATCHER* macros cannot be used inside a function or class. The matcher body must be purely functional (i.e. it cannot have any side effect, and the result must not depend on anything other than the value being matched and the matcher parameters). You can use PrintToString(x) to convert a value x of any type to a string.","title":"Defining Matchers"},{"location":"examples/gtest/docs/cheat_sheet.html#ActionList","text":"Actions specify what a mock function should do when invoked.","title":"Actions"},{"location":"examples/gtest/docs/cheat_sheet.html#returning-a-value","text":"Return() Return from a void mock function. Return(value) Return value . If the type of value is different to the mock function's return type, value is converted to the latter type at the time the expectation is set , not when the action is executed. ReturnArg<N>() Return the N -th (0-based) argument. ReturnNew<T>(a1, ..., ak) Return new T(a1, ..., ak) ; a different object is created each time. ReturnNull() Return a null pointer. ReturnPointee(ptr) Return the value pointed to by ptr . ReturnRef(variable) Return a reference to variable . ReturnRefOfCopy(value) Return a reference to a copy of value ; the copy lives as long as the action.","title":"Returning a Value"},{"location":"examples/gtest/docs/cheat_sheet.html#side-effects","text":"Assign(&variable, value) Assign value to variable. DeleteArg<N>() Delete the N -th (0-based) argument, which must be a pointer. SaveArg<N>(pointer) Save the N -th (0-based) argument to *pointer . SaveArgPointee<N>(pointer) Save the value pointed to by the N -th (0-based) argument to *pointer . SetArgReferee<N>(value) Assign value to the variable referenced by the N -th (0-based) argument. SetArgPointee<N>(value) Assign value to the variable pointed by the N -th (0-based) argument. SetArgumentPointee<N>(value) Same as SetArgPointee<N>(value) . Deprecated. Will be removed in v1.7.0. SetArrayArgument<N>(first, last) Copies the elements in source range [ first , last ) to the array pointed to by the N -th (0-based) argument, which can be either a pointer or an iterator. The action does not take ownership of the elements in the source range. SetErrnoAndReturn(error, value) Set errno to error and return value . Throw(exception) Throws the given exception, which can be any copyable value. Available since v1.1.0.","title":"Side Effects"},{"location":"examples/gtest/docs/cheat_sheet.html#using-a-function-functor-or-lambda-as-an-action","text":"In the following, by \"callable\" we mean a free function, std::function , functor, or lambda. f Invoke f with the arguments passed to the mock function, where f is a callable. Invoke(f) Invoke f with the arguments passed to the mock function, where f can be a global/static function or a functor. Invoke(object_pointer, &class::method) Invoke the method on the object with the arguments passed to the mock function. InvokeWithoutArgs(f) Invoke f , which can be a global/static function or a functor. f must take no arguments. InvokeWithoutArgs(object_pointer, &class::method) Invoke the method on the object, which takes no arguments. InvokeArgument<N>(arg1, arg2, ..., argk) Invoke the mock function's N -th (0-based) argument, which must be a function or a functor, with the k arguments. The return value of the invoked function is used as the return value of the action. When defining a callable to be used with Invoke*() , you can declare any unused parameters as Unused : using :: testing :: Invoke ; double Distance ( Unused , double x , double y ) { return sqrt ( x * x + y * y ); } ... EXPECT_CALL ( mock , Foo ( \"Hi\" , _ , _ )). WillOnce ( Invoke ( Distance )); Invoke(callback) and InvokeWithoutArgs(callback) take ownership of callback , which must be permanent. The type of callback must be a base callback type instead of a derived one, e.g. BlockingClosure * done = new BlockingClosure ; ... Invoke ( done ) ...; // This won't compile! Closure * done2 = new BlockingClosure ; ... Invoke ( done2 ) ...; // This works. In InvokeArgument<N>(...) , if an argument needs to be passed by reference, wrap it inside ByRef() . For example, using :: testing :: ByRef ; using :: testing :: InvokeArgument ; ... InvokeArgument < 2 > ( 5 , string ( \"Hi\" ), ByRef ( foo )) calls the mock function's #2 argument, passing to it 5 and string(\"Hi\") by value, and foo by reference.","title":"Using a Function, Functor, or Lambda as an Action"},{"location":"examples/gtest/docs/cheat_sheet.html#default-action","text":"Matcher Description DoDefault() Do the default action (specified by ON_CALL() or the built-in one). Note: due to technical reasons, DoDefault() cannot be used inside a composite action - trying to do so will result in a run-time error.","title":"Default Action"},{"location":"examples/gtest/docs/cheat_sheet.html#composite-actions","text":"DoAll(a1, a2, ..., an) Do all actions a1 to an and return the result of an in each invocation. The first n - 1 sub-actions must return void. IgnoreResult(a) Perform action a and ignore its result. a must not return void. WithArg<N>(a) Pass the N -th (0-based) argument of the mock function to action a and perform it. WithArgs<N1, N2, ..., Nk>(a) Pass the selected (0-based) arguments of the mock function to action a and perform it. WithoutArgs(a) Perform action a without any arguments.","title":"Composite Actions"},{"location":"examples/gtest/docs/cheat_sheet.html#defining-actions","text":"`struct SumAction {` \u2003`template ` \u2003`T operator()(T x, Ty) { return x + y; }` `};` Defines a generic functor that can be used as an action summing its arguments. ACTION(Sum) { return arg0 + arg1; } Defines an action Sum() to return the sum of the mock function's argument #0 and #1. ACTION_P(Plus, n) { return arg0 + n; } Defines an action Plus(n) to return the sum of the mock function's argument #0 and n . ACTION_Pk(Foo, p1, ..., pk) { statements; } Defines a parameterized action Foo(p1, ..., pk) to execute the given statements . The ACTION* macros cannot be used inside a function or class.","title":"Defining Actions"},{"location":"examples/gtest/docs/cheat_sheet.html#CardinalityList","text":"These are used in Times() to specify how many times a mock function will be called: AnyNumber() The function can be called any number of times. AtLeast(n) The call is expected at least n times. AtMost(n) The call is expected at most n times. Between(m, n) The call is expected between m and n (inclusive) times. Exactly(n) or n The call is expected exactly n times. In particular, the call should never happen when n is 0.","title":"Cardinalities"},{"location":"examples/gtest/docs/cheat_sheet.html#expectation-order","text":"By default, the expectations can be matched in any order. If some or all expectations must be matched in a given order, there are two ways to specify it. They can be used either independently or together.","title":"Expectation Order"},{"location":"examples/gtest/docs/cheat_sheet.html#AfterClause","text":"using :: testing :: Expectation ; ... Expectation init_x = EXPECT_CALL ( foo , InitX ()); Expectation init_y = EXPECT_CALL ( foo , InitY ()); EXPECT_CALL ( foo , Bar ()) . After ( init_x , init_y ); says that Bar() can be called only after both InitX() and InitY() have been called. If you don't know how many pre-requisites an expectation has when you write it, you can use an ExpectationSet to collect them: using :: testing :: ExpectationSet ; ... ExpectationSet all_inits ; for ( int i = 0 ; i < element_count ; i ++ ) { all_inits += EXPECT_CALL ( foo , InitElement ( i )); } EXPECT_CALL ( foo , Bar ()) . After ( all_inits ); says that Bar() can be called only after all elements have been initialized (but we don't care about which elements get initialized before the others). Modifying an ExpectationSet after using it in an .After() doesn't affect the meaning of the .After() .","title":"The After Clause"},{"location":"examples/gtest/docs/cheat_sheet.html#UsingSequences","text":"When you have a long chain of sequential expectations, it's easier to specify the order using sequences , which don't require you to given each expectation in the chain a different name. All expected calls in the same sequence must occur in the order they are specified. using :: testing :: Return ; using :: testing :: Sequence ; Sequence s1 , s2 ; ... EXPECT_CALL ( foo , Reset ()) . InSequence ( s1 , s2 ) . WillOnce ( Return ( true )); EXPECT_CALL ( foo , GetSize ()) . InSequence ( s1 ) . WillOnce ( Return ( 1 )); EXPECT_CALL ( foo , Describe ( A < const char *> ())) . InSequence ( s2 ) . WillOnce ( Return ( \"dummy\" )); says that Reset() must be called before both GetSize() and Describe() , and the latter two can occur in any order. To put many expectations in a sequence conveniently: using :: testing :: InSequence ; { InSequence seq ; EXPECT_CALL (...)...; EXPECT_CALL (...)...; ... EXPECT_CALL (...)...; } says that all expected calls in the scope of seq must occur in strict order. The name seq is irrelevant.","title":"Sequences"},{"location":"examples/gtest/docs/cheat_sheet.html#verifying-and-resetting-a-mock","text":"gMock will verify the expectations on a mock object when it is destructed, or you can do it earlier: using :: testing :: Mock ; ... // Verifies and removes the expectations on mock_obj; // returns true if and only if successful. Mock :: VerifyAndClearExpectations ( & mock_obj ); ... // Verifies and removes the expectations on mock_obj; // also removes the default actions set by ON_CALL(); // returns true if and only if successful. Mock :: VerifyAndClear ( & mock_obj ); You can also tell gMock that a mock object can be leaked and doesn't need to be verified: Mock :: AllowLeak ( & mock_obj );","title":"Verifying and Resetting a Mock"},{"location":"examples/gtest/docs/cheat_sheet.html#mock-classes","text":"gMock defines a convenient mock class template class MockFunction < R ( A1 , ..., An ) > { public : MOCK_METHOD ( R , Call , ( A1 , ..., An )); }; See this recipe for one application of it.","title":"Mock Classes"},{"location":"examples/gtest/docs/cheat_sheet.html#flags","text":"Flag Description --gmock_catch_leaked_mocks=0 Don't report leaked mock objects as failures. --gmock_verbose=LEVEL Sets the default verbosity level ( info , warning , or error ) of Google Mock messages.","title":"Flags"},{"location":"examples/gtest/docs/cook_book.html","text":"gMock Cookbook You can find recipes for using gMock here. If you haven't yet, please read this first to make sure you understand the basics. Note: gMock lives in the testing name space. For readability, it is recommended to write using ::testing::Foo; once in your file before using the name Foo defined by gMock. We omit such using statements in this section for brevity, but you should do it in your own code. Creating Mock Classes Mock classes are defined as normal classes, using the MOCK_METHOD macro to generate mocked methods. The macro gets 3 or 4 parameters: class MyMock { public : MOCK_METHOD ( ReturnType , MethodName , ( Args ...)); MOCK_METHOD ( ReturnType , MethodName , ( Args ...), ( Specs ...)); }; The first 3 parameters are simply the method declaration, split into 3 parts. The 4th parameter accepts a closed list of qualifiers, which affect the generated method: const - Makes the mocked method a const method. Required if overriding a const method. override - Marks the method with override . Recommended if overriding a virtual method. noexcept - Marks the method with noexcept . Required if overriding a noexcept method. Calltype(...) - Sets the call type for the method (e.g. to STDMETHODCALLTYPE ), useful in Windows. Dealing with unprotected commas Unprotected commas, i.e. commas which are not surrounded by parentheses, prevent MOCK_METHOD from parsing its arguments correctly: ```cpp {.bad} class MockFoo { public: MOCK_METHOD(std::pair , GetPair, ()); // Won't compile! MOCK_METHOD(bool, CheckMap, (std::map , bool)); // Won't compile! }; Solution 1 - wrap with parentheses: ```cpp {.good} class MockFoo { public: MOCK_METHOD((std::pair<bool, int>), GetPair, ()); MOCK_METHOD(bool, CheckMap, ((std::map<int, double>), bool)); }; Note that wrapping a return or argument type with parentheses is, in general, invalid C++. MOCK_METHOD removes the parentheses. Solution 2 - define an alias: ```cpp {.good} class MockFoo { public: using BoolAndInt = std::pair ; MOCK_METHOD(BoolAndInt, GetPair, ()); using MapIntDouble = std::map ; MOCK_METHOD(bool, CheckMap, (MapIntDouble, bool)); }; ### Mocking Private or Protected Methods You must always put a mock method definition (`MOCK_METHOD`) in a `public:` section of the mock class, regardless of the method being mocked being `public`, `protected`, or `private` in the base class. This allows `ON_CALL` and `EXPECT_CALL` to reference the mock function from outside of the mock class. (Yes, C++ allows a subclass to change the access level of a virtual function in the base class.) Example: ```cpp class Foo { public: ... virtual bool Transform(Gadget* g) = 0; protected: virtual void Resume(); private: virtual int GetTimeOut(); }; class MockFoo : public Foo { public: ... MOCK_METHOD(bool, Transform, (Gadget* g), (override)); // The following must be in the public section, even though the // methods are protected or private in the base class. MOCK_METHOD(void, Resume, (), (override)); MOCK_METHOD(int, GetTimeOut, (), (override)); }; Mocking Overloaded Methods You can mock overloaded functions as usual. No special attention is required: class Foo { ... // Must be virtual as we'll inherit from Foo. virtual ~ Foo (); // Overloaded on the types and/or numbers of arguments. virtual int Add ( Element x ); virtual int Add ( int times , Element x ); // Overloaded on the const-ness of this object. virtual Bar & GetBar (); virtual const Bar & GetBar () const ; }; class MockFoo : public Foo { ... MOCK_METHOD ( int , Add , ( Element x ), ( override )); MOCK_METHOD ( int , Add , ( int times , Element x ), ( override )); MOCK_METHOD ( Bar & , GetBar , (), ( override )); MOCK_METHOD ( const Bar & , GetBar , (), ( const , override )); }; Note: if you don't mock all versions of the overloaded method, the compiler will give you a warning about some methods in the base class being hidden. To fix that, use using to bring them in scope: class MockFoo : public Foo { ... using Foo :: Add ; MOCK_METHOD ( int , Add , ( Element x ), ( override )); // We don't want to mock int Add(int times, Element x); ... }; Mocking Class Templates You can mock class templates just like any class. template < typename Elem > class StackInterface { ... // Must be virtual as we'll inherit from StackInterface. virtual ~ StackInterface (); virtual int GetSize () const = 0 ; virtual void Push ( const Elem & x ) = 0 ; }; template < typename Elem > class MockStack : public StackInterface < Elem > { ... MOCK_METHOD ( int , GetSize , (), ( override )); MOCK_METHOD ( void , Push , ( const Elem & x ), ( override )); }; Mocking Non-virtual Methods gMock can mock non-virtual functions to be used in Hi-perf dependency injection. In this case, instead of sharing a common base class with the real class, your mock class will be unrelated to the real class, but contain methods with the same signatures. The syntax for mocking non-virtual methods is the same as mocking virtual methods (just don't add override ): // A simple packet stream class. None of its members is virtual. class ConcretePacketStream { public : void AppendPacket ( Packet * new_packet ); const Packet * GetPacket ( size_t packet_number ) const ; size_t NumberOfPackets () const ; ... }; // A mock packet stream class. It inherits from no other, but defines // GetPacket() and NumberOfPackets(). class MockPacketStream { public : MOCK_METHOD ( const Packet * , GetPacket , ( size_t packet_number ), ( const )); MOCK_METHOD ( size_t , NumberOfPackets , (), ( const )); ... }; Note that the mock class doesn't define AppendPacket() , unlike the real class. That's fine as long as the test doesn't need to call it. Next, you need a way to say that you want to use ConcretePacketStream in production code, and use MockPacketStream in tests. Since the functions are not virtual and the two classes are unrelated, you must specify your choice at compile time (as opposed to run time). One way to do it is to templatize your code that needs to use a packet stream. More specifically, you will give your code a template type argument for the type of the packet stream. In production, you will instantiate your template with ConcretePacketStream as the type argument. In tests, you will instantiate the same template with MockPacketStream . For example, you may write: template < class PacketStream > void CreateConnection ( PacketStream * stream ) { ... } template < class PacketStream > class PacketReader { public : void ReadPackets ( PacketStream * stream , size_t packet_num ); }; Then you can use CreateConnection<ConcretePacketStream>() and PacketReader<ConcretePacketStream> in production code, and use CreateConnection<MockPacketStream>() and PacketReader<MockPacketStream> in tests. MockPacketStream mock_stream ; EXPECT_CALL ( mock_stream , ...)...; .. set more expectations on mock_stream ... PacketReader < MockPacketStream > reader ( & mock_stream ); ... exercise reader ... Mocking Free Functions It's possible to use gMock to mock a free function (i.e. a C-style function or a static method). You just need to rewrite your code to use an interface (abstract class). Instead of calling a free function (say, OpenFile ) directly, introduce an interface for it and have a concrete subclass that calls the free function: class FileInterface { public : ... virtual bool Open ( const char * path , const char * mode ) = 0 ; }; class File : public FileInterface { public : ... virtual bool Open ( const char * path , const char * mode ) { return OpenFile ( path , mode ); } }; Your code should talk to FileInterface to open a file. Now it's easy to mock out the function. This may seem like a lot of hassle, but in practice you often have multiple related functions that you can put in the same interface, so the per-function syntactic overhead will be much lower. If you are concerned about the performance overhead incurred by virtual functions, and profiling confirms your concern, you can combine this with the recipe for mocking non-virtual methods . Old-Style MOCK_METHODn Macros Before the generic MOCK_METHOD macro was introduced, mocks where created using a family of macros collectively called MOCK_METHODn . These macros are still supported, though migration to the new MOCK_METHOD is recommended. The macros in the MOCK_METHODn family differ from MOCK_METHOD : The general structure is MOCK_METHODn(MethodName, ReturnType(Args)) , instead of MOCK_METHOD(ReturnType, MethodName, (Args)) . The number n must equal the number of arguments. When mocking a const method, one must use MOCK_CONST_METHODn . When mocking a class template, the macro name must be suffixed with _T . In order to specify the call type, the macro name must be suffixed with _WITH_CALLTYPE , and the call type is the first macro argument. Old macros and their new equivalents: Simple Old MOCK_METHOD1(Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int)) Const Method Old MOCK_CONST_METHOD1(Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (const)) Method in a Class Template Old MOCK_METHOD1_T(Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int)) Const Method in a Class Template Old MOCK_CONST_METHOD1_T(Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (const)) Method with Call Type Old MOCK_METHOD1_WITH_CALLTYPE(STDMETHODCALLTYPE, Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (Calltype(STDMETHODCALLTYPE))) Const Method with Call Type Old MOCK_CONST_METHOD1_WITH_CALLTYPE(STDMETHODCALLTYPE, Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (const, Calltype(STDMETHODCALLTYPE))) Method with Call Type in a Class Template Old MOCK_METHOD1_T_WITH_CALLTYPE(STDMETHODCALLTYPE, Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (Calltype(STDMETHODCALLTYPE))) Const Method with Call Type in a Class Template Old `MOCK_CONST_METHOD1_T_WITH_CALLTYPE(STDMETHODCALLTYPE, Foo, bool(int))` New MOCK_METHOD(bool, Foo, (int), (const, Calltype(STDMETHODCALLTYPE))) The Nice, the Strict, and the Naggy If a mock method has no EXPECT_CALL spec but is called, we say that it's an \"uninteresting call\", and the default action (which can be specified using ON_CALL() ) of the method will be taken. Currently, an uninteresting call will also by default cause gMock to print a warning. (In the future, we might remove this warning by default.) However, sometimes you may want to ignore these uninteresting calls, and sometimes you may want to treat them as errors. gMock lets you make the decision on a per-mock-object basis. Suppose your test uses a mock class MockFoo : TEST (...) { MockFoo mock_foo ; EXPECT_CALL ( mock_foo , DoThis ()); ... code that uses mock_foo ... } If a method of mock_foo other than DoThis() is called, you will get a warning. However, if you rewrite your test to use NiceMock<MockFoo> instead, you can suppress the warning: using :: testing :: NiceMock ; TEST (...) { NiceMock < MockFoo > mock_foo ; EXPECT_CALL ( mock_foo , DoThis ()); ... code that uses mock_foo ... } NiceMock<MockFoo> is a subclass of MockFoo , so it can be used wherever MockFoo is accepted. It also works if MockFoo 's constructor takes some arguments, as NiceMock<MockFoo> \"inherits\" MockFoo 's constructors: using :: testing :: NiceMock ; TEST (...) { NiceMock < MockFoo > mock_foo ( 5 , \"hi\" ); // Calls MockFoo(5, \"hi\"). EXPECT_CALL ( mock_foo , DoThis ()); ... code that uses mock_foo ... } The usage of StrictMock is similar, except that it makes all uninteresting calls failures: using :: testing :: StrictMock ; TEST (...) { StrictMock < MockFoo > mock_foo ; EXPECT_CALL ( mock_foo , DoThis ()); ... code that uses mock_foo ... // The test will fail if a method of mock_foo other than DoThis() // is called. } NOTE: NiceMock and StrictMock only affects uninteresting calls (calls of methods with no expectations); they do not affect unexpected calls (calls of methods with expectations, but they don't match). See Understanding Uninteresting vs Unexpected Calls . There are some caveats though (I dislike them just as much as the next guy, but sadly they are side effects of C++'s limitations): NiceMock<MockFoo> and StrictMock<MockFoo> only work for mock methods defined using the MOCK_METHOD macro directly in the MockFoo class. If a mock method is defined in a base class of MockFoo , the \"nice\" or \"strict\" modifier may not affect it, depending on the compiler. In particular, nesting NiceMock and StrictMock (e.g. NiceMock<StrictMock<MockFoo> > ) is not supported. NiceMock<MockFoo> and StrictMock<MockFoo> may not work correctly if the destructor of MockFoo is not virtual. We would like to fix this, but it requires cleaning up existing tests. http://b/28934720 tracks the issue. During the constructor or destructor of MockFoo , the mock object is not nice or strict. This may cause surprises if the constructor or destructor calls a mock method on this object. (This behavior, however, is consistent with C++'s general rule: if a constructor or destructor calls a virtual method of this object, that method is treated as non-virtual. In other words, to the base class's constructor or destructor, this object behaves like an instance of the base class, not the derived class. This rule is required for safety. Otherwise a base constructor may use members of a derived class before they are initialized, or a base destructor may use members of a derived class after they have been destroyed.) Finally, you should be very cautious about when to use naggy or strict mocks, as they tend to make tests more brittle and harder to maintain. When you refactor your code without changing its externally visible behavior, ideally you shouldn't need to update any tests. If your code interacts with a naggy mock, however, you may start to get spammed with warnings as the result of your change. Worse, if your code interacts with a strict mock, your tests may start to fail and you'll be forced to fix them. Our general recommendation is to use nice mocks (not yet the default) most of the time, use naggy mocks (the current default) when developing or debugging tests, and use strict mocks only as the last resort. Simplifying the Interface without Breaking Existing Code Sometimes a method has a long list of arguments that is mostly uninteresting. For example: class LogSink { public : ... virtual void send ( LogSeverity severity , const char * full_filename , const char * base_filename , int line , const struct tm * tm_time , const char * message , size_t message_len ) = 0 ; }; This method's argument list is lengthy and hard to work with (the message argument is not even 0-terminated). If we mock it as is, using the mock will be awkward. If, however, we try to simplify this interface, we'll need to fix all clients depending on it, which is often infeasible. The trick is to redispatch the method in the mock class: class ScopedMockLog : public LogSink { public : ... virtual void send ( LogSeverity severity , const char * full_filename , const char * base_filename , int line , const tm * tm_time , const char * message , size_t message_len ) { // We are only interested in the log severity, full file name, and // log message. Log ( severity , full_filename , std :: string ( message , message_len )); } // Implements the mock method: // // void Log(LogSeverity severity, // const string& file_path, // const string& message); MOCK_METHOD ( void , Log , ( LogSeverity severity , const string & file_path , const string & message )); }; By defining a new mock method with a trimmed argument list, we make the mock class more user-friendly. This technique may also be applied to make overloaded methods more amenable to mocking. For example, when overloads have been used to implement default arguments: class MockTurtleFactory : public TurtleFactory { public : Turtle * MakeTurtle ( int length , int weight ) override { ... } Turtle * MakeTurtle ( int length , int weight , int speed ) override { ... } // the above methods delegate to this one: MOCK_METHOD ( Turtle * , DoMakeTurtle , ()); }; This allows tests that don't care which overload was invoked to avoid specifying argument matchers: ON_CALL ( factory , DoMakeTurtle ) . WillByDefault ( MakeMockTurtle ()); Alternative to Mocking Concrete Classes Often you may find yourself using classes that don't implement interfaces. In order to test your code that uses such a class (let's call it Concrete ), you may be tempted to make the methods of Concrete virtual and then mock it. Try not to do that. Making a non-virtual function virtual is a big decision. It creates an extension point where subclasses can tweak your class' behavior. This weakens your control on the class because now it's harder to maintain the class invariants. You should make a function virtual only when there is a valid reason for a subclass to override it. Mocking concrete classes directly is problematic as it creates a tight coupling between the class and the tests - any small change in the class may invalidate your tests and make test maintenance a pain. To avoid such problems, many programmers have been practicing \"coding to interfaces\": instead of talking to the Concrete class, your code would define an interface and talk to it. Then you implement that interface as an adaptor on top of Concrete . In tests, you can easily mock that interface to observe how your code is doing. This technique incurs some overhead: You pay the cost of virtual function calls (usually not a problem). There is more abstraction for the programmers to learn. However, it can also bring significant benefits in addition to better testability: Concrete 's API may not fit your problem domain very well, as you may not be the only client it tries to serve. By designing your own interface, you have a chance to tailor it to your need - you may add higher-level functionalities, rename stuff, etc instead of just trimming the class. This allows you to write your code (user of the interface) in a more natural way, which means it will be more readable, more maintainable, and you'll be more productive. If Concrete 's implementation ever has to change, you don't have to rewrite everywhere it is used. Instead, you can absorb the change in your implementation of the interface, and your other code and tests will be insulated from this change. Some people worry that if everyone is practicing this technique, they will end up writing lots of redundant code. This concern is totally understandable. However, there are two reasons why it may not be the case: Different projects may need to use Concrete in different ways, so the best interfaces for them will be different. Therefore, each of them will have its own domain-specific interface on top of Concrete , and they will not be the same code. If enough projects want to use the same interface, they can always share it, just like they have been sharing Concrete . You can check in the interface and the adaptor somewhere near Concrete (perhaps in a contrib sub-directory) and let many projects use it. You need to weigh the pros and cons carefully for your particular problem, but I'd like to assure you that the Java community has been practicing this for a long time and it's a proven effective technique applicable in a wide variety of situations. :-) Delegating Calls to a Fake Some times you have a non-trivial fake implementation of an interface. For example: class Foo { public : virtual ~ Foo () {} virtual char DoThis ( int n ) = 0 ; virtual void DoThat ( const char * s , int * p ) = 0 ; }; class FakeFoo : public Foo { public : char DoThis ( int n ) override { return ( n > 0 ) ? '+' : ( n < 0 ) ? '-' : '0' ; } void DoThat ( const char * s , int * p ) override { * p = strlen ( s ); } }; Now you want to mock this interface such that you can set expectations on it. However, you also want to use FakeFoo for the default behavior, as duplicating it in the mock object is, well, a lot of work. When you define the mock class using gMock, you can have it delegate its default action to a fake class you already have, using this pattern: class MockFoo : public Foo { public : // Normal mock method definitions using gMock. MOCK_METHOD ( char , DoThis , ( int n ), ( override )); MOCK_METHOD ( void , DoThat , ( const char * s , int * p ), ( override )); // Delegates the default actions of the methods to a FakeFoo object. // This must be called *before* the custom ON_CALL() statements. void DelegateToFake () { ON_CALL ( * this , DoThis ). WillByDefault ([ this ]( int n ) { return fake_ . DoThis ( n ); }); ON_CALL ( * this , DoThat ). WillByDefault ([ this ]( const char * s , int * p ) { fake_ . DoThat ( s , p ); }); } private : FakeFoo fake_ ; // Keeps an instance of the fake in the mock. }; With that, you can use MockFoo in your tests as usual. Just remember that if you don't explicitly set an action in an ON_CALL() or EXPECT_CALL() , the fake will be called upon to do it.: using :: testing :: _ ; TEST ( AbcTest , Xyz ) { MockFoo foo ; foo . DelegateToFake (); // Enables the fake for delegation. // Put your ON_CALL(foo, ...)s here, if any. // No action specified, meaning to use the default action. EXPECT_CALL ( foo , DoThis ( 5 )); EXPECT_CALL ( foo , DoThat ( _ , _ )); int n = 0 ; EXPECT_EQ ( '+' , foo . DoThis ( 5 )); // FakeFoo::DoThis() is invoked. foo . DoThat ( \"Hi\" , & n ); // FakeFoo::DoThat() is invoked. EXPECT_EQ ( 2 , n ); } Some tips: If you want, you can still override the default action by providing your own ON_CALL() or using .WillOnce() / .WillRepeatedly() in EXPECT_CALL() . In DelegateToFake() , you only need to delegate the methods whose fake implementation you intend to use. The general technique discussed here works for overloaded methods, but you'll need to tell the compiler which version you mean. To disambiguate a mock function (the one you specify inside the parentheses of ON_CALL() ), use this technique ; to disambiguate a fake function (the one you place inside Invoke() ), use a static_cast to specify the function's type. For instance, if class Foo has methods char DoThis(int n) and bool DoThis(double x) const , and you want to invoke the latter, you need to write Invoke(&fake_, static_cast<bool (FakeFoo::*)(double) const>(&FakeFoo::DoThis)) instead of Invoke(&fake_, &FakeFoo::DoThis) (The strange-looking thing inside the angled brackets of static_cast is the type of a function pointer to the second DoThis() method.). Having to mix a mock and a fake is often a sign of something gone wrong. Perhaps you haven't got used to the interaction-based way of testing yet. Or perhaps your interface is taking on too many roles and should be split up. Therefore, don't abuse this . We would only recommend to do it as an intermediate step when you are refactoring your code. Regarding the tip on mixing a mock and a fake, here's an example on why it may be a bad sign: Suppose you have a class System for low-level system operations. In particular, it does file and I/O operations. And suppose you want to test how your code uses System to do I/O, and you just want the file operations to work normally. If you mock out the entire System class, you'll have to provide a fake implementation for the file operation part, which suggests that System is taking on too many roles. Instead, you can define a FileOps interface and an IOOps interface and split System 's functionalities into the two. Then you can mock IOOps without mocking FileOps . Delegating Calls to a Real Object When using testing doubles (mocks, fakes, stubs, and etc), sometimes their behaviors will differ from those of the real objects. This difference could be either intentional (as in simulating an error such that you can test the error handling code) or unintentional. If your mocks have different behaviors than the real objects by mistake, you could end up with code that passes the tests but fails in production. You can use the delegating-to-real technique to ensure that your mock has the same behavior as the real object while retaining the ability to validate calls. This technique is very similar to the delegating-to-fake technique, the difference being that we use a real object instead of a fake. Here's an example: using :: testing :: AtLeast ; class MockFoo : public Foo { public : MockFoo () { // By default, all calls are delegated to the real object. ON_CALL ( * this , DoThis ). WillByDefault ([ this ]( int n ) { return real_ . DoThis ( n ); }); ON_CALL ( * this , DoThat ). WillByDefault ([ this ]( const char * s , int * p ) { real_ . DoThat ( s , p ); }); ... } MOCK_METHOD ( char , DoThis , ...); MOCK_METHOD ( void , DoThat , ...); ... private : Foo real_ ; }; ... MockFoo mock ; EXPECT_CALL ( mock , DoThis ()) . Times ( 3 ); EXPECT_CALL ( mock , DoThat ( \"Hi\" )) . Times ( AtLeast ( 1 )); ... use mock in test ... With this, gMock will verify that your code made the right calls (with the right arguments, in the right order, called the right number of times, etc), and a real object will answer the calls (so the behavior will be the same as in production). This gives you the best of both worlds. Delegating Calls to a Parent Class Ideally, you should code to interfaces, whose methods are all pure virtual. In reality, sometimes you do need to mock a virtual method that is not pure (i.e, it already has an implementation). For example: class Foo { public : virtual ~ Foo (); virtual void Pure ( int n ) = 0 ; virtual int Concrete ( const char * str ) { ... } }; class MockFoo : public Foo { public : // Mocking a pure method. MOCK_METHOD ( void , Pure , ( int n ), ( override )); // Mocking a concrete method. Foo::Concrete() is shadowed. MOCK_METHOD ( int , Concrete , ( const char * str ), ( override )); }; Sometimes you may want to call Foo::Concrete() instead of MockFoo::Concrete() . Perhaps you want to do it as part of a stub action, or perhaps your test doesn't need to mock Concrete() at all (but it would be oh-so painful to have to define a new mock class whenever you don't need to mock one of its methods). The trick is to leave a back door in your mock class for accessing the real methods in the base class: class MockFoo : public Foo { public : // Mocking a pure method. MOCK_METHOD ( void , Pure , ( int n ), ( override )); // Mocking a concrete method. Foo::Concrete() is shadowed. MOCK_METHOD ( int , Concrete , ( const char * str ), ( override )); // Use this to call Concrete() defined in Foo. int FooConcrete ( const char * str ) { return Foo :: Concrete ( str ); } }; Now, you can call Foo::Concrete() inside an action by: ... EXPECT_CALL ( foo , Concrete ). WillOnce ([ & foo ]( const char * str ) { return foo . FooConcrete ( str ); }); or tell the mock object that you don't want to mock Concrete() : ... ON_CALL ( foo , Concrete ). WillByDefault ([ & foo ]( const char * str ) { return foo . FooConcrete ( str ); }); (Why don't we just write { return foo.Concrete(str); } ? If you do that, MockFoo::Concrete() will be called (and cause an infinite recursion) since Foo::Concrete() is virtual. That's just how C++ works.) Using Matchers Matching Argument Values Exactly You can specify exactly which arguments a mock method is expecting: using :: testing :: Return ; ... EXPECT_CALL ( foo , DoThis ( 5 )) . WillOnce ( Return ( 'a' )); EXPECT_CALL ( foo , DoThat ( \"Hello\" , bar )); Using Simple Matchers You can use matchers to match arguments that have a certain property: using :: testing :: NotNull ; using :: testing :: Return ; ... EXPECT_CALL ( foo , DoThis ( Ge ( 5 ))) // The argument must be >= 5. . WillOnce ( Return ( 'a' )); EXPECT_CALL ( foo , DoThat ( \"Hello\" , NotNull ())); // The second argument must not be NULL. A frequently used matcher is _ , which matches anything: EXPECT_CALL ( foo , DoThat ( _ , NotNull ())); Combining Matchers You can build complex matchers from existing ones using AllOf() , AllOfArray() , AnyOf() , AnyOfArray() and Not() : using :: testing :: AllOf ; using :: testing :: Gt ; using :: testing :: HasSubstr ; using :: testing :: Ne ; using :: testing :: Not ; ... // The argument must be > 5 and != 10. EXPECT_CALL ( foo , DoThis ( AllOf ( Gt ( 5 ), Ne ( 10 )))); // The first argument must not contain sub-string \"blah\". EXPECT_CALL ( foo , DoThat ( Not ( HasSubstr ( \"blah\" )), NULL )); Casting Matchers gMock matchers are statically typed, meaning that the compiler can catch your mistake if you use a matcher of the wrong type (for example, if you use Eq(5) to match a string argument). Good for you! Sometimes, however, you know what you're doing and want the compiler to give you some slack. One example is that you have a matcher for long and the argument you want to match is int . While the two types aren't exactly the same, there is nothing really wrong with using a Matcher<long> to match an int - after all, we can first convert the int argument to a long losslessly before giving it to the matcher. To support this need, gMock gives you the SafeMatcherCast<T>(m) function. It casts a matcher m to type Matcher<T> . To ensure safety, gMock checks that (let U be the type m accepts : Type T can be implicitly cast to type U ; When both T and U are built-in arithmetic types ( bool , integers, and floating-point numbers), the conversion from T to U is not lossy (in other words, any value representable by T can also be represented by U ); and When U is a reference, T must also be a reference (as the underlying matcher may be interested in the address of the U value). The code won't compile if any of these conditions isn't met. Here's one example: using :: testing :: SafeMatcherCast ; // A base class and a child class. class Base { ... }; class Derived : public Base { ... }; class MockFoo : public Foo { public : MOCK_METHOD ( void , DoThis , ( Derived * derived ), ( override )); }; ... MockFoo foo ; // m is a Matcher<Base*> we got from somewhere. EXPECT_CALL ( foo , DoThis ( SafeMatcherCast < Derived *> ( m ))); If you find SafeMatcherCast<T>(m) too limiting, you can use a similar function MatcherCast<T>(m) . The difference is that MatcherCast works as long as you can static_cast type T to type U . MatcherCast essentially lets you bypass C++'s type system ( static_cast isn't always safe as it could throw away information, for example), so be careful not to misuse/abuse it. Selecting Between Overloaded Functions If you expect an overloaded function to be called, the compiler may need some help on which overloaded version it is. To disambiguate functions overloaded on the const-ness of this object, use the Const() argument wrapper. using :: testing :: ReturnRef ; class MockFoo : public Foo { ... MOCK_METHOD ( Bar & , GetBar , (), ( override )); MOCK_METHOD ( const Bar & , GetBar , (), ( const , override )); }; ... MockFoo foo ; Bar bar1 , bar2 ; EXPECT_CALL ( foo , GetBar ()) // The non-const GetBar(). . WillOnce ( ReturnRef ( bar1 )); EXPECT_CALL ( Const ( foo ), GetBar ()) // The const GetBar(). . WillOnce ( ReturnRef ( bar2 )); ( Const() is defined by gMock and returns a const reference to its argument.) To disambiguate overloaded functions with the same number of arguments but different argument types, you may need to specify the exact type of a matcher, either by wrapping your matcher in Matcher<type>() , or using a matcher whose type is fixed ( TypedEq<type> , An<type>() , etc): using :: testing :: An ; using :: testing :: Matcher ; using :: testing :: TypedEq ; class MockPrinter : public Printer { public : MOCK_METHOD ( void , Print , ( int n ), ( override )); MOCK_METHOD ( void , Print , ( char c ), ( override )); }; TEST ( PrinterTest , Print ) { MockPrinter printer ; EXPECT_CALL ( printer , Print ( An < int > ())); // void Print(int); EXPECT_CALL ( printer , Print ( Matcher < int > ( Lt ( 5 )))); // void Print(int); EXPECT_CALL ( printer , Print ( TypedEq < char > ( 'a' ))); // void Print(char); printer . Print ( 3 ); printer . Print ( 6 ); printer . Print ( 'a' ); } Performing Different Actions Based on the Arguments When a mock method is called, the last matching expectation that's still active will be selected (think \"newer overrides older\"). So, you can make a method do different things depending on its argument values like this: using :: testing :: _ ; using :: testing :: Lt ; using :: testing :: Return ; ... // The default case. EXPECT_CALL ( foo , DoThis ( _ )) . WillRepeatedly ( Return ( 'b' )); // The more specific case. EXPECT_CALL ( foo , DoThis ( Lt ( 5 ))) . WillRepeatedly ( Return ( 'a' )); Now, if foo.DoThis() is called with a value less than 5, 'a' will be returned; otherwise 'b' will be returned. Matching Multiple Arguments as a Whole Sometimes it's not enough to match the arguments individually. For example, we may want to say that the first argument must be less than the second argument. The With() clause allows us to match all arguments of a mock function as a whole. For example, using :: testing :: _ ; using :: testing :: Ne ; using :: testing :: Lt ; ... EXPECT_CALL ( foo , InRange ( Ne ( 0 ), _ )) . With ( Lt ()); says that the first argument of InRange() must not be 0, and must be less than the second argument. The expression inside With() must be a matcher of type Matcher< ::std::tuple<A1, ..., An> > , where A1 , ..., An are the types of the function arguments. You can also write AllArgs(m) instead of m inside .With() . The two forms are equivalent, but .With(AllArgs(Lt())) is more readable than .With(Lt()) . You can use Args<k1, ..., kn>(m) to match the n selected arguments (as a tuple) against m . For example, using :: testing :: _ ; using :: testing :: AllOf ; using :: testing :: Args ; using :: testing :: Lt ; ... EXPECT_CALL ( foo , Blah ) . With ( AllOf ( Args < 0 , 1 > ( Lt ()), Args < 1 , 2 > ( Lt ()))); says that Blah will be called with arguments x , y , and z where x < y < z . Note that in this example, it wasn't necessary specify the positional matchers. As a convenience and example, gMock provides some matchers for 2-tuples, including the Lt() matcher above. See here for the complete list. Note that if you want to pass the arguments to a predicate of your own (e.g. .With(Args<0, 1>(Truly(&MyPredicate))) ), that predicate MUST be written to take a ::std::tuple as its argument; gMock will pass the n selected arguments as one single tuple to the predicate. Using Matchers as Predicates Have you noticed that a matcher is just a fancy predicate that also knows how to describe itself? Many existing algorithms take predicates as arguments (e.g. those defined in STL's <algorithm> header), and it would be a shame if gMock matchers were not allowed to participate. Luckily, you can use a matcher where a unary predicate functor is expected by wrapping it inside the Matches() function. For example, #include <algorithm> #include <vector> using :: testing :: Matches ; using :: testing :: Ge ; vector < int > v ; ... // How many elements in v are >= 10? const int count = count_if ( v . begin (), v . end (), Matches ( Ge ( 10 ))); Since you can build complex matchers from simpler ones easily using gMock, this gives you a way to conveniently construct composite predicates (doing the same using STL's <functional> header is just painful). For example, here's a predicate that's satisfied by any number that is >= 0, <= 100, and != 50: using testing :: AllOf ; using testing :: Ge ; using testing :: Le ; using testing :: Matches ; using testing :: Ne ; ... Matches ( AllOf ( Ge ( 0 ), Le ( 100 ), Ne ( 50 ))) Using Matchers in googletest Assertions Since matchers are basically predicates that also know how to describe themselves, there is a way to take advantage of them in googletest assertions. It's called ASSERT_THAT and EXPECT_THAT : ASSERT_THAT ( value , matcher ); // Asserts that value matches matcher. EXPECT_THAT ( value , matcher ); // The non-fatal version. For example, in a googletest test you can write: #include \"gmock/gmock.h\" using :: testing :: AllOf ; using :: testing :: Ge ; using :: testing :: Le ; using :: testing :: MatchesRegex ; using :: testing :: StartsWith ; ... EXPECT_THAT ( Foo (), StartsWith ( \"Hello\" )); EXPECT_THAT ( Bar (), MatchesRegex ( \"Line \\\\ d+\" )); ASSERT_THAT ( Baz (), AllOf ( Ge ( 5 ), Le ( 10 ))); which (as you can probably guess) executes Foo() , Bar() , and Baz() , and verifies that: Foo() returns a string that starts with \"Hello\" . Bar() returns a string that matches regular expression \"Line \\\\d+\" . Baz() returns a number in the range [5, 10]. The nice thing about these macros is that they read like English . They generate informative messages too. For example, if the first EXPECT_THAT() above fails, the message will be something like: Value of : Foo () Actual : \"Hi, world!\" Expected : starts with \"Hello\" Credit: The idea of (ASSERT|EXPECT)_THAT was borrowed from Joe Walnes' Hamcrest project, which adds assertThat() to JUnit. Using Predicates as Matchers gMock provides a built-in set of matchers. In case you find them lacking, you can use an arbitrary unary predicate function or functor as a matcher - as long as the predicate accepts a value of the type you want. You do this by wrapping the predicate inside the Truly() function, for example: using :: testing :: Truly ; int IsEven ( int n ) { return ( n % 2 ) == 0 ? 1 : 0 ; } ... // Bar() must be called with an even number. EXPECT_CALL ( foo , Bar ( Truly ( IsEven ))); Note that the predicate function / functor doesn't have to return bool . It works as long as the return value can be used as the condition in in statement if (condition) ... . Matching Arguments that Are Not Copyable When you do an EXPECT_CALL(mock_obj, Foo(bar)) , gMock saves away a copy of bar . When Foo() is called later, gMock compares the argument to Foo() with the saved copy of bar . This way, you don't need to worry about bar being modified or destroyed after the EXPECT_CALL() is executed. The same is true when you use matchers like Eq(bar) , Le(bar) , and so on. But what if bar cannot be copied (i.e. has no copy constructor)? You could define your own matcher function or callback and use it with Truly() , as the previous couple of recipes have shown. Or, you may be able to get away from it if you can guarantee that bar won't be changed after the EXPECT_CALL() is executed. Just tell gMock that it should save a reference to bar , instead of a copy of it. Here's how: using :: testing :: ByRef ; using :: testing :: Eq ; using :: testing :: Lt ; ... // Expects that Foo()'s argument == bar. EXPECT_CALL ( mock_obj , Foo ( Eq ( ByRef ( bar )))); // Expects that Foo()'s argument < bar. EXPECT_CALL ( mock_obj , Foo ( Lt ( ByRef ( bar )))); Remember: if you do this, don't change bar after the EXPECT_CALL() , or the result is undefined. Validating a Member of an Object Often a mock function takes a reference to object as an argument. When matching the argument, you may not want to compare the entire object against a fixed object, as that may be over-specification. Instead, you may need to validate a certain member variable or the result of a certain getter method of the object. You can do this with Field() and Property() . More specifically, Field ( & Foo :: bar , m ) is a matcher that matches a Foo object whose bar member variable satisfies matcher m . Property ( & Foo :: baz , m ) is a matcher that matches a Foo object whose baz() method returns a value that satisfies matcher m . For example: Expression Description Field(&Foo::number, Ge(3)) Matches x where x.number >= 3 . Property(&Foo::name, StartsWith(\"John \")) Matches x where x.name() starts with \"John \" . Note that in Property(&Foo::baz, ...) , method baz() must take no argument and be declared as const . BTW, Field() and Property() can also match plain pointers to objects. For instance, using :: testing :: Field ; using :: testing :: Ge ; ... Field ( & Foo :: number , Ge ( 3 )) matches a plain pointer p where p->number >= 3 . If p is NULL , the match will always fail regardless of the inner matcher. What if you want to validate more than one members at the same time? Remember that there are AllOf() and AllOfArray() . Finally Field() and Property() provide overloads that take the field or property names as the first argument to include it in the error message. This can be useful when creating combined matchers. using :: testing :: AllOf ; using :: testing :: Field ; using :: testing :: Matcher ; using :: testing :: SafeMatcherCast ; Matcher < Foo > IsFoo ( const Foo & foo ) { return AllOf ( Field ( \"some_field\" , & Foo :: some_field , foo . some_field ), Field ( \"other_field\" , & Foo :: other_field , foo . other_field ), Field ( \"last_field\" , & Foo :: last_field , foo . last_field )); } Validating the Value Pointed to by a Pointer Argument C++ functions often take pointers as arguments. You can use matchers like IsNull() , NotNull() , and other comparison matchers to match a pointer, but what if you want to make sure the value pointed to by the pointer, instead of the pointer itself, has a certain property? Well, you can use the Pointee(m) matcher. Pointee(m) matches a pointer if and only if m matches the value the pointer points to. For example: using :: testing :: Ge ; using :: testing :: Pointee ; ... EXPECT_CALL ( foo , Bar ( Pointee ( Ge ( 3 )))); expects foo.Bar() to be called with a pointer that points to a value greater than or equal to 3. One nice thing about Pointee() is that it treats a NULL pointer as a match failure, so you can write Pointee(m) instead of using :: testing :: AllOf ; using :: testing :: NotNull ; using :: testing :: Pointee ; ... AllOf ( NotNull (), Pointee ( m )) without worrying that a NULL pointer will crash your test. Also, did we tell you that Pointee() works with both raw pointers and smart pointers ( std::unique_ptr , std::shared_ptr , etc)? What if you have a pointer to pointer? You guessed it - you can use nested Pointee() to probe deeper inside the value. For example, Pointee(Pointee(Lt(3))) matches a pointer that points to a pointer that points to a number less than 3 (what a mouthful...). Testing a Certain Property of an Object Sometimes you want to specify that an object argument has a certain property, but there is no existing matcher that does this. If you want good error messages, you should define a matcher . If you want to do it quick and dirty, you could get away with writing an ordinary function. Let's say you have a mock function that takes an object of type Foo , which has an int bar() method and an int baz() method, and you want to constrain that the argument's bar() value plus its baz() value is a given number. Here's how you can define a matcher to do it: using :: testing :: Matcher ; using :: testing :: MatcherInterface ; using :: testing :: MatchResultListener ; class BarPlusBazEqMatcher : public MatcherInterface < const Foo &> { public : explicit BarPlusBazEqMatcher ( int expected_sum ) : expected_sum_ ( expected_sum ) {} bool MatchAndExplain ( const Foo & foo , MatchResultListener * /* listener */ ) const override { return ( foo . bar () + foo . baz ()) == expected_sum_ ; } void DescribeTo ( :: std :: ostream * os ) const override { * os << \"bar() + baz() equals \" << expected_sum_ ; } void DescribeNegationTo ( :: std :: ostream * os ) const override { * os << \"bar() + baz() does not equal \" << expected_sum_ ; } private : const int expected_sum_ ; }; Matcher < const Foo &> BarPlusBazEq ( int expected_sum ) { return MakeMatcher ( new BarPlusBazEqMatcher ( expected_sum )); } ... EXPECT_CALL (..., DoThis ( BarPlusBazEq ( 5 )))...; Matching Containers Sometimes an STL container (e.g. list, vector, map, ...) is passed to a mock function and you may want to validate it. Since most STL containers support the == operator, you can write Eq(expected_container) or simply expected_container to match a container exactly. Sometimes, though, you may want to be more flexible (for example, the first element must be an exact match, but the second element can be any positive number, and so on). Also, containers used in tests often have a small number of elements, and having to define the expected container out-of-line is a bit of a hassle. You can use the ElementsAre() or UnorderedElementsAre() matcher in such cases: using :: testing :: _ ; using :: testing :: ElementsAre ; using :: testing :: Gt ; ... MOCK_METHOD ( void , Foo , ( const vector < int >& numbers ), ( override )); ... EXPECT_CALL ( mock , Foo ( ElementsAre ( 1 , Gt ( 0 ), _ , 5 ))); The above matcher says that the container must have 4 elements, which must be 1, greater than 0, anything, and 5 respectively. If you instead write: using :: testing :: _ ; using :: testing :: Gt ; using :: testing :: UnorderedElementsAre ; ... MOCK_METHOD ( void , Foo , ( const vector < int >& numbers ), ( override )); ... EXPECT_CALL ( mock , Foo ( UnorderedElementsAre ( 1 , Gt ( 0 ), _ , 5 ))); It means that the container must have 4 elements, which (under some permutation) must be 1, greater than 0, anything, and 5 respectively. As an alternative you can place the arguments in a C-style array and use ElementsAreArray() or UnorderedElementsAreArray() instead: using :: testing :: ElementsAreArray ; ... // ElementsAreArray accepts an array of element values. const int expected_vector1 [] = { 1 , 5 , 2 , 4 , ...}; EXPECT_CALL ( mock , Foo ( ElementsAreArray ( expected_vector1 ))); // Or, an array of element matchers. Matcher < int > expected_vector2 [] = { 1 , Gt ( 2 ), _ , 3 , ...}; EXPECT_CALL ( mock , Foo ( ElementsAreArray ( expected_vector2 ))); In case the array needs to be dynamically created (and therefore the array size cannot be inferred by the compiler), you can give ElementsAreArray() an additional argument to specify the array size: using :: testing :: ElementsAreArray ; ... int * const expected_vector3 = new int [ count ]; ... fill expected_vector3 with values ... EXPECT_CALL ( mock , Foo ( ElementsAreArray ( expected_vector3 , count ))); Use Pair when comparing maps or other associative containers. using testing :: ElementsAre ; using testing :: Pair ; ... std :: map < string , int > m = {{ \"a\" , 1 }, { \"b\" , 2 }, { \"c\" , 3 }}; EXPECT_THAT ( m , ElementsAre ( Pair ( \"a\" , 1 ), Pair ( \"b\" , 2 ), Pair ( \"c\" , 3 ))); Tips: ElementsAre*() can be used to match any container that implements the STL iterator pattern (i.e. it has a const_iterator type and supports begin()/end() ), not just the ones defined in STL. It will even work with container types yet to be written - as long as they follows the above pattern. You can use nested ElementsAre*() to match nested (multi-dimensional) containers. If the container is passed by pointer instead of by reference, just write Pointee(ElementsAre*(...)) . The order of elements matters for ElementsAre*() . If you are using it with containers whose element order are undefined (e.g. hash_map ) you should use WhenSorted around ElementsAre . Sharing Matchers Under the hood, a gMock matcher object consists of a pointer to a ref-counted implementation object. Copying matchers is allowed and very efficient, as only the pointer is copied. When the last matcher that references the implementation object dies, the implementation object will be deleted. Therefore, if you have some complex matcher that you want to use again and again, there is no need to build it everytime. Just assign it to a matcher variable and use that variable repeatedly! For example, using :: testing :: AllOf ; using :: testing :: Gt ; using :: testing :: Le ; using :: testing :: Matcher ; ... Matcher < int > in_range = AllOf ( Gt ( 5 ), Le ( 10 )); ... use in_range as a matcher in multiple EXPECT_CALLs ... Matchers must have no side-effects WARNING: gMock does not guarantee when or how many times a matcher will be invoked. Therefore, all matchers must be purely functional : they cannot have any side effects, and the match result must not depend on anything other than the matcher's parameters and the value being matched. This requirement must be satisfied no matter how a matcher is defined (e.g., if it is one of the standard matchers, or a custom matcher). In particular, a matcher can never call a mock function, as that will affect the state of the mock object and gMock. Setting Expectations Knowing When to Expect ON_CALL is likely the single most under-utilized construct in gMock. There are basically two constructs for defining the behavior of a mock object: ON_CALL and EXPECT_CALL . The difference? ON_CALL defines what happens when a mock method is called, but doesn't imply any expectation on the method being called . EXPECT_CALL not only defines the behavior, but also sets an expectation that the method will be called with the given arguments, for the given number of times (and in the given order when you specify the order too). Since EXPECT_CALL does more, isn't it better than ON_CALL ? Not really. Every EXPECT_CALL adds a constraint on the behavior of the code under test. Having more constraints than necessary is baaad - even worse than not having enough constraints. This may be counter-intuitive. How could tests that verify more be worse than tests that verify less? Isn't verification the whole point of tests? The answer lies in what a test should verify. A good test verifies the contract of the code. If a test over-specifies, it doesn't leave enough freedom to the implementation. As a result, changing the implementation without breaking the contract (e.g. refactoring and optimization), which should be perfectly fine to do, can break such tests. Then you have to spend time fixing them, only to see them broken again the next time the implementation is changed. Keep in mind that one doesn't have to verify more than one property in one test. In fact, it's a good style to verify only one thing in one test. If you do that, a bug will likely break only one or two tests instead of dozens (which case would you rather debug?). If you are also in the habit of giving tests descriptive names that tell what they verify, you can often easily guess what's wrong just from the test log itself. So use ON_CALL by default, and only use EXPECT_CALL when you actually intend to verify that the call is made. For example, you may have a bunch of ON_CALL s in your test fixture to set the common mock behavior shared by all tests in the same group, and write (scarcely) different EXPECT_CALL s in different TEST_F s to verify different aspects of the code's behavior. Compared with the style where each TEST has many EXPECT_CALL s, this leads to tests that are more resilient to implementational changes (and thus less likely to require maintenance) and makes the intent of the tests more obvious (so they are easier to maintain when you do need to maintain them). If you are bothered by the \"Uninteresting mock function call\" message printed when a mock method without an EXPECT_CALL is called, you may use a NiceMock instead to suppress all such messages for the mock object, or suppress the message for specific methods by adding EXPECT_CALL(...).Times(AnyNumber()) . DO NOT suppress it by blindly adding an EXPECT_CALL(...) , or you'll have a test that's a pain to maintain. Ignoring Uninteresting Calls If you are not interested in how a mock method is called, just don't say anything about it. In this case, if the method is ever called, gMock will perform its default action to allow the test program to continue. If you are not happy with the default action taken by gMock, you can override it using DefaultValue<T>::Set() (described here ) or ON_CALL() . Please note that once you expressed interest in a particular mock method (via EXPECT_CALL() ), all invocations to it must match some expectation. If this function is called but the arguments don't match any EXPECT_CALL() statement, it will be an error. Disallowing Unexpected Calls If a mock method shouldn't be called at all, explicitly say so: using :: testing :: _ ; ... EXPECT_CALL ( foo , Bar ( _ )) . Times ( 0 ); If some calls to the method are allowed, but the rest are not, just list all the expected calls: using :: testing :: AnyNumber ; using :: testing :: Gt ; ... EXPECT_CALL ( foo , Bar ( 5 )); EXPECT_CALL ( foo , Bar ( Gt ( 10 ))) . Times ( AnyNumber ()); A call to foo.Bar() that doesn't match any of the EXPECT_CALL() statements will be an error. Understanding Uninteresting vs Unexpected Calls Uninteresting calls and unexpected calls are different concepts in gMock. Very different. A call x.Y(...) is uninteresting if there's not even a single EXPECT_CALL(x, Y(...)) set. In other words, the test isn't interested in the x.Y() method at all, as evident in that the test doesn't care to say anything about it. A call x.Y(...) is unexpected if there are some EXPECT_CALL(x, Y(...)) s set, but none of them matches the call. Put another way, the test is interested in the x.Y() method (therefore it explicitly sets some EXPECT_CALL to verify how it's called); however, the verification fails as the test doesn't expect this particular call to happen. An unexpected call is always an error, as the code under test doesn't behave the way the test expects it to behave. By default, an uninteresting call is not an error, as it violates no constraint specified by the test. (gMock's philosophy is that saying nothing means there is no constraint.) However, it leads to a warning, as it might indicate a problem (e.g. the test author might have forgotten to specify a constraint). In gMock, NiceMock and StrictMock can be used to make a mock class \"nice\" or \"strict\". How does this affect uninteresting calls and unexpected calls? A nice mock suppresses uninteresting call warnings . It is less chatty than the default mock, but otherwise is the same. If a test fails with a default mock, it will also fail using a nice mock instead. And vice versa. Don't expect making a mock nice to change the test's result. A strict mock turns uninteresting call warnings into errors. So making a mock strict may change the test's result. Let's look at an example: TEST (...) { NiceMock < MockDomainRegistry > mock_registry ; EXPECT_CALL ( mock_registry , GetDomainOwner ( \"google.com\" )) . WillRepeatedly ( Return ( \"Larry Page\" )); // Use mock_registry in code under test. ... & mock_registry ... } The sole EXPECT_CALL here says that all calls to GetDomainOwner() must have \"google.com\" as the argument. If GetDomainOwner(\"yahoo.com\") is called, it will be an unexpected call, and thus an error. Having a nice mock doesn't change the severity of an unexpected call. So how do we tell gMock that GetDomainOwner() can be called with some other arguments as well? The standard technique is to add a \"catch all\" EXPECT_CALL : EXPECT_CALL ( mock_registry , GetDomainOwner ( _ )) . Times ( AnyNumber ()); // catches all other calls to this method. EXPECT_CALL ( mock_registry , GetDomainOwner ( \"google.com\" )) . WillRepeatedly ( Return ( \"Larry Page\" )); Remember that _ is the wildcard matcher that matches anything. With this, if GetDomainOwner(\"google.com\") is called, it will do what the second EXPECT_CALL says; if it is called with a different argument, it will do what the first EXPECT_CALL says. Note that the order of the two EXPECT_CALL s is important, as a newer EXPECT_CALL takes precedence over an older one. For more on uninteresting calls, nice mocks, and strict mocks, read \"The Nice, the Strict, and the Naggy\" . Ignoring Uninteresting Arguments If your test doesn't care about the parameters (it only cares about the number or order of calls), you can often simply omit the parameter list: // Expect foo.Bar( ... ) twice with any arguments. EXPECT_CALL ( foo , Bar ). Times ( 2 ); // Delegate to the given method whenever the factory is invoked. ON_CALL ( foo_factory , MakeFoo ) . WillByDefault ( & BuildFooForTest ); This functionality is only available when a method is not overloaded; to prevent unexpected behavior it is a compilation error to try to set an expectation on a method where the specific overload is ambiguous. You can work around this by supplying a simpler mock interface than the mocked class provides. This pattern is also useful when the arguments are interesting, but match logic is substantially complex. You can leave the argument list unspecified and use SaveArg actions to save the values for later verification . If you do that, you can easily differentiate calling the method the wrong number of times from calling it with the wrong arguments. Expecting Ordered Calls Although an EXPECT_CALL() statement defined earlier takes precedence when gMock tries to match a function call with an expectation, by default calls don't have to happen in the order EXPECT_CALL() statements are written. For example, if the arguments match the matchers in the third EXPECT_CALL() , but not those in the first two, then the third expectation will be used. If you would rather have all calls occur in the order of the expectations, put the EXPECT_CALL() statements in a block where you define a variable of type InSequence : using :: testing :: _ ; using :: testing :: InSequence ; { InSequence s ; EXPECT_CALL ( foo , DoThis ( 5 )); EXPECT_CALL ( bar , DoThat ( _ )) . Times ( 2 ); EXPECT_CALL ( foo , DoThis ( 6 )); } In this example, we expect a call to foo.DoThis(5) , followed by two calls to bar.DoThat() where the argument can be anything, which are in turn followed by a call to foo.DoThis(6) . If a call occurred out-of-order, gMock will report an error. Expecting Partially Ordered Calls Sometimes requiring everything to occur in a predetermined order can lead to brittle tests. For example, we may care about A occurring before both B and C , but aren't interested in the relative order of B and C . In this case, the test should reflect our real intent, instead of being overly constraining. gMock allows you to impose an arbitrary DAG (directed acyclic graph) on the calls. One way to express the DAG is to use the After clause of EXPECT_CALL . Another way is via the InSequence() clause (not the same as the InSequence class), which we borrowed from jMock 2. It's less flexible than After() , but more convenient when you have long chains of sequential calls, as it doesn't require you to come up with different names for the expectations in the chains. Here's how it works: If we view EXPECT_CALL() statements as nodes in a graph, and add an edge from node A to node B wherever A must occur before B, we can get a DAG. We use the term \"sequence\" to mean a directed path in this DAG. Now, if we decompose the DAG into sequences, we just need to know which sequences each EXPECT_CALL() belongs to in order to be able to reconstruct the original DAG. So, to specify the partial order on the expectations we need to do two things: first to define some Sequence objects, and then for each EXPECT_CALL() say which Sequence objects it is part of. Expectations in the same sequence must occur in the order they are written. For example, using :: testing :: Sequence ; ... Sequence s1 , s2 ; EXPECT_CALL ( foo , A ()) . InSequence ( s1 , s2 ); EXPECT_CALL ( bar , B ()) . InSequence ( s1 ); EXPECT_CALL ( bar , C ()) . InSequence ( s2 ); EXPECT_CALL ( foo , D ()) . InSequence ( s2 ); specifies the following DAG (where s1 is A -> B , and s2 is A -> C -> D ): +---> B | A ---| | +---> C ---> D This means that A must occur before B and C, and C must occur before D. There's no restriction about the order other than these. Controlling When an Expectation Retires When a mock method is called, gMock only considers expectations that are still active. An expectation is active when created, and becomes inactive (aka retires ) when a call that has to occur later has occurred. For example, in using :: testing :: _ ; using :: testing :: Sequence ; ... Sequence s1 , s2 ; EXPECT_CALL ( log , Log ( WARNING , _ , \"File too large.\" )) // #1 . Times ( AnyNumber ()) . InSequence ( s1 , s2 ); EXPECT_CALL ( log , Log ( WARNING , _ , \"Data set is empty.\" )) // #2 . InSequence ( s1 ); EXPECT_CALL ( log , Log ( WARNING , _ , \"User not found.\" )) // #3 . InSequence ( s2 ); as soon as either #2 or #3 is matched, #1 will retire. If a warning \"File too large.\" is logged after this, it will be an error. Note that an expectation doesn't retire automatically when it's saturated. For example, using :: testing :: _ ; ... EXPECT_CALL ( log , Log ( WARNING , _ , _ )); // #1 EXPECT_CALL ( log , Log ( WARNING , _ , \"File too large.\" )); // #2 says that there will be exactly one warning with the message \"File too large.\" . If the second warning contains this message too, #2 will match again and result in an upper-bound-violated error. If this is not what you want, you can ask an expectation to retire as soon as it becomes saturated: using :: testing :: _ ; ... EXPECT_CALL ( log , Log ( WARNING , _ , _ )); // #1 EXPECT_CALL ( log , Log ( WARNING , _ , \"File too large.\" )) // #2 . RetiresOnSaturation (); Here #2 can be used only once, so if you have two warnings with the message \"File too large.\" , the first will match #2 and the second will match #1 - there will be no error. Using Actions Returning References from Mock Methods If a mock function's return type is a reference, you need to use ReturnRef() instead of Return() to return a result: using :: testing :: ReturnRef ; class MockFoo : public Foo { public : MOCK_METHOD ( Bar & , GetBar , (), ( override )); }; ... MockFoo foo ; Bar bar ; EXPECT_CALL ( foo , GetBar ()) . WillOnce ( ReturnRef ( bar )); ... Returning Live Values from Mock Methods The Return(x) action saves a copy of x when the action is created, and always returns the same value whenever it's executed. Sometimes you may want to instead return the live value of x (i.e. its value at the time when the action is executed .). Use either ReturnRef() or ReturnPointee() for this purpose. If the mock function's return type is a reference, you can do it using ReturnRef(x) , as shown in the previous recipe (\"Returning References from Mock Methods\"). However, gMock doesn't let you use ReturnRef() in a mock function whose return type is not a reference, as doing that usually indicates a user error. So, what shall you do? Though you may be tempted, DO NOT use ByRef() : using testing :: ByRef ; using testing :: Return ; class MockFoo : public Foo { public : MOCK_METHOD ( int , GetValue , (), ( override )); }; ... int x = 0 ; MockFoo foo ; EXPECT_CALL ( foo , GetValue ()) . WillRepeatedly ( Return ( ByRef ( x ))); // Wrong! x = 42 ; EXPECT_EQ ( 42 , foo . GetValue ()); Unfortunately, it doesn't work here. The above code will fail with error: Value of: foo.GetValue() Actual: 0 Expected: 42 The reason is that Return(*value*) converts value to the actual return type of the mock function at the time when the action is created , not when it is executed . (This behavior was chosen for the action to be safe when value is a proxy object that references some temporary objects.) As a result, ByRef(x) is converted to an int value (instead of a const int& ) when the expectation is set, and Return(ByRef(x)) will always return 0. ReturnPointee(pointer) was provided to solve this problem specifically. It returns the value pointed to by pointer at the time the action is executed : using testing :: ReturnPointee ; ... int x = 0 ; MockFoo foo ; EXPECT_CALL ( foo , GetValue ()) . WillRepeatedly ( ReturnPointee ( & x )); // Note the & here. x = 42 ; EXPECT_EQ ( 42 , foo . GetValue ()); // This will succeed now. Combining Actions Want to do more than one thing when a function is called? That's fine. DoAll() allow you to do sequence of actions every time. Only the return value of the last action in the sequence will be used. using :: testing :: _ ; using :: testing :: DoAll ; class MockFoo : public Foo { public : MOCK_METHOD ( bool , Bar , ( int n ), ( override )); }; ... EXPECT_CALL ( foo , Bar ( _ )) . WillOnce ( DoAll ( action_1 , action_2 , ... action_n )); Verifying Complex Arguments If you want to verify that a method is called with a particular argument but the match criteria is complex, it can be difficult to distinguish between cardinality failures (calling the method the wrong number of times) and argument match failures. Similarly, if you are matching multiple parameters, it may not be easy to distinguishing which argument failed to match. For example: // Not ideal: this could fail because of a problem with arg1 or arg2, or maybe // just the method wasn't called. EXPECT_CALL ( foo , SendValues ( _ , ElementsAre ( 1 , 4 , 4 , 7 ), EqualsProto ( ... ))); You can instead save the arguments and test them individually: EXPECT_CALL ( foo , SendValues ) . WillOnce ( DoAll ( SaveArg < 1 > ( & actual_array ), SaveArg < 2 > ( & actual_proto ))); ... run the test EXPECT_THAT ( actual_array , ElementsAre ( 1 , 4 , 4 , 7 )); EXPECT_THAT ( actual_proto , EqualsProto ( ... )); Mocking Side Effects Sometimes a method exhibits its effect not via returning a value but via side effects. For example, it may change some global state or modify an output argument. To mock side effects, in general you can define your own action by implementing ::testing::ActionInterface . If all you need to do is to change an output argument, the built-in SetArgPointee() action is convenient: using :: testing :: _ ; using :: testing :: SetArgPointee ; class MockMutator : public Mutator { public : MOCK_METHOD ( void , Mutate , ( bool mutate , int * value ), ( override )); ... } ... MockMutator mutator ; EXPECT_CALL ( mutator , Mutate ( true , _ )) . WillOnce ( SetArgPointee < 1 > ( 5 )); In this example, when mutator.Mutate() is called, we will assign 5 to the int variable pointed to by argument #1 (0-based). SetArgPointee() conveniently makes an internal copy of the value you pass to it, removing the need to keep the value in scope and alive. The implication however is that the value must have a copy constructor and assignment operator. If the mock method also needs to return a value as well, you can chain SetArgPointee() with Return() using DoAll() , remembering to put the Return() statement last: using :: testing :: _ ; using :: testing :: Return ; using :: testing :: SetArgPointee ; class MockMutator : public Mutator { public : ... MOCK_METHOD ( bool , MutateInt , ( int * value ), ( override )); } ... MockMutator mutator ; EXPECT_CALL ( mutator , MutateInt ( _ )) . WillOnce ( DoAll ( SetArgPointee < 0 > ( 5 ), Return ( true ))); Note, however, that if you use the ReturnOKWith() method, it will override the values provided by SetArgPointee() in the response parameters of your function call. If the output argument is an array, use the SetArrayArgument<N>(first, last) action instead. It copies the elements in source range [first, last) to the array pointed to by the N -th (0-based) argument: using :: testing :: NotNull ; using :: testing :: SetArrayArgument ; class MockArrayMutator : public ArrayMutator { public : MOCK_METHOD ( void , Mutate , ( int * values , int num_values ), ( override )); ... } ... MockArrayMutator mutator ; int values [ 5 ] = { 1 , 2 , 3 , 4 , 5 }; EXPECT_CALL ( mutator , Mutate ( NotNull (), 5 )) . WillOnce ( SetArrayArgument < 0 > ( values , values + 5 )); This also works when the argument is an output iterator: using :: testing :: _ ; using :: testing :: SetArrayArgument ; class MockRolodex : public Rolodex { public : MOCK_METHOD ( void , GetNames , ( std :: back_insert_iterator < vector < string >> ), ( override )); ... } ... MockRolodex rolodex ; vector < string > names ; names . push_back ( \"George\" ); names . push_back ( \"John\" ); names . push_back ( \"Thomas\" ); EXPECT_CALL ( rolodex , GetNames ( _ )) . WillOnce ( SetArrayArgument < 0 > ( names . begin (), names . end ())); Changing a Mock Object's Behavior Based on the State If you expect a call to change the behavior of a mock object, you can use ::testing::InSequence to specify different behaviors before and after the call: using :: testing :: InSequence ; using :: testing :: Return ; ... { InSequence seq ; EXPECT_CALL ( my_mock , IsDirty ()) . WillRepeatedly ( Return ( true )); EXPECT_CALL ( my_mock , Flush ()); EXPECT_CALL ( my_mock , IsDirty ()) . WillRepeatedly ( Return ( false )); } my_mock . FlushIfDirty (); This makes my_mock.IsDirty() return true before my_mock.Flush() is called and return false afterwards. If the behavior change is more complex, you can store the effects in a variable and make a mock method get its return value from that variable: using :: testing :: _ ; using :: testing :: SaveArg ; using :: testing :: Return ; ACTION_P ( ReturnPointee , p ) { return * p ; } ... int previous_value = 0 ; EXPECT_CALL ( my_mock , GetPrevValue ) . WillRepeatedly ( ReturnPointee ( & previous_value )); EXPECT_CALL ( my_mock , UpdateValue ) . WillRepeatedly ( SaveArg < 0 > ( & previous_value )); my_mock . DoSomethingToUpdateValue (); Here my_mock.GetPrevValue() will always return the argument of the last UpdateValue() call. Setting the Default Value for a Return Type If a mock method's return type is a built-in C++ type or pointer, by default it will return 0 when invoked. Also, in C++ 11 and above, a mock method whose return type has a default constructor will return a default-constructed value by default. You only need to specify an action if this default value doesn't work for you. Sometimes, you may want to change this default value, or you may want to specify a default value for types gMock doesn't know about. You can do this using the ::testing::DefaultValue class template: using :: testing :: DefaultValue ; class MockFoo : public Foo { public : MOCK_METHOD ( Bar , CalculateBar , (), ( override )); }; ... Bar default_bar ; // Sets the default return value for type Bar. DefaultValue < Bar >:: Set ( default_bar ); MockFoo foo ; // We don't need to specify an action here, as the default // return value works for us. EXPECT_CALL ( foo , CalculateBar ()); foo . CalculateBar (); // This should return default_bar. // Unsets the default return value. DefaultValue < Bar >:: Clear (); Please note that changing the default value for a type can make you tests hard to understand. We recommend you to use this feature judiciously. For example, you may want to make sure the Set() and Clear() calls are right next to the code that uses your mock. Setting the Default Actions for a Mock Method You've learned how to change the default value of a given type. However, this may be too coarse for your purpose: perhaps you have two mock methods with the same return type and you want them to have different behaviors. The ON_CALL() macro allows you to customize your mock's behavior at the method level: using :: testing :: _ ; using :: testing :: AnyNumber ; using :: testing :: Gt ; using :: testing :: Return ; ... ON_CALL ( foo , Sign ( _ )) . WillByDefault ( Return ( -1 )); ON_CALL ( foo , Sign ( 0 )) . WillByDefault ( Return ( 0 )); ON_CALL ( foo , Sign ( Gt ( 0 ))) . WillByDefault ( Return ( 1 )); EXPECT_CALL ( foo , Sign ( _ )) . Times ( AnyNumber ()); foo . Sign ( 5 ); // This should return 1. foo . Sign ( -9 ); // This should return -1. foo . Sign ( 0 ); // This should return 0. As you may have guessed, when there are more than one ON_CALL() statements, the newer ones in the order take precedence over the older ones. In other words, the last one that matches the function arguments will be used. This matching order allows you to set up the common behavior in a mock object's constructor or the test fixture's set-up phase and specialize the mock's behavior later. Note that both ON_CALL and EXPECT_CALL have the same \"later statements take precedence\" rule, but they don't interact. That is, EXPECT_CALL s have their own precedence order distinct from the ON_CALL precedence order. Using Functions/Methods/Functors/Lambdas as Actions If the built-in actions don't suit you, you can use an existing callable (function, std::function , method, functor, lambda as an action. using :: testing :: _ ; using :: testing :: Invoke ; class MockFoo : public Foo { public : MOCK_METHOD ( int , Sum , ( int x , int y ), ( override )); MOCK_METHOD ( bool , ComplexJob , ( int x ), ( override )); }; int CalculateSum ( int x , int y ) { return x + y ; } int Sum3 ( int x , int y , int z ) { return x + y + z ; } class Helper { public : bool ComplexJob ( int x ); }; ... MockFoo foo ; Helper helper ; EXPECT_CALL ( foo , Sum ( _ , _ )) . WillOnce ( & CalculateSum ) . WillRepeatedly ( Invoke ( NewPermanentCallback ( Sum3 , 1 ))); EXPECT_CALL ( foo , ComplexJob ( _ )) . WillOnce ( Invoke ( & helper , & Helper :: ComplexJob )); . WillRepeatedly ([]( int x ) { return x > 0 ; }); foo . Sum ( 5 , 6 ); // Invokes CalculateSum(5, 6). foo . Sum ( 2 , 3 ); // Invokes Sum3(1, 2, 3). foo . ComplexJob ( 10 ); // Invokes helper.ComplexJob(10). foo . ComplexJob ( -1 ); // Invokes the inline lambda. The only requirement is that the type of the function, etc must be compatible with the signature of the mock function, meaning that the latter's arguments can be implicitly converted to the corresponding arguments of the former, and the former's return type can be implicitly converted to that of the latter. So, you can invoke something whose type is not exactly the same as the mock function, as long as it's safe to do so - nice, huh? Note: The action takes ownership of the callback and will delete it when the action itself is destructed. If the type of a callback is derived from a base callback type C , you need to implicitly cast it to C to resolve the overloading, e.g. using :: testing :: Invoke ; ... ResultCallback < bool >* is_ok = ...; ... Invoke ( is_ok ) ...; // This works. BlockingClosure * done = new BlockingClosure ; ... Invoke ( implicit_cast < Closure *> ( done )) ...; // The cast is necessary. Using Functions with Extra Info as Actions The function or functor you call using Invoke() must have the same number of arguments as the mock function you use it for. Sometimes you may have a function that takes more arguments, and you are willing to pass in the extra arguments yourself to fill the gap. You can do this in gMock using callbacks with pre-bound arguments. Here's an example: using :: testing :: Invoke ; class MockFoo : public Foo { public : MOCK_METHOD ( char , DoThis , ( int n ), ( override )); }; char SignOfSum ( int x , int y ) { const int sum = x + y ; return ( sum > 0 ) ? '+' : ( sum < 0 ) ? '-' : '0' ; } TEST_F ( FooTest , Test ) { MockFoo foo ; EXPECT_CALL ( foo , DoThis ( 2 )) . WillOnce ( Invoke ( NewPermanentCallback ( SignOfSum , 5 ))); EXPECT_EQ ( '+' , foo . DoThis ( 2 )); // Invokes SignOfSum(5, 2). } Invoking a Function/Method/Functor/Lambda/Callback Without Arguments Invoke() is very useful for doing actions that are more complex. It passes the mock function's arguments to the function, etc being invoked such that the callee has the full context of the call to work with. If the invoked function is not interested in some or all of the arguments, it can simply ignore them. Yet, a common pattern is that a test author wants to invoke a function without the arguments of the mock function. Invoke() allows her to do that using a wrapper function that throws away the arguments before invoking an underlining nullary function. Needless to say, this can be tedious and obscures the intent of the test. InvokeWithoutArgs() solves this problem. It's like Invoke() except that it doesn't pass the mock function's arguments to the callee. Here's an example: using :: testing :: _ ; using :: testing :: InvokeWithoutArgs ; class MockFoo : public Foo { public : MOCK_METHOD ( bool , ComplexJob , ( int n ), ( override )); }; bool Job1 () { ... } bool Job2 ( int n , char c ) { ... } ... MockFoo foo ; EXPECT_CALL ( foo , ComplexJob ( _ )) . WillOnce ( InvokeWithoutArgs ( Job1 )) . WillOnce ( InvokeWithoutArgs ( NewPermanentCallback ( Job2 , 5 , 'a' ))); foo . ComplexJob ( 10 ); // Invokes Job1(). foo . ComplexJob ( 20 ); // Invokes Job2(5, 'a'). Note: The action takes ownership of the callback and will delete it when the action itself is destructed. If the type of a callback is derived from a base callback type C , you need to implicitly cast it to C to resolve the overloading, e.g. using :: testing :: InvokeWithoutArgs ; ... ResultCallback < bool >* is_ok = ...; ... InvokeWithoutArgs ( is_ok ) ...; // This works. BlockingClosure * done = ...; ... InvokeWithoutArgs ( implicit_cast < Closure *> ( done )) ...; // The cast is necessary. Invoking an Argument of the Mock Function Sometimes a mock function will receive a function pointer, a functor (in other words, a \"callable\") as an argument, e.g. class MockFoo : public Foo { public : MOCK_METHOD ( bool , DoThis , ( int n , ( ResultCallback1 < bool , int >* callback )), ( override )); }; and you may want to invoke this callable argument: using :: testing :: _ ; ... MockFoo foo ; EXPECT_CALL ( foo , DoThis ( _ , _ )) . WillOnce (...); // Will execute callback->Run(5), where callback is the // second argument DoThis() receives. NOTE: The section below is legacy documentation from before C++ had lambdas: Arghh, you need to refer to a mock function argument but C++ has no lambda (yet), so you have to define your own action. :-( Or do you really? Well, gMock has an action to solve exactly this problem: InvokeArgument < N > ( arg_1 , arg_2 , ..., arg_m ) will invoke the N -th (0-based) argument the mock function receives, with arg_1 , arg_2 , ..., and arg_m . No matter if the argument is a function pointer, a functor, or a callback. gMock handles them all. With that, you could write: using :: testing :: _ ; using :: testing :: InvokeArgument ; ... EXPECT_CALL ( foo , DoThis ( _ , _ )) . WillOnce ( InvokeArgument < 1 > ( 5 )); // Will execute callback->Run(5), where callback is the // second argument DoThis() receives. What if the callable takes an argument by reference? No problem - just wrap it inside ByRef() : ... MOCK_METHOD ( bool , Bar , (( ResultCallback2 < bool , int , const Helper &>* callback )), ( override )); ... using :: testing :: _ ; using :: testing :: ByRef ; using :: testing :: InvokeArgument ; ... MockFoo foo ; Helper helper ; ... EXPECT_CALL ( foo , Bar ( _ )) . WillOnce ( InvokeArgument < 0 > ( 5 , ByRef ( helper ))); // ByRef(helper) guarantees that a reference to helper, not a copy of it, // will be passed to the callback. What if the callable takes an argument by reference and we do not wrap the argument in ByRef() ? Then InvokeArgument() will make a copy of the argument, and pass a reference to the copy , instead of a reference to the original value, to the callable. This is especially handy when the argument is a temporary value: ... MOCK_METHOD ( bool , DoThat , ( bool ( * f )( const double & x , const string & s )), ( override )); ... using :: testing :: _ ; using :: testing :: InvokeArgument ; ... MockFoo foo ; ... EXPECT_CALL ( foo , DoThat ( _ )) . WillOnce ( InvokeArgument < 0 > ( 5.0 , string ( \"Hi\" ))); // Will execute (*f)(5.0, string(\"Hi\")), where f is the function pointer // DoThat() receives. Note that the values 5.0 and string(\"Hi\") are // temporary and dead once the EXPECT_CALL() statement finishes. Yet // it's fine to perform this action later, since a copy of the values // are kept inside the InvokeArgument action. Ignoring an Action's Result Sometimes you have an action that returns something , but you need an action that returns void (perhaps you want to use it in a mock function that returns void , or perhaps it needs to be used in DoAll() and it's not the last in the list). IgnoreResult() lets you do that. For example: using :: testing :: _ ; using :: testing :: DoAll ; using :: testing :: IgnoreResult ; using :: testing :: Return ; int Process ( const MyData & data ); string DoSomething (); class MockFoo : public Foo { public : MOCK_METHOD ( void , Abc , ( const MyData & data ), ( override )); MOCK_METHOD ( bool , Xyz , (), ( override )); }; ... MockFoo foo ; EXPECT_CALL ( foo , Abc ( _ )) // .WillOnce(Invoke(Process)); // The above line won't compile as Process() returns int but Abc() needs // to return void. . WillOnce ( IgnoreResult ( Process )); EXPECT_CALL ( foo , Xyz ()) . WillOnce ( DoAll ( IgnoreResult ( DoSomething ), // Ignores the string DoSomething() returns. Return ( true ))); Note that you cannot use IgnoreResult() on an action that already returns void . Doing so will lead to ugly compiler errors. Selecting an Action's Arguments Say you have a mock function Foo() that takes seven arguments, and you have a custom action that you want to invoke when Foo() is called. Trouble is, the custom action only wants three arguments: using :: testing :: _ ; using :: testing :: Invoke ; ... MOCK_METHOD ( bool , Foo , ( bool visible , const string & name , int x , int y , ( const map < pair < int , int >> ), double & weight , double min_weight , double max_wight )); ... bool IsVisibleInQuadrant1 ( bool visible , int x , int y ) { return visible && x >= 0 && y >= 0 ; } ... EXPECT_CALL ( mock , Foo ) . WillOnce ( Invoke ( IsVisibleInQuadrant1 )); // Uh, won't compile. :-( To please the compiler God, you need to define an \"adaptor\" that has the same signature as Foo() and calls the custom action with the right arguments: using :: testing :: _ ; using :: testing :: Invoke ; ... bool MyIsVisibleInQuadrant1 ( bool visible , const string & name , int x , int y , const map < pair < int , int > , double >& weight , double min_weight , double max_wight ) { return IsVisibleInQuadrant1 ( visible , x , y ); } ... EXPECT_CALL ( mock , Foo ) . WillOnce ( Invoke ( MyIsVisibleInQuadrant1 )); // Now it works. But isn't this awkward? gMock provides a generic action adaptor , so you can spend your time minding more important business than writing your own adaptors. Here's the syntax: WithArgs < N1 , N2 , ..., Nk > ( action ) creates an action that passes the arguments of the mock function at the given indices (0-based) to the inner action and performs it. Using WithArgs , our original example can be written as: using :: testing :: _ ; using :: testing :: Invoke ; using :: testing :: WithArgs ; ... EXPECT_CALL ( mock , Foo ) . WillOnce ( WithArgs < 0 , 2 , 3 > ( Invoke ( IsVisibleInQuadrant1 ))); // No need to define your own adaptor. For better readability, gMock also gives you: WithoutArgs(action) when the inner action takes no argument, and WithArg<N>(action) (no s after Arg ) when the inner action takes one argument. As you may have realized, InvokeWithoutArgs(...) is just syntactic sugar for WithoutArgs(Invoke(...)) . Here are more tips: The inner action used in WithArgs and friends does not have to be Invoke() -- it can be anything. You can repeat an argument in the argument list if necessary, e.g. WithArgs<2, 3, 3, 5>(...) . You can change the order of the arguments, e.g. WithArgs<3, 2, 1>(...) . The types of the selected arguments do not have to match the signature of the inner action exactly. It works as long as they can be implicitly converted to the corresponding arguments of the inner action. For example, if the 4-th argument of the mock function is an int and my_action takes a double , WithArg<4>(my_action) will work. Ignoring Arguments in Action Functions The selecting-an-action's-arguments recipe showed us one way to make a mock function and an action with incompatible argument lists fit together. The downside is that wrapping the action in WithArgs<...>() can get tedious for people writing the tests. If you are defining a function (or method, functor, lambda, callback) to be used with Invoke*() , and you are not interested in some of its arguments, an alternative to WithArgs is to declare the uninteresting arguments as Unused . This makes the definition less cluttered and less fragile in case the types of the uninteresting arguments change. It could also increase the chance the action function can be reused. For example, given public : MOCK_METHOD ( double , Foo , double ( const string & label , double x , double y ), ( override )); MOCK_METHOD ( double , Bar , ( int index , double x , double y ), ( override )); instead of using :: testing :: _ ; using :: testing :: Invoke ; double DistanceToOriginWithLabel ( const string & label , double x , double y ) { return sqrt ( x * x + y * y ); } double DistanceToOriginWithIndex ( int index , double x , double y ) { return sqrt ( x * x + y * y ); } ... EXPECT_CALL ( mock , Foo ( \"abc\" , _ , _ )) . WillOnce ( Invoke ( DistanceToOriginWithLabel )); EXPECT_CALL ( mock , Bar ( 5 , _ , _ )) . WillOnce ( Invoke ( DistanceToOriginWithIndex )); you could write using :: testing :: _ ; using :: testing :: Invoke ; using :: testing :: Unused ; double DistanceToOrigin ( Unused , double x , double y ) { return sqrt ( x * x + y * y ); } ... EXPECT_CALL ( mock , Foo ( \"abc\" , _ , _ )) . WillOnce ( Invoke ( DistanceToOrigin )); EXPECT_CALL ( mock , Bar ( 5 , _ , _ )) . WillOnce ( Invoke ( DistanceToOrigin )); Sharing Actions Just like matchers, a gMock action object consists of a pointer to a ref-counted implementation object. Therefore copying actions is also allowed and very efficient. When the last action that references the implementation object dies, the implementation object will be deleted. If you have some complex action that you want to use again and again, you may not have to build it from scratch everytime. If the action doesn't have an internal state (i.e. if it always does the same thing no matter how many times it has been called), you can assign it to an action variable and use that variable repeatedly. For example: using :: testing :: Action ; using :: testing :: DoAll ; using :: testing :: Return ; using :: testing :: SetArgPointee ; ... Action < bool ( int * ) > set_flag = DoAll ( SetArgPointee < 0 > ( 5 ), Return ( true )); ... use set_flag in . WillOnce () and . WillRepeatedly () ... However, if the action has its own state, you may be surprised if you share the action object. Suppose you have an action factory IncrementCounter(init) which creates an action that increments and returns a counter whose initial value is init , using two actions created from the same expression and using a shared action will exhibit different behaviors. Example: EXPECT_CALL ( foo , DoThis ()) . WillRepeatedly ( IncrementCounter ( 0 )); EXPECT_CALL ( foo , DoThat ()) . WillRepeatedly ( IncrementCounter ( 0 )); foo . DoThis (); // Returns 1. foo . DoThis (); // Returns 2. foo . DoThat (); // Returns 1 - Blah() uses a different // counter than Bar()'s. versus using :: testing :: Action ; ... Action < int () > increment = IncrementCounter ( 0 ); EXPECT_CALL ( foo , DoThis ()) . WillRepeatedly ( increment ); EXPECT_CALL ( foo , DoThat ()) . WillRepeatedly ( increment ); foo . DoThis (); // Returns 1. foo . DoThis (); // Returns 2. foo . DoThat (); // Returns 3 - the counter is shared. Testing Asynchronous Behavior One oft-encountered problem with gMock is that it can be hard to test asynchronous behavior. Suppose you had a EventQueue class that you wanted to test, and you created a separate EventDispatcher interface so that you could easily mock it out. However, the implementation of the class fired all the events on a background thread, which made test timings difficult. You could just insert sleep() statements and hope for the best, but that makes your test behavior nondeterministic. A better way is to use gMock actions and Notification objects to force your asynchronous test to behave synchronously. using :: testing :: DoAll ; using :: testing :: InvokeWithoutArgs ; using :: testing :: Return ; class MockEventDispatcher : public EventDispatcher { MOCK_METHOD ( bool , DispatchEvent , ( int32 ), ( override )); }; ACTION_P ( Notify , notification ) { notification -> Notify (); } TEST ( EventQueueTest , EnqueueEventTest ) { MockEventDispatcher mock_event_dispatcher ; EventQueue event_queue ( & mock_event_dispatcher ); const int32 kEventId = 321 ; Notification done ; EXPECT_CALL ( mock_event_dispatcher , DispatchEvent ( kEventId )) . WillOnce ( Notify ( & done )); event_queue . EnqueueEvent ( kEventId ); done . WaitForNotification (); } In the example above, we set our normal gMock expectations, but then add an additional action to notify the Notification object. Now we can just call Notification::WaitForNotification() in the main thread to wait for the asynchronous call to finish. After that, our test suite is complete and we can safely exit. Note: this example has a downside: namely, if the expectation is not satisfied, our test will run forever. It will eventually time-out and fail, but it will take longer and be slightly harder to debug. To alleviate this problem, you can use WaitForNotificationWithTimeout(ms) instead of WaitForNotification() . Misc Recipes on Using gMock Mocking Methods That Use Move-Only Types C++11 introduced move-only types . A move-only-typed value can be moved from one object to another, but cannot be copied. std::unique_ptr<T> is probably the most commonly used move-only type. Mocking a method that takes and/or returns move-only types presents some challenges, but nothing insurmountable. This recipe shows you how you can do it. Note that the support for move-only method arguments was only introduced to gMock in April 2017; in older code, you may find more complex workarounds for lack of this feature. Let\u2019s say we are working on a fictional project that lets one post and share snippets called \u201cbuzzes\u201d. Your code uses these types: enum class AccessLevel { kInternal , kPublic }; class Buzz { public : explicit Buzz ( AccessLevel access ) { ... } ... }; class Buzzer { public : virtual ~ Buzzer () {} virtual std :: unique_ptr < Buzz > MakeBuzz ( StringPiece text ) = 0 ; virtual bool ShareBuzz ( std :: unique_ptr < Buzz > buzz , int64_t timestamp ) = 0 ; ... }; A Buzz object represents a snippet being posted. A class that implements the Buzzer interface is capable of creating and sharing Buzz es. Methods in Buzzer may return a unique_ptr<Buzz> or take a unique_ptr<Buzz> . Now we need to mock Buzzer in our tests. To mock a method that accepts or returns move-only types, you just use the familiar MOCK_METHOD syntax as usual: class MockBuzzer : public Buzzer { public : MOCK_METHOD ( std :: unique_ptr < Buzz > , MakeBuzz , ( StringPiece text ), ( override )); MOCK_METHOD ( bool , ShareBuzz , ( std :: unique_ptr < Buzz > buzz , int64_t timestamp ), ( override )); }; Now that we have the mock class defined, we can use it in tests. In the following code examples, we assume that we have defined a MockBuzzer object named mock_buzzer_ : MockBuzzer mock_buzzer_ ; First let\u2019s see how we can set expectations on the MakeBuzz() method, which returns a unique_ptr<Buzz> . As usual, if you set an expectation without an action (i.e. the .WillOnce() or .WillRepeatedly() clause), when that expectation fires, the default action for that method will be taken. Since unique_ptr<> has a default constructor that returns a null unique_ptr , that\u2019s what you\u2019ll get if you don\u2019t specify an action: // Use the default action. EXPECT_CALL ( mock_buzzer_ , MakeBuzz ( \"hello\" )); // Triggers the previous EXPECT_CALL. EXPECT_EQ ( nullptr , mock_buzzer_ . MakeBuzz ( \"hello\" )); If you are not happy with the default action, you can tweak it as usual; see Setting Default Actions . If you just need to return a pre-defined move-only value, you can use the Return(ByMove(...)) action: // When this fires, the unique_ptr<> specified by ByMove(...) will // be returned. EXPECT_CALL ( mock_buzzer_ , MakeBuzz ( \"world\" )) . WillOnce ( Return ( ByMove ( MakeUnique < Buzz > ( AccessLevel :: kInternal )))); EXPECT_NE ( nullptr , mock_buzzer_ . MakeBuzz ( \"world\" )); Note that ByMove() is essential here - if you drop it, the code won\u2019t compile. Quiz time! What do you think will happen if a Return(ByMove(...)) action is performed more than once (e.g. you write ... .WillRepeatedly(Return(ByMove(...))); )? Come think of it, after the first time the action runs, the source value will be consumed (since it\u2019s a move-only value), so the next time around, there\u2019s no value to move from -- you\u2019ll get a run-time error that Return(ByMove(...)) can only be run once. If you need your mock method to do more than just moving a pre-defined value, remember that you can always use a lambda or a callable object, which can do pretty much anything you want: EXPECT_CALL ( mock_buzzer_ , MakeBuzz ( \"x\" )) . WillRepeatedly ([]( StringPiece text ) { return MakeUnique < Buzz > ( AccessLevel :: kInternal ); }); EXPECT_NE ( nullptr , mock_buzzer_ . MakeBuzz ( \"x\" )); EXPECT_NE ( nullptr , mock_buzzer_ . MakeBuzz ( \"x\" )); Every time this EXPECT_CALL fires, a new unique_ptr<Buzz> will be created and returned. You cannot do this with Return(ByMove(...)) . That covers returning move-only values; but how do we work with methods accepting move-only arguments? The answer is that they work normally, although some actions will not compile when any of method's arguments are move-only. You can always use Return , or a lambda or functor : using :: testing :: Unused ; EXPECT_CALL ( mock_buzzer_ , ShareBuzz ( NotNull (), _ )). WillOnce ( Return ( true )); EXPECT_TRUE ( mock_buzzer_ . ShareBuzz ( MakeUnique < Buzz > ( AccessLevel :: kInternal )), 0 ); EXPECT_CALL ( mock_buzzer_ , ShareBuzz ( _ , _ )). WillOnce ( []( std :: unique_ptr < Buzz > buzz , Unused ) { return buzz != nullptr ; }); EXPECT_FALSE ( mock_buzzer_ . ShareBuzz ( nullptr , 0 )); Many built-in actions ( WithArgs , WithoutArgs , DeleteArg , SaveArg , ...) could in principle support move-only arguments, but the support for this is not implemented yet. If this is blocking you, please file a bug. A few actions (e.g. DoAll ) copy their arguments internally, so they can never work with non-copyable objects; you'll have to use functors instead. Legacy workarounds for move-only types Support for move-only function arguments was only introduced to gMock in April 2017. In older code, you may encounter the following workaround for the lack of this feature (it is no longer necessary - we're including it just for reference): class MockBuzzer : public Buzzer { public : MOCK_METHOD ( bool , DoShareBuzz , ( Buzz * buzz , Time timestamp )); bool ShareBuzz ( std :: unique_ptr < Buzz > buzz , Time timestamp ) override { return DoShareBuzz ( buzz . get (), timestamp ); } }; The trick is to delegate the ShareBuzz() method to a mock method (let\u2019s call it DoShareBuzz() ) that does not take move-only parameters. Then, instead of setting expectations on ShareBuzz() , you set them on the DoShareBuzz() mock method: MockBuzzer mock_buzzer_ ; EXPECT_CALL ( mock_buzzer_ , DoShareBuzz ( NotNull (), _ )); // When one calls ShareBuzz() on the MockBuzzer like this, the call is // forwarded to DoShareBuzz(), which is mocked. Therefore this statement // will trigger the above EXPECT_CALL. mock_buzzer_ . ShareBuzz ( MakeUnique < Buzz > ( AccessLevel :: kInternal ), 0 ); Making the Compilation Faster Believe it or not, the vast majority of the time spent on compiling a mock class is in generating its constructor and destructor, as they perform non-trivial tasks (e.g. verification of the expectations). What's more, mock methods with different signatures have different types and thus their constructors/destructors need to be generated by the compiler separately. As a result, if you mock many different types of methods, compiling your mock class can get really slow. If you are experiencing slow compilation, you can move the definition of your mock class' constructor and destructor out of the class body and into a .cc file. This way, even if you #include your mock class in N files, the compiler only needs to generate its constructor and destructor once, resulting in a much faster compilation. Let's illustrate the idea using an example. Here's the definition of a mock class before applying this recipe: // File mock_foo.h. ... class MockFoo : public Foo { public : // Since we don't declare the constructor or the destructor, // the compiler will generate them in every translation unit // where this mock class is used. MOCK_METHOD ( int , DoThis , (), ( override )); MOCK_METHOD ( bool , DoThat , ( const char * str ), ( override )); ... more mock methods ... }; After the change, it would look like: // File mock_foo.h. ... class MockFoo : public Foo { public : // The constructor and destructor are declared, but not defined, here. MockFoo (); virtual ~ MockFoo (); MOCK_METHOD ( int , DoThis , (), ( override )); MOCK_METHOD ( bool , DoThat , ( const char * str ), ( override )); ... more mock methods ... }; and // File mock_foo.cc. #include \"path/to/mock_foo.h\" // The definitions may appear trivial, but the functions actually do a // lot of things through the constructors/destructors of the member // variables used to implement the mock methods. MockFoo :: MockFoo () {} MockFoo ::~ MockFoo () {} Forcing a Verification When it's being destroyed, your friendly mock object will automatically verify that all expectations on it have been satisfied, and will generate googletest failures if not. This is convenient as it leaves you with one less thing to worry about. That is, unless you are not sure if your mock object will be destroyed. How could it be that your mock object won't eventually be destroyed? Well, it might be created on the heap and owned by the code you are testing. Suppose there's a bug in that code and it doesn't delete the mock object properly - you could end up with a passing test when there's actually a bug. Using a heap checker is a good idea and can alleviate the concern, but its implementation is not 100% reliable. So, sometimes you do want to force gMock to verify a mock object before it is (hopefully) destructed. You can do this with Mock::VerifyAndClearExpectations(&mock_object) : TEST ( MyServerTest , ProcessesRequest ) { using :: testing :: Mock ; MockFoo * const foo = new MockFoo ; EXPECT_CALL ( * foo , ...)...; // ... other expectations ... // server now owns foo. MyServer server ( foo ); server . ProcessRequest (...); // In case that server's destructor will forget to delete foo, // this will verify the expectations anyway. Mock :: VerifyAndClearExpectations ( foo ); } // server is destroyed when it goes out of scope here. Tip: The Mock::VerifyAndClearExpectations() function returns a bool to indicate whether the verification was successful ( true for yes), so you can wrap that function call inside a ASSERT_TRUE() if there is no point going further when the verification has failed. Using Check Points Sometimes you may want to \"reset\" a mock object at various check points in your test: at each check point, you verify that all existing expectations on the mock object have been satisfied, and then you set some new expectations on it as if it's newly created. This allows you to work with a mock object in \"phases\" whose sizes are each manageable. One such scenario is that in your test's SetUp() function, you may want to put the object you are testing into a certain state, with the help from a mock object. Once in the desired state, you want to clear all expectations on the mock, such that in the TEST_F body you can set fresh expectations on it. As you may have figured out, the Mock::VerifyAndClearExpectations() function we saw in the previous recipe can help you here. Or, if you are using ON_CALL() to set default actions on the mock object and want to clear the default actions as well, use Mock::VerifyAndClear(&mock_object) instead. This function does what Mock::VerifyAndClearExpectations(&mock_object) does and returns the same bool , plus it clears the ON_CALL() statements on mock_object too. Another trick you can use to achieve the same effect is to put the expectations in sequences and insert calls to a dummy \"check-point\" function at specific places. Then you can verify that the mock function calls do happen at the right time. For example, if you are exercising code: Foo ( 1 ); Foo ( 2 ); Foo ( 3 ); and want to verify that Foo(1) and Foo(3) both invoke mock.Bar(\"a\") , but Foo(2) doesn't invoke anything. You can write: using :: testing :: MockFunction ; TEST ( FooTest , InvokesBarCorrectly ) { MyMock mock ; // Class MockFunction<F> has exactly one mock method. It is named // Call() and has type F. MockFunction < void ( string check_point_name ) > check ; { InSequence s ; EXPECT_CALL ( mock , Bar ( \"a\" )); EXPECT_CALL ( check , Call ( \"1\" )); EXPECT_CALL ( check , Call ( \"2\" )); EXPECT_CALL ( mock , Bar ( \"a\" )); } Foo ( 1 ); check . Call ( \"1\" ); Foo ( 2 ); check . Call ( \"2\" ); Foo ( 3 ); } The expectation spec says that the first Bar(\"a\") must happen before check point \"1\", the second Bar(\"a\") must happen after check point \"2\", and nothing should happen between the two check points. The explicit check points make it easy to tell which Bar(\"a\") is called by which call to Foo() . Mocking Destructors Sometimes you want to make sure a mock object is destructed at the right time, e.g. after bar->A() is called but before bar->B() is called. We already know that you can specify constraints on the order of mock function calls, so all we need to do is to mock the destructor of the mock function. This sounds simple, except for one problem: a destructor is a special function with special syntax and special semantics, and the MOCK_METHOD macro doesn't work for it: MOCK_METHOD ( void , ~ MockFoo , ()); // Won't compile! The good news is that you can use a simple pattern to achieve the same effect. First, add a mock function Die() to your mock class and call it in the destructor, like this: class MockFoo : public Foo { ... // Add the following two lines to the mock class. MOCK_METHOD ( void , Die , ()); virtual ~ MockFoo () { Die (); } }; (If the name Die() clashes with an existing symbol, choose another name.) Now, we have translated the problem of testing when a MockFoo object dies to testing when its Die() method is called: MockFoo * foo = new MockFoo ; MockBar * bar = new MockBar ; ... { InSequence s ; // Expects *foo to die after bar->A() and before bar->B(). EXPECT_CALL ( * bar , A ()); EXPECT_CALL ( * foo , Die ()); EXPECT_CALL ( * bar , B ()); } And that's that. Using gMock and Threads In a unit test, it's best if you could isolate and test a piece of code in a single-threaded context. That avoids race conditions and dead locks, and makes debugging your test much easier. Yet most programs are multi-threaded, and sometimes to test something we need to pound on it from more than one thread. gMock works for this purpose too. Remember the steps for using a mock: Create a mock object foo . Set its default actions and expectations using ON_CALL() and EXPECT_CALL() . The code under test calls methods of foo . Optionally, verify and reset the mock. Destroy the mock yourself, or let the code under test destroy it. The destructor will automatically verify it. If you follow the following simple rules, your mocks and threads can live happily together: Execute your test code (as opposed to the code being tested) in one thread. This makes your test easy to follow. Obviously, you can do step #1 without locking. When doing step #2 and #5, make sure no other thread is accessing foo . Obvious too, huh? 3 and #4 can be done either in one thread or in multiple threads - anyway you want. gMock takes care of the locking, so you don't have to do any - unless required by your test logic. If you violate the rules (for example, if you set expectations on a mock while another thread is calling its methods), you get undefined behavior. That's not fun, so don't do it. gMock guarantees that the action for a mock function is done in the same thread that called the mock function. For example, in EXPECT_CALL ( mock , Foo ( 1 )) . WillOnce ( action1 ); EXPECT_CALL ( mock , Foo ( 2 )) . WillOnce ( action2 ); if Foo(1) is called in thread 1 and Foo(2) is called in thread 2, gMock will execute action1 in thread 1 and action2 in thread 2. gMock does not impose a sequence on actions performed in different threads (doing so may create deadlocks as the actions may need to cooperate). This means that the execution of action1 and action2 in the above example may interleave. If this is a problem, you should add proper synchronization logic to action1 and action2 to make the test thread-safe. Also, remember that DefaultValue<T> is a global resource that potentially affects all living mock objects in your program. Naturally, you won't want to mess with it from multiple threads or when there still are mocks in action. Controlling How Much Information gMock Prints When gMock sees something that has the potential of being an error (e.g. a mock function with no expectation is called, a.k.a. an uninteresting call, which is allowed but perhaps you forgot to explicitly ban the call), it prints some warning messages, including the arguments of the function, the return value, and the stack trace. Hopefully this will remind you to take a look and see if there is indeed a problem. Sometimes you are confident that your tests are correct and may not appreciate such friendly messages. Some other times, you are debugging your tests or learning about the behavior of the code you are testing, and wish you could observe every mock call that happens (including argument values, the return value, and the stack trace). Clearly, one size doesn't fit all. You can control how much gMock tells you using the --gmock_verbose=LEVEL command-line flag, where LEVEL is a string with three possible values: info : gMock will print all informational messages, warnings, and errors (most verbose). At this setting, gMock will also log any calls to the ON_CALL/EXPECT_CALL macros. It will include a stack trace in \"uninteresting call\" warnings. warning : gMock will print both warnings and errors (less verbose); it will omit the stack traces in \"uninteresting call\" warnings. This is the default. error : gMock will print errors only (least verbose). Alternatively, you can adjust the value of that flag from within your tests like so: :: testing :: FLAGS_gmock_verbose = \"error\" ; If you find gMock printing too many stack frames with its informational or warning messages, remember that you can control their amount with the --gtest_stack_trace_depth=max_depth flag. Now, judiciously use the right flag to enable gMock serve you better! Gaining Super Vision into Mock Calls You have a test using gMock. It fails: gMock tells you some expectations aren't satisfied. However, you aren't sure why: Is there a typo somewhere in the matchers? Did you mess up the order of the EXPECT_CALL s? Or is the code under test doing something wrong? How can you find out the cause? Won't it be nice if you have X-ray vision and can actually see the trace of all EXPECT_CALL s and mock method calls as they are made? For each call, would you like to see its actual argument values and which EXPECT_CALL gMock thinks it matches? If you still need some help to figure out who made these calls, how about being able to see the complete stack trace at each mock call? You can unlock this power by running your test with the --gmock_verbose=info flag. For example, given the test program: #include \"gmock/gmock.h\" using testing :: _ ; using testing :: HasSubstr ; using testing :: Return ; class MockFoo { public : MOCK_METHOD ( void , F , ( const string & x , const string & y )); }; TEST ( Foo , Bar ) { MockFoo mock ; EXPECT_CALL ( mock , F ( _ , _ )). WillRepeatedly ( Return ()); EXPECT_CALL ( mock , F ( \"a\" , \"b\" )); EXPECT_CALL ( mock , F ( \"c\" , HasSubstr ( \"d\" ))); mock . F ( \"a\" , \"good\" ); mock . F ( \"a\" , \"b\" ); } if you run it with --gmock_verbose=info , you will see this output: [ RUN ] Foo.Bar foo_test.cc:14: EXPECT_CALL ( mock, F ( _, _ )) invoked Stack trace: ... foo_test.cc:15: EXPECT_CALL ( mock, F ( \"a\" , \"b\" )) invoked Stack trace: ... foo_test.cc:16: EXPECT_CALL ( mock, F ( \"c\" , HasSubstr ( \"d\" ))) invoked Stack trace: ... foo_test.cc:14: Mock function call matches EXPECT_CALL ( mock, F ( _, _ )) ... Function call: F ( @0x7fff7c8dad40 \"a\" ,@0x7fff7c8dad10 \"good\" ) Stack trace: ... foo_test.cc:15: Mock function call matches EXPECT_CALL ( mock, F ( \"a\" , \"b\" )) ... Function call: F ( @0x7fff7c8dada0 \"a\" ,@0x7fff7c8dad70 \"b\" ) Stack trace: ... foo_test.cc:16: Failure Actual function call count doesn ' t match EXPECT_CALL ( mock, F ( \"c\" , HasSubstr ( \"d\" ))) ... Expected: to be called once Actual: never called - unsatisfied and active [ FAILED ] Foo.Bar Suppose the bug is that the \"c\" in the third EXPECT_CALL is a typo and should actually be \"a\" . With the above message, you should see that the actual F(\"a\", \"good\") call is matched by the first EXPECT_CALL , not the third as you thought. From that it should be obvious that the third EXPECT_CALL is written wrong. Case solved. If you are interested in the mock call trace but not the stack traces, you can combine --gmock_verbose=info with --gtest_stack_trace_depth=0 on the test command line. Running Tests in Emacs If you build and run your tests in Emacs using the M-x google-compile command (as many googletest users do), the source file locations of gMock and googletest errors will be highlighted. Just press <Enter> on one of them and you'll be taken to the offending line. Or, you can just type `C-x`` to jump to the next error. To make it even easier, you can add the following lines to your ~/.emacs file: (global-set-key \"\\M-m\" 'google-compile) ; m is for make (global-set-key [M-down] 'next-error) (global-set-key [M-up] '(lambda () (interactive) (next-error -1))) Then you can type M-m to start a build (if you want to run the test as well, just make sure foo_test.run or runtests is in the build command you supply after typing M-m ), or M-up / M-down to move back and forth between errors. Extending gMock Writing New Matchers Quickly WARNING: gMock does not guarantee when or how many times a matcher will be invoked. Therefore, all matchers must be functionally pure. See this section for more details. The MATCHER* family of macros can be used to define custom matchers easily. The syntax: MATCHER ( name , description_string_expression ) { statements ; } will define a matcher with the given name that executes the statements, which must return a bool to indicate if the match succeeds. Inside the statements, you can refer to the value being matched by arg , and refer to its type by arg_type . The description string is a string -typed expression that documents what the matcher does, and is used to generate the failure message when the match fails. It can (and should) reference the special bool variable negation , and should evaluate to the description of the matcher when negation is false , or that of the matcher's negation when negation is true . For convenience, we allow the description string to be empty ( \"\" ), in which case gMock will use the sequence of words in the matcher name as the description. For example: MATCHER ( IsDivisibleBy7 , \"\" ) { return ( arg % 7 ) == 0 ; } allows you to write // Expects mock_foo.Bar(n) to be called where n is divisible by 7. EXPECT_CALL ( mock_foo , Bar ( IsDivisibleBy7 ())); or, using :: testing :: Not ; ... // Verifies that two values are divisible by 7. EXPECT_THAT ( some_expression , IsDivisibleBy7 ()); EXPECT_THAT ( some_other_expression , Not ( IsDivisibleBy7 ())); If the above assertions fail, they will print something like: Value of: some_expression Expected: is divisible by 7 Actual: 27 ... Value of: some_other_expression Expected: not ( is divisible by 7 ) Actual: 21 where the descriptions \"is divisible by 7\" and \"not (is divisible by 7)\" are automatically calculated from the matcher name IsDivisibleBy7 . As you may have noticed, the auto-generated descriptions (especially those for the negation) may not be so great. You can always override them with a string expression of your own: MATCHER ( IsDivisibleBy7 , absl :: StrCat ( negation ? \"isn't\" : \"is\" , \" divisible by 7\" )) { return ( arg % 7 ) == 0 ; } Optionally, you can stream additional information to a hidden argument named result_listener to explain the match result. For example, a better definition of IsDivisibleBy7 is: MATCHER ( IsDivisibleBy7 , \"\" ) { if (( arg % 7 ) == 0 ) return true ; * result_listener << \"the remainder is \" << ( arg % 7 ); return false ; } With this definition, the above assertion will give a better message: Value of: some_expression Expected: is divisible by 7 Actual: 27 ( the remainder is 6 ) You should let MatchAndExplain() print any additional information that can help a user understand the match result. Note that it should explain why the match succeeds in case of a success (unless it's obvious) - this is useful when the matcher is used inside Not() . There is no need to print the argument value itself, as gMock already prints it for you. NOTE: The type of the value being matched ( arg_type ) is determined by the context in which you use the matcher and is supplied to you by the compiler, so you don't need to worry about declaring it (nor can you). This allows the matcher to be polymorphic. For example, IsDivisibleBy7() can be used to match any type where the value of (arg % 7) == 0 can be implicitly converted to a bool . In the Bar(IsDivisibleBy7()) example above, if method Bar() takes an int , arg_type will be int ; if it takes an unsigned long , arg_type will be unsigned long ; and so on. Writing New Parameterized Matchers Quickly Sometimes you'll want to define a matcher that has parameters. For that you can use the macro: MATCHER_P ( name , param_name , description_string ) { statements ; } where the description string can be either \"\" or a string expression that references negation and param_name . For example: MATCHER_P ( HasAbsoluteValue , value , \"\" ) { return abs ( arg ) == value ; } will allow you to write: EXPECT_THAT ( Blah ( \"a\" ), HasAbsoluteValue ( n )); which may lead to this message (assuming n is 10): Value of: Blah ( \"a\" ) Expected: has absolute value 10 Actual: -9 Note that both the matcher description and its parameter are printed, making the message human-friendly. In the matcher definition body, you can write foo_type to reference the type of a parameter named foo . For example, in the body of MATCHER_P(HasAbsoluteValue, value) above, you can write value_type to refer to the type of value . gMock also provides MATCHER_P2 , MATCHER_P3 , ..., up to MATCHER_P10 to support multi-parameter matchers: MATCHER_Pk ( name , param_1 , ..., param_k , description_string ) { statements ; } Please note that the custom description string is for a particular instance of the matcher, where the parameters have been bound to actual values. Therefore usually you'll want the parameter values to be part of the description. gMock lets you do that by referencing the matcher parameters in the description string expression. For example, using :: testing :: PrintToString ; MATCHER_P2 ( InClosedRange , low , hi , absl :: StrFormat ( \"%s in range [%s, %s]\" , negation ? \"isn't\" : \"is\" , PrintToString ( low ), PrintToString ( hi ))) { return low <= arg && arg <= hi ; } ... EXPECT_THAT ( 3 , InClosedRange ( 4 , 6 )); would generate a failure that contains the message: Expected: is in range [ 4 , 6 ] If you specify \"\" as the description, the failure message will contain the sequence of words in the matcher name followed by the parameter values printed as a tuple. For example, MATCHER_P2 ( InClosedRange , low , hi , \"\" ) { ... } ... EXPECT_THAT ( 3 , InClosedRange ( 4 , 6 )); would generate a failure that contains the text: Expected: in closed range ( 4 , 6 ) For the purpose of typing, you can view MATCHER_Pk ( Foo , p1 , ..., pk , description_string ) { ... } as shorthand for template < typename p1_type , ..., typename pk_type > FooMatcherPk < p1_type , ..., pk_type > Foo ( p1_type p1 , ..., pk_type pk ) { ... } When you write Foo(v1, ..., vk) , the compiler infers the types of the parameters v1 , ..., and vk for you. If you are not happy with the result of the type inference, you can specify the types by explicitly instantiating the template, as in Foo<long, bool>(5, false) . As said earlier, you don't get to (or need to) specify arg_type as that's determined by the context in which the matcher is used. You can assign the result of expression Foo(p1, ..., pk) to a variable of type FooMatcherPk<p1_type, ..., pk_type> . This can be useful when composing matchers. Matchers that don't have a parameter or have only one parameter have special types: you can assign Foo() to a FooMatcher -typed variable, and assign Foo(p) to a FooMatcherP<p_type> -typed variable. While you can instantiate a matcher template with reference types, passing the parameters by pointer usually makes your code more readable. If, however, you still want to pass a parameter by reference, be aware that in the failure message generated by the matcher you will see the value of the referenced object but not its address. You can overload matchers with different numbers of parameters: MATCHER_P ( Blah , a , description_string_1 ) { ... } MATCHER_P2 ( Blah , a , b , description_string_2 ) { ... } While it's tempting to always use the MATCHER* macros when defining a new matcher, you should also consider implementing MatcherInterface or using MakePolymorphicMatcher() instead (see the recipes that follow), especially if you need to use the matcher a lot. While these approaches require more work, they give you more control on the types of the value being matched and the matcher parameters, which in general leads to better compiler error messages that pay off in the long run. They also allow overloading matchers based on parameter types (as opposed to just based on the number of parameters). Writing New Monomorphic Matchers A matcher of argument type T implements ::testing::MatcherInterface<T> and does two things: it tests whether a value of type T matches the matcher, and can describe what kind of values it matches. The latter ability is used for generating readable error messages when expectations are violated. The interface looks like this: class MatchResultListener { public : ... // Streams x to the underlying ostream; does nothing if the ostream // is NULL. template < typename T > MatchResultListener & operator << ( const T & x ); // Returns the underlying ostream. :: std :: ostream * stream (); }; template < typename T > class MatcherInterface { public : virtual ~ MatcherInterface (); // Returns true if and only if the matcher matches x; also explains the match // result to 'listener'. virtual bool MatchAndExplain ( T x , MatchResultListener * listener ) const = 0 ; // Describes this matcher to an ostream. virtual void DescribeTo ( :: std :: ostream * os ) const = 0 ; // Describes the negation of this matcher to an ostream. virtual void DescribeNegationTo ( :: std :: ostream * os ) const ; }; If you need a custom matcher but Truly() is not a good option (for example, you may not be happy with the way Truly(predicate) describes itself, or you may want your matcher to be polymorphic as Eq(value) is), you can define a matcher to do whatever you want in two steps: first implement the matcher interface, and then define a factory function to create a matcher instance. The second step is not strictly needed but it makes the syntax of using the matcher nicer. For example, you can define a matcher to test whether an int is divisible by 7 and then use it like this: using :: testing :: MakeMatcher ; using :: testing :: Matcher ; using :: testing :: MatcherInterface ; using :: testing :: MatchResultListener ; class DivisibleBy7Matcher : public MatcherInterface < int > { public : bool MatchAndExplain ( int n , MatchResultListener * /* listener */ ) const override { return ( n % 7 ) == 0 ; } void DescribeTo ( :: std :: ostream * os ) const override { * os << \"is divisible by 7\" ; } void DescribeNegationTo ( :: std :: ostream * os ) const override { * os << \"is not divisible by 7\" ; } }; Matcher < int > DivisibleBy7 () { return MakeMatcher ( new DivisibleBy7Matcher ); } ... EXPECT_CALL ( foo , Bar ( DivisibleBy7 ())); You may improve the matcher message by streaming additional information to the listener argument in MatchAndExplain() : class DivisibleBy7Matcher : public MatcherInterface < int > { public : bool MatchAndExplain ( int n , MatchResultListener * listener ) const override { const int remainder = n % 7 ; if ( remainder != 0 ) { * listener << \"the remainder is \" << remainder ; } return remainder == 0 ; } ... }; Then, EXPECT_THAT(x, DivisibleBy7()); may generate a message like this: Value of: x Expected: is divisible by 7 Actual: 23 ( the remainder is 2 ) Writing New Polymorphic Matchers You've learned how to write your own matchers in the previous recipe. Just one problem: a matcher created using MakeMatcher() only works for one particular type of arguments. If you want a polymorphic matcher that works with arguments of several types (for instance, Eq(x) can be used to match a value as long as value == x compiles -- value and x don't have to share the same type), you can learn the trick from testing/base/public/gmock-matchers.h but it's a bit involved. Fortunately, most of the time you can define a polymorphic matcher easily with the help of MakePolymorphicMatcher() . Here's how you can define NotNull() as an example: using :: testing :: MakePolymorphicMatcher ; using :: testing :: MatchResultListener ; using :: testing :: PolymorphicMatcher ; class NotNullMatcher { public : // To implement a polymorphic matcher, first define a COPYABLE class // that has three members MatchAndExplain(), DescribeTo(), and // DescribeNegationTo(), like the following. // In this example, we want to use NotNull() with any pointer, so // MatchAndExplain() accepts a pointer of any type as its first argument. // In general, you can define MatchAndExplain() as an ordinary method or // a method template, or even overload it. template < typename T > bool MatchAndExplain ( T * p , MatchResultListener * /* listener */ ) const { return p != NULL ; } // Describes the property of a value matching this matcher. void DescribeTo ( std :: ostream * os ) const { * os << \"is not NULL\" ; } // Describes the property of a value NOT matching this matcher. void DescribeNegationTo ( std :: ostream * os ) const { * os << \"is NULL\" ; } }; // To construct a polymorphic matcher, pass an instance of the class // to MakePolymorphicMatcher(). Note the return type. PolymorphicMatcher < NotNullMatcher > NotNull () { return MakePolymorphicMatcher ( NotNullMatcher ()); } ... EXPECT_CALL ( foo , Bar ( NotNull ())); // The argument must be a non-NULL pointer. Note: Your polymorphic matcher class does not need to inherit from MatcherInterface or any other class, and its methods do not need to be virtual. Like in a monomorphic matcher, you may explain the match result by streaming additional information to the listener argument in MatchAndExplain() . Writing New Cardinalities A cardinality is used in Times() to tell gMock how many times you expect a call to occur. It doesn't have to be exact. For example, you can say AtLeast(5) or Between(2, 4) . If the built-in set of cardinalities doesn't suit you, you are free to define your own by implementing the following interface (in namespace testing ): class CardinalityInterface { public : virtual ~ CardinalityInterface (); // Returns true if and only if call_count calls will satisfy this cardinality. virtual bool IsSatisfiedByCallCount ( int call_count ) const = 0 ; // Returns true if and only if call_count calls will saturate this // cardinality. virtual bool IsSaturatedByCallCount ( int call_count ) const = 0 ; // Describes self to an ostream. virtual void DescribeTo ( std :: ostream * os ) const = 0 ; }; For example, to specify that a call must occur even number of times, you can write using :: testing :: Cardinality ; using :: testing :: CardinalityInterface ; using :: testing :: MakeCardinality ; class EvenNumberCardinality : public CardinalityInterface { public : bool IsSatisfiedByCallCount ( int call_count ) const override { return ( call_count % 2 ) == 0 ; } bool IsSaturatedByCallCount ( int call_count ) const override { return false ; } void DescribeTo ( std :: ostream * os ) const { * os << \"called even number of times\" ; } }; Cardinality EvenNumber () { return MakeCardinality ( new EvenNumberCardinality ); } ... EXPECT_CALL ( foo , Bar ( 3 )) . Times ( EvenNumber ()); Writing New Actions Quickly If the built-in actions don't work for you, you can easily define your own one. Just define a functor class with a (possibly templated) call operator, matching the signature of your action. struct Increment { template < typename T > T operator ()( T * arg ) { return ++ ( * arg ); } } The same approach works with stateful functors (or any callable, really): struct MultiplyBy { template <typename T> T operator()(T arg) { return arg * multiplier; } int multiplier; } // Then use: // EXPECT_CALL(...).WillOnce(MultiplyBy{7}); Legacy macro-based Actions Before C++11, the functor-based actions were not supported; the old way of writing actions was through a set of ACTION* macros. We suggest to avoid them in new code; they hide a lot of logic behind the macro, potentially leading to harder-to-understand compiler errors. Nevertheless, we cover them here for completeness. By writing ACTION ( name ) { statements ; } in a namespace scope (i.e. not inside a class or function), you will define an action with the given name that executes the statements. The value returned by statements will be used as the return value of the action. Inside the statements, you can refer to the K-th (0-based) argument of the mock function as argK . For example: ACTION ( IncrementArg1 ) { return ++ ( * arg1 ); } allows you to write ... WillOnce ( IncrementArg1 ()); Note that you don't need to specify the types of the mock function arguments. Rest assured that your code is type-safe though: you'll get a compiler error if *arg1 doesn't support the ++ operator, or if the type of ++(*arg1) isn't compatible with the mock function's return type. Another example: ACTION ( Foo ) { ( * arg2 )( 5 ); Blah (); * arg1 = 0 ; return arg0 ; } defines an action Foo() that invokes argument #2 (a function pointer) with 5, calls function Blah() , sets the value pointed to by argument #1 to 0, and returns argument #0. For more convenience and flexibility, you can also use the following pre-defined symbols in the body of ACTION : argK_type The type of the K-th (0-based) argument of the mock function args All arguments of the mock function as a tuple args_type The type of all arguments of the mock function as a tuple return_type The return type of the mock function function_type The type of the mock function For example, when using an ACTION as a stub action for mock function: int DoSomething ( bool flag , int * ptr ); we have: Pre-defined Symbol Is Bound To arg0 the value of flag arg0_type the type bool arg1 the value of ptr arg1_type the type int* args the tuple (flag, ptr) args_type the type std::tuple<bool, int*> return_type the type int function_type the type int(bool, int*) Legacy macro-based parameterized Actions Sometimes you'll want to parameterize an action you define. For that we have another macro ACTION_P ( name , param ) { statements ; } For example, ACTION_P ( Add , n ) { return arg0 + n ; } will allow you to write // Returns argument #0 + 5. ... WillOnce ( Add ( 5 )); For convenience, we use the term arguments for the values used to invoke the mock function, and the term parameters for the values used to instantiate an action. Note that you don't need to provide the type of the parameter either. Suppose the parameter is named param , you can also use the gMock-defined symbol param_type to refer to the type of the parameter as inferred by the compiler. For example, in the body of ACTION_P(Add, n) above, you can write n_type for the type of n . gMock also provides ACTION_P2 , ACTION_P3 , and etc to support multi-parameter actions. For example, ACTION_P2 ( ReturnDistanceTo , x , y ) { double dx = arg0 - x ; double dy = arg1 - y ; return sqrt ( dx * dx + dy * dy ); } lets you write ... WillOnce ( ReturnDistanceTo ( 5.0 , 26.5 )); You can view ACTION as a degenerated parameterized action where the number of parameters is 0. You can also easily define actions overloaded on the number of parameters: ACTION_P ( Plus , a ) { ... } ACTION_P2 ( Plus , a , b ) { ... } Restricting the Type of an Argument or Parameter in an ACTION For maximum brevity and reusability, the ACTION* macros don't ask you to provide the types of the mock function arguments and the action parameters. Instead, we let the compiler infer the types for us. Sometimes, however, we may want to be more explicit about the types. There are several tricks to do that. For example: ACTION ( Foo ) { // Makes sure arg0 can be converted to int. int n = arg0 ; ... use n instead of arg0 here ... } ACTION_P ( Bar , param ) { // Makes sure the type of arg1 is const char*. :: testing :: StaticAssertTypeEq < const char * , arg1_type > (); // Makes sure param can be converted to bool. bool flag = param ; } where StaticAssertTypeEq is a compile-time assertion in googletest that verifies two types are the same. Writing New Action Templates Quickly Sometimes you want to give an action explicit template parameters that cannot be inferred from its value parameters. ACTION_TEMPLATE() supports that and can be viewed as an extension to ACTION() and ACTION_P*() . The syntax: ACTION_TEMPLATE ( ActionName , HAS_m_TEMPLATE_PARAMS ( kind1 , name1 , ..., kind_m , name_m ), AND_n_VALUE_PARAMS ( p1 , ..., p_n )) { statements ; } defines an action template that takes m explicit template parameters and n value parameters, where m is in [1, 10] and n is in [0, 10]. name_i is the name of the i -th template parameter, and kind_i specifies whether it's a typename , an integral constant, or a template. p_i is the name of the i -th value parameter. Example: // DuplicateArg<k, T>(output) converts the k-th argument of the mock // function to type T and copies it to *output. ACTION_TEMPLATE ( DuplicateArg , // Note the comma between int and k: HAS_2_TEMPLATE_PARAMS ( int , k , typename , T ), AND_1_VALUE_PARAMS ( output )) { * output = T ( :: std :: get < k > ( args )); } To create an instance of an action template, write: ActionName < t1 , ..., t_m > ( v1 , ..., v_n ) where the t s are the template arguments and the v s are the value arguments. The value argument types are inferred by the compiler. For example: using :: testing :: _ ; ... int n ; EXPECT_CALL ( mock , Foo ). WillOnce ( DuplicateArg < 1 , unsigned char > ( & n )); If you want to explicitly specify the value argument types, you can provide additional template arguments: ActionName < t1 , ..., t_m , u1 , ..., u_k > ( v1 , ..., v_n ) where u_i is the desired type of v_i . ACTION_TEMPLATE and ACTION / ACTION_P* can be overloaded on the number of value parameters, but not on the number of template parameters. Without the restriction, the meaning of the following is unclear: OverloadedAction < int , bool > ( x ); Are we using a single-template-parameter action where bool refers to the type of x , or a two-template-parameter action where the compiler is asked to infer the type of x ? Using the ACTION Object's Type If you are writing a function that returns an ACTION object, you'll need to know its type. The type depends on the macro used to define the action and the parameter types. The rule is relatively simple: Given Definition Expression Has Type ACTION(Foo) Foo() FooAction ACTION_TEMPLATE(Foo, Foo<t1, ..., | FooAction<t1, ..., : HAS_m_TEMPLATE_PARAMS(...), : t_m>() : t_m> : : AND_0_VALUE_PARAMS()) : : : ACTION_P(Bar, param) Bar(int_value) BarActionP<int> ACTION_TEMPLATE(Bar, Bar<t1, ..., t_m> `FooActionP<t1, ..., : HAS_m_TEMPLATE_PARAMS(...), : (int_value) : t_m, int>` : : AND_1_VALUE_PARAMS(p1)) : : : ACTION_P2(Baz, p1, p2) Baz(bool_value, `BazActionP2<bool, : : int_value) : int>` : ACTION_TEMPLATE(Baz, Baz<t1, ..., t_m> `FooActionP2<t1, ..., : HAS_m_TEMPLATE_PARAMS(...), : (bool_value, : t_m, bool, int>` : : AND_2_VALUE_PARAMS(p1, p2)) : int_value) : : ... ... ... Note that we have to pick different suffixes ( Action , ActionP , ActionP2 , and etc) for actions with different numbers of value parameters, or the action definitions cannot be overloaded on the number of them. Writing New Monomorphic Actions While the ACTION* macros are very convenient, sometimes they are inappropriate. For example, despite the tricks shown in the previous recipes, they don't let you directly specify the types of the mock function arguments and the action parameters, which in general leads to unoptimized compiler error messages that can baffle unfamiliar users. They also don't allow overloading actions based on parameter types without jumping through some hoops. An alternative to the ACTION* macros is to implement ::testing::ActionInterface<F> , where F is the type of the mock function in which the action will be used. For example: template < typename F > class ActionInterface { public : virtual ~ ActionInterface (); // Performs the action. Result is the return type of function type // F, and ArgumentTuple is the tuple of arguments of F. // // For example, if F is int(bool, const string&), then Result would // be int, and ArgumentTuple would be ::std::tuple<bool, const string&>. virtual Result Perform ( const ArgumentTuple & args ) = 0 ; }; using :: testing :: _ ; using :: testing :: Action ; using :: testing :: ActionInterface ; using :: testing :: MakeAction ; typedef int IncrementMethod ( int * ); class IncrementArgumentAction : public ActionInterface < IncrementMethod > { public : int Perform ( const :: std :: tuple < int *>& args ) override { int * p = :: std :: get < 0 > ( args ); // Grabs the first argument. return * p ++ ; } }; Action < IncrementMethod > IncrementArgument () { return MakeAction ( new IncrementArgumentAction ); } ... EXPECT_CALL ( foo , Baz ( _ )) . WillOnce ( IncrementArgument ()); int n = 5 ; foo . Baz ( & n ); // Should return 5 and change n to 6. Writing New Polymorphic Actions The previous recipe showed you how to define your own action. This is all good, except that you need to know the type of the function in which the action will be used. Sometimes that can be a problem. For example, if you want to use the action in functions with different types (e.g. like Return() and SetArgPointee() ). If an action can be used in several types of mock functions, we say it's polymorphic . The MakePolymorphicAction() function template makes it easy to define such an action: namespace testing { template < typename Impl > PolymorphicAction < Impl > MakePolymorphicAction ( const Impl & impl ); } // namespace testing As an example, let's define an action that returns the second argument in the mock function's argument list. The first step is to define an implementation class: class ReturnSecondArgumentAction { public : template < typename Result , typename ArgumentTuple > Result Perform ( const ArgumentTuple & args ) const { // To get the i-th (0-based) argument, use ::std::get(args). return :: std :: get < 1 > ( args ); } }; This implementation class does not need to inherit from any particular class. What matters is that it must have a Perform() method template. This method template takes the mock function's arguments as a tuple in a single argument, and returns the result of the action. It can be either const or not, but must be invokable with exactly one template argument, which is the result type. In other words, you must be able to call Perform<R>(args) where R is the mock function's return type and args is its arguments in a tuple. Next, we use MakePolymorphicAction() to turn an instance of the implementation class into the polymorphic action we need. It will be convenient to have a wrapper for this: using :: testing :: MakePolymorphicAction ; using :: testing :: PolymorphicAction ; PolymorphicAction < ReturnSecondArgumentAction > ReturnSecondArgument () { return MakePolymorphicAction ( ReturnSecondArgumentAction ()); } Now, you can use this polymorphic action the same way you use the built-in ones: using :: testing :: _ ; class MockFoo : public Foo { public : MOCK_METHOD ( int , DoThis , ( bool flag , int n ), ( override )); MOCK_METHOD ( string , DoThat , ( int x , const char * str1 , const char * str2 ), ( override )); }; ... MockFoo foo ; EXPECT_CALL ( foo , DoThis ). WillOnce ( ReturnSecondArgument ()); EXPECT_CALL ( foo , DoThat ). WillOnce ( ReturnSecondArgument ()); ... foo . DoThis ( true , 5 ); // Will return 5. foo . DoThat ( 1 , \"Hi\" , \"Bye\" ); // Will return \"Hi\". Teaching gMock How to Print Your Values When an uninteresting or unexpected call occurs, gMock prints the argument values and the stack trace to help you debug. Assertion macros like EXPECT_THAT and EXPECT_EQ also print the values in question when the assertion fails. gMock and googletest do this using googletest's user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. googletest's advanced guide explains how to extend the printer to do a better job at printing your particular type than to dump the bytes. Useful Mocks Created Using gMock Mock std::function std::function is a general function type introduced in C++11. It is a preferred way of passing callbacks to new interfaces. Functions are copiable, and are not usually passed around by pointer, which makes them tricky to mock. But fear not - MockFunction can help you with that. MockFunction<R(T1, ..., Tn)> has a mock method Call() with the signature: R Call ( T1 , ..., Tn ); It also has a AsStdFunction() method, which creates a std::function proxy forwarding to Call: std :: function < R ( T1 , ..., Tn ) > AsStdFunction (); To use MockFunction , first create MockFunction object and set up expectations on its Call method. Then pass proxy obtained from AsStdFunction() to the code you are testing. For example: TEST ( FooTest , RunsCallbackWithBarArgument ) { // 1. Create a mock object. MockFunction < int ( string ) > mock_function ; // 2. Set expectations on Call() method. EXPECT_CALL ( mock_function , Call ( \"bar\" )). WillOnce ( Return ( 1 )); // 3. Exercise code that uses std::function. Foo ( mock_function . AsStdFunction ()); // Foo's signature can be either of: // void Foo(const std::function<int(string)>& fun); // void Foo(std::function<int(string)> fun); // 4. All expectations will be verified when mock_function // goes out of scope and is destroyed. } Remember that function objects created with AsStdFunction() are just forwarders. If you create multiple of them, they will share the same set of expectations. Although std::function supports unlimited number of arguments, MockFunction implementation is limited to ten. If you ever hit that limit... well, your callback has bigger problems than being mockable. :-)","title":"gMock Cookbook"},{"location":"examples/gtest/docs/cook_book.html#gmock-cookbook","text":"You can find recipes for using gMock here. If you haven't yet, please read this first to make sure you understand the basics. Note: gMock lives in the testing name space. For readability, it is recommended to write using ::testing::Foo; once in your file before using the name Foo defined by gMock. We omit such using statements in this section for brevity, but you should do it in your own code.","title":"gMock Cookbook"},{"location":"examples/gtest/docs/cook_book.html#creating-mock-classes","text":"Mock classes are defined as normal classes, using the MOCK_METHOD macro to generate mocked methods. The macro gets 3 or 4 parameters: class MyMock { public : MOCK_METHOD ( ReturnType , MethodName , ( Args ...)); MOCK_METHOD ( ReturnType , MethodName , ( Args ...), ( Specs ...)); }; The first 3 parameters are simply the method declaration, split into 3 parts. The 4th parameter accepts a closed list of qualifiers, which affect the generated method: const - Makes the mocked method a const method. Required if overriding a const method. override - Marks the method with override . Recommended if overriding a virtual method. noexcept - Marks the method with noexcept . Required if overriding a noexcept method. Calltype(...) - Sets the call type for the method (e.g. to STDMETHODCALLTYPE ), useful in Windows.","title":"Creating Mock Classes"},{"location":"examples/gtest/docs/cook_book.html#dealing-with-unprotected-commas","text":"Unprotected commas, i.e. commas which are not surrounded by parentheses, prevent MOCK_METHOD from parsing its arguments correctly: ```cpp {.bad} class MockFoo { public: MOCK_METHOD(std::pair , GetPair, ()); // Won't compile! MOCK_METHOD(bool, CheckMap, (std::map , bool)); // Won't compile! }; Solution 1 - wrap with parentheses: ```cpp {.good} class MockFoo { public: MOCK_METHOD((std::pair<bool, int>), GetPair, ()); MOCK_METHOD(bool, CheckMap, ((std::map<int, double>), bool)); }; Note that wrapping a return or argument type with parentheses is, in general, invalid C++. MOCK_METHOD removes the parentheses. Solution 2 - define an alias: ```cpp {.good} class MockFoo { public: using BoolAndInt = std::pair ; MOCK_METHOD(BoolAndInt, GetPair, ()); using MapIntDouble = std::map ; MOCK_METHOD(bool, CheckMap, (MapIntDouble, bool)); }; ### Mocking Private or Protected Methods You must always put a mock method definition (`MOCK_METHOD`) in a `public:` section of the mock class, regardless of the method being mocked being `public`, `protected`, or `private` in the base class. This allows `ON_CALL` and `EXPECT_CALL` to reference the mock function from outside of the mock class. (Yes, C++ allows a subclass to change the access level of a virtual function in the base class.) Example: ```cpp class Foo { public: ... virtual bool Transform(Gadget* g) = 0; protected: virtual void Resume(); private: virtual int GetTimeOut(); }; class MockFoo : public Foo { public: ... MOCK_METHOD(bool, Transform, (Gadget* g), (override)); // The following must be in the public section, even though the // methods are protected or private in the base class. MOCK_METHOD(void, Resume, (), (override)); MOCK_METHOD(int, GetTimeOut, (), (override)); };","title":"Dealing with unprotected commas"},{"location":"examples/gtest/docs/cook_book.html#mocking-overloaded-methods","text":"You can mock overloaded functions as usual. No special attention is required: class Foo { ... // Must be virtual as we'll inherit from Foo. virtual ~ Foo (); // Overloaded on the types and/or numbers of arguments. virtual int Add ( Element x ); virtual int Add ( int times , Element x ); // Overloaded on the const-ness of this object. virtual Bar & GetBar (); virtual const Bar & GetBar () const ; }; class MockFoo : public Foo { ... MOCK_METHOD ( int , Add , ( Element x ), ( override )); MOCK_METHOD ( int , Add , ( int times , Element x ), ( override )); MOCK_METHOD ( Bar & , GetBar , (), ( override )); MOCK_METHOD ( const Bar & , GetBar , (), ( const , override )); }; Note: if you don't mock all versions of the overloaded method, the compiler will give you a warning about some methods in the base class being hidden. To fix that, use using to bring them in scope: class MockFoo : public Foo { ... using Foo :: Add ; MOCK_METHOD ( int , Add , ( Element x ), ( override )); // We don't want to mock int Add(int times, Element x); ... };","title":"Mocking Overloaded Methods"},{"location":"examples/gtest/docs/cook_book.html#mocking-class-templates","text":"You can mock class templates just like any class. template < typename Elem > class StackInterface { ... // Must be virtual as we'll inherit from StackInterface. virtual ~ StackInterface (); virtual int GetSize () const = 0 ; virtual void Push ( const Elem & x ) = 0 ; }; template < typename Elem > class MockStack : public StackInterface < Elem > { ... MOCK_METHOD ( int , GetSize , (), ( override )); MOCK_METHOD ( void , Push , ( const Elem & x ), ( override )); };","title":"Mocking Class Templates"},{"location":"examples/gtest/docs/cook_book.html#MockingNonVirtualMethods","text":"gMock can mock non-virtual functions to be used in Hi-perf dependency injection. In this case, instead of sharing a common base class with the real class, your mock class will be unrelated to the real class, but contain methods with the same signatures. The syntax for mocking non-virtual methods is the same as mocking virtual methods (just don't add override ): // A simple packet stream class. None of its members is virtual. class ConcretePacketStream { public : void AppendPacket ( Packet * new_packet ); const Packet * GetPacket ( size_t packet_number ) const ; size_t NumberOfPackets () const ; ... }; // A mock packet stream class. It inherits from no other, but defines // GetPacket() and NumberOfPackets(). class MockPacketStream { public : MOCK_METHOD ( const Packet * , GetPacket , ( size_t packet_number ), ( const )); MOCK_METHOD ( size_t , NumberOfPackets , (), ( const )); ... }; Note that the mock class doesn't define AppendPacket() , unlike the real class. That's fine as long as the test doesn't need to call it. Next, you need a way to say that you want to use ConcretePacketStream in production code, and use MockPacketStream in tests. Since the functions are not virtual and the two classes are unrelated, you must specify your choice at compile time (as opposed to run time). One way to do it is to templatize your code that needs to use a packet stream. More specifically, you will give your code a template type argument for the type of the packet stream. In production, you will instantiate your template with ConcretePacketStream as the type argument. In tests, you will instantiate the same template with MockPacketStream . For example, you may write: template < class PacketStream > void CreateConnection ( PacketStream * stream ) { ... } template < class PacketStream > class PacketReader { public : void ReadPackets ( PacketStream * stream , size_t packet_num ); }; Then you can use CreateConnection<ConcretePacketStream>() and PacketReader<ConcretePacketStream> in production code, and use CreateConnection<MockPacketStream>() and PacketReader<MockPacketStream> in tests. MockPacketStream mock_stream ; EXPECT_CALL ( mock_stream , ...)...; .. set more expectations on mock_stream ... PacketReader < MockPacketStream > reader ( & mock_stream ); ... exercise reader ...","title":"Mocking Non-virtual Methods"},{"location":"examples/gtest/docs/cook_book.html#mocking-free-functions","text":"It's possible to use gMock to mock a free function (i.e. a C-style function or a static method). You just need to rewrite your code to use an interface (abstract class). Instead of calling a free function (say, OpenFile ) directly, introduce an interface for it and have a concrete subclass that calls the free function: class FileInterface { public : ... virtual bool Open ( const char * path , const char * mode ) = 0 ; }; class File : public FileInterface { public : ... virtual bool Open ( const char * path , const char * mode ) { return OpenFile ( path , mode ); } }; Your code should talk to FileInterface to open a file. Now it's easy to mock out the function. This may seem like a lot of hassle, but in practice you often have multiple related functions that you can put in the same interface, so the per-function syntactic overhead will be much lower. If you are concerned about the performance overhead incurred by virtual functions, and profiling confirms your concern, you can combine this with the recipe for mocking non-virtual methods .","title":"Mocking Free Functions"},{"location":"examples/gtest/docs/cook_book.html#old-style-mock_methodn-macros","text":"Before the generic MOCK_METHOD macro was introduced, mocks where created using a family of macros collectively called MOCK_METHODn . These macros are still supported, though migration to the new MOCK_METHOD is recommended. The macros in the MOCK_METHODn family differ from MOCK_METHOD : The general structure is MOCK_METHODn(MethodName, ReturnType(Args)) , instead of MOCK_METHOD(ReturnType, MethodName, (Args)) . The number n must equal the number of arguments. When mocking a const method, one must use MOCK_CONST_METHODn . When mocking a class template, the macro name must be suffixed with _T . In order to specify the call type, the macro name must be suffixed with _WITH_CALLTYPE , and the call type is the first macro argument. Old macros and their new equivalents: Simple Old MOCK_METHOD1(Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int)) Const Method Old MOCK_CONST_METHOD1(Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (const)) Method in a Class Template Old MOCK_METHOD1_T(Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int)) Const Method in a Class Template Old MOCK_CONST_METHOD1_T(Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (const)) Method with Call Type Old MOCK_METHOD1_WITH_CALLTYPE(STDMETHODCALLTYPE, Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (Calltype(STDMETHODCALLTYPE))) Const Method with Call Type Old MOCK_CONST_METHOD1_WITH_CALLTYPE(STDMETHODCALLTYPE, Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (const, Calltype(STDMETHODCALLTYPE))) Method with Call Type in a Class Template Old MOCK_METHOD1_T_WITH_CALLTYPE(STDMETHODCALLTYPE, Foo, bool(int)) New MOCK_METHOD(bool, Foo, (int), (Calltype(STDMETHODCALLTYPE))) Const Method with Call Type in a Class Template Old `MOCK_CONST_METHOD1_T_WITH_CALLTYPE(STDMETHODCALLTYPE, Foo, bool(int))` New MOCK_METHOD(bool, Foo, (int), (const, Calltype(STDMETHODCALLTYPE)))","title":"Old-Style MOCK_METHODn Macros"},{"location":"examples/gtest/docs/cook_book.html#NiceStrictNaggy","text":"If a mock method has no EXPECT_CALL spec but is called, we say that it's an \"uninteresting call\", and the default action (which can be specified using ON_CALL() ) of the method will be taken. Currently, an uninteresting call will also by default cause gMock to print a warning. (In the future, we might remove this warning by default.) However, sometimes you may want to ignore these uninteresting calls, and sometimes you may want to treat them as errors. gMock lets you make the decision on a per-mock-object basis. Suppose your test uses a mock class MockFoo : TEST (...) { MockFoo mock_foo ; EXPECT_CALL ( mock_foo , DoThis ()); ... code that uses mock_foo ... } If a method of mock_foo other than DoThis() is called, you will get a warning. However, if you rewrite your test to use NiceMock<MockFoo> instead, you can suppress the warning: using :: testing :: NiceMock ; TEST (...) { NiceMock < MockFoo > mock_foo ; EXPECT_CALL ( mock_foo , DoThis ()); ... code that uses mock_foo ... } NiceMock<MockFoo> is a subclass of MockFoo , so it can be used wherever MockFoo is accepted. It also works if MockFoo 's constructor takes some arguments, as NiceMock<MockFoo> \"inherits\" MockFoo 's constructors: using :: testing :: NiceMock ; TEST (...) { NiceMock < MockFoo > mock_foo ( 5 , \"hi\" ); // Calls MockFoo(5, \"hi\"). EXPECT_CALL ( mock_foo , DoThis ()); ... code that uses mock_foo ... } The usage of StrictMock is similar, except that it makes all uninteresting calls failures: using :: testing :: StrictMock ; TEST (...) { StrictMock < MockFoo > mock_foo ; EXPECT_CALL ( mock_foo , DoThis ()); ... code that uses mock_foo ... // The test will fail if a method of mock_foo other than DoThis() // is called. } NOTE: NiceMock and StrictMock only affects uninteresting calls (calls of methods with no expectations); they do not affect unexpected calls (calls of methods with expectations, but they don't match). See Understanding Uninteresting vs Unexpected Calls . There are some caveats though (I dislike them just as much as the next guy, but sadly they are side effects of C++'s limitations): NiceMock<MockFoo> and StrictMock<MockFoo> only work for mock methods defined using the MOCK_METHOD macro directly in the MockFoo class. If a mock method is defined in a base class of MockFoo , the \"nice\" or \"strict\" modifier may not affect it, depending on the compiler. In particular, nesting NiceMock and StrictMock (e.g. NiceMock<StrictMock<MockFoo> > ) is not supported. NiceMock<MockFoo> and StrictMock<MockFoo> may not work correctly if the destructor of MockFoo is not virtual. We would like to fix this, but it requires cleaning up existing tests. http://b/28934720 tracks the issue. During the constructor or destructor of MockFoo , the mock object is not nice or strict. This may cause surprises if the constructor or destructor calls a mock method on this object. (This behavior, however, is consistent with C++'s general rule: if a constructor or destructor calls a virtual method of this object, that method is treated as non-virtual. In other words, to the base class's constructor or destructor, this object behaves like an instance of the base class, not the derived class. This rule is required for safety. Otherwise a base constructor may use members of a derived class before they are initialized, or a base destructor may use members of a derived class after they have been destroyed.) Finally, you should be very cautious about when to use naggy or strict mocks, as they tend to make tests more brittle and harder to maintain. When you refactor your code without changing its externally visible behavior, ideally you shouldn't need to update any tests. If your code interacts with a naggy mock, however, you may start to get spammed with warnings as the result of your change. Worse, if your code interacts with a strict mock, your tests may start to fail and you'll be forced to fix them. Our general recommendation is to use nice mocks (not yet the default) most of the time, use naggy mocks (the current default) when developing or debugging tests, and use strict mocks only as the last resort.","title":"The Nice, the Strict, and the Naggy"},{"location":"examples/gtest/docs/cook_book.html#SimplerInterfaces","text":"Sometimes a method has a long list of arguments that is mostly uninteresting. For example: class LogSink { public : ... virtual void send ( LogSeverity severity , const char * full_filename , const char * base_filename , int line , const struct tm * tm_time , const char * message , size_t message_len ) = 0 ; }; This method's argument list is lengthy and hard to work with (the message argument is not even 0-terminated). If we mock it as is, using the mock will be awkward. If, however, we try to simplify this interface, we'll need to fix all clients depending on it, which is often infeasible. The trick is to redispatch the method in the mock class: class ScopedMockLog : public LogSink { public : ... virtual void send ( LogSeverity severity , const char * full_filename , const char * base_filename , int line , const tm * tm_time , const char * message , size_t message_len ) { // We are only interested in the log severity, full file name, and // log message. Log ( severity , full_filename , std :: string ( message , message_len )); } // Implements the mock method: // // void Log(LogSeverity severity, // const string& file_path, // const string& message); MOCK_METHOD ( void , Log , ( LogSeverity severity , const string & file_path , const string & message )); }; By defining a new mock method with a trimmed argument list, we make the mock class more user-friendly. This technique may also be applied to make overloaded methods more amenable to mocking. For example, when overloads have been used to implement default arguments: class MockTurtleFactory : public TurtleFactory { public : Turtle * MakeTurtle ( int length , int weight ) override { ... } Turtle * MakeTurtle ( int length , int weight , int speed ) override { ... } // the above methods delegate to this one: MOCK_METHOD ( Turtle * , DoMakeTurtle , ()); }; This allows tests that don't care which overload was invoked to avoid specifying argument matchers: ON_CALL ( factory , DoMakeTurtle ) . WillByDefault ( MakeMockTurtle ());","title":"Simplifying the Interface without Breaking Existing Code"},{"location":"examples/gtest/docs/cook_book.html#alternative-to-mocking-concrete-classes","text":"Often you may find yourself using classes that don't implement interfaces. In order to test your code that uses such a class (let's call it Concrete ), you may be tempted to make the methods of Concrete virtual and then mock it. Try not to do that. Making a non-virtual function virtual is a big decision. It creates an extension point where subclasses can tweak your class' behavior. This weakens your control on the class because now it's harder to maintain the class invariants. You should make a function virtual only when there is a valid reason for a subclass to override it. Mocking concrete classes directly is problematic as it creates a tight coupling between the class and the tests - any small change in the class may invalidate your tests and make test maintenance a pain. To avoid such problems, many programmers have been practicing \"coding to interfaces\": instead of talking to the Concrete class, your code would define an interface and talk to it. Then you implement that interface as an adaptor on top of Concrete . In tests, you can easily mock that interface to observe how your code is doing. This technique incurs some overhead: You pay the cost of virtual function calls (usually not a problem). There is more abstraction for the programmers to learn. However, it can also bring significant benefits in addition to better testability: Concrete 's API may not fit your problem domain very well, as you may not be the only client it tries to serve. By designing your own interface, you have a chance to tailor it to your need - you may add higher-level functionalities, rename stuff, etc instead of just trimming the class. This allows you to write your code (user of the interface) in a more natural way, which means it will be more readable, more maintainable, and you'll be more productive. If Concrete 's implementation ever has to change, you don't have to rewrite everywhere it is used. Instead, you can absorb the change in your implementation of the interface, and your other code and tests will be insulated from this change. Some people worry that if everyone is practicing this technique, they will end up writing lots of redundant code. This concern is totally understandable. However, there are two reasons why it may not be the case: Different projects may need to use Concrete in different ways, so the best interfaces for them will be different. Therefore, each of them will have its own domain-specific interface on top of Concrete , and they will not be the same code. If enough projects want to use the same interface, they can always share it, just like they have been sharing Concrete . You can check in the interface and the adaptor somewhere near Concrete (perhaps in a contrib sub-directory) and let many projects use it. You need to weigh the pros and cons carefully for your particular problem, but I'd like to assure you that the Java community has been practicing this for a long time and it's a proven effective technique applicable in a wide variety of situations. :-)","title":"Alternative to Mocking Concrete Classes"},{"location":"examples/gtest/docs/cook_book.html#DelegatingToFake","text":"Some times you have a non-trivial fake implementation of an interface. For example: class Foo { public : virtual ~ Foo () {} virtual char DoThis ( int n ) = 0 ; virtual void DoThat ( const char * s , int * p ) = 0 ; }; class FakeFoo : public Foo { public : char DoThis ( int n ) override { return ( n > 0 ) ? '+' : ( n < 0 ) ? '-' : '0' ; } void DoThat ( const char * s , int * p ) override { * p = strlen ( s ); } }; Now you want to mock this interface such that you can set expectations on it. However, you also want to use FakeFoo for the default behavior, as duplicating it in the mock object is, well, a lot of work. When you define the mock class using gMock, you can have it delegate its default action to a fake class you already have, using this pattern: class MockFoo : public Foo { public : // Normal mock method definitions using gMock. MOCK_METHOD ( char , DoThis , ( int n ), ( override )); MOCK_METHOD ( void , DoThat , ( const char * s , int * p ), ( override )); // Delegates the default actions of the methods to a FakeFoo object. // This must be called *before* the custom ON_CALL() statements. void DelegateToFake () { ON_CALL ( * this , DoThis ). WillByDefault ([ this ]( int n ) { return fake_ . DoThis ( n ); }); ON_CALL ( * this , DoThat ). WillByDefault ([ this ]( const char * s , int * p ) { fake_ . DoThat ( s , p ); }); } private : FakeFoo fake_ ; // Keeps an instance of the fake in the mock. }; With that, you can use MockFoo in your tests as usual. Just remember that if you don't explicitly set an action in an ON_CALL() or EXPECT_CALL() , the fake will be called upon to do it.: using :: testing :: _ ; TEST ( AbcTest , Xyz ) { MockFoo foo ; foo . DelegateToFake (); // Enables the fake for delegation. // Put your ON_CALL(foo, ...)s here, if any. // No action specified, meaning to use the default action. EXPECT_CALL ( foo , DoThis ( 5 )); EXPECT_CALL ( foo , DoThat ( _ , _ )); int n = 0 ; EXPECT_EQ ( '+' , foo . DoThis ( 5 )); // FakeFoo::DoThis() is invoked. foo . DoThat ( \"Hi\" , & n ); // FakeFoo::DoThat() is invoked. EXPECT_EQ ( 2 , n ); } Some tips: If you want, you can still override the default action by providing your own ON_CALL() or using .WillOnce() / .WillRepeatedly() in EXPECT_CALL() . In DelegateToFake() , you only need to delegate the methods whose fake implementation you intend to use. The general technique discussed here works for overloaded methods, but you'll need to tell the compiler which version you mean. To disambiguate a mock function (the one you specify inside the parentheses of ON_CALL() ), use this technique ; to disambiguate a fake function (the one you place inside Invoke() ), use a static_cast to specify the function's type. For instance, if class Foo has methods char DoThis(int n) and bool DoThis(double x) const , and you want to invoke the latter, you need to write Invoke(&fake_, static_cast<bool (FakeFoo::*)(double) const>(&FakeFoo::DoThis)) instead of Invoke(&fake_, &FakeFoo::DoThis) (The strange-looking thing inside the angled brackets of static_cast is the type of a function pointer to the second DoThis() method.). Having to mix a mock and a fake is often a sign of something gone wrong. Perhaps you haven't got used to the interaction-based way of testing yet. Or perhaps your interface is taking on too many roles and should be split up. Therefore, don't abuse this . We would only recommend to do it as an intermediate step when you are refactoring your code. Regarding the tip on mixing a mock and a fake, here's an example on why it may be a bad sign: Suppose you have a class System for low-level system operations. In particular, it does file and I/O operations. And suppose you want to test how your code uses System to do I/O, and you just want the file operations to work normally. If you mock out the entire System class, you'll have to provide a fake implementation for the file operation part, which suggests that System is taking on too many roles. Instead, you can define a FileOps interface and an IOOps interface and split System 's functionalities into the two. Then you can mock IOOps without mocking FileOps .","title":"Delegating Calls to a Fake"},{"location":"examples/gtest/docs/cook_book.html#delegating-calls-to-a-real-object","text":"When using testing doubles (mocks, fakes, stubs, and etc), sometimes their behaviors will differ from those of the real objects. This difference could be either intentional (as in simulating an error such that you can test the error handling code) or unintentional. If your mocks have different behaviors than the real objects by mistake, you could end up with code that passes the tests but fails in production. You can use the delegating-to-real technique to ensure that your mock has the same behavior as the real object while retaining the ability to validate calls. This technique is very similar to the delegating-to-fake technique, the difference being that we use a real object instead of a fake. Here's an example: using :: testing :: AtLeast ; class MockFoo : public Foo { public : MockFoo () { // By default, all calls are delegated to the real object. ON_CALL ( * this , DoThis ). WillByDefault ([ this ]( int n ) { return real_ . DoThis ( n ); }); ON_CALL ( * this , DoThat ). WillByDefault ([ this ]( const char * s , int * p ) { real_ . DoThat ( s , p ); }); ... } MOCK_METHOD ( char , DoThis , ...); MOCK_METHOD ( void , DoThat , ...); ... private : Foo real_ ; }; ... MockFoo mock ; EXPECT_CALL ( mock , DoThis ()) . Times ( 3 ); EXPECT_CALL ( mock , DoThat ( \"Hi\" )) . Times ( AtLeast ( 1 )); ... use mock in test ... With this, gMock will verify that your code made the right calls (with the right arguments, in the right order, called the right number of times, etc), and a real object will answer the calls (so the behavior will be the same as in production). This gives you the best of both worlds.","title":"Delegating Calls to a Real Object"},{"location":"examples/gtest/docs/cook_book.html#delegating-calls-to-a-parent-class","text":"Ideally, you should code to interfaces, whose methods are all pure virtual. In reality, sometimes you do need to mock a virtual method that is not pure (i.e, it already has an implementation). For example: class Foo { public : virtual ~ Foo (); virtual void Pure ( int n ) = 0 ; virtual int Concrete ( const char * str ) { ... } }; class MockFoo : public Foo { public : // Mocking a pure method. MOCK_METHOD ( void , Pure , ( int n ), ( override )); // Mocking a concrete method. Foo::Concrete() is shadowed. MOCK_METHOD ( int , Concrete , ( const char * str ), ( override )); }; Sometimes you may want to call Foo::Concrete() instead of MockFoo::Concrete() . Perhaps you want to do it as part of a stub action, or perhaps your test doesn't need to mock Concrete() at all (but it would be oh-so painful to have to define a new mock class whenever you don't need to mock one of its methods). The trick is to leave a back door in your mock class for accessing the real methods in the base class: class MockFoo : public Foo { public : // Mocking a pure method. MOCK_METHOD ( void , Pure , ( int n ), ( override )); // Mocking a concrete method. Foo::Concrete() is shadowed. MOCK_METHOD ( int , Concrete , ( const char * str ), ( override )); // Use this to call Concrete() defined in Foo. int FooConcrete ( const char * str ) { return Foo :: Concrete ( str ); } }; Now, you can call Foo::Concrete() inside an action by: ... EXPECT_CALL ( foo , Concrete ). WillOnce ([ & foo ]( const char * str ) { return foo . FooConcrete ( str ); }); or tell the mock object that you don't want to mock Concrete() : ... ON_CALL ( foo , Concrete ). WillByDefault ([ & foo ]( const char * str ) { return foo . FooConcrete ( str ); }); (Why don't we just write { return foo.Concrete(str); } ? If you do that, MockFoo::Concrete() will be called (and cause an infinite recursion) since Foo::Concrete() is virtual. That's just how C++ works.)","title":"Delegating Calls to a Parent Class"},{"location":"examples/gtest/docs/cook_book.html#using-matchers","text":"","title":"Using Matchers"},{"location":"examples/gtest/docs/cook_book.html#matching-argument-values-exactly","text":"You can specify exactly which arguments a mock method is expecting: using :: testing :: Return ; ... EXPECT_CALL ( foo , DoThis ( 5 )) . WillOnce ( Return ( 'a' )); EXPECT_CALL ( foo , DoThat ( \"Hello\" , bar ));","title":"Matching Argument Values Exactly"},{"location":"examples/gtest/docs/cook_book.html#using-simple-matchers","text":"You can use matchers to match arguments that have a certain property: using :: testing :: NotNull ; using :: testing :: Return ; ... EXPECT_CALL ( foo , DoThis ( Ge ( 5 ))) // The argument must be >= 5. . WillOnce ( Return ( 'a' )); EXPECT_CALL ( foo , DoThat ( \"Hello\" , NotNull ())); // The second argument must not be NULL. A frequently used matcher is _ , which matches anything: EXPECT_CALL ( foo , DoThat ( _ , NotNull ()));","title":"Using Simple Matchers"},{"location":"examples/gtest/docs/cook_book.html#CombiningMatchers","text":"You can build complex matchers from existing ones using AllOf() , AllOfArray() , AnyOf() , AnyOfArray() and Not() : using :: testing :: AllOf ; using :: testing :: Gt ; using :: testing :: HasSubstr ; using :: testing :: Ne ; using :: testing :: Not ; ... // The argument must be > 5 and != 10. EXPECT_CALL ( foo , DoThis ( AllOf ( Gt ( 5 ), Ne ( 10 )))); // The first argument must not contain sub-string \"blah\". EXPECT_CALL ( foo , DoThat ( Not ( HasSubstr ( \"blah\" )), NULL ));","title":"Combining Matchers"},{"location":"examples/gtest/docs/cook_book.html#SafeMatcherCast","text":"gMock matchers are statically typed, meaning that the compiler can catch your mistake if you use a matcher of the wrong type (for example, if you use Eq(5) to match a string argument). Good for you! Sometimes, however, you know what you're doing and want the compiler to give you some slack. One example is that you have a matcher for long and the argument you want to match is int . While the two types aren't exactly the same, there is nothing really wrong with using a Matcher<long> to match an int - after all, we can first convert the int argument to a long losslessly before giving it to the matcher. To support this need, gMock gives you the SafeMatcherCast<T>(m) function. It casts a matcher m to type Matcher<T> . To ensure safety, gMock checks that (let U be the type m accepts : Type T can be implicitly cast to type U ; When both T and U are built-in arithmetic types ( bool , integers, and floating-point numbers), the conversion from T to U is not lossy (in other words, any value representable by T can also be represented by U ); and When U is a reference, T must also be a reference (as the underlying matcher may be interested in the address of the U value). The code won't compile if any of these conditions isn't met. Here's one example: using :: testing :: SafeMatcherCast ; // A base class and a child class. class Base { ... }; class Derived : public Base { ... }; class MockFoo : public Foo { public : MOCK_METHOD ( void , DoThis , ( Derived * derived ), ( override )); }; ... MockFoo foo ; // m is a Matcher<Base*> we got from somewhere. EXPECT_CALL ( foo , DoThis ( SafeMatcherCast < Derived *> ( m ))); If you find SafeMatcherCast<T>(m) too limiting, you can use a similar function MatcherCast<T>(m) . The difference is that MatcherCast works as long as you can static_cast type T to type U . MatcherCast essentially lets you bypass C++'s type system ( static_cast isn't always safe as it could throw away information, for example), so be careful not to misuse/abuse it.","title":"Casting Matchers"},{"location":"examples/gtest/docs/cook_book.html#SelectOverload","text":"If you expect an overloaded function to be called, the compiler may need some help on which overloaded version it is. To disambiguate functions overloaded on the const-ness of this object, use the Const() argument wrapper. using :: testing :: ReturnRef ; class MockFoo : public Foo { ... MOCK_METHOD ( Bar & , GetBar , (), ( override )); MOCK_METHOD ( const Bar & , GetBar , (), ( const , override )); }; ... MockFoo foo ; Bar bar1 , bar2 ; EXPECT_CALL ( foo , GetBar ()) // The non-const GetBar(). . WillOnce ( ReturnRef ( bar1 )); EXPECT_CALL ( Const ( foo ), GetBar ()) // The const GetBar(). . WillOnce ( ReturnRef ( bar2 )); ( Const() is defined by gMock and returns a const reference to its argument.) To disambiguate overloaded functions with the same number of arguments but different argument types, you may need to specify the exact type of a matcher, either by wrapping your matcher in Matcher<type>() , or using a matcher whose type is fixed ( TypedEq<type> , An<type>() , etc): using :: testing :: An ; using :: testing :: Matcher ; using :: testing :: TypedEq ; class MockPrinter : public Printer { public : MOCK_METHOD ( void , Print , ( int n ), ( override )); MOCK_METHOD ( void , Print , ( char c ), ( override )); }; TEST ( PrinterTest , Print ) { MockPrinter printer ; EXPECT_CALL ( printer , Print ( An < int > ())); // void Print(int); EXPECT_CALL ( printer , Print ( Matcher < int > ( Lt ( 5 )))); // void Print(int); EXPECT_CALL ( printer , Print ( TypedEq < char > ( 'a' ))); // void Print(char); printer . Print ( 3 ); printer . Print ( 6 ); printer . Print ( 'a' ); }","title":"Selecting Between Overloaded Functions"},{"location":"examples/gtest/docs/cook_book.html#performing-different-actions-based-on-the-arguments","text":"When a mock method is called, the last matching expectation that's still active will be selected (think \"newer overrides older\"). So, you can make a method do different things depending on its argument values like this: using :: testing :: _ ; using :: testing :: Lt ; using :: testing :: Return ; ... // The default case. EXPECT_CALL ( foo , DoThis ( _ )) . WillRepeatedly ( Return ( 'b' )); // The more specific case. EXPECT_CALL ( foo , DoThis ( Lt ( 5 ))) . WillRepeatedly ( Return ( 'a' )); Now, if foo.DoThis() is called with a value less than 5, 'a' will be returned; otherwise 'b' will be returned.","title":"Performing Different Actions Based on the Arguments"},{"location":"examples/gtest/docs/cook_book.html#matching-multiple-arguments-as-a-whole","text":"Sometimes it's not enough to match the arguments individually. For example, we may want to say that the first argument must be less than the second argument. The With() clause allows us to match all arguments of a mock function as a whole. For example, using :: testing :: _ ; using :: testing :: Ne ; using :: testing :: Lt ; ... EXPECT_CALL ( foo , InRange ( Ne ( 0 ), _ )) . With ( Lt ()); says that the first argument of InRange() must not be 0, and must be less than the second argument. The expression inside With() must be a matcher of type Matcher< ::std::tuple<A1, ..., An> > , where A1 , ..., An are the types of the function arguments. You can also write AllArgs(m) instead of m inside .With() . The two forms are equivalent, but .With(AllArgs(Lt())) is more readable than .With(Lt()) . You can use Args<k1, ..., kn>(m) to match the n selected arguments (as a tuple) against m . For example, using :: testing :: _ ; using :: testing :: AllOf ; using :: testing :: Args ; using :: testing :: Lt ; ... EXPECT_CALL ( foo , Blah ) . With ( AllOf ( Args < 0 , 1 > ( Lt ()), Args < 1 , 2 > ( Lt ()))); says that Blah will be called with arguments x , y , and z where x < y < z . Note that in this example, it wasn't necessary specify the positional matchers. As a convenience and example, gMock provides some matchers for 2-tuples, including the Lt() matcher above. See here for the complete list. Note that if you want to pass the arguments to a predicate of your own (e.g. .With(Args<0, 1>(Truly(&MyPredicate))) ), that predicate MUST be written to take a ::std::tuple as its argument; gMock will pass the n selected arguments as one single tuple to the predicate.","title":"Matching Multiple Arguments as a Whole"},{"location":"examples/gtest/docs/cook_book.html#using-matchers-as-predicates","text":"Have you noticed that a matcher is just a fancy predicate that also knows how to describe itself? Many existing algorithms take predicates as arguments (e.g. those defined in STL's <algorithm> header), and it would be a shame if gMock matchers were not allowed to participate. Luckily, you can use a matcher where a unary predicate functor is expected by wrapping it inside the Matches() function. For example, #include <algorithm> #include <vector> using :: testing :: Matches ; using :: testing :: Ge ; vector < int > v ; ... // How many elements in v are >= 10? const int count = count_if ( v . begin (), v . end (), Matches ( Ge ( 10 ))); Since you can build complex matchers from simpler ones easily using gMock, this gives you a way to conveniently construct composite predicates (doing the same using STL's <functional> header is just painful). For example, here's a predicate that's satisfied by any number that is >= 0, <= 100, and != 50: using testing :: AllOf ; using testing :: Ge ; using testing :: Le ; using testing :: Matches ; using testing :: Ne ; ... Matches ( AllOf ( Ge ( 0 ), Le ( 100 ), Ne ( 50 )))","title":"Using Matchers as Predicates"},{"location":"examples/gtest/docs/cook_book.html#using-matchers-in-googletest-assertions","text":"Since matchers are basically predicates that also know how to describe themselves, there is a way to take advantage of them in googletest assertions. It's called ASSERT_THAT and EXPECT_THAT : ASSERT_THAT ( value , matcher ); // Asserts that value matches matcher. EXPECT_THAT ( value , matcher ); // The non-fatal version. For example, in a googletest test you can write: #include \"gmock/gmock.h\" using :: testing :: AllOf ; using :: testing :: Ge ; using :: testing :: Le ; using :: testing :: MatchesRegex ; using :: testing :: StartsWith ; ... EXPECT_THAT ( Foo (), StartsWith ( \"Hello\" )); EXPECT_THAT ( Bar (), MatchesRegex ( \"Line \\\\ d+\" )); ASSERT_THAT ( Baz (), AllOf ( Ge ( 5 ), Le ( 10 ))); which (as you can probably guess) executes Foo() , Bar() , and Baz() , and verifies that: Foo() returns a string that starts with \"Hello\" . Bar() returns a string that matches regular expression \"Line \\\\d+\" . Baz() returns a number in the range [5, 10]. The nice thing about these macros is that they read like English . They generate informative messages too. For example, if the first EXPECT_THAT() above fails, the message will be something like: Value of : Foo () Actual : \"Hi, world!\" Expected : starts with \"Hello\" Credit: The idea of (ASSERT|EXPECT)_THAT was borrowed from Joe Walnes' Hamcrest project, which adds assertThat() to JUnit.","title":"Using Matchers in googletest Assertions"},{"location":"examples/gtest/docs/cook_book.html#using-predicates-as-matchers","text":"gMock provides a built-in set of matchers. In case you find them lacking, you can use an arbitrary unary predicate function or functor as a matcher - as long as the predicate accepts a value of the type you want. You do this by wrapping the predicate inside the Truly() function, for example: using :: testing :: Truly ; int IsEven ( int n ) { return ( n % 2 ) == 0 ? 1 : 0 ; } ... // Bar() must be called with an even number. EXPECT_CALL ( foo , Bar ( Truly ( IsEven ))); Note that the predicate function / functor doesn't have to return bool . It works as long as the return value can be used as the condition in in statement if (condition) ... .","title":"Using Predicates as Matchers"},{"location":"examples/gtest/docs/cook_book.html#matching-arguments-that-are-not-copyable","text":"When you do an EXPECT_CALL(mock_obj, Foo(bar)) , gMock saves away a copy of bar . When Foo() is called later, gMock compares the argument to Foo() with the saved copy of bar . This way, you don't need to worry about bar being modified or destroyed after the EXPECT_CALL() is executed. The same is true when you use matchers like Eq(bar) , Le(bar) , and so on. But what if bar cannot be copied (i.e. has no copy constructor)? You could define your own matcher function or callback and use it with Truly() , as the previous couple of recipes have shown. Or, you may be able to get away from it if you can guarantee that bar won't be changed after the EXPECT_CALL() is executed. Just tell gMock that it should save a reference to bar , instead of a copy of it. Here's how: using :: testing :: ByRef ; using :: testing :: Eq ; using :: testing :: Lt ; ... // Expects that Foo()'s argument == bar. EXPECT_CALL ( mock_obj , Foo ( Eq ( ByRef ( bar )))); // Expects that Foo()'s argument < bar. EXPECT_CALL ( mock_obj , Foo ( Lt ( ByRef ( bar )))); Remember: if you do this, don't change bar after the EXPECT_CALL() , or the result is undefined.","title":"Matching Arguments that Are Not Copyable"},{"location":"examples/gtest/docs/cook_book.html#validating-a-member-of-an-object","text":"Often a mock function takes a reference to object as an argument. When matching the argument, you may not want to compare the entire object against a fixed object, as that may be over-specification. Instead, you may need to validate a certain member variable or the result of a certain getter method of the object. You can do this with Field() and Property() . More specifically, Field ( & Foo :: bar , m ) is a matcher that matches a Foo object whose bar member variable satisfies matcher m . Property ( & Foo :: baz , m ) is a matcher that matches a Foo object whose baz() method returns a value that satisfies matcher m . For example: Expression Description Field(&Foo::number, Ge(3)) Matches x where x.number >= 3 . Property(&Foo::name, StartsWith(\"John \")) Matches x where x.name() starts with \"John \" . Note that in Property(&Foo::baz, ...) , method baz() must take no argument and be declared as const . BTW, Field() and Property() can also match plain pointers to objects. For instance, using :: testing :: Field ; using :: testing :: Ge ; ... Field ( & Foo :: number , Ge ( 3 )) matches a plain pointer p where p->number >= 3 . If p is NULL , the match will always fail regardless of the inner matcher. What if you want to validate more than one members at the same time? Remember that there are AllOf() and AllOfArray() . Finally Field() and Property() provide overloads that take the field or property names as the first argument to include it in the error message. This can be useful when creating combined matchers. using :: testing :: AllOf ; using :: testing :: Field ; using :: testing :: Matcher ; using :: testing :: SafeMatcherCast ; Matcher < Foo > IsFoo ( const Foo & foo ) { return AllOf ( Field ( \"some_field\" , & Foo :: some_field , foo . some_field ), Field ( \"other_field\" , & Foo :: other_field , foo . other_field ), Field ( \"last_field\" , & Foo :: last_field , foo . last_field )); }","title":"Validating a Member of an Object"},{"location":"examples/gtest/docs/cook_book.html#validating-the-value-pointed-to-by-a-pointer-argument","text":"C++ functions often take pointers as arguments. You can use matchers like IsNull() , NotNull() , and other comparison matchers to match a pointer, but what if you want to make sure the value pointed to by the pointer, instead of the pointer itself, has a certain property? Well, you can use the Pointee(m) matcher. Pointee(m) matches a pointer if and only if m matches the value the pointer points to. For example: using :: testing :: Ge ; using :: testing :: Pointee ; ... EXPECT_CALL ( foo , Bar ( Pointee ( Ge ( 3 )))); expects foo.Bar() to be called with a pointer that points to a value greater than or equal to 3. One nice thing about Pointee() is that it treats a NULL pointer as a match failure, so you can write Pointee(m) instead of using :: testing :: AllOf ; using :: testing :: NotNull ; using :: testing :: Pointee ; ... AllOf ( NotNull (), Pointee ( m )) without worrying that a NULL pointer will crash your test. Also, did we tell you that Pointee() works with both raw pointers and smart pointers ( std::unique_ptr , std::shared_ptr , etc)? What if you have a pointer to pointer? You guessed it - you can use nested Pointee() to probe deeper inside the value. For example, Pointee(Pointee(Lt(3))) matches a pointer that points to a pointer that points to a number less than 3 (what a mouthful...).","title":"Validating the Value Pointed to by a Pointer Argument"},{"location":"examples/gtest/docs/cook_book.html#testing-a-certain-property-of-an-object","text":"Sometimes you want to specify that an object argument has a certain property, but there is no existing matcher that does this. If you want good error messages, you should define a matcher . If you want to do it quick and dirty, you could get away with writing an ordinary function. Let's say you have a mock function that takes an object of type Foo , which has an int bar() method and an int baz() method, and you want to constrain that the argument's bar() value plus its baz() value is a given number. Here's how you can define a matcher to do it: using :: testing :: Matcher ; using :: testing :: MatcherInterface ; using :: testing :: MatchResultListener ; class BarPlusBazEqMatcher : public MatcherInterface < const Foo &> { public : explicit BarPlusBazEqMatcher ( int expected_sum ) : expected_sum_ ( expected_sum ) {} bool MatchAndExplain ( const Foo & foo , MatchResultListener * /* listener */ ) const override { return ( foo . bar () + foo . baz ()) == expected_sum_ ; } void DescribeTo ( :: std :: ostream * os ) const override { * os << \"bar() + baz() equals \" << expected_sum_ ; } void DescribeNegationTo ( :: std :: ostream * os ) const override { * os << \"bar() + baz() does not equal \" << expected_sum_ ; } private : const int expected_sum_ ; }; Matcher < const Foo &> BarPlusBazEq ( int expected_sum ) { return MakeMatcher ( new BarPlusBazEqMatcher ( expected_sum )); } ... EXPECT_CALL (..., DoThis ( BarPlusBazEq ( 5 )))...;","title":"Testing a Certain Property of an Object"},{"location":"examples/gtest/docs/cook_book.html#matching-containers","text":"Sometimes an STL container (e.g. list, vector, map, ...) is passed to a mock function and you may want to validate it. Since most STL containers support the == operator, you can write Eq(expected_container) or simply expected_container to match a container exactly. Sometimes, though, you may want to be more flexible (for example, the first element must be an exact match, but the second element can be any positive number, and so on). Also, containers used in tests often have a small number of elements, and having to define the expected container out-of-line is a bit of a hassle. You can use the ElementsAre() or UnorderedElementsAre() matcher in such cases: using :: testing :: _ ; using :: testing :: ElementsAre ; using :: testing :: Gt ; ... MOCK_METHOD ( void , Foo , ( const vector < int >& numbers ), ( override )); ... EXPECT_CALL ( mock , Foo ( ElementsAre ( 1 , Gt ( 0 ), _ , 5 ))); The above matcher says that the container must have 4 elements, which must be 1, greater than 0, anything, and 5 respectively. If you instead write: using :: testing :: _ ; using :: testing :: Gt ; using :: testing :: UnorderedElementsAre ; ... MOCK_METHOD ( void , Foo , ( const vector < int >& numbers ), ( override )); ... EXPECT_CALL ( mock , Foo ( UnorderedElementsAre ( 1 , Gt ( 0 ), _ , 5 ))); It means that the container must have 4 elements, which (under some permutation) must be 1, greater than 0, anything, and 5 respectively. As an alternative you can place the arguments in a C-style array and use ElementsAreArray() or UnorderedElementsAreArray() instead: using :: testing :: ElementsAreArray ; ... // ElementsAreArray accepts an array of element values. const int expected_vector1 [] = { 1 , 5 , 2 , 4 , ...}; EXPECT_CALL ( mock , Foo ( ElementsAreArray ( expected_vector1 ))); // Or, an array of element matchers. Matcher < int > expected_vector2 [] = { 1 , Gt ( 2 ), _ , 3 , ...}; EXPECT_CALL ( mock , Foo ( ElementsAreArray ( expected_vector2 ))); In case the array needs to be dynamically created (and therefore the array size cannot be inferred by the compiler), you can give ElementsAreArray() an additional argument to specify the array size: using :: testing :: ElementsAreArray ; ... int * const expected_vector3 = new int [ count ]; ... fill expected_vector3 with values ... EXPECT_CALL ( mock , Foo ( ElementsAreArray ( expected_vector3 , count ))); Use Pair when comparing maps or other associative containers. using testing :: ElementsAre ; using testing :: Pair ; ... std :: map < string , int > m = {{ \"a\" , 1 }, { \"b\" , 2 }, { \"c\" , 3 }}; EXPECT_THAT ( m , ElementsAre ( Pair ( \"a\" , 1 ), Pair ( \"b\" , 2 ), Pair ( \"c\" , 3 ))); Tips: ElementsAre*() can be used to match any container that implements the STL iterator pattern (i.e. it has a const_iterator type and supports begin()/end() ), not just the ones defined in STL. It will even work with container types yet to be written - as long as they follows the above pattern. You can use nested ElementsAre*() to match nested (multi-dimensional) containers. If the container is passed by pointer instead of by reference, just write Pointee(ElementsAre*(...)) . The order of elements matters for ElementsAre*() . If you are using it with containers whose element order are undefined (e.g. hash_map ) you should use WhenSorted around ElementsAre .","title":"Matching Containers"},{"location":"examples/gtest/docs/cook_book.html#sharing-matchers","text":"Under the hood, a gMock matcher object consists of a pointer to a ref-counted implementation object. Copying matchers is allowed and very efficient, as only the pointer is copied. When the last matcher that references the implementation object dies, the implementation object will be deleted. Therefore, if you have some complex matcher that you want to use again and again, there is no need to build it everytime. Just assign it to a matcher variable and use that variable repeatedly! For example, using :: testing :: AllOf ; using :: testing :: Gt ; using :: testing :: Le ; using :: testing :: Matcher ; ... Matcher < int > in_range = AllOf ( Gt ( 5 ), Le ( 10 )); ... use in_range as a matcher in multiple EXPECT_CALLs ...","title":"Sharing Matchers"},{"location":"examples/gtest/docs/cook_book.html#PureMatchers","text":"WARNING: gMock does not guarantee when or how many times a matcher will be invoked. Therefore, all matchers must be purely functional : they cannot have any side effects, and the match result must not depend on anything other than the matcher's parameters and the value being matched. This requirement must be satisfied no matter how a matcher is defined (e.g., if it is one of the standard matchers, or a custom matcher). In particular, a matcher can never call a mock function, as that will affect the state of the mock object and gMock.","title":"Matchers must have no side-effects"},{"location":"examples/gtest/docs/cook_book.html#setting-expectations","text":"","title":"Setting Expectations"},{"location":"examples/gtest/docs/cook_book.html#UseOnCall","text":"ON_CALL is likely the single most under-utilized construct in gMock. There are basically two constructs for defining the behavior of a mock object: ON_CALL and EXPECT_CALL . The difference? ON_CALL defines what happens when a mock method is called, but doesn't imply any expectation on the method being called . EXPECT_CALL not only defines the behavior, but also sets an expectation that the method will be called with the given arguments, for the given number of times (and in the given order when you specify the order too). Since EXPECT_CALL does more, isn't it better than ON_CALL ? Not really. Every EXPECT_CALL adds a constraint on the behavior of the code under test. Having more constraints than necessary is baaad - even worse than not having enough constraints. This may be counter-intuitive. How could tests that verify more be worse than tests that verify less? Isn't verification the whole point of tests? The answer lies in what a test should verify. A good test verifies the contract of the code. If a test over-specifies, it doesn't leave enough freedom to the implementation. As a result, changing the implementation without breaking the contract (e.g. refactoring and optimization), which should be perfectly fine to do, can break such tests. Then you have to spend time fixing them, only to see them broken again the next time the implementation is changed. Keep in mind that one doesn't have to verify more than one property in one test. In fact, it's a good style to verify only one thing in one test. If you do that, a bug will likely break only one or two tests instead of dozens (which case would you rather debug?). If you are also in the habit of giving tests descriptive names that tell what they verify, you can often easily guess what's wrong just from the test log itself. So use ON_CALL by default, and only use EXPECT_CALL when you actually intend to verify that the call is made. For example, you may have a bunch of ON_CALL s in your test fixture to set the common mock behavior shared by all tests in the same group, and write (scarcely) different EXPECT_CALL s in different TEST_F s to verify different aspects of the code's behavior. Compared with the style where each TEST has many EXPECT_CALL s, this leads to tests that are more resilient to implementational changes (and thus less likely to require maintenance) and makes the intent of the tests more obvious (so they are easier to maintain when you do need to maintain them). If you are bothered by the \"Uninteresting mock function call\" message printed when a mock method without an EXPECT_CALL is called, you may use a NiceMock instead to suppress all such messages for the mock object, or suppress the message for specific methods by adding EXPECT_CALL(...).Times(AnyNumber()) . DO NOT suppress it by blindly adding an EXPECT_CALL(...) , or you'll have a test that's a pain to maintain.","title":"Knowing When to Expect"},{"location":"examples/gtest/docs/cook_book.html#ignoring-uninteresting-calls","text":"If you are not interested in how a mock method is called, just don't say anything about it. In this case, if the method is ever called, gMock will perform its default action to allow the test program to continue. If you are not happy with the default action taken by gMock, you can override it using DefaultValue<T>::Set() (described here ) or ON_CALL() . Please note that once you expressed interest in a particular mock method (via EXPECT_CALL() ), all invocations to it must match some expectation. If this function is called but the arguments don't match any EXPECT_CALL() statement, it will be an error.","title":"Ignoring Uninteresting Calls"},{"location":"examples/gtest/docs/cook_book.html#disallowing-unexpected-calls","text":"If a mock method shouldn't be called at all, explicitly say so: using :: testing :: _ ; ... EXPECT_CALL ( foo , Bar ( _ )) . Times ( 0 ); If some calls to the method are allowed, but the rest are not, just list all the expected calls: using :: testing :: AnyNumber ; using :: testing :: Gt ; ... EXPECT_CALL ( foo , Bar ( 5 )); EXPECT_CALL ( foo , Bar ( Gt ( 10 ))) . Times ( AnyNumber ()); A call to foo.Bar() that doesn't match any of the EXPECT_CALL() statements will be an error.","title":"Disallowing Unexpected Calls"},{"location":"examples/gtest/docs/cook_book.html#uninteresting-vs-unexpected","text":"Uninteresting calls and unexpected calls are different concepts in gMock. Very different. A call x.Y(...) is uninteresting if there's not even a single EXPECT_CALL(x, Y(...)) set. In other words, the test isn't interested in the x.Y() method at all, as evident in that the test doesn't care to say anything about it. A call x.Y(...) is unexpected if there are some EXPECT_CALL(x, Y(...)) s set, but none of them matches the call. Put another way, the test is interested in the x.Y() method (therefore it explicitly sets some EXPECT_CALL to verify how it's called); however, the verification fails as the test doesn't expect this particular call to happen. An unexpected call is always an error, as the code under test doesn't behave the way the test expects it to behave. By default, an uninteresting call is not an error, as it violates no constraint specified by the test. (gMock's philosophy is that saying nothing means there is no constraint.) However, it leads to a warning, as it might indicate a problem (e.g. the test author might have forgotten to specify a constraint). In gMock, NiceMock and StrictMock can be used to make a mock class \"nice\" or \"strict\". How does this affect uninteresting calls and unexpected calls? A nice mock suppresses uninteresting call warnings . It is less chatty than the default mock, but otherwise is the same. If a test fails with a default mock, it will also fail using a nice mock instead. And vice versa. Don't expect making a mock nice to change the test's result. A strict mock turns uninteresting call warnings into errors. So making a mock strict may change the test's result. Let's look at an example: TEST (...) { NiceMock < MockDomainRegistry > mock_registry ; EXPECT_CALL ( mock_registry , GetDomainOwner ( \"google.com\" )) . WillRepeatedly ( Return ( \"Larry Page\" )); // Use mock_registry in code under test. ... & mock_registry ... } The sole EXPECT_CALL here says that all calls to GetDomainOwner() must have \"google.com\" as the argument. If GetDomainOwner(\"yahoo.com\") is called, it will be an unexpected call, and thus an error. Having a nice mock doesn't change the severity of an unexpected call. So how do we tell gMock that GetDomainOwner() can be called with some other arguments as well? The standard technique is to add a \"catch all\" EXPECT_CALL : EXPECT_CALL ( mock_registry , GetDomainOwner ( _ )) . Times ( AnyNumber ()); // catches all other calls to this method. EXPECT_CALL ( mock_registry , GetDomainOwner ( \"google.com\" )) . WillRepeatedly ( Return ( \"Larry Page\" )); Remember that _ is the wildcard matcher that matches anything. With this, if GetDomainOwner(\"google.com\") is called, it will do what the second EXPECT_CALL says; if it is called with a different argument, it will do what the first EXPECT_CALL says. Note that the order of the two EXPECT_CALL s is important, as a newer EXPECT_CALL takes precedence over an older one. For more on uninteresting calls, nice mocks, and strict mocks, read \"The Nice, the Strict, and the Naggy\" .","title":"Understanding Uninteresting vs Unexpected Calls"},{"location":"examples/gtest/docs/cook_book.html#ParameterlessExpectations","text":"If your test doesn't care about the parameters (it only cares about the number or order of calls), you can often simply omit the parameter list: // Expect foo.Bar( ... ) twice with any arguments. EXPECT_CALL ( foo , Bar ). Times ( 2 ); // Delegate to the given method whenever the factory is invoked. ON_CALL ( foo_factory , MakeFoo ) . WillByDefault ( & BuildFooForTest ); This functionality is only available when a method is not overloaded; to prevent unexpected behavior it is a compilation error to try to set an expectation on a method where the specific overload is ambiguous. You can work around this by supplying a simpler mock interface than the mocked class provides. This pattern is also useful when the arguments are interesting, but match logic is substantially complex. You can leave the argument list unspecified and use SaveArg actions to save the values for later verification . If you do that, you can easily differentiate calling the method the wrong number of times from calling it with the wrong arguments.","title":"Ignoring Uninteresting Arguments"},{"location":"examples/gtest/docs/cook_book.html#OrderedCalls","text":"Although an EXPECT_CALL() statement defined earlier takes precedence when gMock tries to match a function call with an expectation, by default calls don't have to happen in the order EXPECT_CALL() statements are written. For example, if the arguments match the matchers in the third EXPECT_CALL() , but not those in the first two, then the third expectation will be used. If you would rather have all calls occur in the order of the expectations, put the EXPECT_CALL() statements in a block where you define a variable of type InSequence : using :: testing :: _ ; using :: testing :: InSequence ; { InSequence s ; EXPECT_CALL ( foo , DoThis ( 5 )); EXPECT_CALL ( bar , DoThat ( _ )) . Times ( 2 ); EXPECT_CALL ( foo , DoThis ( 6 )); } In this example, we expect a call to foo.DoThis(5) , followed by two calls to bar.DoThat() where the argument can be anything, which are in turn followed by a call to foo.DoThis(6) . If a call occurred out-of-order, gMock will report an error.","title":"Expecting Ordered Calls"},{"location":"examples/gtest/docs/cook_book.html#PartialOrder","text":"Sometimes requiring everything to occur in a predetermined order can lead to brittle tests. For example, we may care about A occurring before both B and C , but aren't interested in the relative order of B and C . In this case, the test should reflect our real intent, instead of being overly constraining. gMock allows you to impose an arbitrary DAG (directed acyclic graph) on the calls. One way to express the DAG is to use the After clause of EXPECT_CALL . Another way is via the InSequence() clause (not the same as the InSequence class), which we borrowed from jMock 2. It's less flexible than After() , but more convenient when you have long chains of sequential calls, as it doesn't require you to come up with different names for the expectations in the chains. Here's how it works: If we view EXPECT_CALL() statements as nodes in a graph, and add an edge from node A to node B wherever A must occur before B, we can get a DAG. We use the term \"sequence\" to mean a directed path in this DAG. Now, if we decompose the DAG into sequences, we just need to know which sequences each EXPECT_CALL() belongs to in order to be able to reconstruct the original DAG. So, to specify the partial order on the expectations we need to do two things: first to define some Sequence objects, and then for each EXPECT_CALL() say which Sequence objects it is part of. Expectations in the same sequence must occur in the order they are written. For example, using :: testing :: Sequence ; ... Sequence s1 , s2 ; EXPECT_CALL ( foo , A ()) . InSequence ( s1 , s2 ); EXPECT_CALL ( bar , B ()) . InSequence ( s1 ); EXPECT_CALL ( bar , C ()) . InSequence ( s2 ); EXPECT_CALL ( foo , D ()) . InSequence ( s2 ); specifies the following DAG (where s1 is A -> B , and s2 is A -> C -> D ): +---> B | A ---| | +---> C ---> D This means that A must occur before B and C, and C must occur before D. There's no restriction about the order other than these.","title":"Expecting Partially Ordered Calls"},{"location":"examples/gtest/docs/cook_book.html#controlling-when-an-expectation-retires","text":"When a mock method is called, gMock only considers expectations that are still active. An expectation is active when created, and becomes inactive (aka retires ) when a call that has to occur later has occurred. For example, in using :: testing :: _ ; using :: testing :: Sequence ; ... Sequence s1 , s2 ; EXPECT_CALL ( log , Log ( WARNING , _ , \"File too large.\" )) // #1 . Times ( AnyNumber ()) . InSequence ( s1 , s2 ); EXPECT_CALL ( log , Log ( WARNING , _ , \"Data set is empty.\" )) // #2 . InSequence ( s1 ); EXPECT_CALL ( log , Log ( WARNING , _ , \"User not found.\" )) // #3 . InSequence ( s2 ); as soon as either #2 or #3 is matched, #1 will retire. If a warning \"File too large.\" is logged after this, it will be an error. Note that an expectation doesn't retire automatically when it's saturated. For example, using :: testing :: _ ; ... EXPECT_CALL ( log , Log ( WARNING , _ , _ )); // #1 EXPECT_CALL ( log , Log ( WARNING , _ , \"File too large.\" )); // #2 says that there will be exactly one warning with the message \"File too large.\" . If the second warning contains this message too, #2 will match again and result in an upper-bound-violated error. If this is not what you want, you can ask an expectation to retire as soon as it becomes saturated: using :: testing :: _ ; ... EXPECT_CALL ( log , Log ( WARNING , _ , _ )); // #1 EXPECT_CALL ( log , Log ( WARNING , _ , \"File too large.\" )) // #2 . RetiresOnSaturation (); Here #2 can be used only once, so if you have two warnings with the message \"File too large.\" , the first will match #2 and the second will match #1 - there will be no error.","title":"Controlling When an Expectation Retires"},{"location":"examples/gtest/docs/cook_book.html#using-actions","text":"","title":"Using Actions"},{"location":"examples/gtest/docs/cook_book.html#returning-references-from-mock-methods","text":"If a mock function's return type is a reference, you need to use ReturnRef() instead of Return() to return a result: using :: testing :: ReturnRef ; class MockFoo : public Foo { public : MOCK_METHOD ( Bar & , GetBar , (), ( override )); }; ... MockFoo foo ; Bar bar ; EXPECT_CALL ( foo , GetBar ()) . WillOnce ( ReturnRef ( bar )); ...","title":"Returning References from Mock Methods"},{"location":"examples/gtest/docs/cook_book.html#returning-live-values-from-mock-methods","text":"The Return(x) action saves a copy of x when the action is created, and always returns the same value whenever it's executed. Sometimes you may want to instead return the live value of x (i.e. its value at the time when the action is executed .). Use either ReturnRef() or ReturnPointee() for this purpose. If the mock function's return type is a reference, you can do it using ReturnRef(x) , as shown in the previous recipe (\"Returning References from Mock Methods\"). However, gMock doesn't let you use ReturnRef() in a mock function whose return type is not a reference, as doing that usually indicates a user error. So, what shall you do? Though you may be tempted, DO NOT use ByRef() : using testing :: ByRef ; using testing :: Return ; class MockFoo : public Foo { public : MOCK_METHOD ( int , GetValue , (), ( override )); }; ... int x = 0 ; MockFoo foo ; EXPECT_CALL ( foo , GetValue ()) . WillRepeatedly ( Return ( ByRef ( x ))); // Wrong! x = 42 ; EXPECT_EQ ( 42 , foo . GetValue ()); Unfortunately, it doesn't work here. The above code will fail with error: Value of: foo.GetValue() Actual: 0 Expected: 42 The reason is that Return(*value*) converts value to the actual return type of the mock function at the time when the action is created , not when it is executed . (This behavior was chosen for the action to be safe when value is a proxy object that references some temporary objects.) As a result, ByRef(x) is converted to an int value (instead of a const int& ) when the expectation is set, and Return(ByRef(x)) will always return 0. ReturnPointee(pointer) was provided to solve this problem specifically. It returns the value pointed to by pointer at the time the action is executed : using testing :: ReturnPointee ; ... int x = 0 ; MockFoo foo ; EXPECT_CALL ( foo , GetValue ()) . WillRepeatedly ( ReturnPointee ( & x )); // Note the & here. x = 42 ; EXPECT_EQ ( 42 , foo . GetValue ()); // This will succeed now.","title":"Returning Live Values from Mock Methods"},{"location":"examples/gtest/docs/cook_book.html#combining-actions","text":"Want to do more than one thing when a function is called? That's fine. DoAll() allow you to do sequence of actions every time. Only the return value of the last action in the sequence will be used. using :: testing :: _ ; using :: testing :: DoAll ; class MockFoo : public Foo { public : MOCK_METHOD ( bool , Bar , ( int n ), ( override )); }; ... EXPECT_CALL ( foo , Bar ( _ )) . WillOnce ( DoAll ( action_1 , action_2 , ... action_n ));","title":"Combining Actions"},{"location":"examples/gtest/docs/cook_book.html#SaveArgVerify","text":"If you want to verify that a method is called with a particular argument but the match criteria is complex, it can be difficult to distinguish between cardinality failures (calling the method the wrong number of times) and argument match failures. Similarly, if you are matching multiple parameters, it may not be easy to distinguishing which argument failed to match. For example: // Not ideal: this could fail because of a problem with arg1 or arg2, or maybe // just the method wasn't called. EXPECT_CALL ( foo , SendValues ( _ , ElementsAre ( 1 , 4 , 4 , 7 ), EqualsProto ( ... ))); You can instead save the arguments and test them individually: EXPECT_CALL ( foo , SendValues ) . WillOnce ( DoAll ( SaveArg < 1 > ( & actual_array ), SaveArg < 2 > ( & actual_proto ))); ... run the test EXPECT_THAT ( actual_array , ElementsAre ( 1 , 4 , 4 , 7 )); EXPECT_THAT ( actual_proto , EqualsProto ( ... ));","title":"Verifying Complex Arguments"},{"location":"examples/gtest/docs/cook_book.html#MockingSideEffects","text":"Sometimes a method exhibits its effect not via returning a value but via side effects. For example, it may change some global state or modify an output argument. To mock side effects, in general you can define your own action by implementing ::testing::ActionInterface . If all you need to do is to change an output argument, the built-in SetArgPointee() action is convenient: using :: testing :: _ ; using :: testing :: SetArgPointee ; class MockMutator : public Mutator { public : MOCK_METHOD ( void , Mutate , ( bool mutate , int * value ), ( override )); ... } ... MockMutator mutator ; EXPECT_CALL ( mutator , Mutate ( true , _ )) . WillOnce ( SetArgPointee < 1 > ( 5 )); In this example, when mutator.Mutate() is called, we will assign 5 to the int variable pointed to by argument #1 (0-based). SetArgPointee() conveniently makes an internal copy of the value you pass to it, removing the need to keep the value in scope and alive. The implication however is that the value must have a copy constructor and assignment operator. If the mock method also needs to return a value as well, you can chain SetArgPointee() with Return() using DoAll() , remembering to put the Return() statement last: using :: testing :: _ ; using :: testing :: Return ; using :: testing :: SetArgPointee ; class MockMutator : public Mutator { public : ... MOCK_METHOD ( bool , MutateInt , ( int * value ), ( override )); } ... MockMutator mutator ; EXPECT_CALL ( mutator , MutateInt ( _ )) . WillOnce ( DoAll ( SetArgPointee < 0 > ( 5 ), Return ( true ))); Note, however, that if you use the ReturnOKWith() method, it will override the values provided by SetArgPointee() in the response parameters of your function call. If the output argument is an array, use the SetArrayArgument<N>(first, last) action instead. It copies the elements in source range [first, last) to the array pointed to by the N -th (0-based) argument: using :: testing :: NotNull ; using :: testing :: SetArrayArgument ; class MockArrayMutator : public ArrayMutator { public : MOCK_METHOD ( void , Mutate , ( int * values , int num_values ), ( override )); ... } ... MockArrayMutator mutator ; int values [ 5 ] = { 1 , 2 , 3 , 4 , 5 }; EXPECT_CALL ( mutator , Mutate ( NotNull (), 5 )) . WillOnce ( SetArrayArgument < 0 > ( values , values + 5 )); This also works when the argument is an output iterator: using :: testing :: _ ; using :: testing :: SetArrayArgument ; class MockRolodex : public Rolodex { public : MOCK_METHOD ( void , GetNames , ( std :: back_insert_iterator < vector < string >> ), ( override )); ... } ... MockRolodex rolodex ; vector < string > names ; names . push_back ( \"George\" ); names . push_back ( \"John\" ); names . push_back ( \"Thomas\" ); EXPECT_CALL ( rolodex , GetNames ( _ )) . WillOnce ( SetArrayArgument < 0 > ( names . begin (), names . end ()));","title":"Mocking Side Effects"},{"location":"examples/gtest/docs/cook_book.html#changing-a-mock-objects-behavior-based-on-the-state","text":"If you expect a call to change the behavior of a mock object, you can use ::testing::InSequence to specify different behaviors before and after the call: using :: testing :: InSequence ; using :: testing :: Return ; ... { InSequence seq ; EXPECT_CALL ( my_mock , IsDirty ()) . WillRepeatedly ( Return ( true )); EXPECT_CALL ( my_mock , Flush ()); EXPECT_CALL ( my_mock , IsDirty ()) . WillRepeatedly ( Return ( false )); } my_mock . FlushIfDirty (); This makes my_mock.IsDirty() return true before my_mock.Flush() is called and return false afterwards. If the behavior change is more complex, you can store the effects in a variable and make a mock method get its return value from that variable: using :: testing :: _ ; using :: testing :: SaveArg ; using :: testing :: Return ; ACTION_P ( ReturnPointee , p ) { return * p ; } ... int previous_value = 0 ; EXPECT_CALL ( my_mock , GetPrevValue ) . WillRepeatedly ( ReturnPointee ( & previous_value )); EXPECT_CALL ( my_mock , UpdateValue ) . WillRepeatedly ( SaveArg < 0 > ( & previous_value )); my_mock . DoSomethingToUpdateValue (); Here my_mock.GetPrevValue() will always return the argument of the last UpdateValue() call.","title":"Changing a Mock Object's Behavior Based on the State"},{"location":"examples/gtest/docs/cook_book.html#DefaultValue","text":"If a mock method's return type is a built-in C++ type or pointer, by default it will return 0 when invoked. Also, in C++ 11 and above, a mock method whose return type has a default constructor will return a default-constructed value by default. You only need to specify an action if this default value doesn't work for you. Sometimes, you may want to change this default value, or you may want to specify a default value for types gMock doesn't know about. You can do this using the ::testing::DefaultValue class template: using :: testing :: DefaultValue ; class MockFoo : public Foo { public : MOCK_METHOD ( Bar , CalculateBar , (), ( override )); }; ... Bar default_bar ; // Sets the default return value for type Bar. DefaultValue < Bar >:: Set ( default_bar ); MockFoo foo ; // We don't need to specify an action here, as the default // return value works for us. EXPECT_CALL ( foo , CalculateBar ()); foo . CalculateBar (); // This should return default_bar. // Unsets the default return value. DefaultValue < Bar >:: Clear (); Please note that changing the default value for a type can make you tests hard to understand. We recommend you to use this feature judiciously. For example, you may want to make sure the Set() and Clear() calls are right next to the code that uses your mock.","title":"Setting the Default Value for a Return Type"},{"location":"examples/gtest/docs/cook_book.html#setting-the-default-actions-for-a-mock-method","text":"You've learned how to change the default value of a given type. However, this may be too coarse for your purpose: perhaps you have two mock methods with the same return type and you want them to have different behaviors. The ON_CALL() macro allows you to customize your mock's behavior at the method level: using :: testing :: _ ; using :: testing :: AnyNumber ; using :: testing :: Gt ; using :: testing :: Return ; ... ON_CALL ( foo , Sign ( _ )) . WillByDefault ( Return ( -1 )); ON_CALL ( foo , Sign ( 0 )) . WillByDefault ( Return ( 0 )); ON_CALL ( foo , Sign ( Gt ( 0 ))) . WillByDefault ( Return ( 1 )); EXPECT_CALL ( foo , Sign ( _ )) . Times ( AnyNumber ()); foo . Sign ( 5 ); // This should return 1. foo . Sign ( -9 ); // This should return -1. foo . Sign ( 0 ); // This should return 0. As you may have guessed, when there are more than one ON_CALL() statements, the newer ones in the order take precedence over the older ones. In other words, the last one that matches the function arguments will be used. This matching order allows you to set up the common behavior in a mock object's constructor or the test fixture's set-up phase and specialize the mock's behavior later. Note that both ON_CALL and EXPECT_CALL have the same \"later statements take precedence\" rule, but they don't interact. That is, EXPECT_CALL s have their own precedence order distinct from the ON_CALL precedence order.","title":"Setting the Default Actions for a Mock Method"},{"location":"examples/gtest/docs/cook_book.html#FunctionsAsActions","text":"If the built-in actions don't suit you, you can use an existing callable (function, std::function , method, functor, lambda as an action. using :: testing :: _ ; using :: testing :: Invoke ; class MockFoo : public Foo { public : MOCK_METHOD ( int , Sum , ( int x , int y ), ( override )); MOCK_METHOD ( bool , ComplexJob , ( int x ), ( override )); }; int CalculateSum ( int x , int y ) { return x + y ; } int Sum3 ( int x , int y , int z ) { return x + y + z ; } class Helper { public : bool ComplexJob ( int x ); }; ... MockFoo foo ; Helper helper ; EXPECT_CALL ( foo , Sum ( _ , _ )) . WillOnce ( & CalculateSum ) . WillRepeatedly ( Invoke ( NewPermanentCallback ( Sum3 , 1 ))); EXPECT_CALL ( foo , ComplexJob ( _ )) . WillOnce ( Invoke ( & helper , & Helper :: ComplexJob )); . WillRepeatedly ([]( int x ) { return x > 0 ; }); foo . Sum ( 5 , 6 ); // Invokes CalculateSum(5, 6). foo . Sum ( 2 , 3 ); // Invokes Sum3(1, 2, 3). foo . ComplexJob ( 10 ); // Invokes helper.ComplexJob(10). foo . ComplexJob ( -1 ); // Invokes the inline lambda. The only requirement is that the type of the function, etc must be compatible with the signature of the mock function, meaning that the latter's arguments can be implicitly converted to the corresponding arguments of the former, and the former's return type can be implicitly converted to that of the latter. So, you can invoke something whose type is not exactly the same as the mock function, as long as it's safe to do so - nice, huh? Note: The action takes ownership of the callback and will delete it when the action itself is destructed. If the type of a callback is derived from a base callback type C , you need to implicitly cast it to C to resolve the overloading, e.g. using :: testing :: Invoke ; ... ResultCallback < bool >* is_ok = ...; ... Invoke ( is_ok ) ...; // This works. BlockingClosure * done = new BlockingClosure ; ... Invoke ( implicit_cast < Closure *> ( done )) ...; // The cast is necessary.","title":"Using Functions/Methods/Functors/Lambdas as Actions"},{"location":"examples/gtest/docs/cook_book.html#using-functions-with-extra-info-as-actions","text":"The function or functor you call using Invoke() must have the same number of arguments as the mock function you use it for. Sometimes you may have a function that takes more arguments, and you are willing to pass in the extra arguments yourself to fill the gap. You can do this in gMock using callbacks with pre-bound arguments. Here's an example: using :: testing :: Invoke ; class MockFoo : public Foo { public : MOCK_METHOD ( char , DoThis , ( int n ), ( override )); }; char SignOfSum ( int x , int y ) { const int sum = x + y ; return ( sum > 0 ) ? '+' : ( sum < 0 ) ? '-' : '0' ; } TEST_F ( FooTest , Test ) { MockFoo foo ; EXPECT_CALL ( foo , DoThis ( 2 )) . WillOnce ( Invoke ( NewPermanentCallback ( SignOfSum , 5 ))); EXPECT_EQ ( '+' , foo . DoThis ( 2 )); // Invokes SignOfSum(5, 2). }","title":"Using Functions with Extra Info as Actions"},{"location":"examples/gtest/docs/cook_book.html#invoking-a-functionmethodfunctorlambdacallback-without-arguments","text":"Invoke() is very useful for doing actions that are more complex. It passes the mock function's arguments to the function, etc being invoked such that the callee has the full context of the call to work with. If the invoked function is not interested in some or all of the arguments, it can simply ignore them. Yet, a common pattern is that a test author wants to invoke a function without the arguments of the mock function. Invoke() allows her to do that using a wrapper function that throws away the arguments before invoking an underlining nullary function. Needless to say, this can be tedious and obscures the intent of the test. InvokeWithoutArgs() solves this problem. It's like Invoke() except that it doesn't pass the mock function's arguments to the callee. Here's an example: using :: testing :: _ ; using :: testing :: InvokeWithoutArgs ; class MockFoo : public Foo { public : MOCK_METHOD ( bool , ComplexJob , ( int n ), ( override )); }; bool Job1 () { ... } bool Job2 ( int n , char c ) { ... } ... MockFoo foo ; EXPECT_CALL ( foo , ComplexJob ( _ )) . WillOnce ( InvokeWithoutArgs ( Job1 )) . WillOnce ( InvokeWithoutArgs ( NewPermanentCallback ( Job2 , 5 , 'a' ))); foo . ComplexJob ( 10 ); // Invokes Job1(). foo . ComplexJob ( 20 ); // Invokes Job2(5, 'a'). Note: The action takes ownership of the callback and will delete it when the action itself is destructed. If the type of a callback is derived from a base callback type C , you need to implicitly cast it to C to resolve the overloading, e.g. using :: testing :: InvokeWithoutArgs ; ... ResultCallback < bool >* is_ok = ...; ... InvokeWithoutArgs ( is_ok ) ...; // This works. BlockingClosure * done = ...; ... InvokeWithoutArgs ( implicit_cast < Closure *> ( done )) ...; // The cast is necessary.","title":"Invoking a Function/Method/Functor/Lambda/Callback Without Arguments"},{"location":"examples/gtest/docs/cook_book.html#invoking-an-argument-of-the-mock-function","text":"Sometimes a mock function will receive a function pointer, a functor (in other words, a \"callable\") as an argument, e.g. class MockFoo : public Foo { public : MOCK_METHOD ( bool , DoThis , ( int n , ( ResultCallback1 < bool , int >* callback )), ( override )); }; and you may want to invoke this callable argument: using :: testing :: _ ; ... MockFoo foo ; EXPECT_CALL ( foo , DoThis ( _ , _ )) . WillOnce (...); // Will execute callback->Run(5), where callback is the // second argument DoThis() receives. NOTE: The section below is legacy documentation from before C++ had lambdas: Arghh, you need to refer to a mock function argument but C++ has no lambda (yet), so you have to define your own action. :-( Or do you really? Well, gMock has an action to solve exactly this problem: InvokeArgument < N > ( arg_1 , arg_2 , ..., arg_m ) will invoke the N -th (0-based) argument the mock function receives, with arg_1 , arg_2 , ..., and arg_m . No matter if the argument is a function pointer, a functor, or a callback. gMock handles them all. With that, you could write: using :: testing :: _ ; using :: testing :: InvokeArgument ; ... EXPECT_CALL ( foo , DoThis ( _ , _ )) . WillOnce ( InvokeArgument < 1 > ( 5 )); // Will execute callback->Run(5), where callback is the // second argument DoThis() receives. What if the callable takes an argument by reference? No problem - just wrap it inside ByRef() : ... MOCK_METHOD ( bool , Bar , (( ResultCallback2 < bool , int , const Helper &>* callback )), ( override )); ... using :: testing :: _ ; using :: testing :: ByRef ; using :: testing :: InvokeArgument ; ... MockFoo foo ; Helper helper ; ... EXPECT_CALL ( foo , Bar ( _ )) . WillOnce ( InvokeArgument < 0 > ( 5 , ByRef ( helper ))); // ByRef(helper) guarantees that a reference to helper, not a copy of it, // will be passed to the callback. What if the callable takes an argument by reference and we do not wrap the argument in ByRef() ? Then InvokeArgument() will make a copy of the argument, and pass a reference to the copy , instead of a reference to the original value, to the callable. This is especially handy when the argument is a temporary value: ... MOCK_METHOD ( bool , DoThat , ( bool ( * f )( const double & x , const string & s )), ( override )); ... using :: testing :: _ ; using :: testing :: InvokeArgument ; ... MockFoo foo ; ... EXPECT_CALL ( foo , DoThat ( _ )) . WillOnce ( InvokeArgument < 0 > ( 5.0 , string ( \"Hi\" ))); // Will execute (*f)(5.0, string(\"Hi\")), where f is the function pointer // DoThat() receives. Note that the values 5.0 and string(\"Hi\") are // temporary and dead once the EXPECT_CALL() statement finishes. Yet // it's fine to perform this action later, since a copy of the values // are kept inside the InvokeArgument action.","title":"Invoking an Argument of the Mock Function"},{"location":"examples/gtest/docs/cook_book.html#ignoring-an-actions-result","text":"Sometimes you have an action that returns something , but you need an action that returns void (perhaps you want to use it in a mock function that returns void , or perhaps it needs to be used in DoAll() and it's not the last in the list). IgnoreResult() lets you do that. For example: using :: testing :: _ ; using :: testing :: DoAll ; using :: testing :: IgnoreResult ; using :: testing :: Return ; int Process ( const MyData & data ); string DoSomething (); class MockFoo : public Foo { public : MOCK_METHOD ( void , Abc , ( const MyData & data ), ( override )); MOCK_METHOD ( bool , Xyz , (), ( override )); }; ... MockFoo foo ; EXPECT_CALL ( foo , Abc ( _ )) // .WillOnce(Invoke(Process)); // The above line won't compile as Process() returns int but Abc() needs // to return void. . WillOnce ( IgnoreResult ( Process )); EXPECT_CALL ( foo , Xyz ()) . WillOnce ( DoAll ( IgnoreResult ( DoSomething ), // Ignores the string DoSomething() returns. Return ( true ))); Note that you cannot use IgnoreResult() on an action that already returns void . Doing so will lead to ugly compiler errors.","title":"Ignoring an Action's Result"},{"location":"examples/gtest/docs/cook_book.html#SelectingArgs","text":"Say you have a mock function Foo() that takes seven arguments, and you have a custom action that you want to invoke when Foo() is called. Trouble is, the custom action only wants three arguments: using :: testing :: _ ; using :: testing :: Invoke ; ... MOCK_METHOD ( bool , Foo , ( bool visible , const string & name , int x , int y , ( const map < pair < int , int >> ), double & weight , double min_weight , double max_wight )); ... bool IsVisibleInQuadrant1 ( bool visible , int x , int y ) { return visible && x >= 0 && y >= 0 ; } ... EXPECT_CALL ( mock , Foo ) . WillOnce ( Invoke ( IsVisibleInQuadrant1 )); // Uh, won't compile. :-( To please the compiler God, you need to define an \"adaptor\" that has the same signature as Foo() and calls the custom action with the right arguments: using :: testing :: _ ; using :: testing :: Invoke ; ... bool MyIsVisibleInQuadrant1 ( bool visible , const string & name , int x , int y , const map < pair < int , int > , double >& weight , double min_weight , double max_wight ) { return IsVisibleInQuadrant1 ( visible , x , y ); } ... EXPECT_CALL ( mock , Foo ) . WillOnce ( Invoke ( MyIsVisibleInQuadrant1 )); // Now it works. But isn't this awkward? gMock provides a generic action adaptor , so you can spend your time minding more important business than writing your own adaptors. Here's the syntax: WithArgs < N1 , N2 , ..., Nk > ( action ) creates an action that passes the arguments of the mock function at the given indices (0-based) to the inner action and performs it. Using WithArgs , our original example can be written as: using :: testing :: _ ; using :: testing :: Invoke ; using :: testing :: WithArgs ; ... EXPECT_CALL ( mock , Foo ) . WillOnce ( WithArgs < 0 , 2 , 3 > ( Invoke ( IsVisibleInQuadrant1 ))); // No need to define your own adaptor. For better readability, gMock also gives you: WithoutArgs(action) when the inner action takes no argument, and WithArg<N>(action) (no s after Arg ) when the inner action takes one argument. As you may have realized, InvokeWithoutArgs(...) is just syntactic sugar for WithoutArgs(Invoke(...)) . Here are more tips: The inner action used in WithArgs and friends does not have to be Invoke() -- it can be anything. You can repeat an argument in the argument list if necessary, e.g. WithArgs<2, 3, 3, 5>(...) . You can change the order of the arguments, e.g. WithArgs<3, 2, 1>(...) . The types of the selected arguments do not have to match the signature of the inner action exactly. It works as long as they can be implicitly converted to the corresponding arguments of the inner action. For example, if the 4-th argument of the mock function is an int and my_action takes a double , WithArg<4>(my_action) will work.","title":"Selecting an Action's Arguments"},{"location":"examples/gtest/docs/cook_book.html#ignoring-arguments-in-action-functions","text":"The selecting-an-action's-arguments recipe showed us one way to make a mock function and an action with incompatible argument lists fit together. The downside is that wrapping the action in WithArgs<...>() can get tedious for people writing the tests. If you are defining a function (or method, functor, lambda, callback) to be used with Invoke*() , and you are not interested in some of its arguments, an alternative to WithArgs is to declare the uninteresting arguments as Unused . This makes the definition less cluttered and less fragile in case the types of the uninteresting arguments change. It could also increase the chance the action function can be reused. For example, given public : MOCK_METHOD ( double , Foo , double ( const string & label , double x , double y ), ( override )); MOCK_METHOD ( double , Bar , ( int index , double x , double y ), ( override )); instead of using :: testing :: _ ; using :: testing :: Invoke ; double DistanceToOriginWithLabel ( const string & label , double x , double y ) { return sqrt ( x * x + y * y ); } double DistanceToOriginWithIndex ( int index , double x , double y ) { return sqrt ( x * x + y * y ); } ... EXPECT_CALL ( mock , Foo ( \"abc\" , _ , _ )) . WillOnce ( Invoke ( DistanceToOriginWithLabel )); EXPECT_CALL ( mock , Bar ( 5 , _ , _ )) . WillOnce ( Invoke ( DistanceToOriginWithIndex )); you could write using :: testing :: _ ; using :: testing :: Invoke ; using :: testing :: Unused ; double DistanceToOrigin ( Unused , double x , double y ) { return sqrt ( x * x + y * y ); } ... EXPECT_CALL ( mock , Foo ( \"abc\" , _ , _ )) . WillOnce ( Invoke ( DistanceToOrigin )); EXPECT_CALL ( mock , Bar ( 5 , _ , _ )) . WillOnce ( Invoke ( DistanceToOrigin ));","title":"Ignoring Arguments in Action Functions"},{"location":"examples/gtest/docs/cook_book.html#sharing-actions","text":"Just like matchers, a gMock action object consists of a pointer to a ref-counted implementation object. Therefore copying actions is also allowed and very efficient. When the last action that references the implementation object dies, the implementation object will be deleted. If you have some complex action that you want to use again and again, you may not have to build it from scratch everytime. If the action doesn't have an internal state (i.e. if it always does the same thing no matter how many times it has been called), you can assign it to an action variable and use that variable repeatedly. For example: using :: testing :: Action ; using :: testing :: DoAll ; using :: testing :: Return ; using :: testing :: SetArgPointee ; ... Action < bool ( int * ) > set_flag = DoAll ( SetArgPointee < 0 > ( 5 ), Return ( true )); ... use set_flag in . WillOnce () and . WillRepeatedly () ... However, if the action has its own state, you may be surprised if you share the action object. Suppose you have an action factory IncrementCounter(init) which creates an action that increments and returns a counter whose initial value is init , using two actions created from the same expression and using a shared action will exhibit different behaviors. Example: EXPECT_CALL ( foo , DoThis ()) . WillRepeatedly ( IncrementCounter ( 0 )); EXPECT_CALL ( foo , DoThat ()) . WillRepeatedly ( IncrementCounter ( 0 )); foo . DoThis (); // Returns 1. foo . DoThis (); // Returns 2. foo . DoThat (); // Returns 1 - Blah() uses a different // counter than Bar()'s. versus using :: testing :: Action ; ... Action < int () > increment = IncrementCounter ( 0 ); EXPECT_CALL ( foo , DoThis ()) . WillRepeatedly ( increment ); EXPECT_CALL ( foo , DoThat ()) . WillRepeatedly ( increment ); foo . DoThis (); // Returns 1. foo . DoThis (); // Returns 2. foo . DoThat (); // Returns 3 - the counter is shared.","title":"Sharing Actions"},{"location":"examples/gtest/docs/cook_book.html#testing-asynchronous-behavior","text":"One oft-encountered problem with gMock is that it can be hard to test asynchronous behavior. Suppose you had a EventQueue class that you wanted to test, and you created a separate EventDispatcher interface so that you could easily mock it out. However, the implementation of the class fired all the events on a background thread, which made test timings difficult. You could just insert sleep() statements and hope for the best, but that makes your test behavior nondeterministic. A better way is to use gMock actions and Notification objects to force your asynchronous test to behave synchronously. using :: testing :: DoAll ; using :: testing :: InvokeWithoutArgs ; using :: testing :: Return ; class MockEventDispatcher : public EventDispatcher { MOCK_METHOD ( bool , DispatchEvent , ( int32 ), ( override )); }; ACTION_P ( Notify , notification ) { notification -> Notify (); } TEST ( EventQueueTest , EnqueueEventTest ) { MockEventDispatcher mock_event_dispatcher ; EventQueue event_queue ( & mock_event_dispatcher ); const int32 kEventId = 321 ; Notification done ; EXPECT_CALL ( mock_event_dispatcher , DispatchEvent ( kEventId )) . WillOnce ( Notify ( & done )); event_queue . EnqueueEvent ( kEventId ); done . WaitForNotification (); } In the example above, we set our normal gMock expectations, but then add an additional action to notify the Notification object. Now we can just call Notification::WaitForNotification() in the main thread to wait for the asynchronous call to finish. After that, our test suite is complete and we can safely exit. Note: this example has a downside: namely, if the expectation is not satisfied, our test will run forever. It will eventually time-out and fail, but it will take longer and be slightly harder to debug. To alleviate this problem, you can use WaitForNotificationWithTimeout(ms) instead of WaitForNotification() .","title":"Testing Asynchronous Behavior"},{"location":"examples/gtest/docs/cook_book.html#misc-recipes-on-using-gmock","text":"","title":"Misc Recipes on Using gMock"},{"location":"examples/gtest/docs/cook_book.html#mocking-methods-that-use-move-only-types","text":"C++11 introduced move-only types . A move-only-typed value can be moved from one object to another, but cannot be copied. std::unique_ptr<T> is probably the most commonly used move-only type. Mocking a method that takes and/or returns move-only types presents some challenges, but nothing insurmountable. This recipe shows you how you can do it. Note that the support for move-only method arguments was only introduced to gMock in April 2017; in older code, you may find more complex workarounds for lack of this feature. Let\u2019s say we are working on a fictional project that lets one post and share snippets called \u201cbuzzes\u201d. Your code uses these types: enum class AccessLevel { kInternal , kPublic }; class Buzz { public : explicit Buzz ( AccessLevel access ) { ... } ... }; class Buzzer { public : virtual ~ Buzzer () {} virtual std :: unique_ptr < Buzz > MakeBuzz ( StringPiece text ) = 0 ; virtual bool ShareBuzz ( std :: unique_ptr < Buzz > buzz , int64_t timestamp ) = 0 ; ... }; A Buzz object represents a snippet being posted. A class that implements the Buzzer interface is capable of creating and sharing Buzz es. Methods in Buzzer may return a unique_ptr<Buzz> or take a unique_ptr<Buzz> . Now we need to mock Buzzer in our tests. To mock a method that accepts or returns move-only types, you just use the familiar MOCK_METHOD syntax as usual: class MockBuzzer : public Buzzer { public : MOCK_METHOD ( std :: unique_ptr < Buzz > , MakeBuzz , ( StringPiece text ), ( override )); MOCK_METHOD ( bool , ShareBuzz , ( std :: unique_ptr < Buzz > buzz , int64_t timestamp ), ( override )); }; Now that we have the mock class defined, we can use it in tests. In the following code examples, we assume that we have defined a MockBuzzer object named mock_buzzer_ : MockBuzzer mock_buzzer_ ; First let\u2019s see how we can set expectations on the MakeBuzz() method, which returns a unique_ptr<Buzz> . As usual, if you set an expectation without an action (i.e. the .WillOnce() or .WillRepeatedly() clause), when that expectation fires, the default action for that method will be taken. Since unique_ptr<> has a default constructor that returns a null unique_ptr , that\u2019s what you\u2019ll get if you don\u2019t specify an action: // Use the default action. EXPECT_CALL ( mock_buzzer_ , MakeBuzz ( \"hello\" )); // Triggers the previous EXPECT_CALL. EXPECT_EQ ( nullptr , mock_buzzer_ . MakeBuzz ( \"hello\" )); If you are not happy with the default action, you can tweak it as usual; see Setting Default Actions . If you just need to return a pre-defined move-only value, you can use the Return(ByMove(...)) action: // When this fires, the unique_ptr<> specified by ByMove(...) will // be returned. EXPECT_CALL ( mock_buzzer_ , MakeBuzz ( \"world\" )) . WillOnce ( Return ( ByMove ( MakeUnique < Buzz > ( AccessLevel :: kInternal )))); EXPECT_NE ( nullptr , mock_buzzer_ . MakeBuzz ( \"world\" )); Note that ByMove() is essential here - if you drop it, the code won\u2019t compile. Quiz time! What do you think will happen if a Return(ByMove(...)) action is performed more than once (e.g. you write ... .WillRepeatedly(Return(ByMove(...))); )? Come think of it, after the first time the action runs, the source value will be consumed (since it\u2019s a move-only value), so the next time around, there\u2019s no value to move from -- you\u2019ll get a run-time error that Return(ByMove(...)) can only be run once. If you need your mock method to do more than just moving a pre-defined value, remember that you can always use a lambda or a callable object, which can do pretty much anything you want: EXPECT_CALL ( mock_buzzer_ , MakeBuzz ( \"x\" )) . WillRepeatedly ([]( StringPiece text ) { return MakeUnique < Buzz > ( AccessLevel :: kInternal ); }); EXPECT_NE ( nullptr , mock_buzzer_ . MakeBuzz ( \"x\" )); EXPECT_NE ( nullptr , mock_buzzer_ . MakeBuzz ( \"x\" )); Every time this EXPECT_CALL fires, a new unique_ptr<Buzz> will be created and returned. You cannot do this with Return(ByMove(...)) . That covers returning move-only values; but how do we work with methods accepting move-only arguments? The answer is that they work normally, although some actions will not compile when any of method's arguments are move-only. You can always use Return , or a lambda or functor : using :: testing :: Unused ; EXPECT_CALL ( mock_buzzer_ , ShareBuzz ( NotNull (), _ )). WillOnce ( Return ( true )); EXPECT_TRUE ( mock_buzzer_ . ShareBuzz ( MakeUnique < Buzz > ( AccessLevel :: kInternal )), 0 ); EXPECT_CALL ( mock_buzzer_ , ShareBuzz ( _ , _ )). WillOnce ( []( std :: unique_ptr < Buzz > buzz , Unused ) { return buzz != nullptr ; }); EXPECT_FALSE ( mock_buzzer_ . ShareBuzz ( nullptr , 0 )); Many built-in actions ( WithArgs , WithoutArgs , DeleteArg , SaveArg , ...) could in principle support move-only arguments, but the support for this is not implemented yet. If this is blocking you, please file a bug. A few actions (e.g. DoAll ) copy their arguments internally, so they can never work with non-copyable objects; you'll have to use functors instead.","title":"Mocking Methods That Use Move-Only Types"},{"location":"examples/gtest/docs/cook_book.html#LegacyMoveOnly","text":"Support for move-only function arguments was only introduced to gMock in April 2017. In older code, you may encounter the following workaround for the lack of this feature (it is no longer necessary - we're including it just for reference): class MockBuzzer : public Buzzer { public : MOCK_METHOD ( bool , DoShareBuzz , ( Buzz * buzz , Time timestamp )); bool ShareBuzz ( std :: unique_ptr < Buzz > buzz , Time timestamp ) override { return DoShareBuzz ( buzz . get (), timestamp ); } }; The trick is to delegate the ShareBuzz() method to a mock method (let\u2019s call it DoShareBuzz() ) that does not take move-only parameters. Then, instead of setting expectations on ShareBuzz() , you set them on the DoShareBuzz() mock method: MockBuzzer mock_buzzer_ ; EXPECT_CALL ( mock_buzzer_ , DoShareBuzz ( NotNull (), _ )); // When one calls ShareBuzz() on the MockBuzzer like this, the call is // forwarded to DoShareBuzz(), which is mocked. Therefore this statement // will trigger the above EXPECT_CALL. mock_buzzer_ . ShareBuzz ( MakeUnique < Buzz > ( AccessLevel :: kInternal ), 0 );","title":"Legacy workarounds for move-only types"},{"location":"examples/gtest/docs/cook_book.html#making-the-compilation-faster","text":"Believe it or not, the vast majority of the time spent on compiling a mock class is in generating its constructor and destructor, as they perform non-trivial tasks (e.g. verification of the expectations). What's more, mock methods with different signatures have different types and thus their constructors/destructors need to be generated by the compiler separately. As a result, if you mock many different types of methods, compiling your mock class can get really slow. If you are experiencing slow compilation, you can move the definition of your mock class' constructor and destructor out of the class body and into a .cc file. This way, even if you #include your mock class in N files, the compiler only needs to generate its constructor and destructor once, resulting in a much faster compilation. Let's illustrate the idea using an example. Here's the definition of a mock class before applying this recipe: // File mock_foo.h. ... class MockFoo : public Foo { public : // Since we don't declare the constructor or the destructor, // the compiler will generate them in every translation unit // where this mock class is used. MOCK_METHOD ( int , DoThis , (), ( override )); MOCK_METHOD ( bool , DoThat , ( const char * str ), ( override )); ... more mock methods ... }; After the change, it would look like: // File mock_foo.h. ... class MockFoo : public Foo { public : // The constructor and destructor are declared, but not defined, here. MockFoo (); virtual ~ MockFoo (); MOCK_METHOD ( int , DoThis , (), ( override )); MOCK_METHOD ( bool , DoThat , ( const char * str ), ( override )); ... more mock methods ... }; and // File mock_foo.cc. #include \"path/to/mock_foo.h\" // The definitions may appear trivial, but the functions actually do a // lot of things through the constructors/destructors of the member // variables used to implement the mock methods. MockFoo :: MockFoo () {} MockFoo ::~ MockFoo () {}","title":"Making the Compilation Faster"},{"location":"examples/gtest/docs/cook_book.html#forcing-a-verification","text":"When it's being destroyed, your friendly mock object will automatically verify that all expectations on it have been satisfied, and will generate googletest failures if not. This is convenient as it leaves you with one less thing to worry about. That is, unless you are not sure if your mock object will be destroyed. How could it be that your mock object won't eventually be destroyed? Well, it might be created on the heap and owned by the code you are testing. Suppose there's a bug in that code and it doesn't delete the mock object properly - you could end up with a passing test when there's actually a bug. Using a heap checker is a good idea and can alleviate the concern, but its implementation is not 100% reliable. So, sometimes you do want to force gMock to verify a mock object before it is (hopefully) destructed. You can do this with Mock::VerifyAndClearExpectations(&mock_object) : TEST ( MyServerTest , ProcessesRequest ) { using :: testing :: Mock ; MockFoo * const foo = new MockFoo ; EXPECT_CALL ( * foo , ...)...; // ... other expectations ... // server now owns foo. MyServer server ( foo ); server . ProcessRequest (...); // In case that server's destructor will forget to delete foo, // this will verify the expectations anyway. Mock :: VerifyAndClearExpectations ( foo ); } // server is destroyed when it goes out of scope here. Tip: The Mock::VerifyAndClearExpectations() function returns a bool to indicate whether the verification was successful ( true for yes), so you can wrap that function call inside a ASSERT_TRUE() if there is no point going further when the verification has failed.","title":"Forcing a Verification"},{"location":"examples/gtest/docs/cook_book.html#UsingCheckPoints","text":"Sometimes you may want to \"reset\" a mock object at various check points in your test: at each check point, you verify that all existing expectations on the mock object have been satisfied, and then you set some new expectations on it as if it's newly created. This allows you to work with a mock object in \"phases\" whose sizes are each manageable. One such scenario is that in your test's SetUp() function, you may want to put the object you are testing into a certain state, with the help from a mock object. Once in the desired state, you want to clear all expectations on the mock, such that in the TEST_F body you can set fresh expectations on it. As you may have figured out, the Mock::VerifyAndClearExpectations() function we saw in the previous recipe can help you here. Or, if you are using ON_CALL() to set default actions on the mock object and want to clear the default actions as well, use Mock::VerifyAndClear(&mock_object) instead. This function does what Mock::VerifyAndClearExpectations(&mock_object) does and returns the same bool , plus it clears the ON_CALL() statements on mock_object too. Another trick you can use to achieve the same effect is to put the expectations in sequences and insert calls to a dummy \"check-point\" function at specific places. Then you can verify that the mock function calls do happen at the right time. For example, if you are exercising code: Foo ( 1 ); Foo ( 2 ); Foo ( 3 ); and want to verify that Foo(1) and Foo(3) both invoke mock.Bar(\"a\") , but Foo(2) doesn't invoke anything. You can write: using :: testing :: MockFunction ; TEST ( FooTest , InvokesBarCorrectly ) { MyMock mock ; // Class MockFunction<F> has exactly one mock method. It is named // Call() and has type F. MockFunction < void ( string check_point_name ) > check ; { InSequence s ; EXPECT_CALL ( mock , Bar ( \"a\" )); EXPECT_CALL ( check , Call ( \"1\" )); EXPECT_CALL ( check , Call ( \"2\" )); EXPECT_CALL ( mock , Bar ( \"a\" )); } Foo ( 1 ); check . Call ( \"1\" ); Foo ( 2 ); check . Call ( \"2\" ); Foo ( 3 ); } The expectation spec says that the first Bar(\"a\") must happen before check point \"1\", the second Bar(\"a\") must happen after check point \"2\", and nothing should happen between the two check points. The explicit check points make it easy to tell which Bar(\"a\") is called by which call to Foo() .","title":"Using Check Points"},{"location":"examples/gtest/docs/cook_book.html#mocking-destructors","text":"Sometimes you want to make sure a mock object is destructed at the right time, e.g. after bar->A() is called but before bar->B() is called. We already know that you can specify constraints on the order of mock function calls, so all we need to do is to mock the destructor of the mock function. This sounds simple, except for one problem: a destructor is a special function with special syntax and special semantics, and the MOCK_METHOD macro doesn't work for it: MOCK_METHOD ( void , ~ MockFoo , ()); // Won't compile! The good news is that you can use a simple pattern to achieve the same effect. First, add a mock function Die() to your mock class and call it in the destructor, like this: class MockFoo : public Foo { ... // Add the following two lines to the mock class. MOCK_METHOD ( void , Die , ()); virtual ~ MockFoo () { Die (); } }; (If the name Die() clashes with an existing symbol, choose another name.) Now, we have translated the problem of testing when a MockFoo object dies to testing when its Die() method is called: MockFoo * foo = new MockFoo ; MockBar * bar = new MockBar ; ... { InSequence s ; // Expects *foo to die after bar->A() and before bar->B(). EXPECT_CALL ( * bar , A ()); EXPECT_CALL ( * foo , Die ()); EXPECT_CALL ( * bar , B ()); } And that's that.","title":"Mocking Destructors"},{"location":"examples/gtest/docs/cook_book.html#UsingThreads","text":"In a unit test, it's best if you could isolate and test a piece of code in a single-threaded context. That avoids race conditions and dead locks, and makes debugging your test much easier. Yet most programs are multi-threaded, and sometimes to test something we need to pound on it from more than one thread. gMock works for this purpose too. Remember the steps for using a mock: Create a mock object foo . Set its default actions and expectations using ON_CALL() and EXPECT_CALL() . The code under test calls methods of foo . Optionally, verify and reset the mock. Destroy the mock yourself, or let the code under test destroy it. The destructor will automatically verify it. If you follow the following simple rules, your mocks and threads can live happily together: Execute your test code (as opposed to the code being tested) in one thread. This makes your test easy to follow. Obviously, you can do step #1 without locking. When doing step #2 and #5, make sure no other thread is accessing foo . Obvious too, huh?","title":"Using gMock and Threads"},{"location":"examples/gtest/docs/cook_book.html#3-and-4-can-be-done-either-in-one-thread-or-in-multiple-threads-anyway","text":"you want. gMock takes care of the locking, so you don't have to do any - unless required by your test logic. If you violate the rules (for example, if you set expectations on a mock while another thread is calling its methods), you get undefined behavior. That's not fun, so don't do it. gMock guarantees that the action for a mock function is done in the same thread that called the mock function. For example, in EXPECT_CALL ( mock , Foo ( 1 )) . WillOnce ( action1 ); EXPECT_CALL ( mock , Foo ( 2 )) . WillOnce ( action2 ); if Foo(1) is called in thread 1 and Foo(2) is called in thread 2, gMock will execute action1 in thread 1 and action2 in thread 2. gMock does not impose a sequence on actions performed in different threads (doing so may create deadlocks as the actions may need to cooperate). This means that the execution of action1 and action2 in the above example may interleave. If this is a problem, you should add proper synchronization logic to action1 and action2 to make the test thread-safe. Also, remember that DefaultValue<T> is a global resource that potentially affects all living mock objects in your program. Naturally, you won't want to mess with it from multiple threads or when there still are mocks in action.","title":"3 and #4 can be done either in one thread or in multiple threads - anyway"},{"location":"examples/gtest/docs/cook_book.html#controlling-how-much-information-gmock-prints","text":"When gMock sees something that has the potential of being an error (e.g. a mock function with no expectation is called, a.k.a. an uninteresting call, which is allowed but perhaps you forgot to explicitly ban the call), it prints some warning messages, including the arguments of the function, the return value, and the stack trace. Hopefully this will remind you to take a look and see if there is indeed a problem. Sometimes you are confident that your tests are correct and may not appreciate such friendly messages. Some other times, you are debugging your tests or learning about the behavior of the code you are testing, and wish you could observe every mock call that happens (including argument values, the return value, and the stack trace). Clearly, one size doesn't fit all. You can control how much gMock tells you using the --gmock_verbose=LEVEL command-line flag, where LEVEL is a string with three possible values: info : gMock will print all informational messages, warnings, and errors (most verbose). At this setting, gMock will also log any calls to the ON_CALL/EXPECT_CALL macros. It will include a stack trace in \"uninteresting call\" warnings. warning : gMock will print both warnings and errors (less verbose); it will omit the stack traces in \"uninteresting call\" warnings. This is the default. error : gMock will print errors only (least verbose). Alternatively, you can adjust the value of that flag from within your tests like so: :: testing :: FLAGS_gmock_verbose = \"error\" ; If you find gMock printing too many stack frames with its informational or warning messages, remember that you can control their amount with the --gtest_stack_trace_depth=max_depth flag. Now, judiciously use the right flag to enable gMock serve you better!","title":"Controlling How Much Information gMock Prints"},{"location":"examples/gtest/docs/cook_book.html#gaining-super-vision-into-mock-calls","text":"You have a test using gMock. It fails: gMock tells you some expectations aren't satisfied. However, you aren't sure why: Is there a typo somewhere in the matchers? Did you mess up the order of the EXPECT_CALL s? Or is the code under test doing something wrong? How can you find out the cause? Won't it be nice if you have X-ray vision and can actually see the trace of all EXPECT_CALL s and mock method calls as they are made? For each call, would you like to see its actual argument values and which EXPECT_CALL gMock thinks it matches? If you still need some help to figure out who made these calls, how about being able to see the complete stack trace at each mock call? You can unlock this power by running your test with the --gmock_verbose=info flag. For example, given the test program: #include \"gmock/gmock.h\" using testing :: _ ; using testing :: HasSubstr ; using testing :: Return ; class MockFoo { public : MOCK_METHOD ( void , F , ( const string & x , const string & y )); }; TEST ( Foo , Bar ) { MockFoo mock ; EXPECT_CALL ( mock , F ( _ , _ )). WillRepeatedly ( Return ()); EXPECT_CALL ( mock , F ( \"a\" , \"b\" )); EXPECT_CALL ( mock , F ( \"c\" , HasSubstr ( \"d\" ))); mock . F ( \"a\" , \"good\" ); mock . F ( \"a\" , \"b\" ); } if you run it with --gmock_verbose=info , you will see this output: [ RUN ] Foo.Bar foo_test.cc:14: EXPECT_CALL ( mock, F ( _, _ )) invoked Stack trace: ... foo_test.cc:15: EXPECT_CALL ( mock, F ( \"a\" , \"b\" )) invoked Stack trace: ... foo_test.cc:16: EXPECT_CALL ( mock, F ( \"c\" , HasSubstr ( \"d\" ))) invoked Stack trace: ... foo_test.cc:14: Mock function call matches EXPECT_CALL ( mock, F ( _, _ )) ... Function call: F ( @0x7fff7c8dad40 \"a\" ,@0x7fff7c8dad10 \"good\" ) Stack trace: ... foo_test.cc:15: Mock function call matches EXPECT_CALL ( mock, F ( \"a\" , \"b\" )) ... Function call: F ( @0x7fff7c8dada0 \"a\" ,@0x7fff7c8dad70 \"b\" ) Stack trace: ... foo_test.cc:16: Failure Actual function call count doesn ' t match EXPECT_CALL ( mock, F ( \"c\" , HasSubstr ( \"d\" ))) ... Expected: to be called once Actual: never called - unsatisfied and active [ FAILED ] Foo.Bar Suppose the bug is that the \"c\" in the third EXPECT_CALL is a typo and should actually be \"a\" . With the above message, you should see that the actual F(\"a\", \"good\") call is matched by the first EXPECT_CALL , not the third as you thought. From that it should be obvious that the third EXPECT_CALL is written wrong. Case solved. If you are interested in the mock call trace but not the stack traces, you can combine --gmock_verbose=info with --gtest_stack_trace_depth=0 on the test command line.","title":"Gaining Super Vision into Mock Calls"},{"location":"examples/gtest/docs/cook_book.html#running-tests-in-emacs","text":"If you build and run your tests in Emacs using the M-x google-compile command (as many googletest users do), the source file locations of gMock and googletest errors will be highlighted. Just press <Enter> on one of them and you'll be taken to the offending line. Or, you can just type `C-x`` to jump to the next error. To make it even easier, you can add the following lines to your ~/.emacs file: (global-set-key \"\\M-m\" 'google-compile) ; m is for make (global-set-key [M-down] 'next-error) (global-set-key [M-up] '(lambda () (interactive) (next-error -1))) Then you can type M-m to start a build (if you want to run the test as well, just make sure foo_test.run or runtests is in the build command you supply after typing M-m ), or M-up / M-down to move back and forth between errors.","title":"Running Tests in Emacs"},{"location":"examples/gtest/docs/cook_book.html#extending-gmock","text":"","title":"Extending gMock"},{"location":"examples/gtest/docs/cook_book.html#NewMatchers","text":"WARNING: gMock does not guarantee when or how many times a matcher will be invoked. Therefore, all matchers must be functionally pure. See this section for more details. The MATCHER* family of macros can be used to define custom matchers easily. The syntax: MATCHER ( name , description_string_expression ) { statements ; } will define a matcher with the given name that executes the statements, which must return a bool to indicate if the match succeeds. Inside the statements, you can refer to the value being matched by arg , and refer to its type by arg_type . The description string is a string -typed expression that documents what the matcher does, and is used to generate the failure message when the match fails. It can (and should) reference the special bool variable negation , and should evaluate to the description of the matcher when negation is false , or that of the matcher's negation when negation is true . For convenience, we allow the description string to be empty ( \"\" ), in which case gMock will use the sequence of words in the matcher name as the description. For example: MATCHER ( IsDivisibleBy7 , \"\" ) { return ( arg % 7 ) == 0 ; } allows you to write // Expects mock_foo.Bar(n) to be called where n is divisible by 7. EXPECT_CALL ( mock_foo , Bar ( IsDivisibleBy7 ())); or, using :: testing :: Not ; ... // Verifies that two values are divisible by 7. EXPECT_THAT ( some_expression , IsDivisibleBy7 ()); EXPECT_THAT ( some_other_expression , Not ( IsDivisibleBy7 ())); If the above assertions fail, they will print something like: Value of: some_expression Expected: is divisible by 7 Actual: 27 ... Value of: some_other_expression Expected: not ( is divisible by 7 ) Actual: 21 where the descriptions \"is divisible by 7\" and \"not (is divisible by 7)\" are automatically calculated from the matcher name IsDivisibleBy7 . As you may have noticed, the auto-generated descriptions (especially those for the negation) may not be so great. You can always override them with a string expression of your own: MATCHER ( IsDivisibleBy7 , absl :: StrCat ( negation ? \"isn't\" : \"is\" , \" divisible by 7\" )) { return ( arg % 7 ) == 0 ; } Optionally, you can stream additional information to a hidden argument named result_listener to explain the match result. For example, a better definition of IsDivisibleBy7 is: MATCHER ( IsDivisibleBy7 , \"\" ) { if (( arg % 7 ) == 0 ) return true ; * result_listener << \"the remainder is \" << ( arg % 7 ); return false ; } With this definition, the above assertion will give a better message: Value of: some_expression Expected: is divisible by 7 Actual: 27 ( the remainder is 6 ) You should let MatchAndExplain() print any additional information that can help a user understand the match result. Note that it should explain why the match succeeds in case of a success (unless it's obvious) - this is useful when the matcher is used inside Not() . There is no need to print the argument value itself, as gMock already prints it for you. NOTE: The type of the value being matched ( arg_type ) is determined by the context in which you use the matcher and is supplied to you by the compiler, so you don't need to worry about declaring it (nor can you). This allows the matcher to be polymorphic. For example, IsDivisibleBy7() can be used to match any type where the value of (arg % 7) == 0 can be implicitly converted to a bool . In the Bar(IsDivisibleBy7()) example above, if method Bar() takes an int , arg_type will be int ; if it takes an unsigned long , arg_type will be unsigned long ; and so on.","title":"Writing New Matchers Quickly"},{"location":"examples/gtest/docs/cook_book.html#writing-new-parameterized-matchers-quickly","text":"Sometimes you'll want to define a matcher that has parameters. For that you can use the macro: MATCHER_P ( name , param_name , description_string ) { statements ; } where the description string can be either \"\" or a string expression that references negation and param_name . For example: MATCHER_P ( HasAbsoluteValue , value , \"\" ) { return abs ( arg ) == value ; } will allow you to write: EXPECT_THAT ( Blah ( \"a\" ), HasAbsoluteValue ( n )); which may lead to this message (assuming n is 10): Value of: Blah ( \"a\" ) Expected: has absolute value 10 Actual: -9 Note that both the matcher description and its parameter are printed, making the message human-friendly. In the matcher definition body, you can write foo_type to reference the type of a parameter named foo . For example, in the body of MATCHER_P(HasAbsoluteValue, value) above, you can write value_type to refer to the type of value . gMock also provides MATCHER_P2 , MATCHER_P3 , ..., up to MATCHER_P10 to support multi-parameter matchers: MATCHER_Pk ( name , param_1 , ..., param_k , description_string ) { statements ; } Please note that the custom description string is for a particular instance of the matcher, where the parameters have been bound to actual values. Therefore usually you'll want the parameter values to be part of the description. gMock lets you do that by referencing the matcher parameters in the description string expression. For example, using :: testing :: PrintToString ; MATCHER_P2 ( InClosedRange , low , hi , absl :: StrFormat ( \"%s in range [%s, %s]\" , negation ? \"isn't\" : \"is\" , PrintToString ( low ), PrintToString ( hi ))) { return low <= arg && arg <= hi ; } ... EXPECT_THAT ( 3 , InClosedRange ( 4 , 6 )); would generate a failure that contains the message: Expected: is in range [ 4 , 6 ] If you specify \"\" as the description, the failure message will contain the sequence of words in the matcher name followed by the parameter values printed as a tuple. For example, MATCHER_P2 ( InClosedRange , low , hi , \"\" ) { ... } ... EXPECT_THAT ( 3 , InClosedRange ( 4 , 6 )); would generate a failure that contains the text: Expected: in closed range ( 4 , 6 ) For the purpose of typing, you can view MATCHER_Pk ( Foo , p1 , ..., pk , description_string ) { ... } as shorthand for template < typename p1_type , ..., typename pk_type > FooMatcherPk < p1_type , ..., pk_type > Foo ( p1_type p1 , ..., pk_type pk ) { ... } When you write Foo(v1, ..., vk) , the compiler infers the types of the parameters v1 , ..., and vk for you. If you are not happy with the result of the type inference, you can specify the types by explicitly instantiating the template, as in Foo<long, bool>(5, false) . As said earlier, you don't get to (or need to) specify arg_type as that's determined by the context in which the matcher is used. You can assign the result of expression Foo(p1, ..., pk) to a variable of type FooMatcherPk<p1_type, ..., pk_type> . This can be useful when composing matchers. Matchers that don't have a parameter or have only one parameter have special types: you can assign Foo() to a FooMatcher -typed variable, and assign Foo(p) to a FooMatcherP<p_type> -typed variable. While you can instantiate a matcher template with reference types, passing the parameters by pointer usually makes your code more readable. If, however, you still want to pass a parameter by reference, be aware that in the failure message generated by the matcher you will see the value of the referenced object but not its address. You can overload matchers with different numbers of parameters: MATCHER_P ( Blah , a , description_string_1 ) { ... } MATCHER_P2 ( Blah , a , b , description_string_2 ) { ... } While it's tempting to always use the MATCHER* macros when defining a new matcher, you should also consider implementing MatcherInterface or using MakePolymorphicMatcher() instead (see the recipes that follow), especially if you need to use the matcher a lot. While these approaches require more work, they give you more control on the types of the value being matched and the matcher parameters, which in general leads to better compiler error messages that pay off in the long run. They also allow overloading matchers based on parameter types (as opposed to just based on the number of parameters).","title":"Writing New Parameterized Matchers Quickly"},{"location":"examples/gtest/docs/cook_book.html#writing-new-monomorphic-matchers","text":"A matcher of argument type T implements ::testing::MatcherInterface<T> and does two things: it tests whether a value of type T matches the matcher, and can describe what kind of values it matches. The latter ability is used for generating readable error messages when expectations are violated. The interface looks like this: class MatchResultListener { public : ... // Streams x to the underlying ostream; does nothing if the ostream // is NULL. template < typename T > MatchResultListener & operator << ( const T & x ); // Returns the underlying ostream. :: std :: ostream * stream (); }; template < typename T > class MatcherInterface { public : virtual ~ MatcherInterface (); // Returns true if and only if the matcher matches x; also explains the match // result to 'listener'. virtual bool MatchAndExplain ( T x , MatchResultListener * listener ) const = 0 ; // Describes this matcher to an ostream. virtual void DescribeTo ( :: std :: ostream * os ) const = 0 ; // Describes the negation of this matcher to an ostream. virtual void DescribeNegationTo ( :: std :: ostream * os ) const ; }; If you need a custom matcher but Truly() is not a good option (for example, you may not be happy with the way Truly(predicate) describes itself, or you may want your matcher to be polymorphic as Eq(value) is), you can define a matcher to do whatever you want in two steps: first implement the matcher interface, and then define a factory function to create a matcher instance. The second step is not strictly needed but it makes the syntax of using the matcher nicer. For example, you can define a matcher to test whether an int is divisible by 7 and then use it like this: using :: testing :: MakeMatcher ; using :: testing :: Matcher ; using :: testing :: MatcherInterface ; using :: testing :: MatchResultListener ; class DivisibleBy7Matcher : public MatcherInterface < int > { public : bool MatchAndExplain ( int n , MatchResultListener * /* listener */ ) const override { return ( n % 7 ) == 0 ; } void DescribeTo ( :: std :: ostream * os ) const override { * os << \"is divisible by 7\" ; } void DescribeNegationTo ( :: std :: ostream * os ) const override { * os << \"is not divisible by 7\" ; } }; Matcher < int > DivisibleBy7 () { return MakeMatcher ( new DivisibleBy7Matcher ); } ... EXPECT_CALL ( foo , Bar ( DivisibleBy7 ())); You may improve the matcher message by streaming additional information to the listener argument in MatchAndExplain() : class DivisibleBy7Matcher : public MatcherInterface < int > { public : bool MatchAndExplain ( int n , MatchResultListener * listener ) const override { const int remainder = n % 7 ; if ( remainder != 0 ) { * listener << \"the remainder is \" << remainder ; } return remainder == 0 ; } ... }; Then, EXPECT_THAT(x, DivisibleBy7()); may generate a message like this: Value of: x Expected: is divisible by 7 Actual: 23 ( the remainder is 2 )","title":"Writing New Monomorphic Matchers"},{"location":"examples/gtest/docs/cook_book.html#writing-new-polymorphic-matchers","text":"You've learned how to write your own matchers in the previous recipe. Just one problem: a matcher created using MakeMatcher() only works for one particular type of arguments. If you want a polymorphic matcher that works with arguments of several types (for instance, Eq(x) can be used to match a value as long as value == x compiles -- value and x don't have to share the same type), you can learn the trick from testing/base/public/gmock-matchers.h but it's a bit involved. Fortunately, most of the time you can define a polymorphic matcher easily with the help of MakePolymorphicMatcher() . Here's how you can define NotNull() as an example: using :: testing :: MakePolymorphicMatcher ; using :: testing :: MatchResultListener ; using :: testing :: PolymorphicMatcher ; class NotNullMatcher { public : // To implement a polymorphic matcher, first define a COPYABLE class // that has three members MatchAndExplain(), DescribeTo(), and // DescribeNegationTo(), like the following. // In this example, we want to use NotNull() with any pointer, so // MatchAndExplain() accepts a pointer of any type as its first argument. // In general, you can define MatchAndExplain() as an ordinary method or // a method template, or even overload it. template < typename T > bool MatchAndExplain ( T * p , MatchResultListener * /* listener */ ) const { return p != NULL ; } // Describes the property of a value matching this matcher. void DescribeTo ( std :: ostream * os ) const { * os << \"is not NULL\" ; } // Describes the property of a value NOT matching this matcher. void DescribeNegationTo ( std :: ostream * os ) const { * os << \"is NULL\" ; } }; // To construct a polymorphic matcher, pass an instance of the class // to MakePolymorphicMatcher(). Note the return type. PolymorphicMatcher < NotNullMatcher > NotNull () { return MakePolymorphicMatcher ( NotNullMatcher ()); } ... EXPECT_CALL ( foo , Bar ( NotNull ())); // The argument must be a non-NULL pointer. Note: Your polymorphic matcher class does not need to inherit from MatcherInterface or any other class, and its methods do not need to be virtual. Like in a monomorphic matcher, you may explain the match result by streaming additional information to the listener argument in MatchAndExplain() .","title":"Writing New Polymorphic Matchers"},{"location":"examples/gtest/docs/cook_book.html#writing-new-cardinalities","text":"A cardinality is used in Times() to tell gMock how many times you expect a call to occur. It doesn't have to be exact. For example, you can say AtLeast(5) or Between(2, 4) . If the built-in set of cardinalities doesn't suit you, you are free to define your own by implementing the following interface (in namespace testing ): class CardinalityInterface { public : virtual ~ CardinalityInterface (); // Returns true if and only if call_count calls will satisfy this cardinality. virtual bool IsSatisfiedByCallCount ( int call_count ) const = 0 ; // Returns true if and only if call_count calls will saturate this // cardinality. virtual bool IsSaturatedByCallCount ( int call_count ) const = 0 ; // Describes self to an ostream. virtual void DescribeTo ( std :: ostream * os ) const = 0 ; }; For example, to specify that a call must occur even number of times, you can write using :: testing :: Cardinality ; using :: testing :: CardinalityInterface ; using :: testing :: MakeCardinality ; class EvenNumberCardinality : public CardinalityInterface { public : bool IsSatisfiedByCallCount ( int call_count ) const override { return ( call_count % 2 ) == 0 ; } bool IsSaturatedByCallCount ( int call_count ) const override { return false ; } void DescribeTo ( std :: ostream * os ) const { * os << \"called even number of times\" ; } }; Cardinality EvenNumber () { return MakeCardinality ( new EvenNumberCardinality ); } ... EXPECT_CALL ( foo , Bar ( 3 )) . Times ( EvenNumber ());","title":"Writing New Cardinalities"},{"location":"examples/gtest/docs/cook_book.html#QuickNewActions","text":"If the built-in actions don't work for you, you can easily define your own one. Just define a functor class with a (possibly templated) call operator, matching the signature of your action. struct Increment { template < typename T > T operator ()( T * arg ) { return ++ ( * arg ); } } The same approach works with stateful functors (or any callable, really): struct MultiplyBy { template <typename T> T operator()(T arg) { return arg * multiplier; } int multiplier; } // Then use: // EXPECT_CALL(...).WillOnce(MultiplyBy{7});","title":"Writing New Actions Quickly"},{"location":"examples/gtest/docs/cook_book.html#legacy-macro-based-actions","text":"Before C++11, the functor-based actions were not supported; the old way of writing actions was through a set of ACTION* macros. We suggest to avoid them in new code; they hide a lot of logic behind the macro, potentially leading to harder-to-understand compiler errors. Nevertheless, we cover them here for completeness. By writing ACTION ( name ) { statements ; } in a namespace scope (i.e. not inside a class or function), you will define an action with the given name that executes the statements. The value returned by statements will be used as the return value of the action. Inside the statements, you can refer to the K-th (0-based) argument of the mock function as argK . For example: ACTION ( IncrementArg1 ) { return ++ ( * arg1 ); } allows you to write ... WillOnce ( IncrementArg1 ()); Note that you don't need to specify the types of the mock function arguments. Rest assured that your code is type-safe though: you'll get a compiler error if *arg1 doesn't support the ++ operator, or if the type of ++(*arg1) isn't compatible with the mock function's return type. Another example: ACTION ( Foo ) { ( * arg2 )( 5 ); Blah (); * arg1 = 0 ; return arg0 ; } defines an action Foo() that invokes argument #2 (a function pointer) with 5, calls function Blah() , sets the value pointed to by argument #1 to 0, and returns argument #0. For more convenience and flexibility, you can also use the following pre-defined symbols in the body of ACTION : argK_type The type of the K-th (0-based) argument of the mock function args All arguments of the mock function as a tuple args_type The type of all arguments of the mock function as a tuple return_type The return type of the mock function function_type The type of the mock function For example, when using an ACTION as a stub action for mock function: int DoSomething ( bool flag , int * ptr ); we have: Pre-defined Symbol Is Bound To arg0 the value of flag arg0_type the type bool arg1 the value of ptr arg1_type the type int* args the tuple (flag, ptr) args_type the type std::tuple<bool, int*> return_type the type int function_type the type int(bool, int*)","title":"Legacy macro-based Actions"},{"location":"examples/gtest/docs/cook_book.html#legacy-macro-based-parameterized-actions","text":"Sometimes you'll want to parameterize an action you define. For that we have another macro ACTION_P ( name , param ) { statements ; } For example, ACTION_P ( Add , n ) { return arg0 + n ; } will allow you to write // Returns argument #0 + 5. ... WillOnce ( Add ( 5 )); For convenience, we use the term arguments for the values used to invoke the mock function, and the term parameters for the values used to instantiate an action. Note that you don't need to provide the type of the parameter either. Suppose the parameter is named param , you can also use the gMock-defined symbol param_type to refer to the type of the parameter as inferred by the compiler. For example, in the body of ACTION_P(Add, n) above, you can write n_type for the type of n . gMock also provides ACTION_P2 , ACTION_P3 , and etc to support multi-parameter actions. For example, ACTION_P2 ( ReturnDistanceTo , x , y ) { double dx = arg0 - x ; double dy = arg1 - y ; return sqrt ( dx * dx + dy * dy ); } lets you write ... WillOnce ( ReturnDistanceTo ( 5.0 , 26.5 )); You can view ACTION as a degenerated parameterized action where the number of parameters is 0. You can also easily define actions overloaded on the number of parameters: ACTION_P ( Plus , a ) { ... } ACTION_P2 ( Plus , a , b ) { ... }","title":"Legacy macro-based parameterized Actions"},{"location":"examples/gtest/docs/cook_book.html#restricting-the-type-of-an-argument-or-parameter-in-an-action","text":"For maximum brevity and reusability, the ACTION* macros don't ask you to provide the types of the mock function arguments and the action parameters. Instead, we let the compiler infer the types for us. Sometimes, however, we may want to be more explicit about the types. There are several tricks to do that. For example: ACTION ( Foo ) { // Makes sure arg0 can be converted to int. int n = arg0 ; ... use n instead of arg0 here ... } ACTION_P ( Bar , param ) { // Makes sure the type of arg1 is const char*. :: testing :: StaticAssertTypeEq < const char * , arg1_type > (); // Makes sure param can be converted to bool. bool flag = param ; } where StaticAssertTypeEq is a compile-time assertion in googletest that verifies two types are the same.","title":"Restricting the Type of an Argument or Parameter in an ACTION"},{"location":"examples/gtest/docs/cook_book.html#writing-new-action-templates-quickly","text":"Sometimes you want to give an action explicit template parameters that cannot be inferred from its value parameters. ACTION_TEMPLATE() supports that and can be viewed as an extension to ACTION() and ACTION_P*() . The syntax: ACTION_TEMPLATE ( ActionName , HAS_m_TEMPLATE_PARAMS ( kind1 , name1 , ..., kind_m , name_m ), AND_n_VALUE_PARAMS ( p1 , ..., p_n )) { statements ; } defines an action template that takes m explicit template parameters and n value parameters, where m is in [1, 10] and n is in [0, 10]. name_i is the name of the i -th template parameter, and kind_i specifies whether it's a typename , an integral constant, or a template. p_i is the name of the i -th value parameter. Example: // DuplicateArg<k, T>(output) converts the k-th argument of the mock // function to type T and copies it to *output. ACTION_TEMPLATE ( DuplicateArg , // Note the comma between int and k: HAS_2_TEMPLATE_PARAMS ( int , k , typename , T ), AND_1_VALUE_PARAMS ( output )) { * output = T ( :: std :: get < k > ( args )); } To create an instance of an action template, write: ActionName < t1 , ..., t_m > ( v1 , ..., v_n ) where the t s are the template arguments and the v s are the value arguments. The value argument types are inferred by the compiler. For example: using :: testing :: _ ; ... int n ; EXPECT_CALL ( mock , Foo ). WillOnce ( DuplicateArg < 1 , unsigned char > ( & n )); If you want to explicitly specify the value argument types, you can provide additional template arguments: ActionName < t1 , ..., t_m , u1 , ..., u_k > ( v1 , ..., v_n ) where u_i is the desired type of v_i . ACTION_TEMPLATE and ACTION / ACTION_P* can be overloaded on the number of value parameters, but not on the number of template parameters. Without the restriction, the meaning of the following is unclear: OverloadedAction < int , bool > ( x ); Are we using a single-template-parameter action where bool refers to the type of x , or a two-template-parameter action where the compiler is asked to infer the type of x ?","title":"Writing New Action Templates Quickly"},{"location":"examples/gtest/docs/cook_book.html#using-the-action-objects-type","text":"If you are writing a function that returns an ACTION object, you'll need to know its type. The type depends on the macro used to define the action and the parameter types. The rule is relatively simple: Given Definition Expression Has Type ACTION(Foo) Foo() FooAction ACTION_TEMPLATE(Foo, Foo<t1, ..., | FooAction<t1, ..., : HAS_m_TEMPLATE_PARAMS(...), : t_m>() : t_m> : : AND_0_VALUE_PARAMS()) : : : ACTION_P(Bar, param) Bar(int_value) BarActionP<int> ACTION_TEMPLATE(Bar, Bar<t1, ..., t_m> `FooActionP<t1, ..., : HAS_m_TEMPLATE_PARAMS(...), : (int_value) : t_m, int>` : : AND_1_VALUE_PARAMS(p1)) : : : ACTION_P2(Baz, p1, p2) Baz(bool_value, `BazActionP2<bool, : : int_value) : int>` : ACTION_TEMPLATE(Baz, Baz<t1, ..., t_m> `FooActionP2<t1, ..., : HAS_m_TEMPLATE_PARAMS(...), : (bool_value, : t_m, bool, int>` : : AND_2_VALUE_PARAMS(p1, p2)) : int_value) : : ... ... ... Note that we have to pick different suffixes ( Action , ActionP , ActionP2 , and etc) for actions with different numbers of value parameters, or the action definitions cannot be overloaded on the number of them.","title":"Using the ACTION Object's Type"},{"location":"examples/gtest/docs/cook_book.html#NewMonoActions","text":"While the ACTION* macros are very convenient, sometimes they are inappropriate. For example, despite the tricks shown in the previous recipes, they don't let you directly specify the types of the mock function arguments and the action parameters, which in general leads to unoptimized compiler error messages that can baffle unfamiliar users. They also don't allow overloading actions based on parameter types without jumping through some hoops. An alternative to the ACTION* macros is to implement ::testing::ActionInterface<F> , where F is the type of the mock function in which the action will be used. For example: template < typename F > class ActionInterface { public : virtual ~ ActionInterface (); // Performs the action. Result is the return type of function type // F, and ArgumentTuple is the tuple of arguments of F. // // For example, if F is int(bool, const string&), then Result would // be int, and ArgumentTuple would be ::std::tuple<bool, const string&>. virtual Result Perform ( const ArgumentTuple & args ) = 0 ; }; using :: testing :: _ ; using :: testing :: Action ; using :: testing :: ActionInterface ; using :: testing :: MakeAction ; typedef int IncrementMethod ( int * ); class IncrementArgumentAction : public ActionInterface < IncrementMethod > { public : int Perform ( const :: std :: tuple < int *>& args ) override { int * p = :: std :: get < 0 > ( args ); // Grabs the first argument. return * p ++ ; } }; Action < IncrementMethod > IncrementArgument () { return MakeAction ( new IncrementArgumentAction ); } ... EXPECT_CALL ( foo , Baz ( _ )) . WillOnce ( IncrementArgument ()); int n = 5 ; foo . Baz ( & n ); // Should return 5 and change n to 6.","title":"Writing New Monomorphic Actions"},{"location":"examples/gtest/docs/cook_book.html#NewPolyActions","text":"The previous recipe showed you how to define your own action. This is all good, except that you need to know the type of the function in which the action will be used. Sometimes that can be a problem. For example, if you want to use the action in functions with different types (e.g. like Return() and SetArgPointee() ). If an action can be used in several types of mock functions, we say it's polymorphic . The MakePolymorphicAction() function template makes it easy to define such an action: namespace testing { template < typename Impl > PolymorphicAction < Impl > MakePolymorphicAction ( const Impl & impl ); } // namespace testing As an example, let's define an action that returns the second argument in the mock function's argument list. The first step is to define an implementation class: class ReturnSecondArgumentAction { public : template < typename Result , typename ArgumentTuple > Result Perform ( const ArgumentTuple & args ) const { // To get the i-th (0-based) argument, use ::std::get(args). return :: std :: get < 1 > ( args ); } }; This implementation class does not need to inherit from any particular class. What matters is that it must have a Perform() method template. This method template takes the mock function's arguments as a tuple in a single argument, and returns the result of the action. It can be either const or not, but must be invokable with exactly one template argument, which is the result type. In other words, you must be able to call Perform<R>(args) where R is the mock function's return type and args is its arguments in a tuple. Next, we use MakePolymorphicAction() to turn an instance of the implementation class into the polymorphic action we need. It will be convenient to have a wrapper for this: using :: testing :: MakePolymorphicAction ; using :: testing :: PolymorphicAction ; PolymorphicAction < ReturnSecondArgumentAction > ReturnSecondArgument () { return MakePolymorphicAction ( ReturnSecondArgumentAction ()); } Now, you can use this polymorphic action the same way you use the built-in ones: using :: testing :: _ ; class MockFoo : public Foo { public : MOCK_METHOD ( int , DoThis , ( bool flag , int n ), ( override )); MOCK_METHOD ( string , DoThat , ( int x , const char * str1 , const char * str2 ), ( override )); }; ... MockFoo foo ; EXPECT_CALL ( foo , DoThis ). WillOnce ( ReturnSecondArgument ()); EXPECT_CALL ( foo , DoThat ). WillOnce ( ReturnSecondArgument ()); ... foo . DoThis ( true , 5 ); // Will return 5. foo . DoThat ( 1 , \"Hi\" , \"Bye\" ); // Will return \"Hi\".","title":"Writing New Polymorphic Actions"},{"location":"examples/gtest/docs/cook_book.html#teaching-gmock-how-to-print-your-values","text":"When an uninteresting or unexpected call occurs, gMock prints the argument values and the stack trace to help you debug. Assertion macros like EXPECT_THAT and EXPECT_EQ also print the values in question when the assertion fails. gMock and googletest do this using googletest's user-extensible value printer. This printer knows how to print built-in C++ types, native arrays, STL containers, and any type that supports the << operator. For other types, it prints the raw bytes in the value and hopes that you the user can figure it out. googletest's advanced guide explains how to extend the printer to do a better job at printing your particular type than to dump the bytes.","title":"Teaching gMock How to Print Your Values"},{"location":"examples/gtest/docs/cook_book.html#useful-mocks-created-using-gmock","text":"","title":"Useful Mocks Created Using gMock"},{"location":"examples/gtest/docs/cook_book.html#MockFunction","text":"std::function is a general function type introduced in C++11. It is a preferred way of passing callbacks to new interfaces. Functions are copiable, and are not usually passed around by pointer, which makes them tricky to mock. But fear not - MockFunction can help you with that. MockFunction<R(T1, ..., Tn)> has a mock method Call() with the signature: R Call ( T1 , ..., Tn ); It also has a AsStdFunction() method, which creates a std::function proxy forwarding to Call: std :: function < R ( T1 , ..., Tn ) > AsStdFunction (); To use MockFunction , first create MockFunction object and set up expectations on its Call method. Then pass proxy obtained from AsStdFunction() to the code you are testing. For example: TEST ( FooTest , RunsCallbackWithBarArgument ) { // 1. Create a mock object. MockFunction < int ( string ) > mock_function ; // 2. Set expectations on Call() method. EXPECT_CALL ( mock_function , Call ( \"bar\" )). WillOnce ( Return ( 1 )); // 3. Exercise code that uses std::function. Foo ( mock_function . AsStdFunction ()); // Foo's signature can be either of: // void Foo(const std::function<int(string)>& fun); // void Foo(std::function<int(string)> fun); // 4. All expectations will be verified when mock_function // goes out of scope and is destroyed. } Remember that function objects created with AsStdFunction() are just forwarders. If you create multiple of them, they will share the same set of expectations. Although std::function supports unlimited number of arguments, MockFunction implementation is limited to ten. If you ever hit that limit... well, your callback has bigger problems than being mockable. :-)","title":"Mock std::function"},{"location":"examples/gtest/docs/faq.html","text":"Googletest FAQ Why should test suite names and test names not contain underscore? Underscore ( _ ) is special, as C++ reserves the following to be used by the compiler and the standard library: any identifier that starts with an _ followed by an upper-case letter, and any identifier that contains two consecutive underscores (i.e. __ ) anywhere in its name. User code is prohibited from using such identifiers. Now let's look at what this means for TEST and TEST_F . Currently TEST(TestSuiteName, TestName) generates a class named TestSuiteName_TestName_Test . What happens if TestSuiteName or TestName contains _ ? If TestSuiteName starts with an _ followed by an upper-case letter (say, _Foo ), we end up with _Foo_TestName_Test , which is reserved and thus invalid. If TestSuiteName ends with an _ (say, Foo_ ), we get Foo__TestName_Test , which is invalid. If TestName starts with an _ (say, _Bar ), we get TestSuiteName__Bar_Test , which is invalid. If TestName ends with an _ (say, Bar_ ), we get TestSuiteName_Bar__Test , which is invalid. So clearly TestSuiteName and TestName cannot start or end with _ (Actually, TestSuiteName can start with _ -- as long as the _ isn't followed by an upper-case letter. But that's getting complicated. So for simplicity we just say that it cannot start with _ .). It may seem fine for TestSuiteName and TestName to contain _ in the middle. However, consider this: TEST ( Time , Flies_Like_An_Arrow ) { ... } TEST ( Time_Flies , Like_An_Arrow ) { ... } Now, the two TEST s will both generate the same class ( Time_Flies_Like_An_Arrow_Test ). That's not good. So for simplicity, we just ask the users to avoid _ in TestSuiteName and TestName . The rule is more constraining than necessary, but it's simple and easy to remember. It also gives googletest some wiggle room in case its implementation needs to change in the future. If you violate the rule, there may not be immediate consequences, but your test may (just may) break with a new compiler (or a new version of the compiler you are using) or with a new version of googletest. Therefore it's best to follow the rule. Why does googletest support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr) ? First of all you can use EXPECT_NE(nullptr, ptr) and ASSERT_NE(nullptr, ptr) . This is the preferred syntax in the style guide because nullptr does not have the type problems that NULL does. Which is why NULL does not work. Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of googletest harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr != NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of the gMock matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros. I need to test that different implementations of an interface satisfy some common requirements. Should I use typed tests or value-parameterized tests? For testing various implementations of the same interface, either typed tests or value-parameterized tests can get it done. It's really up to you the user to decide which is more convenient for you, depending on your particular case. Some rough guidelines: Typed tests can be easier to write if instances of the different implementations can be created the same way, modulo the type. For example, if all these implementations have a public default constructor (such that you can write new TypeParam ), or if their factory functions have the same form (e.g. CreateInstance<TypeParam>() ). Value-parameterized tests can be easier to write if you need different code patterns to create different implementations' instances, e.g. new Foo vs new Bar(5) . To accommodate for the differences, you can write factory function wrappers and pass these function pointers to the tests as their parameters. When a typed test fails, the default output includes the name of the type, which can help you quickly identify which implementation is wrong. Value-parameterized tests only show the number of the failed iteration by default. You will need to define a function that returns the iteration name and pass it as the third parameter to INSTANTIATE_TEST_SUITE_P to have more useful output. When using typed tests, you need to make sure you are testing against the interface type, not the concrete types (in other words, you want to make sure implicit_cast<MyInterface*>(my_concrete_impl) works, not just that my_concrete_impl works). It's less likely to make mistakes in this area when using value-parameterized tests. I hope I didn't confuse you more. :-) If you don't mind, I'd suggest you to give both approaches a try. Practice is a much better way to grasp the subtle differences between the two tools. Once you have some concrete experience, you can much more easily decide which one to use the next time. I got some run-time errors about invalid proto descriptors when using ProtocolMessageEquals . Help! Note: ProtocolMessageEquals and ProtocolMessageEquiv are deprecated now. Please use EqualsProto , etc instead. ProtocolMessageEquals and ProtocolMessageEquiv were redefined recently and are now less tolerant of invalid protocol buffer definitions. In particular, if you have a foo.proto that doesn't fully qualify the type of a protocol message it references (e.g. message<Bar> where it should be message<blah.Bar> ), you will now get run-time errors like: ... descriptor.cc:...] Invalid proto descriptor for file \"path/to/foo.proto\": ... descriptor.cc:...] blah.MyMessage.my_field: \".Bar\" is not defined. If you see this, your .proto file is broken and needs to be fixed by making the types fully qualified. The new definition of ProtocolMessageEquals and ProtocolMessageEquiv just happen to reveal your bug. My death test modifies some state, but the change seems lost after the death test finishes. Why? Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less. In particular, if you use mocking and the death test statement invokes some mock methods, the parent process will think the calls have never occurred. Therefore, you may want to move your EXPECT_CALL statements inside the EXPECT_DEATH macro. EXPECT_EQ(htonl(blah), blah_blah) generates weird compiler errors in opt mode. Is this a googletest bug? Actually, the bug is in htonl() . According to 'man htonl' , htonl() is a function , which means it's valid to use htonl as a function pointer. However, in opt mode htonl() is defined as a macro , which breaks this usage. Worse, the macro definition of htonl() uses a gcc extension and is not standard C++. That hacky implementation has some ad hoc limitations. In particular, it prevents you from writing Foo<sizeof(htonl(x))>() , where Foo is a template that has an integral argument. The implementation of EXPECT_EQ(a, b) uses sizeof(... a ...) inside a template argument, and thus doesn't compile in opt mode when a contains a call to htonl() . It is difficult to make EXPECT_EQ bypass the htonl() bug, as the solution must work with different compilers on various platforms. htonl() has some other problems as described in //util/endian/endian.h , which defines ghtonl() to replace it. ghtonl() does the same thing htonl() does, only without its problems. We suggest you to use ghtonl() instead of htonl() , both in your tests and production code. //util/endian/endian.h also defines ghtons() , which solves similar problems in htons() . Don't forget to add //util/endian to the list of dependencies in the BUILD file wherever ghtonl() and ghtons() are used. The library consists of a single header file and will not bloat your binary. The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong? If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100 ; }; You also need to define it outside of the class body in foo.cc : const int Foo :: kBar ; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in googletest comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error. The fact that \"it used to work\" doesn't mean it's valid. It just means that you were lucky. :-) Can I derive a test fixture from another? Yes. Each test fixture has a corresponding and same named test suite. This means only one test suite can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test suites don't leak important system resources like fonts and brushes. In googletest, you share a fixture among test suites by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test suite that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public :: testing :: Test { protected : ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected : void SetUp () override { BaseTest :: SetUp (); // Sets up the base fixture first. ... additional set - up work ... } void TearDown () override { ... clean - up work for FooTest ... BaseTest :: TearDown (); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F ( FooTest , Bar ) { ... } TEST_F ( FooTest , Baz ) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. googletest has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see sample5_unittest.cc $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample5_unittest.cc). My compiler complains \"void value not ignored as it ought to be.\" What does this mean? You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions, due to exceptions being disabled by our build system. Please see more details here . My death test hangs (or seg-faults). How do I fix it? In googletest, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this . In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . For example, you may want to use mocks or fake objects instead of real ones in your tests. Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry! Should I use the constructor/destructor of the test fixture or SetUp()/TearDown()? The first thing to remember is that googletest does not reuse the same test fixture object across multiple tests. For each TEST_F , googletest will create a fresh test fixture object, immediately call SetUp() , run the test body, call TearDown() , and then delete the test fixture object. When you need to write per-test set-up and tear-down logic, you have the choice between using the test fixture constructor/destructor or SetUp()/TearDown() . The former is usually preferred, as it has the following benefits: By initializing a member variable in the constructor, we have the option to make it const , which helps prevent accidental changes to its value and makes the tests more obviously correct. In case we need to subclass the test fixture class, the subclass' constructor is guaranteed to call the base class' constructor first , and the subclass' destructor is guaranteed to call the base class' destructor afterward . With SetUp()/TearDown() , a subclass may make the mistake of forgetting to call the base class' SetUp()/TearDown() or call them at the wrong time. You may still want to use SetUp()/TearDown() in the following cases: C++ does not allow virtual function calls in constructors and destructors. You can call a method declared as virtual, but it will not use dynamic dispatch, it will use the definition from the class the constructor of which is currently executing. This is because calling a virtual method before the derived class constructor has a chance to run is very dangerous - the virtual method might operate on uninitialized data. Therefore, if you need to call a method that will be overridden in a derived class, you have to use SetUp()/TearDown() . In the body of a constructor (or destructor), it's not possible to use the ASSERT_xx macros. Therefore, if the set-up operation could cause a fatal test failure that should prevent the test from running, it's necessary to use abort and abort the whole test executable, or to use SetUp() instead of a constructor. If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. The googletest team is considering making the assertion macros throw on platforms where exceptions are enabled (e.g. Windows, Mac OS, and Linux client-side), which will eliminate the need for the user to propagate failures from a subroutine to its caller. Therefore, you shouldn't use googletest assertions in a destructor if your code could run on such a platform. The compiler complains \"no matching function to call\" when I use ASSERT_PRED*. How do I fix it? If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive ( int n ) { return n > 0 ; } bool IsPositive ( double x ) { return x > 0 ; } you will get a compiler error if you write EXPECT_PRED1 ( IsPositive , 5 ); However, this will work: EXPECT_PRED1 ( static_cast < bool ( * )( int ) > ( IsPositive ), 5 ); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template < typename T > bool IsNegative ( T x ) { return x < 0 ; } you can use it in a predicate assertion like this: ASSERT_PRED1 ( IsNegative < int > , -5 ); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2 ( GreaterThan < int , int > , 5 , 0 ); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2 (( GreaterThan < int , int > ), 5 , 0 ); My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why? Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS (); they write RUN_ALL_TESTS (); This is wrong and dangerous . The testing services needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a googletest assertion failure. Very bad. We have decided to fix this (thanks to Michael Chastain for the idea). Now, your code will no longer be able to ignore RUN_ALL_TESTS() when compiled with gcc . If you do so, you'll get a compiler error. If you see the compiler complaining about you ignoring the return value of RUN_ALL_TESTS() , the fix is simple: just make sure its value is used as the return value of main() . But how could we introduce a change that breaks existing tests? Well, in this case, the code was already broken in the first place, so we didn't break it. :-) My compiler complains that a constructor (or destructor) cannot return a value. What's going on? Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ ( 1 , Foo ()) << \"blah blah\" << foo ; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it. My SetUp() function is not called. Why? C++ is case-sensitive. Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestSuite() as SetupTestSuite() and wonder why it's never called. I have several test suites which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious. You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } class BarTest : public BaseTest {}; TEST_F ( BarTest , Abc ) { ... } TEST_F ( BarTest , Def ) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest ; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } typedef BaseTest BarTest ; TEST_F ( BarTest , Abc ) { ... } TEST_F ( BarTest , Def ) { ... } googletest output is buried in a whole bunch of LOG messages. What do I do? The googletest output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the googletest output, making it hard to read. However, there is an easy solution to this problem. Since LOG messages go to stderr, we decided to let googletest output go to stdout. This way, you can easily separate the two using redirection. For example: $ ./my_test > gtest_output.txt Why should I prefer test fixtures over global variables? There are several good reasons: It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. Global variables pollute the global namespace. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test suites have something in common. What can the statement argument in ASSERT_DEATH() be? ASSERT_DEATH(*statement*, *regex*) (or any death assertion macro) can be used wherever *statement* is valid. So basically *statement* can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: a simple function call (often the case), a complex expression, or a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST ( MyDeathTest , FunctionCall ) { ASSERT_DEATH ( Xyz ( 5 ), \"Xyz failed\" ); } // Or a complex expression that references variables and functions. TEST ( MyDeathTest , ComplexExpression ) { const bool c = Condition (); ASSERT_DEATH (( c ? Func1 ( 0 ) : object2 . Method ( \"test\" )), \"(Func1|Method) failed\" ); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST ( MyDeathTest , InsideLoop ) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for ( int i = 0 ; i < 5 ; i ++ ) { EXPECT_DEATH_M ( Foo ( i ), \"Foo has \\\\ d+ errors\" , :: testing :: Message () << \"where i is \" << i ); } } // A death assertion can contain a compound statement. TEST ( MyDeathTest , CompoundStatement ) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH ({ for ( int i = 0 ; i < 5 ; i ++ ) { Bar ( i ); } }, \"Bar has \\\\ d+ errors\" ); } gtest-death-test_test.cc contains more examples if you are interested. I have a fixture class FooTest , but TEST_F(FooTest, Bar) gives me error \"no matching function for call to `FooTest::FooTest()'\" . Why? Googletest needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: If you explicitly declare a non-default constructor for class FooTest ( DISALLOW_EVIL_CONSTRUCTORS() does this), then you need to define a default constructor, even if it would be empty. If FooTest has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .) Why does ASSERT_DEATH complain about previous threads that were already joined? With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this. Why does googletest require the entire test suite, instead of individual tests, to be named *DeathTest when it uses ASSERT_DEATH? googletest does not interleave tests from different test suites. That is, it runs all tests in one test suite first, and then runs all tests in the next test suite, and so on. googletest does this because it needs to set up a test suite before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F ( FooTest , AbcDeathTest ) { ... } TEST_F ( FooTest , Uvw ) { ... } TEST_F ( BarTest , DefDeathTest ) { ... } TEST_F ( BarTest , Xyz ) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test suites, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw . But I don't like calling my entire test suite *DeathTest when it contains both death tests and non-death tests. What do I do? You don't have to, but if you like, you may split up the test suite into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public :: testing :: Test { ... }; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } using FooDeathTest = FooTest ; TEST_F ( FooDeathTest , Uvw ) { ... EXPECT_DEATH (...) ... } TEST_F ( FooDeathTest , Xyz ) { ... ASSERT_DEATH (...) ... } googletest prints the LOG messages in a death test's child process only when the test fails. How can I see the LOG messages when the death test succeeds? Printing the LOG messages generated by the statement inside EXPECT_DEATH() makes it harder to search for real problems in the parent's log. Therefore, googletest only prints them when the death test has failed. If you really need to see such LOG messages, a workaround is to temporarily break the death test (e.g. by changing the regex pattern it is expected to match). Admittedly, this is a hack. We'll consider a more permanent solution after the fork-and-exec-style death tests are implemented. The compiler complains about \"no match for 'operator<<'\" when I use an assertion. What gives? If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space. See https://abseil.io/tips/49 for details. How do I suppress the memory leak messages on Windows? Since the statically initialized googletest singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines. How can my code detect if it is running in a test? If you write code that sniffs whether it's running in a test and does different things accordingly, you are leaking test-only logic into production code and there is no easy way to ensure that the test-only code paths aren't run by mistake in production. Such cleverness also leads to Heisenbugs . Therefore we strongly advise against the practice, and googletest doesn't provide a way to do it. In general, the recommended way to cause the code to behave differently under test is Dependency Injection . You can inject different functionality from the test and from the production code. Since your production code doesn't link in the for-test logic at all (the testonly attribute for BUILD targets helps to ensure that), there is no danger in accidentally running it. However, if you really , really , really have no choice, and if you follow the rule of ending your test program names with _test , you can use the horrible hack of sniffing your executable name ( argv[0] in main() ) to know whether the code is under test. How do I temporarily disable a test? If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0, as disabled tests are still compiled (and thus won't rot). To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag. Is it OK if I have two separate TEST(Foo, Bar) test methods defined in different namespaces? Yes. The rule is all test methods in the same test suite must use the same fixture class. This means that the following is allowed because both tests use the same fixture class ( ::testing::Test ). namespace foo { TEST ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo namespace bar { TEST ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace bar However, the following code is not allowed and will produce a runtime error from googletest because the test methods are using different test fixture classes with the same test suite name. namespace foo { class CoolTest : public :: testing :: Test {}; // Fixture foo::CoolTest TEST_F ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo namespace bar { class CoolTest : public :: testing :: Test {}; // Fixture: bar::CoolTest TEST_F ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace bar","title":"Googletest FAQ"},{"location":"examples/gtest/docs/faq.html#googletest-faq","text":"","title":"Googletest FAQ"},{"location":"examples/gtest/docs/faq.html#why-should-test-suite-names-and-test-names-not-contain-underscore","text":"Underscore ( _ ) is special, as C++ reserves the following to be used by the compiler and the standard library: any identifier that starts with an _ followed by an upper-case letter, and any identifier that contains two consecutive underscores (i.e. __ ) anywhere in its name. User code is prohibited from using such identifiers. Now let's look at what this means for TEST and TEST_F . Currently TEST(TestSuiteName, TestName) generates a class named TestSuiteName_TestName_Test . What happens if TestSuiteName or TestName contains _ ? If TestSuiteName starts with an _ followed by an upper-case letter (say, _Foo ), we end up with _Foo_TestName_Test , which is reserved and thus invalid. If TestSuiteName ends with an _ (say, Foo_ ), we get Foo__TestName_Test , which is invalid. If TestName starts with an _ (say, _Bar ), we get TestSuiteName__Bar_Test , which is invalid. If TestName ends with an _ (say, Bar_ ), we get TestSuiteName_Bar__Test , which is invalid. So clearly TestSuiteName and TestName cannot start or end with _ (Actually, TestSuiteName can start with _ -- as long as the _ isn't followed by an upper-case letter. But that's getting complicated. So for simplicity we just say that it cannot start with _ .). It may seem fine for TestSuiteName and TestName to contain _ in the middle. However, consider this: TEST ( Time , Flies_Like_An_Arrow ) { ... } TEST ( Time_Flies , Like_An_Arrow ) { ... } Now, the two TEST s will both generate the same class ( Time_Flies_Like_An_Arrow_Test ). That's not good. So for simplicity, we just ask the users to avoid _ in TestSuiteName and TestName . The rule is more constraining than necessary, but it's simple and easy to remember. It also gives googletest some wiggle room in case its implementation needs to change in the future. If you violate the rule, there may not be immediate consequences, but your test may (just may) break with a new compiler (or a new version of the compiler you are using) or with a new version of googletest. Therefore it's best to follow the rule.","title":"Why should test suite names and test names not contain underscore?"},{"location":"examples/gtest/docs/faq.html#why-does-googletest-support-expect_eqnull-ptr-and-assert_eqnull-ptr-but-not-expect_nenull-ptr-and-assert_nenull-ptr","text":"First of all you can use EXPECT_NE(nullptr, ptr) and ASSERT_NE(nullptr, ptr) . This is the preferred syntax in the style guide because nullptr does not have the type problems that NULL does. Which is why NULL does not work. Due to some peculiarity of C++, it requires some non-trivial template meta programming tricks to support using NULL as an argument of the EXPECT_XX() and ASSERT_XX() macros. Therefore we only do it where it's most needed (otherwise we make the implementation of googletest harder to maintain and more error-prone than necessary). The EXPECT_EQ() macro takes the expected value as its first argument and the actual value as the second. It's reasonable that someone wants to write EXPECT_EQ(NULL, some_expression) , and this indeed was requested several times. Therefore we implemented it. The need for EXPECT_NE(NULL, ptr) isn't nearly as strong. When the assertion fails, you already know that ptr must be NULL , so it doesn't add any information to print ptr in this case. That means EXPECT_TRUE(ptr != NULL) works just as well. If we were to support EXPECT_NE(NULL, ptr) , for consistency we'll have to support EXPECT_NE(ptr, NULL) as well, as unlike EXPECT_EQ , we don't have a convention on the order of the two arguments for EXPECT_NE . This means using the template meta programming tricks twice in the implementation, making it even harder to understand and maintain. We believe the benefit doesn't justify the cost. Finally, with the growth of the gMock matcher library, we are encouraging people to use the unified EXPECT_THAT(value, matcher) syntax more often in tests. One significant advantage of the matcher approach is that matchers can be easily combined to form new matchers, while the EXPECT_NE , etc, macros cannot be easily combined. Therefore we want to invest more in the matchers than in the EXPECT_XX() macros.","title":"Why does googletest support EXPECT_EQ(NULL, ptr) and ASSERT_EQ(NULL, ptr) but not EXPECT_NE(NULL, ptr) and ASSERT_NE(NULL, ptr)?"},{"location":"examples/gtest/docs/faq.html#i-need-to-test-that-different-implementations-of-an-interface-satisfy-some-common-requirements-should-i-use-typed-tests-or-value-parameterized-tests","text":"For testing various implementations of the same interface, either typed tests or value-parameterized tests can get it done. It's really up to you the user to decide which is more convenient for you, depending on your particular case. Some rough guidelines: Typed tests can be easier to write if instances of the different implementations can be created the same way, modulo the type. For example, if all these implementations have a public default constructor (such that you can write new TypeParam ), or if their factory functions have the same form (e.g. CreateInstance<TypeParam>() ). Value-parameterized tests can be easier to write if you need different code patterns to create different implementations' instances, e.g. new Foo vs new Bar(5) . To accommodate for the differences, you can write factory function wrappers and pass these function pointers to the tests as their parameters. When a typed test fails, the default output includes the name of the type, which can help you quickly identify which implementation is wrong. Value-parameterized tests only show the number of the failed iteration by default. You will need to define a function that returns the iteration name and pass it as the third parameter to INSTANTIATE_TEST_SUITE_P to have more useful output. When using typed tests, you need to make sure you are testing against the interface type, not the concrete types (in other words, you want to make sure implicit_cast<MyInterface*>(my_concrete_impl) works, not just that my_concrete_impl works). It's less likely to make mistakes in this area when using value-parameterized tests. I hope I didn't confuse you more. :-) If you don't mind, I'd suggest you to give both approaches a try. Practice is a much better way to grasp the subtle differences between the two tools. Once you have some concrete experience, you can much more easily decide which one to use the next time.","title":"I need to test that different implementations of an interface satisfy some common requirements. Should I use typed tests or value-parameterized tests?"},{"location":"examples/gtest/docs/faq.html#i-got-some-run-time-errors-about-invalid-proto-descriptors-when-using-protocolmessageequals-help","text":"Note: ProtocolMessageEquals and ProtocolMessageEquiv are deprecated now. Please use EqualsProto , etc instead. ProtocolMessageEquals and ProtocolMessageEquiv were redefined recently and are now less tolerant of invalid protocol buffer definitions. In particular, if you have a foo.proto that doesn't fully qualify the type of a protocol message it references (e.g. message<Bar> where it should be message<blah.Bar> ), you will now get run-time errors like: ... descriptor.cc:...] Invalid proto descriptor for file \"path/to/foo.proto\": ... descriptor.cc:...] blah.MyMessage.my_field: \".Bar\" is not defined. If you see this, your .proto file is broken and needs to be fixed by making the types fully qualified. The new definition of ProtocolMessageEquals and ProtocolMessageEquiv just happen to reveal your bug.","title":"I got some run-time errors about invalid proto descriptors when using ProtocolMessageEquals. Help!"},{"location":"examples/gtest/docs/faq.html#my-death-test-modifies-some-state-but-the-change-seems-lost-after-the-death-test-finishes-why","text":"Death tests ( EXPECT_DEATH , etc) are executed in a sub-process s.t. the expected crash won't kill the test program (i.e. the parent process). As a result, any in-memory side effects they incur are observable in their respective sub-processes, but not in the parent process. You can think of them as running in a parallel universe, more or less. In particular, if you use mocking and the death test statement invokes some mock methods, the parent process will think the calls have never occurred. Therefore, you may want to move your EXPECT_CALL statements inside the EXPECT_DEATH macro.","title":"My death test modifies some state, but the change seems lost after the death test finishes. Why?"},{"location":"examples/gtest/docs/faq.html#expect_eqhtonlblah-blah_blah-generates-weird-compiler-errors-in-opt-mode-is-this-a-googletest-bug","text":"Actually, the bug is in htonl() . According to 'man htonl' , htonl() is a function , which means it's valid to use htonl as a function pointer. However, in opt mode htonl() is defined as a macro , which breaks this usage. Worse, the macro definition of htonl() uses a gcc extension and is not standard C++. That hacky implementation has some ad hoc limitations. In particular, it prevents you from writing Foo<sizeof(htonl(x))>() , where Foo is a template that has an integral argument. The implementation of EXPECT_EQ(a, b) uses sizeof(... a ...) inside a template argument, and thus doesn't compile in opt mode when a contains a call to htonl() . It is difficult to make EXPECT_EQ bypass the htonl() bug, as the solution must work with different compilers on various platforms. htonl() has some other problems as described in //util/endian/endian.h , which defines ghtonl() to replace it. ghtonl() does the same thing htonl() does, only without its problems. We suggest you to use ghtonl() instead of htonl() , both in your tests and production code. //util/endian/endian.h also defines ghtons() , which solves similar problems in htons() . Don't forget to add //util/endian to the list of dependencies in the BUILD file wherever ghtonl() and ghtons() are used. The library consists of a single header file and will not bloat your binary.","title":"EXPECT_EQ(htonl(blah), blah_blah) generates weird compiler errors in opt mode. Is this a googletest bug?"},{"location":"examples/gtest/docs/faq.html#the-compiler-complains-about-undefined-references-to-some-static-const-member-variables-but-i-did-define-them-in-the-class-body-whats-wrong","text":"If your class has a static data member: // foo.h class Foo { ... static const int kBar = 100 ; }; You also need to define it outside of the class body in foo.cc : const int Foo :: kBar ; // No initializer here. Otherwise your code is invalid C++ , and may break in unexpected ways. In particular, using it in googletest comparison assertions ( EXPECT_EQ , etc) will generate an \"undefined reference\" linker error. The fact that \"it used to work\" doesn't mean it's valid. It just means that you were lucky. :-)","title":"The compiler complains about \"undefined references\" to some static const member variables, but I did define them in the class body. What's wrong?"},{"location":"examples/gtest/docs/faq.html#can-i-derive-a-test-fixture-from-another","text":"Yes. Each test fixture has a corresponding and same named test suite. This means only one test suite can use a particular fixture. Sometimes, however, multiple test cases may want to use the same or slightly different fixtures. For example, you may want to make sure that all of a GUI library's test suites don't leak important system resources like fonts and brushes. In googletest, you share a fixture among test suites by putting the shared logic in a base test fixture, then deriving from that base a separate fixture for each test suite that wants to use this common logic. You then use TEST_F() to write tests using each derived fixture. Typically, your code looks like this: // Defines a base test fixture. class BaseTest : public :: testing :: Test { protected : ... }; // Derives a fixture FooTest from BaseTest. class FooTest : public BaseTest { protected : void SetUp () override { BaseTest :: SetUp (); // Sets up the base fixture first. ... additional set - up work ... } void TearDown () override { ... clean - up work for FooTest ... BaseTest :: TearDown (); // Remember to tear down the base fixture // after cleaning up FooTest! } ... functions and variables for FooTest ... }; // Tests that use the fixture FooTest. TEST_F ( FooTest , Bar ) { ... } TEST_F ( FooTest , Baz ) { ... } ... additional fixtures derived from BaseTest ... If necessary, you can continue to derive test fixtures from a derived fixture. googletest has no limit on how deep the hierarchy can be. For a complete example using derived test fixtures, see sample5_unittest.cc $HEXAGON_SDK_ROOT/utils/googletest/gtest/samples/sample5_unittest.cc).","title":"Can I derive a test fixture from another?"},{"location":"examples/gtest/docs/faq.html#my-compiler-complains-void-value-not-ignored-as-it-ought-to-be-what-does-this-mean","text":"You're probably using an ASSERT_*() in a function that doesn't return void . ASSERT_*() can only be used in void functions, due to exceptions being disabled by our build system. Please see more details here .","title":"My compiler complains \"void value not ignored as it ought to be.\" What does this mean?"},{"location":"examples/gtest/docs/faq.html#my-death-test-hangs-or-seg-faults-how-do-i-fix-it","text":"In googletest, death tests are run in a child process and the way they work is delicate. To write death tests you really need to understand how they work. Please make sure you have read this . In particular, death tests don't like having multiple threads in the parent process. So the first thing you can try is to eliminate creating threads outside of EXPECT_DEATH() . For example, you may want to use mocks or fake objects instead of real ones in your tests. Sometimes this is impossible as some library you must use may be creating threads before main() is even reached. In this case, you can try to minimize the chance of conflicts by either moving as many activities as possible inside EXPECT_DEATH() (in the extreme case, you want to move everything inside), or leaving as few things as possible in it. Also, you can try to set the death test style to \"threadsafe\" , which is safer but slower, and see if it helps. If you go with thread-safe death tests, remember that they rerun the test program from the beginning in the child process. Therefore make sure your program can run side-by-side with itself and is deterministic. In the end, this boils down to good concurrent programming. You have to make sure that there is no race conditions or dead locks in your program. No silver bullet - sorry!","title":"My death test hangs (or seg-faults). How do I fix it?"},{"location":"examples/gtest/docs/faq.html#CtorVsSetUp","text":"The first thing to remember is that googletest does not reuse the same test fixture object across multiple tests. For each TEST_F , googletest will create a fresh test fixture object, immediately call SetUp() , run the test body, call TearDown() , and then delete the test fixture object. When you need to write per-test set-up and tear-down logic, you have the choice between using the test fixture constructor/destructor or SetUp()/TearDown() . The former is usually preferred, as it has the following benefits: By initializing a member variable in the constructor, we have the option to make it const , which helps prevent accidental changes to its value and makes the tests more obviously correct. In case we need to subclass the test fixture class, the subclass' constructor is guaranteed to call the base class' constructor first , and the subclass' destructor is guaranteed to call the base class' destructor afterward . With SetUp()/TearDown() , a subclass may make the mistake of forgetting to call the base class' SetUp()/TearDown() or call them at the wrong time. You may still want to use SetUp()/TearDown() in the following cases: C++ does not allow virtual function calls in constructors and destructors. You can call a method declared as virtual, but it will not use dynamic dispatch, it will use the definition from the class the constructor of which is currently executing. This is because calling a virtual method before the derived class constructor has a chance to run is very dangerous - the virtual method might operate on uninitialized data. Therefore, if you need to call a method that will be overridden in a derived class, you have to use SetUp()/TearDown() . In the body of a constructor (or destructor), it's not possible to use the ASSERT_xx macros. Therefore, if the set-up operation could cause a fatal test failure that should prevent the test from running, it's necessary to use abort and abort the whole test executable, or to use SetUp() instead of a constructor. If the tear-down operation could throw an exception, you must use TearDown() as opposed to the destructor, as throwing in a destructor leads to undefined behavior and usually will kill your program right away. Note that many standard libraries (like STL) may throw when exceptions are enabled in the compiler. Therefore you should prefer TearDown() if you want to write portable tests that work with or without exceptions. The googletest team is considering making the assertion macros throw on platforms where exceptions are enabled (e.g. Windows, Mac OS, and Linux client-side), which will eliminate the need for the user to propagate failures from a subroutine to its caller. Therefore, you shouldn't use googletest assertions in a destructor if your code could run on such a platform.","title":"Should I use the constructor/destructor of the test fixture or SetUp()/TearDown()?"},{"location":"examples/gtest/docs/faq.html#the-compiler-complains-no-matching-function-to-call-when-i-use-assert_pred-how-do-i-fix-it","text":"If the predicate function you use in ASSERT_PRED* or EXPECT_PRED* is overloaded or a template, the compiler will have trouble figuring out which overloaded version it should use. ASSERT_PRED_FORMAT* and EXPECT_PRED_FORMAT* don't have this problem. If you see this error, you might want to switch to (ASSERT|EXPECT)_PRED_FORMAT* , which will also give you a better failure message. If, however, that is not an option, you can resolve the problem by explicitly telling the compiler which version to pick. For example, suppose you have bool IsPositive ( int n ) { return n > 0 ; } bool IsPositive ( double x ) { return x > 0 ; } you will get a compiler error if you write EXPECT_PRED1 ( IsPositive , 5 ); However, this will work: EXPECT_PRED1 ( static_cast < bool ( * )( int ) > ( IsPositive ), 5 ); (The stuff inside the angled brackets for the static_cast operator is the type of the function pointer for the int -version of IsPositive() .) As another example, when you have a template function template < typename T > bool IsNegative ( T x ) { return x < 0 ; } you can use it in a predicate assertion like this: ASSERT_PRED1 ( IsNegative < int > , -5 ); Things are more interesting if your template has more than one parameters. The following won't compile: ASSERT_PRED2 ( GreaterThan < int , int > , 5 , 0 ); as the C++ pre-processor thinks you are giving ASSERT_PRED2 4 arguments, which is one more than expected. The workaround is to wrap the predicate function in parentheses: ASSERT_PRED2 (( GreaterThan < int , int > ), 5 , 0 );","title":"The compiler complains \"no matching function to call\" when I use ASSERT_PRED*. How do I fix it?"},{"location":"examples/gtest/docs/faq.html#my-compiler-complains-about-ignoring-return-value-when-i-call-run_all_tests-why","text":"Some people had been ignoring the return value of RUN_ALL_TESTS() . That is, instead of return RUN_ALL_TESTS (); they write RUN_ALL_TESTS (); This is wrong and dangerous . The testing services needs to see the return value of RUN_ALL_TESTS() in order to determine if a test has passed. If your main() function ignores it, your test will be considered successful even if it has a googletest assertion failure. Very bad. We have decided to fix this (thanks to Michael Chastain for the idea). Now, your code will no longer be able to ignore RUN_ALL_TESTS() when compiled with gcc . If you do so, you'll get a compiler error. If you see the compiler complaining about you ignoring the return value of RUN_ALL_TESTS() , the fix is simple: just make sure its value is used as the return value of main() . But how could we introduce a change that breaks existing tests? Well, in this case, the code was already broken in the first place, so we didn't break it. :-)","title":"My compiler complains about \"ignoring return value\" when I call RUN_ALL_TESTS(). Why?"},{"location":"examples/gtest/docs/faq.html#my-compiler-complains-that-a-constructor-or-destructor-cannot-return-a-value-whats-going-on","text":"Due to a peculiarity of C++, in order to support the syntax for streaming messages to an ASSERT_* , e.g. ASSERT_EQ ( 1 , Foo ()) << \"blah blah\" << foo ; we had to give up using ASSERT* and FAIL* (but not EXPECT* and ADD_FAILURE* ) in constructors and destructors. The workaround is to move the content of your constructor/destructor to a private void member function, or switch to EXPECT_*() if that works. This section in the user's guide explains it.","title":"My compiler complains that a constructor (or destructor) cannot return a value. What's going on?"},{"location":"examples/gtest/docs/faq.html#my-setup-function-is-not-called-why","text":"C++ is case-sensitive. Did you spell it as Setup() ? Similarly, sometimes people spell SetUpTestSuite() as SetupTestSuite() and wonder why it's never called.","title":"My SetUp() function is not called. Why?"},{"location":"examples/gtest/docs/faq.html#i-have-several-test-suites-which-share-the-same-test-fixture-logic-do-i-have-to-define-a-new-test-fixture-class-for-each-of-them-this-seems-pretty-tedious","text":"You don't have to. Instead of class FooTest : public BaseTest {}; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } class BarTest : public BaseTest {}; TEST_F ( BarTest , Abc ) { ... } TEST_F ( BarTest , Def ) { ... } you can simply typedef the test fixtures: typedef BaseTest FooTest ; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } typedef BaseTest BarTest ; TEST_F ( BarTest , Abc ) { ... } TEST_F ( BarTest , Def ) { ... }","title":"I have several test suites which share the same test fixture logic, do I have to define a new test fixture class for each of them? This seems pretty tedious."},{"location":"examples/gtest/docs/faq.html#googletest-output-is-buried-in-a-whole-bunch-of-log-messages-what-do-i-do","text":"The googletest output is meant to be a concise and human-friendly report. If your test generates textual output itself, it will mix with the googletest output, making it hard to read. However, there is an easy solution to this problem. Since LOG messages go to stderr, we decided to let googletest output go to stdout. This way, you can easily separate the two using redirection. For example: $ ./my_test > gtest_output.txt","title":"googletest output is buried in a whole bunch of LOG messages. What do I do?"},{"location":"examples/gtest/docs/faq.html#why-should-i-prefer-test-fixtures-over-global-variables","text":"There are several good reasons: It's likely your test needs to change the states of its global variables. This makes it difficult to keep side effects from escaping one test and contaminating others, making debugging difficult. By using fixtures, each test has a fresh set of variables that's different (but with the same names). Thus, tests are kept independent of each other. Global variables pollute the global namespace. Test fixtures can be reused via subclassing, which cannot be done easily with global variables. This is useful if many test suites have something in common.","title":"Why should I prefer test fixtures over global variables?"},{"location":"examples/gtest/docs/faq.html#what-can-the-statement-argument-in-assert_death-be","text":"ASSERT_DEATH(*statement*, *regex*) (or any death assertion macro) can be used wherever *statement* is valid. So basically *statement* can be any C++ statement that makes sense in the current context. In particular, it can reference global and/or local variables, and can be: a simple function call (often the case), a complex expression, or a compound statement. Some examples are shown here: // A death test can be a simple function call. TEST ( MyDeathTest , FunctionCall ) { ASSERT_DEATH ( Xyz ( 5 ), \"Xyz failed\" ); } // Or a complex expression that references variables and functions. TEST ( MyDeathTest , ComplexExpression ) { const bool c = Condition (); ASSERT_DEATH (( c ? Func1 ( 0 ) : object2 . Method ( \"test\" )), \"(Func1|Method) failed\" ); } // Death assertions can be used any where in a function. In // particular, they can be inside a loop. TEST ( MyDeathTest , InsideLoop ) { // Verifies that Foo(0), Foo(1), ..., and Foo(4) all die. for ( int i = 0 ; i < 5 ; i ++ ) { EXPECT_DEATH_M ( Foo ( i ), \"Foo has \\\\ d+ errors\" , :: testing :: Message () << \"where i is \" << i ); } } // A death assertion can contain a compound statement. TEST ( MyDeathTest , CompoundStatement ) { // Verifies that at lease one of Bar(0), Bar(1), ..., and // Bar(4) dies. ASSERT_DEATH ({ for ( int i = 0 ; i < 5 ; i ++ ) { Bar ( i ); } }, \"Bar has \\\\ d+ errors\" ); } gtest-death-test_test.cc contains more examples if you are interested.","title":"What can the statement argument in ASSERT_DEATH() be?"},{"location":"examples/gtest/docs/faq.html#i-have-a-fixture-class-footest-but-test_ffootest-bar-gives-me-error-no-matching-function-for-call-to-footestfootest-why","text":"Googletest needs to be able to create objects of your test fixture class, so it must have a default constructor. Normally the compiler will define one for you. However, there are cases where you have to define your own: If you explicitly declare a non-default constructor for class FooTest ( DISALLOW_EVIL_CONSTRUCTORS() does this), then you need to define a default constructor, even if it would be empty. If FooTest has a const non-static data member, then you have to define the default constructor and initialize the const member in the initializer list of the constructor. (Early versions of gcc doesn't force you to initialize the const member. It's a bug that has been fixed in gcc 4 .)","title":"I have a fixture class FooTest, but TEST_F(FooTest, Bar) gives me error \"no matching function for call to `FooTest::FooTest()'\". Why?"},{"location":"examples/gtest/docs/faq.html#why-does-assert_death-complain-about-previous-threads-that-were-already-joined","text":"With the Linux pthread library, there is no turning back once you cross the line from single thread to multiple threads. The first time you create a thread, a manager thread is created in addition, so you get 3, not 2, threads. Later when the thread you create joins the main thread, the thread count decrements by 1, but the manager thread will never be killed, so you still have 2 threads, which means you cannot safely run a death test. The new NPTL thread library doesn't suffer from this problem, as it doesn't create a manager thread. However, if you don't control which machine your test runs on, you shouldn't depend on this.","title":"Why does ASSERT_DEATH complain about previous threads that were already joined?"},{"location":"examples/gtest/docs/faq.html#why-does-googletest-require-the-entire-test-suite-instead-of-individual-tests-to-be-named-deathtest-when-it-uses-assert_death","text":"googletest does not interleave tests from different test suites. That is, it runs all tests in one test suite first, and then runs all tests in the next test suite, and so on. googletest does this because it needs to set up a test suite before the first test in it is run, and tear it down afterwords. Splitting up the test case would require multiple set-up and tear-down processes, which is inefficient and makes the semantics unclean. If we were to determine the order of tests based on test name instead of test case name, then we would have a problem with the following situation: TEST_F ( FooTest , AbcDeathTest ) { ... } TEST_F ( FooTest , Uvw ) { ... } TEST_F ( BarTest , DefDeathTest ) { ... } TEST_F ( BarTest , Xyz ) { ... } Since FooTest.AbcDeathTest needs to run before BarTest.Xyz , and we don't interleave tests from different test suites, we need to run all tests in the FooTest case before running any test in the BarTest case. This contradicts with the requirement to run BarTest.DefDeathTest before FooTest.Uvw .","title":"Why does googletest require the entire test suite, instead of individual tests, to be named *DeathTest when it uses ASSERT_DEATH?"},{"location":"examples/gtest/docs/faq.html#but-i-dont-like-calling-my-entire-test-suite-deathtest-when-it-contains-both-death-tests-and-non-death-tests-what-do-i-do","text":"You don't have to, but if you like, you may split up the test suite into FooTest and FooDeathTest , where the names make it clear that they are related: class FooTest : public :: testing :: Test { ... }; TEST_F ( FooTest , Abc ) { ... } TEST_F ( FooTest , Def ) { ... } using FooDeathTest = FooTest ; TEST_F ( FooDeathTest , Uvw ) { ... EXPECT_DEATH (...) ... } TEST_F ( FooDeathTest , Xyz ) { ... ASSERT_DEATH (...) ... }","title":"But I don't like calling my entire test suite *DeathTest when it contains both death tests and non-death tests. What do I do?"},{"location":"examples/gtest/docs/faq.html#googletest-prints-the-log-messages-in-a-death-tests-child-process-only-when-the-test-fails-how-can-i-see-the-log-messages-when-the-death-test-succeeds","text":"Printing the LOG messages generated by the statement inside EXPECT_DEATH() makes it harder to search for real problems in the parent's log. Therefore, googletest only prints them when the death test has failed. If you really need to see such LOG messages, a workaround is to temporarily break the death test (e.g. by changing the regex pattern it is expected to match). Admittedly, this is a hack. We'll consider a more permanent solution after the fork-and-exec-style death tests are implemented.","title":"googletest prints the LOG messages in a death test's child process only when the test fails. How can I see the LOG messages when the death test succeeds?"},{"location":"examples/gtest/docs/faq.html#the-compiler-complains-about-no-match-for-operator-when-i-use-an-assertion-what-gives","text":"If you use a user-defined type FooType in an assertion, you must make sure there is an std::ostream& operator<<(std::ostream&, const FooType&) function defined such that we can print a value of FooType . In addition, if FooType is declared in a name space, the << operator also needs to be defined in the same name space. See https://abseil.io/tips/49 for details.","title":"The compiler complains about \"no match for 'operator&lt;&lt;'\" when I use an assertion. What gives?"},{"location":"examples/gtest/docs/faq.html#how-do-i-suppress-the-memory-leak-messages-on-windows","text":"Since the statically initialized googletest singleton requires allocations on the heap, the Visual C++ memory leak detector will report memory leaks at the end of the program run. The easiest way to avoid this is to use the _CrtMemCheckpoint and _CrtMemDumpAllObjectsSince calls to not report any statically initialized heap objects. See MSDN for more details and additional heap check/debug routines.","title":"How do I suppress the memory leak messages on Windows?"},{"location":"examples/gtest/docs/faq.html#how-can-my-code-detect-if-it-is-running-in-a-test","text":"If you write code that sniffs whether it's running in a test and does different things accordingly, you are leaking test-only logic into production code and there is no easy way to ensure that the test-only code paths aren't run by mistake in production. Such cleverness also leads to Heisenbugs . Therefore we strongly advise against the practice, and googletest doesn't provide a way to do it. In general, the recommended way to cause the code to behave differently under test is Dependency Injection . You can inject different functionality from the test and from the production code. Since your production code doesn't link in the for-test logic at all (the testonly attribute for BUILD targets helps to ensure that), there is no danger in accidentally running it. However, if you really , really , really have no choice, and if you follow the rule of ending your test program names with _test , you can use the horrible hack of sniffing your executable name ( argv[0] in main() ) to know whether the code is under test.","title":"How can my code detect if it is running in a test?"},{"location":"examples/gtest/docs/faq.html#how-do-i-temporarily-disable-a-test","text":"If you have a broken test that you cannot fix right away, you can add the DISABLED_ prefix to its name. This will exclude it from execution. This is better than commenting out the code or using #if 0, as disabled tests are still compiled (and thus won't rot). To include disabled tests in test execution, just invoke the test program with the --gtest_also_run_disabled_tests flag.","title":"How do I temporarily disable a test?"},{"location":"examples/gtest/docs/faq.html#is-it-ok-if-i-have-two-separate-testfoo-bar-test-methods-defined-in-different-namespaces","text":"Yes. The rule is all test methods in the same test suite must use the same fixture class. This means that the following is allowed because both tests use the same fixture class ( ::testing::Test ). namespace foo { TEST ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo namespace bar { TEST ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace bar However, the following code is not allowed and will produce a runtime error from googletest because the test methods are using different test fixture classes with the same test suite name. namespace foo { class CoolTest : public :: testing :: Test {}; // Fixture foo::CoolTest TEST_F ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace foo namespace bar { class CoolTest : public :: testing :: Test {}; // Fixture: bar::CoolTest TEST_F ( CoolTest , DoSomething ) { SUCCEED (); } } // namespace bar","title":"Is it OK if I have two separate TEST(Foo, Bar) test methods defined in different namespaces?"},{"location":"examples/gtest/docs/for_dummies.html","text":"gMock for Dummies What Is gMock? When you write a prototype or test, often it's not feasible or wise to rely on real objects entirely. A mock object implements the same interface as a real object (so it can be used as one), but lets you specify at run time how it will be used and what it should do (which methods will be called? in which order? how many times? with what arguments? what will they return? etc). Note: It is easy to confuse the term fake objects with mock objects. Fakes and mocks actually mean very different things in the Test-Driven Development (TDD) community: Fake objects have working implementations, but usually take some shortcut (perhaps to make the operations less expensive), which makes them not suitable for production. An in-memory file system would be an example of a fake. Mocks are objects pre-programmed with expectations , which form a specification of the calls they are expected to receive. If all this seems too abstract for you, don't worry - the most important thing to remember is that a mock allows you to check the interaction between itself and code that uses it. The difference between fakes and mocks shall become much clearer once you start to use mocks. gMock is a library (sometimes we also call it a \"framework\" to make it sound cool) for creating mock classes and using them. It does to C++ what jMock/EasyMock does to Java (well, more or less). When using gMock, first, you use some simple macros to describe the interface you want to mock, and they will expand to the implementation of your mock class; next, you create some mock objects and specify its expectations and behavior using an intuitive syntax; then you exercise code that uses the mock objects. gMock will catch any violation to the expectations as soon as it arises. Why gMock? While mock objects help you remove unnecessary dependencies in tests and make them fast and reliable, using mocks manually in C++ is hard : Someone has to implement the mocks. The job is usually tedious and error-prone. No wonder people go great distance to avoid it. The quality of those manually written mocks is a bit, uh, unpredictable. You may see some really polished ones, but you may also see some that were hacked up in a hurry and have all sorts of ad hoc restrictions. The knowledge you gained from using one mock doesn't transfer to the next one. In contrast, Java and Python programmers have some fine mock frameworks (jMock, EasyMock, Mox , etc), which automate the creation of mocks. As a result, mocking is a proven effective technique and widely adopted practice in those communities. Having the right tool absolutely makes the difference. gMock was built to help C++ programmers. It was inspired by jMock and EasyMock, but designed with C++'s specifics in mind. It is your friend if any of the following problems is bothering you: You are stuck with a sub-optimal design and wish you had done more prototyping before it was too late, but prototyping in C++ is by no means \"rapid\". Your tests are slow as they depend on too many libraries or use expensive resources (e.g. a database). Your tests are brittle as some resources they use are unreliable (e.g. the network). You want to test how your code handles a failure (e.g. a file checksum error), but it's not easy to cause one. You need to make sure that your module interacts with other modules in the right way, but it's hard to observe the interaction; therefore you resort to observing the side effects at the end of the action, but it's awkward at best. You want to \"mock out\" your dependencies, except that they don't have mock implementations yet; and, frankly, you aren't thrilled by some of those hand-written mocks. We encourage you to use gMock as a design tool, for it lets you experiment with your interface design early and often. More iterations lead to better designs! a testing tool to cut your tests' outbound dependencies and probe the interaction between your module and its collaborators. Getting Started gMock is bundled with googletest. A Case for Mock Turtles Let's look at an example. Suppose you are developing a graphics program that relies on a LOGO -like API for drawing. How would you test that it does the right thing? Well, you can run it and compare the screen with a golden screen snapshot, but let's admit it: tests like this are expensive to run and fragile (What if you just upgraded to a shiny new graphics card that has better anti-aliasing? Suddenly you have to update all your golden images.). It would be too painful if all your tests are like this. Fortunately, you learned about Dependency Injection and know the right thing to do: instead of having your application talk to the system API directly, wrap the API in an interface (say, Turtle ) and code to that interface: class Turtle { ... virtual ~ Turtle () {}; virtual void PenUp () = 0 ; virtual void PenDown () = 0 ; virtual void Forward ( int distance ) = 0 ; virtual void Turn ( int degrees ) = 0 ; virtual void GoTo ( int x , int y ) = 0 ; virtual int GetX () const = 0 ; virtual int GetY () const = 0 ; }; (Note that the destructor of Turtle must be virtual, as is the case for all classes you intend to inherit from - otherwise the destructor of the derived class will not be called when you delete an object through a base pointer, and you'll get corrupted program states like memory leaks.) You can control whether the turtle's movement will leave a trace using PenUp() and PenDown() , and control its movement using Forward() , Turn() , and GoTo() . Finally, GetX() and GetY() tell you the current position of the turtle. Your program will normally use a real implementation of this interface. In tests, you can use a mock implementation instead. This allows you to easily check what drawing primitives your program is calling, with what arguments, and in which order. Tests written this way are much more robust (they won't break because your new machine does anti-aliasing differently), easier to read and maintain (the intent of a test is expressed in the code, not in some binary images), and run much, much faster . Writing the Mock Class If you are lucky, the mocks you need to use have already been implemented by some nice people. If, however, you find yourself in the position to write a mock class, relax - gMock turns this task into a fun game! (Well, almost.) How to Define It Using the Turtle interface as example, here are the simple steps you need to follow: Derive a class MockTurtle from Turtle . Take a virtual function of Turtle (while it's possible to mock non-virtual methods using templates , it's much more involved). In the public: section of the child class, write MOCK_METHOD(); Now comes the fun part: you take the function signature, cut-and-paste it into the macro, and add two commas - one between the return type and the name, another between the name and the argument list. If you're mocking a const method, add a 4th parameter containing (const) (the parentheses are required). Since you're overriding a virtual method, we suggest adding the override keyword. For const methods the 4th parameter becomes (const, override) , for non-const methods just (override) . This isn't mandatory. Repeat until all virtual functions you want to mock are done. (It goes without saying that all pure virtual methods in your abstract class must be either mocked or overridden.) After the process, you should have something like: #include \"gmock/gmock.h\" // Brings in gMock. class MockTurtle : public Turtle { public : ... MOCK_METHOD ( void , PenUp , (), ( override )); MOCK_METHOD ( void , PenDown , (), ( override )); MOCK_METHOD ( void , Forward , ( int distance ), ( override )); MOCK_METHOD ( void , Turn , ( int degrees ), ( override )); MOCK_METHOD ( void , GoTo , ( int x , int y ), ( override )); MOCK_METHOD ( int , GetX , (), ( const , override )); MOCK_METHOD ( int , GetY , (), ( const , override )); }; You don't need to define these mock methods somewhere else - the MOCK_METHOD macro will generate the definitions for you. It's that simple! Where to Put It When you define a mock class, you need to decide where to put its definition. Some people put it in a _test.cc . This is fine when the interface being mocked (say, Foo ) is owned by the same person or team. Otherwise, when the owner of Foo changes it, your test could break. (You can't really expect Foo 's maintainer to fix every test that uses Foo , can you?) So, the rule of thumb is: if you need to mock Foo and it's owned by others, define the mock class in Foo 's package (better, in a testing sub-package such that you can clearly separate production code and testing utilities), put it in a .h and a cc_library . Then everyone can reference them from their tests. If Foo ever changes, there is only one copy of MockFoo to change, and only tests that depend on the changed methods need to be fixed. Another way to do it: you can introduce a thin layer FooAdaptor on top of Foo and code to this new interface. Since you own FooAdaptor , you can absorb changes in Foo much more easily. While this is more work initially, carefully choosing the adaptor interface can make your code easier to write and more readable (a net win in the long run), as you can choose FooAdaptor to fit your specific domain much better than Foo does. Using Mocks in Tests Once you have a mock class, using it is easy. The typical work flow is: Import the gMock names from the testing namespace such that you can use them unqualified (You only have to do it once per file. Remember that namespaces are a good idea. Create some mock objects. Specify your expectations on them (How many times will a method be called? With what arguments? What should it do? etc.). Exercise some code that uses the mocks; optionally, check the result using googletest assertions. If a mock method is called more than expected or with wrong arguments, you'll get an error immediately. When a mock is destructed, gMock will automatically check whether all expectations on it have been satisfied. Here's an example: #include \"path/to/mock-turtle.h\" #include \"gmock/gmock.h\" #include \"gtest/gtest.h\" using :: testing :: AtLeast ; // #1 TEST ( PainterTest , CanDrawSomething ) { MockTurtle turtle ; // #2 EXPECT_CALL ( turtle , PenDown ()) // #3 . Times ( AtLeast ( 1 )); Painter painter ( & turtle ); // #4 EXPECT_TRUE ( painter . DrawCircle ( 0 , 0 , 10 )); // #5 } As you might have guessed, this test checks that PenDown() is called at least once. If the painter object didn't call this method, your test will fail with a message like this: path/to/my_test.cc:119: Failure Actual function call count doesn't match this expectation: Actually: never called; Expected: called at least once. Stack trace: ... Tip 1: If you run the test from an Emacs buffer, you can hit on the line number to jump right to the failed expectation. Tip 2: If your mock objects are never deleted, the final verification won't happen. Therefore it's a good idea to turn on the heap checker in your tests when you allocate mocks on the heap. You get that automatically if you use the gtest_main library already. Important note: gMock requires expectations to be set before the mock functions are called, otherwise the behavior is undefined . In particular, you mustn't interleave EXPECT_CALL()s and calls to the mock functions. This means EXPECT_CALL() should be read as expecting that a call will occur in the future , not that a call has occurred. Why does gMock work like that? Well, specifying the expectation beforehand allows gMock to report a violation as soon as it rises, when the context (stack trace, etc) is still available. This makes debugging much easier. Admittedly, this test is contrived and doesn't do much. You can easily achieve the same effect without using gMock. However, as we shall reveal soon, gMock allows you to do so much more with the mocks. Setting Expectations The key to using a mock object successfully is to set the right expectations on it. If you set the expectations too strict, your test will fail as the result of unrelated changes. If you set them too loose, bugs can slip through. You want to do it just right such that your test can catch exactly the kind of bugs you intend it to catch. gMock provides the necessary means for you to do it \"just right.\" General Syntax In gMock we use the EXPECT_CALL() macro to set an expectation on a mock method. The general syntax is: EXPECT_CALL ( mock_object , method ( matchers )) . Times ( cardinality ) . WillOnce ( action ) . WillRepeatedly ( action ); The macro has two arguments: first the mock object, and then the method and its arguments. Note that the two are separated by a comma ( , ), not a period ( . ). (Why using a comma? The answer is that it was necessary for technical reasons.) If the method is not overloaded, the macro can also be called without matchers: EXPECT_CALL ( mock_object , non - overloaded - method ) . Times ( cardinality ) . WillOnce ( action ) . WillRepeatedly ( action ); This syntax allows the test writer to specify \"called with any arguments\" without explicitly specifying the number or types of arguments. To avoid unintended ambiguity, this syntax may only be used for methods which are not overloaded Either form of the macro can be followed by some optional clauses that provide more information about the expectation. We'll discuss how each clause works in the coming sections. This syntax is designed to make an expectation read like English. For example, you can probably guess that using :: testing :: Return ; ... EXPECT_CALL ( turtle , GetX ()) . Times ( 5 ) . WillOnce ( Return ( 100 )) . WillOnce ( Return ( 150 )) . WillRepeatedly ( Return ( 200 )); says that the turtle object's GetX() method will be called five times, it will return 100 the first time, 150 the second time, and then 200 every time. Some people like to call this style of syntax a Domain-Specific Language (DSL). Note: Why do we use a macro to do this? Well it serves two purposes: first it makes expectations easily identifiable (either by gsearch or by a human reader), and second it allows gMock to include the source file location of a failed expectation in messages, making debugging easier. Matchers: What Arguments Do We Expect? When a mock function takes arguments, we may specify what arguments we are expecting, for example: // Expects the turtle to move forward by 100 units. EXPECT_CALL ( turtle , Forward ( 100 )); Oftentimes you do not want to be too specific. Remember that talk about tests being too rigid? Over specification leads to brittle tests and obscures the intent of tests. Therefore we encourage you to specify only what's necessary\u2014no more, no less. If you aren't interested in the value of an argument, write _ as the argument, which means \"anything goes\": using :: testing :: _ ; ... // Expects that the turtle jumps to somewhere on the x=50 line. EXPECT_CALL ( turtle , GoTo ( 50 , _ )); _ is an instance of what we call matchers . A matcher is like a predicate and can test whether an argument is what we'd expect. You can use a matcher inside EXPECT_CALL() wherever a function argument is expected. _ is a convenient way of saying \"any value\". In the above examples, 100 and 50 are also matchers; implicitly, they are the same as Eq(100) and Eq(50) , which specify that the argument must be equal (using operator== ) to the matcher argument. There are many built-in matchers for common types (as well as custom matchers ); for example: using :: testing :: Ge ; ... // Expects the turtle moves forward by at least 100. EXPECT_CALL ( turtle , Forward ( Ge ( 100 ))); If you don't care about any arguments, rather than specify _ for each of them you may instead omit the parameter list: // Expects the turtle to move forward. EXPECT_CALL ( turtle , Forward ); // Expects the turtle to jump somewhere. EXPECT_CALL ( turtle , GoTo ); This works for all non-overloaded methods; if a method is overloaded, you need to help gMock resolve which overload is expected by specifying the number of arguments and possibly also the types of the arguments . Cardinalities: How Many Times Will It Be Called? The first clause we can specify following an EXPECT_CALL() is Times() . We call its argument a cardinality as it tells how many times the call should occur. It allows us to repeat an expectation many times without actually writing it as many times. More importantly, a cardinality can be \"fuzzy\", just like a matcher can be. This allows a user to express the intent of a test exactly. An interesting special case is when we say Times(0) . You may have guessed - it means that the function shouldn't be called with the given arguments at all, and gMock will report a googletest failure whenever the function is (wrongfully) called. We've seen AtLeast(n) as an example of fuzzy cardinalities earlier. For the list of built-in cardinalities you can use, see here . The Times() clause can be omitted. If you omit Times() , gMock will infer the cardinality for you. The rules are easy to remember: If neither WillOnce() nor WillRepeatedly() is in the EXPECT_CALL() , the inferred cardinality is Times(1) . If there are n WillOnce() 's but no WillRepeatedly() , where n >= 1, the cardinality is Times(n) . If there are n WillOnce() 's and one WillRepeatedly() , where n >= 0, the cardinality is Times(AtLeast(n)) . Quick quiz: what do you think will happen if a function is expected to be called twice but actually called four times? Actions: What Should It Do? Remember that a mock object doesn't really have a working implementation? We as users have to tell it what to do when a method is invoked. This is easy in gMock. First, if the return type of a mock function is a built-in type or a pointer, the function has a default action (a void function will just return, a bool function will return false , and other functions will return 0). In addition, in C++ 11 and above, a mock function whose return type is default-constructible (i.e. has a default constructor) has a default action of returning a default-constructed value. If you don't say anything, this behavior will be used. Second, if a mock function doesn't have a default action, or the default action doesn't suit you, you can specify the action to be taken each time the expectation matches using a series of WillOnce() clauses followed by an optional WillRepeatedly() . For example, using :: testing :: Return ; ... EXPECT_CALL ( turtle , GetX ()) . WillOnce ( Return ( 100 )) . WillOnce ( Return ( 200 )) . WillOnce ( Return ( 300 )); says that turtle.GetX() will be called exactly three times (gMock inferred this from how many WillOnce() clauses we've written, since we didn't explicitly write Times() ), and will return 100, 200, and 300 respectively. using :: testing :: Return ; ... EXPECT_CALL ( turtle , GetY ()) . WillOnce ( Return ( 100 )) . WillOnce ( Return ( 200 )) . WillRepeatedly ( Return ( 300 )); says that turtle.GetY() will be called at least twice (gMock knows this as we've written two WillOnce() clauses and a WillRepeatedly() while having no explicit Times() ), will return 100 and 200 respectively the first two times, and 300 from the third time on. Of course, if you explicitly write a Times() , gMock will not try to infer the cardinality itself. What if the number you specified is larger than there are WillOnce() clauses? Well, after all WillOnce() s are used up, gMock will do the default action for the function every time (unless, of course, you have a WillRepeatedly() .). What can we do inside WillOnce() besides Return() ? You can return a reference using ReturnRef(*variable*) , or invoke a pre-defined function, among others . Important note: The EXPECT_CALL() statement evaluates the action clause only once, even though the action may be performed many times. Therefore you must be careful about side effects. The following may not do what you want: using :: testing :: Return ; ... int n = 100 ; EXPECT_CALL ( turtle , GetX ()) . Times ( 4 ) . WillRepeatedly ( Return ( n ++ )); Instead of returning 100, 101, 102, ..., consecutively, this mock function will always return 100 as n++ is only evaluated once. Similarly, Return(new Foo) will create a new Foo object when the EXPECT_CALL() is executed, and will return the same pointer every time. If you want the side effect to happen every time, you need to define a custom action, which we'll teach in the cook book . Time for another quiz! What do you think the following means? using :: testing :: Return ; ... EXPECT_CALL ( turtle , GetY ()) . Times ( 4 ) . WillOnce ( Return ( 100 )); Obviously turtle.GetY() is expected to be called four times. But if you think it will return 100 every time, think twice! Remember that one WillOnce() clause will be consumed each time the function is invoked and the default action will be taken afterwards. So the right answer is that turtle.GetY() will return 100 the first time, but return 0 from the second time on , as returning 0 is the default action for int functions. Using Multiple Expectations So far we've only shown examples where you have a single expectation. More realistically, you'll specify expectations on multiple mock methods which may be from multiple mock objects. By default, when a mock method is invoked, gMock will search the expectations in the reverse order they are defined, and stop when an active expectation that matches the arguments is found (you can think of it as \"newer rules override older ones.\"). If the matching expectation cannot take any more calls, you will get an upper-bound-violated failure. Here's an example: using :: testing :: _ ; ... EXPECT_CALL ( turtle , Forward ( _ )); // #1 EXPECT_CALL ( turtle , Forward ( 10 )) // #2 . Times ( 2 ); If Forward(10) is called three times in a row, the third time it will be an error, as the last matching expectation (#2) has been saturated. If, however, the third Forward(10) call is replaced by Forward(20) , then it would be OK, as now #1 will be the matching expectation. Note: Why does gMock search for a match in the reverse order of the expectations? The reason is that this allows a user to set up the default expectations in a mock object's constructor or the test fixture's set-up phase and then customize the mock by writing more specific expectations in the test body. So, if you have two expectations on the same method, you want to put the one with more specific matchers after the other, or the more specific rule would be shadowed by the more general one that comes after it. Tip: It is very common to start with a catch-all expectation for a method and Times(AnyNumber()) (omitting arguments, or with _ for all arguments, if overloaded). This makes any calls to the method expected. This is not necessary for methods that are not mentioned at all (these are \"uninteresting\"), but is useful for methods that have some expectations, but for which other calls are ok. See Understanding Uninteresting vs Unexpected Calls . Ordered vs Unordered Calls By default, an expectation can match a call even though an earlier expectation hasn't been satisfied. In other words, the calls don't have to occur in the order the expectations are specified. Sometimes, you may want all the expected calls to occur in a strict order. To say this in gMock is easy: using :: testing :: InSequence ; ... TEST ( FooTest , DrawsLineSegment ) { ... { InSequence seq ; EXPECT_CALL ( turtle , PenDown ()); EXPECT_CALL ( turtle , Forward ( 100 )); EXPECT_CALL ( turtle , PenUp ()); } Foo (); } By creating an object of type InSequence , all expectations in its scope are put into a sequence and have to occur sequentially . Since we are just relying on the constructor and destructor of this object to do the actual work, its name is really irrelevant. In this example, we test that Foo() calls the three expected functions in the order as written. If a call is made out-of-order, it will be an error. (What if you care about the relative order of some of the calls, but not all of them? Can you specify an arbitrary partial order? The answer is ... yes! The details can be found here .) All Expectations Are Sticky (Unless Said Otherwise) Now let's do a quick quiz to see how well you can use this mock stuff already. How would you test that the turtle is asked to go to the origin exactly twice (you want to ignore any other instructions it receives)? After you've come up with your answer, take a look at ours and compare notes (solve it yourself first - don't cheat!): using :: testing :: _ ; using :: testing :: AnyNumber ; ... EXPECT_CALL ( turtle , GoTo ( _ , _ )) // #1 . Times ( AnyNumber ()); EXPECT_CALL ( turtle , GoTo ( 0 , 0 )) // #2 . Times ( 2 ); Suppose turtle.GoTo(0, 0) is called three times. In the third time, gMock will see that the arguments match expectation #2 (remember that we always pick the last matching expectation). Now, since we said that there should be only two such calls, gMock will report an error immediately. This is basically what we've told you in the Using Multiple Expectations section above. This example shows that expectations in gMock are \"sticky\" by default , in the sense that they remain active even after we have reached their invocation upper bounds. This is an important rule to remember, as it affects the meaning of the spec, and is different to how it's done in many other mocking frameworks (Why'd we do that? Because we think our rule makes the common cases easier to express and understand.). Simple? Let's see if you've really understood it: what does the following code say? using :: testing :: Return ; ... for ( int i = n ; i > 0 ; i -- ) { EXPECT_CALL ( turtle , GetX ()) . WillOnce ( Return ( 10 * i )); } If you think it says that turtle.GetX() will be called n times and will return 10, 20, 30, ..., consecutively, think twice! The problem is that, as we said, expectations are sticky. So, the second time turtle.GetX() is called, the last (latest) EXPECT_CALL() statement will match, and will immediately lead to an \"upper bound violated\" error - this piece of code is not very useful! One correct way of saying that turtle.GetX() will return 10, 20, 30, ..., is to explicitly say that the expectations are not sticky. In other words, they should retire as soon as they are saturated: using :: testing :: Return ; ... for ( int i = n ; i > 0 ; i -- ) { EXPECT_CALL ( turtle , GetX ()) . WillOnce ( Return ( 10 * i )) . RetiresOnSaturation (); } And, there's a better way to do it: in this case, we expect the calls to occur in a specific order, and we line up the actions to match the order. Since the order is important here, we should make it explicit using a sequence: using :: testing :: InSequence ; using :: testing :: Return ; ... { InSequence s ; for ( int i = 1 ; i <= n ; i ++ ) { EXPECT_CALL ( turtle , GetX ()) . WillOnce ( Return ( 10 * i )) . RetiresOnSaturation (); } } By the way, the other situation where an expectation may not be sticky is when it's in a sequence - as soon as another expectation that comes after it in the sequence has been used, it automatically retires (and will never be used to match any call). Uninteresting Calls A mock object may have many methods, and not all of them are that interesting. For example, in some tests we may not care about how many times GetX() and GetY() get called. In gMock, if you are not interested in a method, just don't say anything about it. If a call to this method occurs, you'll see a warning in the test output, but it won't be a failure. This is called \"naggy\" behavior; to change, see The Nice, the Strict, and the Naggy .","title":"For dummies"},{"location":"examples/gtest/docs/for_dummies.html#GMockForDummies","text":"","title":"gMock for Dummies"},{"location":"examples/gtest/docs/for_dummies.html#what-is-gmock","text":"When you write a prototype or test, often it's not feasible or wise to rely on real objects entirely. A mock object implements the same interface as a real object (so it can be used as one), but lets you specify at run time how it will be used and what it should do (which methods will be called? in which order? how many times? with what arguments? what will they return? etc). Note: It is easy to confuse the term fake objects with mock objects. Fakes and mocks actually mean very different things in the Test-Driven Development (TDD) community: Fake objects have working implementations, but usually take some shortcut (perhaps to make the operations less expensive), which makes them not suitable for production. An in-memory file system would be an example of a fake. Mocks are objects pre-programmed with expectations , which form a specification of the calls they are expected to receive. If all this seems too abstract for you, don't worry - the most important thing to remember is that a mock allows you to check the interaction between itself and code that uses it. The difference between fakes and mocks shall become much clearer once you start to use mocks. gMock is a library (sometimes we also call it a \"framework\" to make it sound cool) for creating mock classes and using them. It does to C++ what jMock/EasyMock does to Java (well, more or less). When using gMock, first, you use some simple macros to describe the interface you want to mock, and they will expand to the implementation of your mock class; next, you create some mock objects and specify its expectations and behavior using an intuitive syntax; then you exercise code that uses the mock objects. gMock will catch any violation to the expectations as soon as it arises.","title":"What Is gMock?"},{"location":"examples/gtest/docs/for_dummies.html#why-gmock","text":"While mock objects help you remove unnecessary dependencies in tests and make them fast and reliable, using mocks manually in C++ is hard : Someone has to implement the mocks. The job is usually tedious and error-prone. No wonder people go great distance to avoid it. The quality of those manually written mocks is a bit, uh, unpredictable. You may see some really polished ones, but you may also see some that were hacked up in a hurry and have all sorts of ad hoc restrictions. The knowledge you gained from using one mock doesn't transfer to the next one. In contrast, Java and Python programmers have some fine mock frameworks (jMock, EasyMock, Mox , etc), which automate the creation of mocks. As a result, mocking is a proven effective technique and widely adopted practice in those communities. Having the right tool absolutely makes the difference. gMock was built to help C++ programmers. It was inspired by jMock and EasyMock, but designed with C++'s specifics in mind. It is your friend if any of the following problems is bothering you: You are stuck with a sub-optimal design and wish you had done more prototyping before it was too late, but prototyping in C++ is by no means \"rapid\". Your tests are slow as they depend on too many libraries or use expensive resources (e.g. a database). Your tests are brittle as some resources they use are unreliable (e.g. the network). You want to test how your code handles a failure (e.g. a file checksum error), but it's not easy to cause one. You need to make sure that your module interacts with other modules in the right way, but it's hard to observe the interaction; therefore you resort to observing the side effects at the end of the action, but it's awkward at best. You want to \"mock out\" your dependencies, except that they don't have mock implementations yet; and, frankly, you aren't thrilled by some of those hand-written mocks. We encourage you to use gMock as a design tool, for it lets you experiment with your interface design early and often. More iterations lead to better designs! a testing tool to cut your tests' outbound dependencies and probe the interaction between your module and its collaborators.","title":"Why gMock?"},{"location":"examples/gtest/docs/for_dummies.html#getting-started","text":"gMock is bundled with googletest.","title":"Getting Started"},{"location":"examples/gtest/docs/for_dummies.html#a-case-for-mock-turtles","text":"Let's look at an example. Suppose you are developing a graphics program that relies on a LOGO -like API for drawing. How would you test that it does the right thing? Well, you can run it and compare the screen with a golden screen snapshot, but let's admit it: tests like this are expensive to run and fragile (What if you just upgraded to a shiny new graphics card that has better anti-aliasing? Suddenly you have to update all your golden images.). It would be too painful if all your tests are like this. Fortunately, you learned about Dependency Injection and know the right thing to do: instead of having your application talk to the system API directly, wrap the API in an interface (say, Turtle ) and code to that interface: class Turtle { ... virtual ~ Turtle () {}; virtual void PenUp () = 0 ; virtual void PenDown () = 0 ; virtual void Forward ( int distance ) = 0 ; virtual void Turn ( int degrees ) = 0 ; virtual void GoTo ( int x , int y ) = 0 ; virtual int GetX () const = 0 ; virtual int GetY () const = 0 ; }; (Note that the destructor of Turtle must be virtual, as is the case for all classes you intend to inherit from - otherwise the destructor of the derived class will not be called when you delete an object through a base pointer, and you'll get corrupted program states like memory leaks.) You can control whether the turtle's movement will leave a trace using PenUp() and PenDown() , and control its movement using Forward() , Turn() , and GoTo() . Finally, GetX() and GetY() tell you the current position of the turtle. Your program will normally use a real implementation of this interface. In tests, you can use a mock implementation instead. This allows you to easily check what drawing primitives your program is calling, with what arguments, and in which order. Tests written this way are much more robust (they won't break because your new machine does anti-aliasing differently), easier to read and maintain (the intent of a test is expressed in the code, not in some binary images), and run much, much faster .","title":"A Case for Mock Turtles"},{"location":"examples/gtest/docs/for_dummies.html#writing-the-mock-class","text":"If you are lucky, the mocks you need to use have already been implemented by some nice people. If, however, you find yourself in the position to write a mock class, relax - gMock turns this task into a fun game! (Well, almost.)","title":"Writing the Mock Class"},{"location":"examples/gtest/docs/for_dummies.html#how-to-define-it","text":"Using the Turtle interface as example, here are the simple steps you need to follow: Derive a class MockTurtle from Turtle . Take a virtual function of Turtle (while it's possible to mock non-virtual methods using templates , it's much more involved). In the public: section of the child class, write MOCK_METHOD(); Now comes the fun part: you take the function signature, cut-and-paste it into the macro, and add two commas - one between the return type and the name, another between the name and the argument list. If you're mocking a const method, add a 4th parameter containing (const) (the parentheses are required). Since you're overriding a virtual method, we suggest adding the override keyword. For const methods the 4th parameter becomes (const, override) , for non-const methods just (override) . This isn't mandatory. Repeat until all virtual functions you want to mock are done. (It goes without saying that all pure virtual methods in your abstract class must be either mocked or overridden.) After the process, you should have something like: #include \"gmock/gmock.h\" // Brings in gMock. class MockTurtle : public Turtle { public : ... MOCK_METHOD ( void , PenUp , (), ( override )); MOCK_METHOD ( void , PenDown , (), ( override )); MOCK_METHOD ( void , Forward , ( int distance ), ( override )); MOCK_METHOD ( void , Turn , ( int degrees ), ( override )); MOCK_METHOD ( void , GoTo , ( int x , int y ), ( override )); MOCK_METHOD ( int , GetX , (), ( const , override )); MOCK_METHOD ( int , GetY , (), ( const , override )); }; You don't need to define these mock methods somewhere else - the MOCK_METHOD macro will generate the definitions for you. It's that simple!","title":"How to Define It"},{"location":"examples/gtest/docs/for_dummies.html#where-to-put-it","text":"When you define a mock class, you need to decide where to put its definition. Some people put it in a _test.cc . This is fine when the interface being mocked (say, Foo ) is owned by the same person or team. Otherwise, when the owner of Foo changes it, your test could break. (You can't really expect Foo 's maintainer to fix every test that uses Foo , can you?) So, the rule of thumb is: if you need to mock Foo and it's owned by others, define the mock class in Foo 's package (better, in a testing sub-package such that you can clearly separate production code and testing utilities), put it in a .h and a cc_library . Then everyone can reference them from their tests. If Foo ever changes, there is only one copy of MockFoo to change, and only tests that depend on the changed methods need to be fixed. Another way to do it: you can introduce a thin layer FooAdaptor on top of Foo and code to this new interface. Since you own FooAdaptor , you can absorb changes in Foo much more easily. While this is more work initially, carefully choosing the adaptor interface can make your code easier to write and more readable (a net win in the long run), as you can choose FooAdaptor to fit your specific domain much better than Foo does.","title":"Where to Put It"},{"location":"examples/gtest/docs/for_dummies.html#using-mocks-in-tests","text":"Once you have a mock class, using it is easy. The typical work flow is: Import the gMock names from the testing namespace such that you can use them unqualified (You only have to do it once per file. Remember that namespaces are a good idea. Create some mock objects. Specify your expectations on them (How many times will a method be called? With what arguments? What should it do? etc.). Exercise some code that uses the mocks; optionally, check the result using googletest assertions. If a mock method is called more than expected or with wrong arguments, you'll get an error immediately. When a mock is destructed, gMock will automatically check whether all expectations on it have been satisfied. Here's an example: #include \"path/to/mock-turtle.h\" #include \"gmock/gmock.h\" #include \"gtest/gtest.h\" using :: testing :: AtLeast ; // #1 TEST ( PainterTest , CanDrawSomething ) { MockTurtle turtle ; // #2 EXPECT_CALL ( turtle , PenDown ()) // #3 . Times ( AtLeast ( 1 )); Painter painter ( & turtle ); // #4 EXPECT_TRUE ( painter . DrawCircle ( 0 , 0 , 10 )); // #5 } As you might have guessed, this test checks that PenDown() is called at least once. If the painter object didn't call this method, your test will fail with a message like this: path/to/my_test.cc:119: Failure Actual function call count doesn't match this expectation: Actually: never called; Expected: called at least once. Stack trace: ... Tip 1: If you run the test from an Emacs buffer, you can hit on the line number to jump right to the failed expectation. Tip 2: If your mock objects are never deleted, the final verification won't happen. Therefore it's a good idea to turn on the heap checker in your tests when you allocate mocks on the heap. You get that automatically if you use the gtest_main library already. Important note: gMock requires expectations to be set before the mock functions are called, otherwise the behavior is undefined . In particular, you mustn't interleave EXPECT_CALL()s and calls to the mock functions. This means EXPECT_CALL() should be read as expecting that a call will occur in the future , not that a call has occurred. Why does gMock work like that? Well, specifying the expectation beforehand allows gMock to report a violation as soon as it rises, when the context (stack trace, etc) is still available. This makes debugging much easier. Admittedly, this test is contrived and doesn't do much. You can easily achieve the same effect without using gMock. However, as we shall reveal soon, gMock allows you to do so much more with the mocks.","title":"Using Mocks in Tests"},{"location":"examples/gtest/docs/for_dummies.html#setting-expectations","text":"The key to using a mock object successfully is to set the right expectations on it. If you set the expectations too strict, your test will fail as the result of unrelated changes. If you set them too loose, bugs can slip through. You want to do it just right such that your test can catch exactly the kind of bugs you intend it to catch. gMock provides the necessary means for you to do it \"just right.\"","title":"Setting Expectations"},{"location":"examples/gtest/docs/for_dummies.html#general-syntax","text":"In gMock we use the EXPECT_CALL() macro to set an expectation on a mock method. The general syntax is: EXPECT_CALL ( mock_object , method ( matchers )) . Times ( cardinality ) . WillOnce ( action ) . WillRepeatedly ( action ); The macro has two arguments: first the mock object, and then the method and its arguments. Note that the two are separated by a comma ( , ), not a period ( . ). (Why using a comma? The answer is that it was necessary for technical reasons.) If the method is not overloaded, the macro can also be called without matchers: EXPECT_CALL ( mock_object , non - overloaded - method ) . Times ( cardinality ) . WillOnce ( action ) . WillRepeatedly ( action ); This syntax allows the test writer to specify \"called with any arguments\" without explicitly specifying the number or types of arguments. To avoid unintended ambiguity, this syntax may only be used for methods which are not overloaded Either form of the macro can be followed by some optional clauses that provide more information about the expectation. We'll discuss how each clause works in the coming sections. This syntax is designed to make an expectation read like English. For example, you can probably guess that using :: testing :: Return ; ... EXPECT_CALL ( turtle , GetX ()) . Times ( 5 ) . WillOnce ( Return ( 100 )) . WillOnce ( Return ( 150 )) . WillRepeatedly ( Return ( 200 )); says that the turtle object's GetX() method will be called five times, it will return 100 the first time, 150 the second time, and then 200 every time. Some people like to call this style of syntax a Domain-Specific Language (DSL). Note: Why do we use a macro to do this? Well it serves two purposes: first it makes expectations easily identifiable (either by gsearch or by a human reader), and second it allows gMock to include the source file location of a failed expectation in messages, making debugging easier.","title":"General Syntax"},{"location":"examples/gtest/docs/for_dummies.html#matchers-what-arguments-do-we-expect","text":"When a mock function takes arguments, we may specify what arguments we are expecting, for example: // Expects the turtle to move forward by 100 units. EXPECT_CALL ( turtle , Forward ( 100 )); Oftentimes you do not want to be too specific. Remember that talk about tests being too rigid? Over specification leads to brittle tests and obscures the intent of tests. Therefore we encourage you to specify only what's necessary\u2014no more, no less. If you aren't interested in the value of an argument, write _ as the argument, which means \"anything goes\": using :: testing :: _ ; ... // Expects that the turtle jumps to somewhere on the x=50 line. EXPECT_CALL ( turtle , GoTo ( 50 , _ )); _ is an instance of what we call matchers . A matcher is like a predicate and can test whether an argument is what we'd expect. You can use a matcher inside EXPECT_CALL() wherever a function argument is expected. _ is a convenient way of saying \"any value\". In the above examples, 100 and 50 are also matchers; implicitly, they are the same as Eq(100) and Eq(50) , which specify that the argument must be equal (using operator== ) to the matcher argument. There are many built-in matchers for common types (as well as custom matchers ); for example: using :: testing :: Ge ; ... // Expects the turtle moves forward by at least 100. EXPECT_CALL ( turtle , Forward ( Ge ( 100 ))); If you don't care about any arguments, rather than specify _ for each of them you may instead omit the parameter list: // Expects the turtle to move forward. EXPECT_CALL ( turtle , Forward ); // Expects the turtle to jump somewhere. EXPECT_CALL ( turtle , GoTo ); This works for all non-overloaded methods; if a method is overloaded, you need to help gMock resolve which overload is expected by specifying the number of arguments and possibly also the types of the arguments .","title":"Matchers: What Arguments Do We Expect?"},{"location":"examples/gtest/docs/for_dummies.html#cardinalities-how-many-times-will-it-be-called","text":"The first clause we can specify following an EXPECT_CALL() is Times() . We call its argument a cardinality as it tells how many times the call should occur. It allows us to repeat an expectation many times without actually writing it as many times. More importantly, a cardinality can be \"fuzzy\", just like a matcher can be. This allows a user to express the intent of a test exactly. An interesting special case is when we say Times(0) . You may have guessed - it means that the function shouldn't be called with the given arguments at all, and gMock will report a googletest failure whenever the function is (wrongfully) called. We've seen AtLeast(n) as an example of fuzzy cardinalities earlier. For the list of built-in cardinalities you can use, see here . The Times() clause can be omitted. If you omit Times() , gMock will infer the cardinality for you. The rules are easy to remember: If neither WillOnce() nor WillRepeatedly() is in the EXPECT_CALL() , the inferred cardinality is Times(1) . If there are n WillOnce() 's but no WillRepeatedly() , where n >= 1, the cardinality is Times(n) . If there are n WillOnce() 's and one WillRepeatedly() , where n >= 0, the cardinality is Times(AtLeast(n)) . Quick quiz: what do you think will happen if a function is expected to be called twice but actually called four times?","title":"Cardinalities: How Many Times Will It Be Called?"},{"location":"examples/gtest/docs/for_dummies.html#actions-what-should-it-do","text":"Remember that a mock object doesn't really have a working implementation? We as users have to tell it what to do when a method is invoked. This is easy in gMock. First, if the return type of a mock function is a built-in type or a pointer, the function has a default action (a void function will just return, a bool function will return false , and other functions will return 0). In addition, in C++ 11 and above, a mock function whose return type is default-constructible (i.e. has a default constructor) has a default action of returning a default-constructed value. If you don't say anything, this behavior will be used. Second, if a mock function doesn't have a default action, or the default action doesn't suit you, you can specify the action to be taken each time the expectation matches using a series of WillOnce() clauses followed by an optional WillRepeatedly() . For example, using :: testing :: Return ; ... EXPECT_CALL ( turtle , GetX ()) . WillOnce ( Return ( 100 )) . WillOnce ( Return ( 200 )) . WillOnce ( Return ( 300 )); says that turtle.GetX() will be called exactly three times (gMock inferred this from how many WillOnce() clauses we've written, since we didn't explicitly write Times() ), and will return 100, 200, and 300 respectively. using :: testing :: Return ; ... EXPECT_CALL ( turtle , GetY ()) . WillOnce ( Return ( 100 )) . WillOnce ( Return ( 200 )) . WillRepeatedly ( Return ( 300 )); says that turtle.GetY() will be called at least twice (gMock knows this as we've written two WillOnce() clauses and a WillRepeatedly() while having no explicit Times() ), will return 100 and 200 respectively the first two times, and 300 from the third time on. Of course, if you explicitly write a Times() , gMock will not try to infer the cardinality itself. What if the number you specified is larger than there are WillOnce() clauses? Well, after all WillOnce() s are used up, gMock will do the default action for the function every time (unless, of course, you have a WillRepeatedly() .). What can we do inside WillOnce() besides Return() ? You can return a reference using ReturnRef(*variable*) , or invoke a pre-defined function, among others . Important note: The EXPECT_CALL() statement evaluates the action clause only once, even though the action may be performed many times. Therefore you must be careful about side effects. The following may not do what you want: using :: testing :: Return ; ... int n = 100 ; EXPECT_CALL ( turtle , GetX ()) . Times ( 4 ) . WillRepeatedly ( Return ( n ++ )); Instead of returning 100, 101, 102, ..., consecutively, this mock function will always return 100 as n++ is only evaluated once. Similarly, Return(new Foo) will create a new Foo object when the EXPECT_CALL() is executed, and will return the same pointer every time. If you want the side effect to happen every time, you need to define a custom action, which we'll teach in the cook book . Time for another quiz! What do you think the following means? using :: testing :: Return ; ... EXPECT_CALL ( turtle , GetY ()) . Times ( 4 ) . WillOnce ( Return ( 100 )); Obviously turtle.GetY() is expected to be called four times. But if you think it will return 100 every time, think twice! Remember that one WillOnce() clause will be consumed each time the function is invoked and the default action will be taken afterwards. So the right answer is that turtle.GetY() will return 100 the first time, but return 0 from the second time on , as returning 0 is the default action for int functions.","title":"Actions: What Should It Do?"},{"location":"examples/gtest/docs/for_dummies.html#MultiExpectations","text":"So far we've only shown examples where you have a single expectation. More realistically, you'll specify expectations on multiple mock methods which may be from multiple mock objects. By default, when a mock method is invoked, gMock will search the expectations in the reverse order they are defined, and stop when an active expectation that matches the arguments is found (you can think of it as \"newer rules override older ones.\"). If the matching expectation cannot take any more calls, you will get an upper-bound-violated failure. Here's an example: using :: testing :: _ ; ... EXPECT_CALL ( turtle , Forward ( _ )); // #1 EXPECT_CALL ( turtle , Forward ( 10 )) // #2 . Times ( 2 ); If Forward(10) is called three times in a row, the third time it will be an error, as the last matching expectation (#2) has been saturated. If, however, the third Forward(10) call is replaced by Forward(20) , then it would be OK, as now #1 will be the matching expectation. Note: Why does gMock search for a match in the reverse order of the expectations? The reason is that this allows a user to set up the default expectations in a mock object's constructor or the test fixture's set-up phase and then customize the mock by writing more specific expectations in the test body. So, if you have two expectations on the same method, you want to put the one with more specific matchers after the other, or the more specific rule would be shadowed by the more general one that comes after it. Tip: It is very common to start with a catch-all expectation for a method and Times(AnyNumber()) (omitting arguments, or with _ for all arguments, if overloaded). This makes any calls to the method expected. This is not necessary for methods that are not mentioned at all (these are \"uninteresting\"), but is useful for methods that have some expectations, but for which other calls are ok. See Understanding Uninteresting vs Unexpected Calls .","title":"Using Multiple Expectations"},{"location":"examples/gtest/docs/for_dummies.html#OrderedCalls","text":"By default, an expectation can match a call even though an earlier expectation hasn't been satisfied. In other words, the calls don't have to occur in the order the expectations are specified. Sometimes, you may want all the expected calls to occur in a strict order. To say this in gMock is easy: using :: testing :: InSequence ; ... TEST ( FooTest , DrawsLineSegment ) { ... { InSequence seq ; EXPECT_CALL ( turtle , PenDown ()); EXPECT_CALL ( turtle , Forward ( 100 )); EXPECT_CALL ( turtle , PenUp ()); } Foo (); } By creating an object of type InSequence , all expectations in its scope are put into a sequence and have to occur sequentially . Since we are just relying on the constructor and destructor of this object to do the actual work, its name is really irrelevant. In this example, we test that Foo() calls the three expected functions in the order as written. If a call is made out-of-order, it will be an error. (What if you care about the relative order of some of the calls, but not all of them? Can you specify an arbitrary partial order? The answer is ... yes! The details can be found here .)","title":"Ordered vs Unordered Calls"},{"location":"examples/gtest/docs/for_dummies.html#StickyExpectations","text":"Now let's do a quick quiz to see how well you can use this mock stuff already. How would you test that the turtle is asked to go to the origin exactly twice (you want to ignore any other instructions it receives)? After you've come up with your answer, take a look at ours and compare notes (solve it yourself first - don't cheat!): using :: testing :: _ ; using :: testing :: AnyNumber ; ... EXPECT_CALL ( turtle , GoTo ( _ , _ )) // #1 . Times ( AnyNumber ()); EXPECT_CALL ( turtle , GoTo ( 0 , 0 )) // #2 . Times ( 2 ); Suppose turtle.GoTo(0, 0) is called three times. In the third time, gMock will see that the arguments match expectation #2 (remember that we always pick the last matching expectation). Now, since we said that there should be only two such calls, gMock will report an error immediately. This is basically what we've told you in the Using Multiple Expectations section above. This example shows that expectations in gMock are \"sticky\" by default , in the sense that they remain active even after we have reached their invocation upper bounds. This is an important rule to remember, as it affects the meaning of the spec, and is different to how it's done in many other mocking frameworks (Why'd we do that? Because we think our rule makes the common cases easier to express and understand.). Simple? Let's see if you've really understood it: what does the following code say? using :: testing :: Return ; ... for ( int i = n ; i > 0 ; i -- ) { EXPECT_CALL ( turtle , GetX ()) . WillOnce ( Return ( 10 * i )); } If you think it says that turtle.GetX() will be called n times and will return 10, 20, 30, ..., consecutively, think twice! The problem is that, as we said, expectations are sticky. So, the second time turtle.GetX() is called, the last (latest) EXPECT_CALL() statement will match, and will immediately lead to an \"upper bound violated\" error - this piece of code is not very useful! One correct way of saying that turtle.GetX() will return 10, 20, 30, ..., is to explicitly say that the expectations are not sticky. In other words, they should retire as soon as they are saturated: using :: testing :: Return ; ... for ( int i = n ; i > 0 ; i -- ) { EXPECT_CALL ( turtle , GetX ()) . WillOnce ( Return ( 10 * i )) . RetiresOnSaturation (); } And, there's a better way to do it: in this case, we expect the calls to occur in a specific order, and we line up the actions to match the order. Since the order is important here, we should make it explicit using a sequence: using :: testing :: InSequence ; using :: testing :: Return ; ... { InSequence s ; for ( int i = 1 ; i <= n ; i ++ ) { EXPECT_CALL ( turtle , GetX ()) . WillOnce ( Return ( 10 * i )) . RetiresOnSaturation (); } } By the way, the other situation where an expectation may not be sticky is when it's in a sequence - as soon as another expectation that comes after it in the sequence has been used, it automatically retires (and will never be used to match any call).","title":"All Expectations Are Sticky (Unless Said Otherwise)"},{"location":"examples/gtest/docs/for_dummies.html#uninteresting-calls","text":"A mock object may have many methods, and not all of them are that interesting. For example, in some tests we may not care about how many times GetX() and GetY() get called. In gMock, if you are not interested in a method, just don't say anything about it. If a call to this method occurs, you'll see a warning in the test output, but it won't be a failure. This is called \"naggy\" behavior; to change, see The Nice, the Strict, and the Naggy .","title":"Uninteresting Calls"},{"location":"examples/gtest/docs/gmock_faq.html","text":"Legacy gMock FAQ When I call a method on my mock object, the method for the real object is invoked instead. What's the problem? In order for a method to be mocked, it must be virtual , unless you use the high-perf dependency injection technique . Can I mock a variadic function? You cannot mock a variadic function (i.e. a function taking ellipsis ( ... ) arguments) directly in gMock. The problem is that in general, there is no way for a mock object to know how many arguments are passed to the variadic method, and what the arguments' types are. Only the author of the base class knows the protocol, and we cannot look into his or her head. Therefore, to mock such a function, the user must teach the mock object how to figure out the number of arguments and their types. One way to do it is to provide overloaded versions of the function. Ellipsis arguments are inherited from C and not really a C++ feature. They are unsafe to use and don't work with arguments that have constructors or destructors. Therefore we recommend to avoid them in C++ as much as possible. MSVC gives me warning C4301 or C4373 when I define a mock method with a const parameter. Why? If you compile this using Microsoft Visual C++ 2005 SP1: class Foo { ... virtual void Bar ( const int i ) = 0 ; }; class MockFoo : public Foo { ... MOCK_METHOD ( void , Bar , ( const int i ), ( override )); }; You may get the following warning: warning C4301: 'MockFoo::Bar' : overriding virtual function only differs from 'Foo::Bar' by const/volatile qualifier This is a MSVC bug. The same code compiles fine with gcc, for example. If you use Visual C++ 2008 SP1, you would get the warning: warning C4373: 'MockFoo::Bar' : virtual function overrides 'Foo::Bar' , previous versions of the compiler did not override when parameters only differed by const/volatile qualifiers In C++, if you declare a function with a const parameter, the const modifier is ignored. Therefore, the Foo base class above is equivalent to: class Foo { ... virtual void Bar ( int i ) = 0 ; // int or const int? Makes no difference. }; In fact, you can declare Bar() with an int parameter, and define it with a const int parameter. The compiler will still match them up. Since making a parameter const is meaningless in the method declaration, we recommend to remove it in both Foo and MockFoo . That should workaround the VC bug. Note that we are talking about the top-level const modifier here. If the function parameter is passed by pointer or reference, declaring the pointee or referee as const is still meaningful. For example, the following two declarations are not equivalent: void Bar ( int * p ); // Neither p nor *p is const. void Bar ( const int * p ); // p is not const, but *p is. I can't figure out why gMock thinks my expectations are not satisfied. What should I do? You might want to run your test with --gmock_verbose=info . This flag lets gMock print a trace of every mock function call it receives. By studying the trace, you'll gain insights on why the expectations you set are not met. If you see the message \"The mock function has no default action set, and its return type has no default value set.\", then try adding a default action . Due to a known issue, unexpected calls on mocks without default actions don't print out a detailed comparison between the actual arguments and the expected arguments. My program crashed and ScopedMockLog spit out tons of messages. Is it a gMock bug? gMock and ScopedMockLog are likely doing the right thing here. When a test crashes, the failure signal handler will try to log a lot of information (the stack trace, and the address map, for example). The messages are compounded if you have many threads with depth stacks. When ScopedMockLog intercepts these messages and finds that they don't match any expectations, it prints an error for each of them. You can learn to ignore the errors, or you can rewrite your expectations to make your test more robust, for example, by adding something like: using :: testing :: AnyNumber ; using :: testing :: Not ; ... // Ignores any log not done by us. EXPECT_CALL ( log , Log ( _ , Not ( EndsWith ( \"/my_file.cc\" )), _ )) . Times ( AnyNumber ()); How can I assert that a function is NEVER called? using :: testing :: _ ; ... EXPECT_CALL ( foo , Bar ( _ )) . Times ( 0 ); I have a failed test where gMock tells me TWICE that a particular expectation is not satisfied. Isn't this redundant? When gMock detects a failure, it prints relevant information (the mock function arguments, the state of relevant expectations, and etc) to help the user debug. If another failure is detected, gMock will do the same, including printing the state of relevant expectations. Sometimes an expectation's state didn't change between two failures, and you'll see the same description of the state twice. They are however not redundant, as they refer to different points in time . The fact they are the same is interesting information. I get a heapcheck failure when using a mock object, but using a real object is fine. What can be wrong? Does the class (hopefully a pure interface) you are mocking have a virtual destructor? Whenever you derive from a base class, make sure its destructor is virtual. Otherwise Bad Things will happen. Consider the following code: class Base { public : // Not virtual, but should be. ~ Base () { ... } ... }; class Derived : public Base { public : ... private : std :: string value_ ; }; ... Base * p = new Derived ; ... delete p ; // Surprise! ~Base() will be called, but ~Derived() will not // - value_ is leaked. By changing ~Base() to virtual, ~Derived() will be correctly called when delete p is executed, and the heap checker will be happy. The \"newer expectations override older ones\" rule makes writing expectations awkward. Why does gMock do that? When people complain about this, often they are referring to code like: using :: testing :: Return ; ... // foo.Bar() should be called twice, return 1 the first time, and return // 2 the second time. However, I have to write the expectations in the // reverse order. This sucks big time!!! EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 2 )) . RetiresOnSaturation (); EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 1 )) . RetiresOnSaturation (); The problem, is that they didn't pick the best way to express the test's intent. By default, expectations don't have to be matched in any particular order. If you want them to match in a certain order, you need to be explicit. This is gMock's (and jMock's) fundamental philosophy: it's easy to accidentally over-specify your tests, and we want to make it harder to do so. There are two better ways to write the test spec. You could either put the expectations in sequence: using :: testing :: Return ; ... // foo.Bar() should be called twice, return 1 the first time, and return // 2 the second time. Using a sequence, we can write the expectations // in their natural order. { InSequence s ; EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 1 )) . RetiresOnSaturation (); EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 2 )) . RetiresOnSaturation (); } or you can put the sequence of actions in the same expectation: using :: testing :: Return ; ... // foo.Bar() should be called twice, return 1 the first time, and return // 2 the second time. EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 1 )) . WillOnce ( Return ( 2 )) . RetiresOnSaturation (); Back to the original questions: why does gMock search the expectations (and ON_CALL s) from back to front? Because this allows a user to set up a mock's behavior for the common case early (e.g. in the mock's constructor or the test fixture's set-up phase) and customize it with more specific rules later. If gMock searches from front to back, this very useful pattern won't be possible. gMock prints a warning when a function without EXPECT_CALL is called, even if I have set its behavior using ON_CALL. Would it be reasonable not to show the warning in this case? When choosing between being neat and being safe, we lean toward the latter. So the answer is that we think it's better to show the warning. Often people write ON_CALL s in the mock object's constructor or SetUp() , as the default behavior rarely changes from test to test. Then in the test body they set the expectations, which are often different for each test. Having an ON_CALL in the set-up part of a test doesn't mean that the calls are expected. If there's no EXPECT_CALL and the method is called, it's possibly an error. If we quietly let the call go through without notifying the user, bugs may creep in unnoticed. If, however, you are sure that the calls are OK, you can write using :: testing :: _ ; ... EXPECT_CALL ( foo , Bar ( _ )) . WillRepeatedly (...); instead of using :: testing :: _ ; ... ON_CALL ( foo , Bar ( _ )) . WillByDefault (...); This tells gMock that you do expect the calls and no warning should be printed. Also, you can control the verbosity by specifying --gmock_verbose=error . Other values are info and warning . If you find the output too noisy when debugging, just choose a less verbose level. How can I delete the mock function's argument in an action? If your mock function takes a pointer argument and you want to delete that argument, you can use testing::DeleteArg () to delete the N'th (zero-indexed) argument: using :: testing :: _ ; ... MOCK_METHOD ( void , Bar , ( X * x , const Y & y )); ... EXPECT_CALL ( mock_foo_ , Bar ( _ , _ )) . WillOnce ( testing :: DeleteArg < 0 > ())); How can I perform an arbitrary action on a mock function's argument? If you find yourself needing to perform some action that's not supported by gMock directly, remember that you can define your own actions using MakeAction() or MakePolymorphicAction() , or you can write a stub function and invoke it using Invoke() . using :: testing :: _ ; using :: testing :: Invoke ; ... MOCK_METHOD ( void , Bar , ( X * p )); ... EXPECT_CALL ( mock_foo_ , Bar ( _ )) . WillOnce ( Invoke ( MyAction (...))); My code calls a static/global function. Can I mock it? You can, but you need to make some changes. In general, if you find yourself needing to mock a static function, it's a sign that your modules are too tightly coupled (and less flexible, less reusable, less testable, etc). You are probably better off defining a small interface and call the function through that interface, which then can be easily mocked. It's a bit of work initially, but usually pays for itself quickly. This Google Testing Blog post says it excellently. Check it out. My mock object needs to do complex stuff. It's a lot of pain to specify the actions. gMock sucks! I know it's not a question, but you get an answer for free any way. :-) With gMock, you can create mocks in C++ easily. And people might be tempted to use them everywhere. Sometimes they work great, and sometimes you may find them, well, a pain to use. So, what's wrong in the latter case? When you write a test without using mocks, you exercise the code and assert that it returns the correct value or that the system is in an expected state. This is sometimes called \"state-based testing\". Mocks are great for what some call \"interaction-based\" testing: instead of checking the system state at the very end, mock objects verify that they are invoked the right way and report an error as soon as it arises, giving you a handle on the precise context in which the error was triggered. This is often more effective and economical to do than state-based testing. If you are doing state-based testing and using a test double just to simulate the real object, you are probably better off using a fake. Using a mock in this case causes pain, as it's not a strong point for mocks to perform complex actions. If you experience this and think that mocks suck, you are just not using the right tool for your problem. Or, you might be trying to solve the wrong problem. :-) I got a warning \"Uninteresting function call encountered - default action taken..\" Should I panic? By all means, NO! It's just an FYI. :-) What it means is that you have a mock function, you haven't set any expectations on it (by gMock's rule this means that you are not interested in calls to this function and therefore it can be called any number of times), and it is called. That's OK - you didn't say it's not OK to call the function! What if you actually meant to disallow this function to be called, but forgot to write EXPECT_CALL(foo, Bar()).Times(0) ? While one can argue that it's the user's fault, gMock tries to be nice and prints you a note. So, when you see the message and believe that there shouldn't be any uninteresting calls, you should investigate what's going on. To make your life easier, gMock dumps the stack trace when an uninteresting call is encountered. From that you can figure out which mock function it is, and how it is called. I want to define a custom action. Should I use Invoke() or implement the ActionInterface interface? Either way is fine - you want to choose the one that's more convenient for your circumstance. Usually, if your action is for a particular function type, defining it using Invoke() should be easier; if your action can be used in functions of different types (e.g. if you are defining Return(*value*) ), MakePolymorphicAction() is easiest. Sometimes you want precise control on what types of functions the action can be used in, and implementing ActionInterface is the way to go here. See the implementation of Return() in testing/base/public/gmock-actions.h for an example. I use SetArgPointee() in WillOnce(), but gcc complains about \"conflicting return type specified\". What does it mean? You got this error as gMock has no idea what value it should return when the mock method is called. SetArgPointee() says what the side effect is, but doesn't say what the return value should be. You need DoAll() to chain a SetArgPointee() with a Return() that provides a value appropriate to the API being mocked. See this recipe for more details and an example. I have a huge mock class, and Microsoft Visual C++ runs out of memory when compiling it. What can I do? We've noticed that when the /clr compiler flag is used, Visual C++ uses 5~6 times as much memory when compiling a mock class. We suggest to avoid /clr when compiling native C++ mocks.","title":"Gmock faq"},{"location":"examples/gtest/docs/gmock_faq.html#GMockFaq","text":"","title":"Legacy gMock FAQ"},{"location":"examples/gtest/docs/gmock_faq.html#when-i-call-a-method-on-my-mock-object-the-method-for-the-real-object-is-invoked-instead-whats-the-problem","text":"In order for a method to be mocked, it must be virtual , unless you use the high-perf dependency injection technique .","title":"When I call a method on my mock object, the method for the real object is invoked instead. What's the problem?"},{"location":"examples/gtest/docs/gmock_faq.html#can-i-mock-a-variadic-function","text":"You cannot mock a variadic function (i.e. a function taking ellipsis ( ... ) arguments) directly in gMock. The problem is that in general, there is no way for a mock object to know how many arguments are passed to the variadic method, and what the arguments' types are. Only the author of the base class knows the protocol, and we cannot look into his or her head. Therefore, to mock such a function, the user must teach the mock object how to figure out the number of arguments and their types. One way to do it is to provide overloaded versions of the function. Ellipsis arguments are inherited from C and not really a C++ feature. They are unsafe to use and don't work with arguments that have constructors or destructors. Therefore we recommend to avoid them in C++ as much as possible.","title":"Can I mock a variadic function?"},{"location":"examples/gtest/docs/gmock_faq.html#msvc-gives-me-warning-c4301-or-c4373-when-i-define-a-mock-method-with-a-const-parameter-why","text":"If you compile this using Microsoft Visual C++ 2005 SP1: class Foo { ... virtual void Bar ( const int i ) = 0 ; }; class MockFoo : public Foo { ... MOCK_METHOD ( void , Bar , ( const int i ), ( override )); }; You may get the following warning: warning C4301: 'MockFoo::Bar' : overriding virtual function only differs from 'Foo::Bar' by const/volatile qualifier This is a MSVC bug. The same code compiles fine with gcc, for example. If you use Visual C++ 2008 SP1, you would get the warning: warning C4373: 'MockFoo::Bar' : virtual function overrides 'Foo::Bar' , previous versions of the compiler did not override when parameters only differed by const/volatile qualifiers In C++, if you declare a function with a const parameter, the const modifier is ignored. Therefore, the Foo base class above is equivalent to: class Foo { ... virtual void Bar ( int i ) = 0 ; // int or const int? Makes no difference. }; In fact, you can declare Bar() with an int parameter, and define it with a const int parameter. The compiler will still match them up. Since making a parameter const is meaningless in the method declaration, we recommend to remove it in both Foo and MockFoo . That should workaround the VC bug. Note that we are talking about the top-level const modifier here. If the function parameter is passed by pointer or reference, declaring the pointee or referee as const is still meaningful. For example, the following two declarations are not equivalent: void Bar ( int * p ); // Neither p nor *p is const. void Bar ( const int * p ); // p is not const, but *p is.","title":"MSVC gives me warning C4301 or C4373 when I define a mock method with a const parameter. Why?"},{"location":"examples/gtest/docs/gmock_faq.html#i-cant-figure-out-why-gmock-thinks-my-expectations-are-not-satisfied-what-should-i-do","text":"You might want to run your test with --gmock_verbose=info . This flag lets gMock print a trace of every mock function call it receives. By studying the trace, you'll gain insights on why the expectations you set are not met. If you see the message \"The mock function has no default action set, and its return type has no default value set.\", then try adding a default action . Due to a known issue, unexpected calls on mocks without default actions don't print out a detailed comparison between the actual arguments and the expected arguments.","title":"I can't figure out why gMock thinks my expectations are not satisfied. What should I do?"},{"location":"examples/gtest/docs/gmock_faq.html#my-program-crashed-and-scopedmocklog-spit-out-tons-of-messages-is-it-a-gmock-bug","text":"gMock and ScopedMockLog are likely doing the right thing here. When a test crashes, the failure signal handler will try to log a lot of information (the stack trace, and the address map, for example). The messages are compounded if you have many threads with depth stacks. When ScopedMockLog intercepts these messages and finds that they don't match any expectations, it prints an error for each of them. You can learn to ignore the errors, or you can rewrite your expectations to make your test more robust, for example, by adding something like: using :: testing :: AnyNumber ; using :: testing :: Not ; ... // Ignores any log not done by us. EXPECT_CALL ( log , Log ( _ , Not ( EndsWith ( \"/my_file.cc\" )), _ )) . Times ( AnyNumber ());","title":"My program crashed and ScopedMockLog spit out tons of messages. Is it a gMock bug?"},{"location":"examples/gtest/docs/gmock_faq.html#how-can-i-assert-that-a-function-is-never-called","text":"using :: testing :: _ ; ... EXPECT_CALL ( foo , Bar ( _ )) . Times ( 0 );","title":"How can I assert that a function is NEVER called?"},{"location":"examples/gtest/docs/gmock_faq.html#i-have-a-failed-test-where-gmock-tells-me-twice-that-a-particular-expectation-is-not-satisfied-isnt-this-redundant","text":"When gMock detects a failure, it prints relevant information (the mock function arguments, the state of relevant expectations, and etc) to help the user debug. If another failure is detected, gMock will do the same, including printing the state of relevant expectations. Sometimes an expectation's state didn't change between two failures, and you'll see the same description of the state twice. They are however not redundant, as they refer to different points in time . The fact they are the same is interesting information.","title":"I have a failed test where gMock tells me TWICE that a particular expectation is not satisfied. Isn't this redundant?"},{"location":"examples/gtest/docs/gmock_faq.html#i-get-a-heapcheck-failure-when-using-a-mock-object-but-using-a-real-object-is-fine-what-can-be-wrong","text":"Does the class (hopefully a pure interface) you are mocking have a virtual destructor? Whenever you derive from a base class, make sure its destructor is virtual. Otherwise Bad Things will happen. Consider the following code: class Base { public : // Not virtual, but should be. ~ Base () { ... } ... }; class Derived : public Base { public : ... private : std :: string value_ ; }; ... Base * p = new Derived ; ... delete p ; // Surprise! ~Base() will be called, but ~Derived() will not // - value_ is leaked. By changing ~Base() to virtual, ~Derived() will be correctly called when delete p is executed, and the heap checker will be happy.","title":"I get a heapcheck failure when using a mock object, but using a real object is fine. What can be wrong?"},{"location":"examples/gtest/docs/gmock_faq.html#the-newer-expectations-override-older-ones-rule-makes-writing-expectations-awkward-why-does-gmock-do-that","text":"When people complain about this, often they are referring to code like: using :: testing :: Return ; ... // foo.Bar() should be called twice, return 1 the first time, and return // 2 the second time. However, I have to write the expectations in the // reverse order. This sucks big time!!! EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 2 )) . RetiresOnSaturation (); EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 1 )) . RetiresOnSaturation (); The problem, is that they didn't pick the best way to express the test's intent. By default, expectations don't have to be matched in any particular order. If you want them to match in a certain order, you need to be explicit. This is gMock's (and jMock's) fundamental philosophy: it's easy to accidentally over-specify your tests, and we want to make it harder to do so. There are two better ways to write the test spec. You could either put the expectations in sequence: using :: testing :: Return ; ... // foo.Bar() should be called twice, return 1 the first time, and return // 2 the second time. Using a sequence, we can write the expectations // in their natural order. { InSequence s ; EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 1 )) . RetiresOnSaturation (); EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 2 )) . RetiresOnSaturation (); } or you can put the sequence of actions in the same expectation: using :: testing :: Return ; ... // foo.Bar() should be called twice, return 1 the first time, and return // 2 the second time. EXPECT_CALL ( foo , Bar ()) . WillOnce ( Return ( 1 )) . WillOnce ( Return ( 2 )) . RetiresOnSaturation (); Back to the original questions: why does gMock search the expectations (and ON_CALL s) from back to front? Because this allows a user to set up a mock's behavior for the common case early (e.g. in the mock's constructor or the test fixture's set-up phase) and customize it with more specific rules later. If gMock searches from front to back, this very useful pattern won't be possible.","title":"The \"newer expectations override older ones\" rule makes writing expectations awkward. Why does gMock do that?"},{"location":"examples/gtest/docs/gmock_faq.html#gmock-prints-a-warning-when-a-function-without-expect_call-is-called-even-if-i-have-set-its-behavior-using-on_call-would-it-be-reasonable-not-to-show-the-warning-in-this-case","text":"When choosing between being neat and being safe, we lean toward the latter. So the answer is that we think it's better to show the warning. Often people write ON_CALL s in the mock object's constructor or SetUp() , as the default behavior rarely changes from test to test. Then in the test body they set the expectations, which are often different for each test. Having an ON_CALL in the set-up part of a test doesn't mean that the calls are expected. If there's no EXPECT_CALL and the method is called, it's possibly an error. If we quietly let the call go through without notifying the user, bugs may creep in unnoticed. If, however, you are sure that the calls are OK, you can write using :: testing :: _ ; ... EXPECT_CALL ( foo , Bar ( _ )) . WillRepeatedly (...); instead of using :: testing :: _ ; ... ON_CALL ( foo , Bar ( _ )) . WillByDefault (...); This tells gMock that you do expect the calls and no warning should be printed. Also, you can control the verbosity by specifying --gmock_verbose=error . Other values are info and warning . If you find the output too noisy when debugging, just choose a less verbose level.","title":"gMock prints a warning when a function without EXPECT_CALL is called, even if I have set its behavior using ON_CALL. Would it be reasonable not to show the warning in this case?"},{"location":"examples/gtest/docs/gmock_faq.html#how-can-i-delete-the-mock-functions-argument-in-an-action","text":"If your mock function takes a pointer argument and you want to delete that argument, you can use testing::DeleteArg () to delete the N'th (zero-indexed) argument: using :: testing :: _ ; ... MOCK_METHOD ( void , Bar , ( X * x , const Y & y )); ... EXPECT_CALL ( mock_foo_ , Bar ( _ , _ )) . WillOnce ( testing :: DeleteArg < 0 > ()));","title":"How can I delete the mock function's argument in an action?"},{"location":"examples/gtest/docs/gmock_faq.html#how-can-i-perform-an-arbitrary-action-on-a-mock-functions-argument","text":"If you find yourself needing to perform some action that's not supported by gMock directly, remember that you can define your own actions using MakeAction() or MakePolymorphicAction() , or you can write a stub function and invoke it using Invoke() . using :: testing :: _ ; using :: testing :: Invoke ; ... MOCK_METHOD ( void , Bar , ( X * p )); ... EXPECT_CALL ( mock_foo_ , Bar ( _ )) . WillOnce ( Invoke ( MyAction (...)));","title":"How can I perform an arbitrary action on a mock function's argument?"},{"location":"examples/gtest/docs/gmock_faq.html#my-code-calls-a-staticglobal-function-can-i-mock-it","text":"You can, but you need to make some changes. In general, if you find yourself needing to mock a static function, it's a sign that your modules are too tightly coupled (and less flexible, less reusable, less testable, etc). You are probably better off defining a small interface and call the function through that interface, which then can be easily mocked. It's a bit of work initially, but usually pays for itself quickly. This Google Testing Blog post says it excellently. Check it out.","title":"My code calls a static/global function. Can I mock it?"},{"location":"examples/gtest/docs/gmock_faq.html#my-mock-object-needs-to-do-complex-stuff-its-a-lot-of-pain-to-specify-the-actions-gmock-sucks","text":"I know it's not a question, but you get an answer for free any way. :-) With gMock, you can create mocks in C++ easily. And people might be tempted to use them everywhere. Sometimes they work great, and sometimes you may find them, well, a pain to use. So, what's wrong in the latter case? When you write a test without using mocks, you exercise the code and assert that it returns the correct value or that the system is in an expected state. This is sometimes called \"state-based testing\". Mocks are great for what some call \"interaction-based\" testing: instead of checking the system state at the very end, mock objects verify that they are invoked the right way and report an error as soon as it arises, giving you a handle on the precise context in which the error was triggered. This is often more effective and economical to do than state-based testing. If you are doing state-based testing and using a test double just to simulate the real object, you are probably better off using a fake. Using a mock in this case causes pain, as it's not a strong point for mocks to perform complex actions. If you experience this and think that mocks suck, you are just not using the right tool for your problem. Or, you might be trying to solve the wrong problem. :-)","title":"My mock object needs to do complex stuff. It's a lot of pain to specify the actions. gMock sucks!"},{"location":"examples/gtest/docs/gmock_faq.html#i-got-a-warning-uninteresting-function-call-encountered-default-action-taken-should-i-panic","text":"By all means, NO! It's just an FYI. :-) What it means is that you have a mock function, you haven't set any expectations on it (by gMock's rule this means that you are not interested in calls to this function and therefore it can be called any number of times), and it is called. That's OK - you didn't say it's not OK to call the function! What if you actually meant to disallow this function to be called, but forgot to write EXPECT_CALL(foo, Bar()).Times(0) ? While one can argue that it's the user's fault, gMock tries to be nice and prints you a note. So, when you see the message and believe that there shouldn't be any uninteresting calls, you should investigate what's going on. To make your life easier, gMock dumps the stack trace when an uninteresting call is encountered. From that you can figure out which mock function it is, and how it is called.","title":"I got a warning \"Uninteresting function call encountered - default action taken..\" Should I panic?"},{"location":"examples/gtest/docs/gmock_faq.html#i-want-to-define-a-custom-action-should-i-use-invoke-or-implement-the-actioninterface-interface","text":"Either way is fine - you want to choose the one that's more convenient for your circumstance. Usually, if your action is for a particular function type, defining it using Invoke() should be easier; if your action can be used in functions of different types (e.g. if you are defining Return(*value*) ), MakePolymorphicAction() is easiest. Sometimes you want precise control on what types of functions the action can be used in, and implementing ActionInterface is the way to go here. See the implementation of Return() in testing/base/public/gmock-actions.h for an example.","title":"I want to define a custom action. Should I use Invoke() or implement the ActionInterface interface?"},{"location":"examples/gtest/docs/gmock_faq.html#i-use-setargpointee-in-willonce-but-gcc-complains-about-conflicting-return-type-specified-what-does-it-mean","text":"You got this error as gMock has no idea what value it should return when the mock method is called. SetArgPointee() says what the side effect is, but doesn't say what the return value should be. You need DoAll() to chain a SetArgPointee() with a Return() that provides a value appropriate to the API being mocked. See this recipe for more details and an example.","title":"I use SetArgPointee() in WillOnce(), but gcc complains about \"conflicting return type specified\". What does it mean?"},{"location":"examples/gtest/docs/gmock_faq.html#i-have-a-huge-mock-class-and-microsoft-visual-c-runs-out-of-memory-when-compiling-it-what-can-i-do","text":"We've noticed that when the /clr compiler flag is used, Visual C++ uses 5~6 times as much memory when compiling a mock class. We suggest to avoid /clr when compiling native C++ mocks.","title":"I have a huge mock class, and Microsoft Visual C++ runs out of memory when compiling it. What can I do?"},{"location":"examples/gtest/docs/primer.html","text":"Googletest Primer Introduction: Why googletest? googletest helps you write better C++ tests. googletest is a testing framework developed by the Testing Technology team with Google's specific requirements and constraints in mind. Whether you work on Linux, Windows, or a Mac, if you write C++ code, googletest can help you. And it supports any kind of tests, not just unit tests. So what makes a good test, and how does googletest fit in? We believe: Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. googletest isolates the tests by running each of them on a different object. When a test fails, googletest allows you to run it in isolation for quick debugging. Tests should be well organized and reflect the structure of the tested code. googletest groups related tests into test suites that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. Tests should be portable and reusable . Google has a lot of code that is platform-neutral; its tests should also be platform-neutral. googletest works on different OSes, with different compilers, with or without exceptions, so googletest tests can work with a variety of configurations. When tests fail, they should provide as much information about the problem as possible. googletest doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . googletest automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. Tests should be fast . With googletest, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since googletest is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go! Beware of the nomenclature Note: There might be some confusion arising from different definitions of the terms Test , Test Case and Test Suite , so beware of misunderstanding these. Historically, googletest started to use the term Test Case for grouping related tests, whereas current publications, including International Software Testing Qualifications Board ( ISTQB ) materials and various textbooks on software quality, use the term Test Suite for this. The related term Test , as it is used in googletest, corresponds to the term Test Case of ISTQB and others. The term Test is commonly of broad enough sense, including ISTQB's definition of Test Case , so it's not much of a problem here. But the term Test Case as was used in Google Test is of contradictory sense and thus confusing. googletest recently started replacing the term Test Case with Test Suite . The preferred API is TestSuite . The older TestCase API is being slowly deprecated and refactored away. So please be aware of the different definitions of the terms: Meaning googletest Term ISTQB Term Exercise a particular program path with specific input values and verify the results TEST() Test Case Basic Concepts When using googletest, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test suite contains one or many tests. You should group your tests into test suites that reflect the structure of the tested code. When multiple tests in a test suite need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test suites. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test suites. Assertions googletest assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, googletest prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to googletest's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failure to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator or a sequence of such operators. An example: ASSERT_EQ ( x . size (), y . size ()) << \"Vectors x and y are of unequal length\" ; for ( int i = 0 ; i < x . size (); ++ i ) { EXPECT_EQ ( x [ i ], y [ i ]) << \"Vectors x and y differ at index \" << i ; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed. Basic Assertions These assertions do basic true/false condition testing. Fatal assertion Nonfatal assertion Verifies ASSERT_TRUE(condition); EXPECT_TRUE(condition); condition is true ASSERT_FALSE(condition); EXPECT_FALSE(condition); condition is false Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac. Binary Comparison This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ(val1, val2); EXPECT_EQ(val1, val2); val1 == val2 ASSERT_NE(val1, val2); EXPECT_NE(val1, val2); val1 != val2 ASSERT_LT(val1, val2); EXPECT_LT(val1, val2); val1 < val2 ASSERT_LE(val1, val2); EXPECT_LE(val1, val2); val1 <= val2 ASSERT_GT(val1, val2); EXPECT_GT(val1, val2); val1 > val2 ASSERT_GE(val1, val2); EXPECT_GE(val1, val2); val1 >= val2 Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. We used to require the arguments to support the << operator for streaming to an ostream , but this is no longer necessary. If << is supported, it will be called to print the arguments when the assertion fails; otherwise googletest will attempt to print them in the best way it can. For more details and how to customize the printing of the arguments, see the documentation . These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g., == or < ). Since this is discouraged by the Google C++ Style Guide , you may need to use ASSERT_TRUE() or EXPECT_TRUE() to assert the equality of two objects of a user-defined type. However, when possible, ASSERT_EQ(actual, expected) is preferred to ASSERT_TRUE(actual == expected) , since it tells you actual and expected 's values on failure. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e., the compiler is free to choose any order), and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(c_string, NULL) . Consider using ASSERT_EQ(c_string, nullptr) if c++11 is supported. To compare two string objects, you should use ASSERT_EQ . When doing pointer comparisons use *_EQ(ptr, nullptr) and *_NE(ptr, nullptr) instead of *_EQ(ptr, NULL) and *_NE(ptr, NULL) . This is because nullptr is typed, while NULL is not. See the FAQ for more details. If you're working with floating point numbers, you may want to use the floating point variations of some of these macros in order to avoid problems caused by rounding. See Advanced googletest Topics for details. Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac. Historical note : Before February 2016 *_EQ had a convention of calling it as ASSERT_EQ(expected, actual) , so lots of existing code uses this order. Now *_EQ treats both parameters in the same way. String Comparison The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ(str1,str2); EXPECT_STREQ(str1,str2); the two C strings have the same content ASSERT_STRNE(str1,str2); EXPECT_STRNE(str1,str2); the two C strings have different contents ASSERT_STRCASEEQ(str1,str2); EXPECT_STRCASEEQ(str1,str2); the two C strings have the same content, ignoring case ASSERT_STRCASENE(str1,str2); EXPECT_STRCASENE(str1,str2); the two C strings have different contents, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. A NULL pointer and an empty string are considered different . *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. Availability : Linux, Windows, Mac. See also : For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see this in the Advanced googletest Guide. Simple Tests To create a test: Use the TEST() macro to define and name a test function. These are ordinary C++ functions that don't return a value. In this function, along with any valid C++ statements you want to include, use the various googletest assertions to check values. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST ( TestSuiteName , TestName ) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test suite, and the second argument is the test's name within the test case. Both names must be valid C++ identifiers, and they should not contain any underscores ( _ ). A test's full name consists of its containing test suite and its individual name. Tests from different test suites can have the same individual name. For example, let's take a simple integer function: int Factorial ( int n ); // Returns the factorial of n A test suite for this function might look like: // Tests factorial of 0. TEST ( FactorialTest , HandlesZeroInput ) { EXPECT_EQ ( Factorial ( 0 ), 1 ); } // Tests factorial of positive numbers. TEST ( FactorialTest , HandlesPositiveInput ) { EXPECT_EQ ( Factorial ( 1 ), 1 ); EXPECT_EQ ( Factorial ( 2 ), 2 ); EXPECT_EQ ( Factorial ( 3 ), 6 ); EXPECT_EQ ( Factorial ( 8 ), 40320 ); } googletest groups the test results by test suites, so logically related tests should be in the same test suite; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test suite FactorialTest . When naming your test suites and tests, you should follow the same convention as for naming functions and classes . Availability : Linux, Windows, Mac. Test Fixtures: Using the Same Data Configuration for Multiple Tests If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . This allows you to reuse the same configuration of objects for several different tests. To create a fixture: Derive a class from ::testing::Test . Start its body with protected: , as we'll want to access fixture members from sub-classes. Inside the class, declare any objects you plan to use. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - Use override in C++11 to make sure you spelled it correctly. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read the FAQ . If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F ( TestFixtureName , TestName ) { ... test body ... } Like TEST() , the first argument is the test suite name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , googletest will create a fresh test fixture at runtime, immediately initialize it via SetUp() , run the test, clean up by calling TearDown() , and then delete the test fixture. Note that different tests in the same test suite have different test fixture objects, and googletest always deletes a test fixture before it creates the next one. googletest does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template < typename E > // E is the element type. class Queue { public : Queue (); void Enqueue ( const E & element ); E * Dequeue (); // Returns NULL if the queue is empty. size_t size () const ; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public :: testing :: Test { protected : void SetUp () override { q1_ . Enqueue ( 1 ); q2_ . Enqueue ( 2 ); q2_ . Enqueue ( 3 ); } // void TearDown() override {} Queue < int > q0_ ; Queue < int > q1_ ; Queue < int > q2_ ; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F ( QueueTest , IsEmptyInitially ) { EXPECT_EQ ( q0_ . size (), 0 ); } TEST_F ( QueueTest , DequeueWorks ) { int * n = q0_ . Dequeue (); EXPECT_EQ ( n , nullptr ); n = q1_ . Dequeue (); ASSERT_NE ( n , nullptr ); EXPECT_EQ ( * n , 1 ); EXPECT_EQ ( q1_ . size (), 0 ); delete n ; n = q2_ . Dequeue (); ASSERT_NE ( n , nullptr ); EXPECT_EQ ( * n , 2 ); EXPECT_EQ ( q2_ . size (), 1 ); delete n ; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_NE(nullptr, n) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: googletest constructs a QueueTest object (let's call it t1 ). t1.SetUp() initializes t1 . The first test ( IsEmptyInitially ) runs on t1 . t1.TearDown() cleans up after the test finishes. t1 is destructed. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac. Invoking the Tests TEST() and TEST_F() implicitly register their tests with googletest. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit--they can be from different test suites, or even different source files. When invoked, the RUN_ALL_TESTS() macro: Saves the state of all googletest flags. Creates a test fixture object for the first test. Initializes it via SetUp() . Runs the test on the fixture object. Cleans up the fixture via TearDown() . Deletes the fixture. Restores the state of all googletest flags. Repeats the above steps for the next test, until all tests have run. If a fatal failure happens the subsequent steps will be skipped. IMPORTANT: You must not ignore the return value of RUN_ALL_TESTS() , or you will get a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced googletest features (e.g., thread-safe death tests ) and thus is not supported. Availability : Linux, Windows, Mac. Writing the main() Function Write your own main() function, which should return the value of RUN_ALL_TESTS() . You can start from this boilerplate: #include \"this/package/foo.h\" #include \"gtest/gtest.h\" namespace { // The fixture for testing class Foo. class FooTest : public :: testing :: Test { protected : // You can remove any or all of the following functions if its body // is empty. FooTest () { // You can do set-up work for each test here. } ~ FooTest () override { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: void SetUp () override { // Code here will be called immediately after the constructor (right // before each test). } void TearDown () override { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test suite for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F ( FooTest , MethodBarDoesAbc ) { const std :: string input_filepath = \"this/package/testdata/myinputfile.dat\" ; const std :: string output_filepath = \"this/package/testdata/myoutputfile.dat\" ; Foo f ; EXPECT_EQ ( f . Bar ( input_filepath , output_filepath ), 0 ); } // Tests that Foo does Xyz. TEST_F ( FooTest , DoesXyz ) { // Exercises the Xyz feature of Foo. } } // namespace int main ( int argc , char ** argv ) { :: testing :: InitGoogleTest ( & argc , argv ); return RUN_ALL_TESTS (); } The ::testing::InitGoogleTest() function parses the command line for googletest flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in the AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely, and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go. NOTE: ParseGUnitFlags() is deprecated in favor of InitGoogleTest() . Known Limitations Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"Googletest Primer"},{"location":"examples/gtest/docs/primer.html#googletest-primer","text":"","title":"Googletest Primer"},{"location":"examples/gtest/docs/primer.html#introduction-why-googletest","text":"googletest helps you write better C++ tests. googletest is a testing framework developed by the Testing Technology team with Google's specific requirements and constraints in mind. Whether you work on Linux, Windows, or a Mac, if you write C++ code, googletest can help you. And it supports any kind of tests, not just unit tests. So what makes a good test, and how does googletest fit in? We believe: Tests should be independent and repeatable . It's a pain to debug a test that succeeds or fails as a result of other tests. googletest isolates the tests by running each of them on a different object. When a test fails, googletest allows you to run it in isolation for quick debugging. Tests should be well organized and reflect the structure of the tested code. googletest groups related tests into test suites that can share data and subroutines. This common pattern is easy to recognize and makes tests easy to maintain. Such consistency is especially helpful when people switch projects and start to work on a new code base. Tests should be portable and reusable . Google has a lot of code that is platform-neutral; its tests should also be platform-neutral. googletest works on different OSes, with different compilers, with or without exceptions, so googletest tests can work with a variety of configurations. When tests fail, they should provide as much information about the problem as possible. googletest doesn't stop at the first test failure. Instead, it only stops the current test and continues with the next. You can also set up tests that report non-fatal failures after which the current test continues. Thus, you can detect and fix multiple bugs in a single run-edit-compile cycle. The testing framework should liberate test writers from housekeeping chores and let them focus on the test content . googletest automatically keeps track of all tests defined, and doesn't require the user to enumerate them in order to run them. Tests should be fast . With googletest, you can reuse shared resources across tests and pay for the set-up/tear-down only once, without making tests depend on each other. Since googletest is based on the popular xUnit architecture, you'll feel right at home if you've used JUnit or PyUnit before. If not, it will take you about 10 minutes to learn the basics and get started. So let's go!","title":"Introduction: Why googletest?"},{"location":"examples/gtest/docs/primer.html#beware-of-the-nomenclature","text":"Note: There might be some confusion arising from different definitions of the terms Test , Test Case and Test Suite , so beware of misunderstanding these. Historically, googletest started to use the term Test Case for grouping related tests, whereas current publications, including International Software Testing Qualifications Board ( ISTQB ) materials and various textbooks on software quality, use the term Test Suite for this. The related term Test , as it is used in googletest, corresponds to the term Test Case of ISTQB and others. The term Test is commonly of broad enough sense, including ISTQB's definition of Test Case , so it's not much of a problem here. But the term Test Case as was used in Google Test is of contradictory sense and thus confusing. googletest recently started replacing the term Test Case with Test Suite . The preferred API is TestSuite . The older TestCase API is being slowly deprecated and refactored away. So please be aware of the different definitions of the terms: Meaning googletest Term ISTQB Term Exercise a particular program path with specific input values and verify the results TEST() Test Case","title":"Beware of the nomenclature"},{"location":"examples/gtest/docs/primer.html#basic-concepts","text":"When using googletest, you start by writing assertions , which are statements that check whether a condition is true. An assertion's result can be success , nonfatal failure , or fatal failure . If a fatal failure occurs, it aborts the current function; otherwise the program continues normally. Tests use assertions to verify the tested code's behavior. If a test crashes or has a failed assertion, then it fails ; otherwise it succeeds . A test suite contains one or many tests. You should group your tests into test suites that reflect the structure of the tested code. When multiple tests in a test suite need to share common objects and subroutines, you can put them into a test fixture class. A test program can contain multiple test suites. We'll now explain how to write a test program, starting at the individual assertion level and building up to tests and test suites.","title":"Basic Concepts"},{"location":"examples/gtest/docs/primer.html#assertions","text":"googletest assertions are macros that resemble function calls. You test a class or function by making assertions about its behavior. When an assertion fails, googletest prints the assertion's source file and line number location, along with a failure message. You may also supply a custom failure message which will be appended to googletest's message. The assertions come in pairs that test the same thing but have different effects on the current function. ASSERT_* versions generate fatal failures when they fail, and abort the current function . EXPECT_* versions generate nonfatal failures, which don't abort the current function. Usually EXPECT_* are preferred, as they allow more than one failure to be reported in a test. However, you should use ASSERT_* if it doesn't make sense to continue when the assertion in question fails. Since a failed ASSERT_* returns from the current function immediately, possibly skipping clean-up code that comes after it, it may cause a space leak. Depending on the nature of the leak, it may or may not be worth fixing - so keep this in mind if you get a heap checker error in addition to assertion errors. To provide a custom failure message, simply stream it into the macro using the << operator or a sequence of such operators. An example: ASSERT_EQ ( x . size (), y . size ()) << \"Vectors x and y are of unequal length\" ; for ( int i = 0 ; i < x . size (); ++ i ) { EXPECT_EQ ( x [ i ], y [ i ]) << \"Vectors x and y differ at index \" << i ; } Anything that can be streamed to an ostream can be streamed to an assertion macro--in particular, C strings and string objects. If a wide string ( wchar_t* , TCHAR* in UNICODE mode on Windows, or std::wstring ) is streamed to an assertion, it will be translated to UTF-8 when printed.","title":"Assertions"},{"location":"examples/gtest/docs/primer.html#basic-assertions","text":"These assertions do basic true/false condition testing. Fatal assertion Nonfatal assertion Verifies ASSERT_TRUE(condition); EXPECT_TRUE(condition); condition is true ASSERT_FALSE(condition); EXPECT_FALSE(condition); condition is false Remember, when they fail, ASSERT_* yields a fatal failure and returns from the current function, while EXPECT_* yields a nonfatal failure, allowing the function to continue running. In either case, an assertion failure means its containing test fails. Availability : Linux, Windows, Mac.","title":"Basic Assertions"},{"location":"examples/gtest/docs/primer.html#binary-comparison","text":"This section describes assertions that compare two values. Fatal assertion Nonfatal assertion Verifies ASSERT_EQ(val1, val2); EXPECT_EQ(val1, val2); val1 == val2 ASSERT_NE(val1, val2); EXPECT_NE(val1, val2); val1 != val2 ASSERT_LT(val1, val2); EXPECT_LT(val1, val2); val1 < val2 ASSERT_LE(val1, val2); EXPECT_LE(val1, val2); val1 <= val2 ASSERT_GT(val1, val2); EXPECT_GT(val1, val2); val1 > val2 ASSERT_GE(val1, val2); EXPECT_GE(val1, val2); val1 >= val2 Value arguments must be comparable by the assertion's comparison operator or you'll get a compiler error. We used to require the arguments to support the << operator for streaming to an ostream , but this is no longer necessary. If << is supported, it will be called to print the arguments when the assertion fails; otherwise googletest will attempt to print them in the best way it can. For more details and how to customize the printing of the arguments, see the documentation . These assertions can work with a user-defined type, but only if you define the corresponding comparison operator (e.g., == or < ). Since this is discouraged by the Google C++ Style Guide , you may need to use ASSERT_TRUE() or EXPECT_TRUE() to assert the equality of two objects of a user-defined type. However, when possible, ASSERT_EQ(actual, expected) is preferred to ASSERT_TRUE(actual == expected) , since it tells you actual and expected 's values on failure. Arguments are always evaluated exactly once. Therefore, it's OK for the arguments to have side effects. However, as with any ordinary C/C++ function, the arguments' evaluation order is undefined (i.e., the compiler is free to choose any order), and your code should not depend on any particular argument evaluation order. ASSERT_EQ() does pointer equality on pointers. If used on two C strings, it tests if they are in the same memory location, not if they have the same value. Therefore, if you want to compare C strings (e.g. const char* ) by value, use ASSERT_STREQ() , which will be described later on. In particular, to assert that a C string is NULL , use ASSERT_STREQ(c_string, NULL) . Consider using ASSERT_EQ(c_string, nullptr) if c++11 is supported. To compare two string objects, you should use ASSERT_EQ . When doing pointer comparisons use *_EQ(ptr, nullptr) and *_NE(ptr, nullptr) instead of *_EQ(ptr, NULL) and *_NE(ptr, NULL) . This is because nullptr is typed, while NULL is not. See the FAQ for more details. If you're working with floating point numbers, you may want to use the floating point variations of some of these macros in order to avoid problems caused by rounding. See Advanced googletest Topics for details. Macros in this section work with both narrow and wide string objects ( string and wstring ). Availability : Linux, Windows, Mac. Historical note : Before February 2016 *_EQ had a convention of calling it as ASSERT_EQ(expected, actual) , so lots of existing code uses this order. Now *_EQ treats both parameters in the same way.","title":"Binary Comparison"},{"location":"examples/gtest/docs/primer.html#string-comparison","text":"The assertions in this group compare two C strings . If you want to compare two string objects, use EXPECT_EQ , EXPECT_NE , and etc instead. Fatal assertion Nonfatal assertion Verifies ASSERT_STREQ(str1,str2); EXPECT_STREQ(str1,str2); the two C strings have the same content ASSERT_STRNE(str1,str2); EXPECT_STRNE(str1,str2); the two C strings have different contents ASSERT_STRCASEEQ(str1,str2); EXPECT_STRCASEEQ(str1,str2); the two C strings have the same content, ignoring case ASSERT_STRCASENE(str1,str2); EXPECT_STRCASENE(str1,str2); the two C strings have different contents, ignoring case Note that \"CASE\" in an assertion name means that case is ignored. A NULL pointer and an empty string are considered different . *STREQ* and *STRNE* also accept wide C strings ( wchar_t* ). If a comparison of two wide strings fails, their values will be printed as UTF-8 narrow strings. Availability : Linux, Windows, Mac. See also : For more string comparison tricks (substring, prefix, suffix, and regular expression matching, for example), see this in the Advanced googletest Guide.","title":"String Comparison"},{"location":"examples/gtest/docs/primer.html#simple-tests","text":"To create a test: Use the TEST() macro to define and name a test function. These are ordinary C++ functions that don't return a value. In this function, along with any valid C++ statements you want to include, use the various googletest assertions to check values. The test's result is determined by the assertions; if any assertion in the test fails (either fatally or non-fatally), or if the test crashes, the entire test fails. Otherwise, it succeeds. TEST ( TestSuiteName , TestName ) { ... test body ... } TEST() arguments go from general to specific. The first argument is the name of the test suite, and the second argument is the test's name within the test case. Both names must be valid C++ identifiers, and they should not contain any underscores ( _ ). A test's full name consists of its containing test suite and its individual name. Tests from different test suites can have the same individual name. For example, let's take a simple integer function: int Factorial ( int n ); // Returns the factorial of n A test suite for this function might look like: // Tests factorial of 0. TEST ( FactorialTest , HandlesZeroInput ) { EXPECT_EQ ( Factorial ( 0 ), 1 ); } // Tests factorial of positive numbers. TEST ( FactorialTest , HandlesPositiveInput ) { EXPECT_EQ ( Factorial ( 1 ), 1 ); EXPECT_EQ ( Factorial ( 2 ), 2 ); EXPECT_EQ ( Factorial ( 3 ), 6 ); EXPECT_EQ ( Factorial ( 8 ), 40320 ); } googletest groups the test results by test suites, so logically related tests should be in the same test suite; in other words, the first argument to their TEST() should be the same. In the above example, we have two tests, HandlesZeroInput and HandlesPositiveInput , that belong to the same test suite FactorialTest . When naming your test suites and tests, you should follow the same convention as for naming functions and classes . Availability : Linux, Windows, Mac.","title":"Simple Tests"},{"location":"examples/gtest/docs/primer.html#same-data-multiple-tests","text":"If you find yourself writing two or more tests that operate on similar data, you can use a test fixture . This allows you to reuse the same configuration of objects for several different tests. To create a fixture: Derive a class from ::testing::Test . Start its body with protected: , as we'll want to access fixture members from sub-classes. Inside the class, declare any objects you plan to use. If necessary, write a default constructor or SetUp() function to prepare the objects for each test. A common mistake is to spell SetUp() as Setup() with a small u - Use override in C++11 to make sure you spelled it correctly. If necessary, write a destructor or TearDown() function to release any resources you allocated in SetUp() . To learn when you should use the constructor/destructor and when you should use SetUp()/TearDown() , read the FAQ . If needed, define subroutines for your tests to share. When using a fixture, use TEST_F() instead of TEST() as it allows you to access objects and subroutines in the test fixture: TEST_F ( TestFixtureName , TestName ) { ... test body ... } Like TEST() , the first argument is the test suite name, but for TEST_F() this must be the name of the test fixture class. You've probably guessed: _F is for fixture. Unfortunately, the C++ macro system does not allow us to create a single macro that can handle both types of tests. Using the wrong macro causes a compiler error. Also, you must first define a test fixture class before using it in a TEST_F() , or you'll get the compiler error \" virtual outside class declaration \". For each test defined with TEST_F() , googletest will create a fresh test fixture at runtime, immediately initialize it via SetUp() , run the test, clean up by calling TearDown() , and then delete the test fixture. Note that different tests in the same test suite have different test fixture objects, and googletest always deletes a test fixture before it creates the next one. googletest does not reuse the same test fixture for multiple tests. Any changes one test makes to the fixture do not affect other tests. As an example, let's write tests for a FIFO queue class named Queue , which has the following interface: template < typename E > // E is the element type. class Queue { public : Queue (); void Enqueue ( const E & element ); E * Dequeue (); // Returns NULL if the queue is empty. size_t size () const ; ... }; First, define a fixture class. By convention, you should give it the name FooTest where Foo is the class being tested. class QueueTest : public :: testing :: Test { protected : void SetUp () override { q1_ . Enqueue ( 1 ); q2_ . Enqueue ( 2 ); q2_ . Enqueue ( 3 ); } // void TearDown() override {} Queue < int > q0_ ; Queue < int > q1_ ; Queue < int > q2_ ; }; In this case, TearDown() is not needed since we don't have to clean up after each test, other than what's already done by the destructor. Now we'll write tests using TEST_F() and this fixture. TEST_F ( QueueTest , IsEmptyInitially ) { EXPECT_EQ ( q0_ . size (), 0 ); } TEST_F ( QueueTest , DequeueWorks ) { int * n = q0_ . Dequeue (); EXPECT_EQ ( n , nullptr ); n = q1_ . Dequeue (); ASSERT_NE ( n , nullptr ); EXPECT_EQ ( * n , 1 ); EXPECT_EQ ( q1_ . size (), 0 ); delete n ; n = q2_ . Dequeue (); ASSERT_NE ( n , nullptr ); EXPECT_EQ ( * n , 2 ); EXPECT_EQ ( q2_ . size (), 1 ); delete n ; } The above uses both ASSERT_* and EXPECT_* assertions. The rule of thumb is to use EXPECT_* when you want the test to continue to reveal more errors after the assertion failure, and use ASSERT_* when continuing after failure doesn't make sense. For example, the second assertion in the Dequeue test is ASSERT_NE(nullptr, n) , as we need to dereference the pointer n later, which would lead to a segfault when n is NULL . When these tests run, the following happens: googletest constructs a QueueTest object (let's call it t1 ). t1.SetUp() initializes t1 . The first test ( IsEmptyInitially ) runs on t1 . t1.TearDown() cleans up after the test finishes. t1 is destructed. The above steps are repeated on another QueueTest object, this time running the DequeueWorks test. Availability : Linux, Windows, Mac.","title":"Test Fixtures: Using the Same Data Configuration for Multiple Tests"},{"location":"examples/gtest/docs/primer.html#invoking-the-tests","text":"TEST() and TEST_F() implicitly register their tests with googletest. So, unlike with many other C++ testing frameworks, you don't have to re-list all your defined tests in order to run them. After defining your tests, you can run them with RUN_ALL_TESTS() , which returns 0 if all the tests are successful, or 1 otherwise. Note that RUN_ALL_TESTS() runs all tests in your link unit--they can be from different test suites, or even different source files. When invoked, the RUN_ALL_TESTS() macro: Saves the state of all googletest flags. Creates a test fixture object for the first test. Initializes it via SetUp() . Runs the test on the fixture object. Cleans up the fixture via TearDown() . Deletes the fixture. Restores the state of all googletest flags. Repeats the above steps for the next test, until all tests have run. If a fatal failure happens the subsequent steps will be skipped. IMPORTANT: You must not ignore the return value of RUN_ALL_TESTS() , or you will get a compiler error. The rationale for this design is that the automated testing service determines whether a test has passed based on its exit code, not on its stdout/stderr output; thus your main() function must return the value of RUN_ALL_TESTS() . Also, you should call RUN_ALL_TESTS() only once . Calling it more than once conflicts with some advanced googletest features (e.g., thread-safe death tests ) and thus is not supported. Availability : Linux, Windows, Mac.","title":"Invoking the Tests"},{"location":"examples/gtest/docs/primer.html#writing-the-main-function","text":"Write your own main() function, which should return the value of RUN_ALL_TESTS() . You can start from this boilerplate: #include \"this/package/foo.h\" #include \"gtest/gtest.h\" namespace { // The fixture for testing class Foo. class FooTest : public :: testing :: Test { protected : // You can remove any or all of the following functions if its body // is empty. FooTest () { // You can do set-up work for each test here. } ~ FooTest () override { // You can do clean-up work that doesn't throw exceptions here. } // If the constructor and destructor are not enough for setting up // and cleaning up each test, you can define the following methods: void SetUp () override { // Code here will be called immediately after the constructor (right // before each test). } void TearDown () override { // Code here will be called immediately after each test (right // before the destructor). } // Objects declared here can be used by all tests in the test suite for Foo. }; // Tests that the Foo::Bar() method does Abc. TEST_F ( FooTest , MethodBarDoesAbc ) { const std :: string input_filepath = \"this/package/testdata/myinputfile.dat\" ; const std :: string output_filepath = \"this/package/testdata/myoutputfile.dat\" ; Foo f ; EXPECT_EQ ( f . Bar ( input_filepath , output_filepath ), 0 ); } // Tests that Foo does Xyz. TEST_F ( FooTest , DoesXyz ) { // Exercises the Xyz feature of Foo. } } // namespace int main ( int argc , char ** argv ) { :: testing :: InitGoogleTest ( & argc , argv ); return RUN_ALL_TESTS (); } The ::testing::InitGoogleTest() function parses the command line for googletest flags, and removes all recognized flags. This allows the user to control a test program's behavior via various flags, which we'll cover in the AdvancedGuide . You must call this function before calling RUN_ALL_TESTS() , or the flags won't be properly initialized. On Windows, InitGoogleTest() also works with wide strings, so it can be used in programs compiled in UNICODE mode as well. But maybe you think that writing all those main() functions is too much work? We agree with you completely, and that's why Google Test provides a basic implementation of main(). If it fits your needs, then just link your test with gtest_main library and you are good to go. NOTE: ParseGUnitFlags() is deprecated in favor of InitGoogleTest() .","title":"Writing the main() Function"},{"location":"examples/gtest/docs/primer.html#known-limitations","text":"Google Test is designed to be thread-safe. The implementation is thread-safe on systems where the pthreads library is available. It is currently unsafe to use Google Test assertions from two threads concurrently on other systems (e.g. Windows). In most tests this is not an issue as usually the assertions are done in the main thread. If you want to help, you can volunteer to implement the necessary synchronization primitives in gtest-port.h for your platform.","title":"Known Limitations"},{"location":"examples/gtest/docs/pump_manual.html","text":"P ump is U seful for M eta P rogramming. The Problem Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code. Our Solution Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain. Highlights The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode. Examples The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template < size_t N > class Foo0 { blah a ; }; // Foo1 does blah for 1-ary predicates. template < size_t N , typename A1 > class Foo1 { blah b ; }; // Foo2 does blah for 2-ary predicates. template < size_t N , typename A1 , typename A2 > class Foo2 { blah b ; }; // Foo3 does blah for 3-ary predicates. template < size_t N , typename A1 , typename A2 , typename A3 > class Foo3 { blah c ; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func (); // If n is 0. Func ( a1 ); // If n is 1. Func ( a1 + a2 ); // If n is 2. Func ( a1 + a2 + a3 ); // If n is 3. // And so on... Constructs We support the following meta programming constructs: | $var id = exp | Defines a named constant value. $id is | : : valid util the end of the current meta : : : lexical block. : | :------------------------------- | :--------------------------------------- | | $range id exp..exp | Sets the range of an iteration variable, | : : which can be reused in multiple loops : : : later. : | $for id sep [[ code ]] | Iteration. The range of id must have | : : been defined earlier. $id is valid in : : : code . : | $($) | Generates a single $ character. | | $id | Value of the named constant or iteration | : : variable. : | $(exp) | Value of the expression. | | $if exp [[ code ]] else_branch | Conditional. | | [[ code ]] | Meta lexical block. | | cpp_code | Raw C++ code. | | $$ comment | Meta comment. | Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output. Grammar code :: = atomic_code * atomic_code ::= $ var id = exp | $ var id = [[ code ]] | $ range id exp . . exp | $ for id sep [[ code ]] | $($) | $ id | $( exp ) | $ if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep :: = cpp_code | empty_string else_branch ::= $ else [[ code ]] | $ elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax Code You can find the source code of Pump in $HEXAGON_SDK_ROOT/utils/googletest/gtest/scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump. Real Examples You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h . Tips If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"Pump manual"},{"location":"examples/gtest/docs/pump_manual.html#the-problem","text":"Template and macro libraries often need to define many classes, functions, or macros that vary only (or almost only) in the number of arguments they take. It's a lot of repetitive, mechanical, and error-prone work. Variadic templates and variadic macros can alleviate the problem. However, while both are being considered by the C++ committee, neither is in the standard yet or widely supported by compilers. Thus they are often not a good choice, especially when your code needs to be portable. And their capabilities are still limited. As a result, authors of such libraries often have to write scripts to generate their implementation. However, our experience is that it's tedious to write such scripts, which tend to reflect the structure of the generated code poorly and are often hard to read and edit. For example, a small change needed in the generated code may require some non-intuitive, non-trivial changes in the script. This is especially painful when experimenting with the code.","title":"The Problem"},{"location":"examples/gtest/docs/pump_manual.html#our-solution","text":"Pump (for Pump is Useful for Meta Programming, Pretty Useful for Meta Programming, or Practical Utility for Meta Programming, whichever you prefer) is a simple meta-programming tool for C++. The idea is that a programmer writes a foo.pump file which contains C++ code plus meta code that manipulates the C++ code. The meta code can handle iterations over a range, nested iterations, local meta variable definitions, simple arithmetic, and conditional expressions. You can view it as a small Domain-Specific Language. The meta language is designed to be non-intrusive (s.t. it won't confuse Emacs' C++ mode, for example) and concise, making Pump code intuitive and easy to maintain.","title":"Our Solution"},{"location":"examples/gtest/docs/pump_manual.html#highlights","text":"The implementation is in a single Python script and thus ultra portable: no build or installation is needed and it works cross platforms. Pump tries to be smart with respect to Google's style guide : it breaks long lines (easy to have when they are generated) at acceptable places to fit within 80 columns and indent the continuation lines correctly. The format is human-readable and more concise than XML. The format works relatively well with Emacs' C++ mode.","title":"Highlights"},{"location":"examples/gtest/docs/pump_manual.html#examples","text":"The following Pump code (where meta keywords start with $ , [[ and ]] are meta brackets, and $$ starts a meta comment that ends with the line): $var n = 3 $$ Defines a meta variable n. $range i 0..n $$ Declares the range of meta iterator i (inclusive). $for i [[ $$ Meta loop. // Foo$i does blah for $i-ary predicates. $range j 1..i template <size_t N $for j [[, typename A$j]]> class Foo$i { $if i == 0 [[ blah a; ]] $elif i <= 2 [[ blah b; ]] $else [[ blah c; ]] }; ]] will be translated by the Pump compiler to: // Foo0 does blah for 0-ary predicates. template < size_t N > class Foo0 { blah a ; }; // Foo1 does blah for 1-ary predicates. template < size_t N , typename A1 > class Foo1 { blah b ; }; // Foo2 does blah for 2-ary predicates. template < size_t N , typename A1 , typename A2 > class Foo2 { blah b ; }; // Foo3 does blah for 3-ary predicates. template < size_t N , typename A1 , typename A2 , typename A3 > class Foo3 { blah c ; }; In another example, $range i 1..n Func($for i + [[a$i]]); $$ The text between i and [[ is the separator between iterations. will generate one of the following lines (without the comments), depending on the value of n : Func (); // If n is 0. Func ( a1 ); // If n is 1. Func ( a1 + a2 ); // If n is 2. Func ( a1 + a2 + a3 ); // If n is 3. // And so on...","title":"Examples"},{"location":"examples/gtest/docs/pump_manual.html#constructs","text":"We support the following meta programming constructs: | $var id = exp | Defines a named constant value. $id is | : : valid util the end of the current meta : : : lexical block. : | :------------------------------- | :--------------------------------------- | | $range id exp..exp | Sets the range of an iteration variable, | : : which can be reused in multiple loops : : : later. : | $for id sep [[ code ]] | Iteration. The range of id must have | : : been defined earlier. $id is valid in : : : code . : | $($) | Generates a single $ character. | | $id | Value of the named constant or iteration | : : variable. : | $(exp) | Value of the expression. | | $if exp [[ code ]] else_branch | Conditional. | | [[ code ]] | Meta lexical block. | | cpp_code | Raw C++ code. | | $$ comment | Meta comment. | Note: To give the user some freedom in formatting the Pump source code, Pump ignores a new-line character if it's right after $for foo or next to [[ or ]] . Without this rule you'll often be forced to write very long lines to get the desired output. Therefore sometimes you may need to insert an extra new-line in such places for a new-line to show up in your output.","title":"Constructs"},{"location":"examples/gtest/docs/pump_manual.html#grammar","text":"code :: = atomic_code * atomic_code ::= $ var id = exp | $ var id = [[ code ]] | $ range id exp . . exp | $ for id sep [[ code ]] | $($) | $ id | $( exp ) | $ if exp [[ code ]] else_branch | [[ code ]] | cpp_code sep :: = cpp_code | empty_string else_branch ::= $ else [[ code ]] | $ elif exp [[ code ]] else_branch | empty_string exp ::= simple_expression_in_Python_syntax","title":"Grammar"},{"location":"examples/gtest/docs/pump_manual.html#code","text":"You can find the source code of Pump in $HEXAGON_SDK_ROOT/utils/googletest/gtest/scripts/pump.py . It is still very unpolished and lacks automated tests, although it has been successfully used many times. If you find a chance to use it in your project, please let us know what you think! We also welcome help on improving Pump.","title":"Code"},{"location":"examples/gtest/docs/pump_manual.html#real-examples","text":"You can find real-world applications of Pump in Google Test and Google Mock . The source file foo.h.pump generates foo.h .","title":"Real Examples"},{"location":"examples/gtest/docs/pump_manual.html#tips","text":"If a meta variable is followed by a letter or digit, you can separate them using [[]] , which inserts an empty string. For example Foo$j[[]]Helper generate Foo1Helper when j is 1. To avoid extra-long Pump source lines, you can break a line anywhere you want by inserting [[]] followed by a new line. Since any new-line character next to [[ or ]] is ignored, the generated code won't contain this new line.","title":"Tips"},{"location":"examples/gtest/docs/samples.html","text":"Googletest Samples If you're like us, you'd like to look at googletest samples. The sample directory has a number of well-commented samples showing how to use a variety of googletest features. Sample #1 shows the basic steps of using googletest to test C++ functions. Sample #2 shows a more complex unit test for a class with multiple member functions. Sample #3 uses a test fixture. Sample #4 teaches you how to use googletest and googletest.h together to get the best of both libraries. Sample #5 puts shared testing logic in a base test fixture, and reuses it in derived fixtures. Sample #6 demonstrates type-parameterized tests. Sample #7 teaches the basics of value-parameterized tests. Sample #8 shows using Combine() in value-parameterized tests. Sample #9 shows use of the listener API to modify Google Test's console output and the use of its reflection API to inspect test results. Sample #10 shows use of the listener API to implement a primitive memory leak checker.","title":"Googletest Samples {#samples}"},{"location":"examples/gtest/docs/samples.html#samples","text":"If you're like us, you'd like to look at googletest samples. The sample directory has a number of well-commented samples showing how to use a variety of googletest features. Sample #1 shows the basic steps of using googletest to test C++ functions. Sample #2 shows a more complex unit test for a class with multiple member functions. Sample #3 uses a test fixture. Sample #4 teaches you how to use googletest and googletest.h together to get the best of both libraries. Sample #5 puts shared testing logic in a base test fixture, and reuses it in derived fixtures. Sample #6 demonstrates type-parameterized tests. Sample #7 teaches the basics of value-parameterized tests. Sample #8 shows using Combine() in value-parameterized tests. Sample #9 shows use of the listener API to modify Google Test's console output and the use of its reflection API to inspect test results. Sample #10 shows use of the listener API to implement a primitive memory leak checker.","title":"Googletest Samples"},{"location":"examples/hap_example/index.html","text":"HAP example Overview The hap_example illustrates the usage of the remote APIs API and the following HAP APIs: HAP_compute_res HAP_farf HAP_mem HAP_perf HAP_power sysmon_cachelock HAP_vtcm_mgr Project structure Makefile Root makefile that invokes variant-specific min files to either build the application processor source code or the Hexagon DSP source code. android.min , hexagon.min Contains the make.d directives used to build the application processor and Hexagon DSP source code. inc/hap_example.idl IDL interface that defines the hap_example API. This IDL file is compiled by the QAIC IDL compiler into the following files: hap_example.h : C/C++ header file hap_example_stub.c : Stub source that needs to be built for the HLOS (Android, etc...) hap_example_skel.c : Skel source that needs to be built for the Hexagon DSP src_app/ : Source files for the HLOS executable that offloads the task to DSP hap_example_main.c : Contains main() function of the hap_example that runs on application processor hap_example.c : Invokes call to DSP functions that demonstrate the HAP APIs hap_unit_test.c : Invokes unit test on DSP that validates some of the HAP APIs src_dsp/ : Source files for the Hexagon-side implementation of the hap_example that is compiled into a shared object. Demonstrates usage of the HAP APIs and contains a unit test. hap_example_imp.c : Contains function to open and close handle to a cdsp domain hap_example_compute_res_imp.c : Compute resource manager framework APIs to request and release compute resources on CDSP hap_example_farf_runtime.c : FARF API to control compile-time and run-time logging hap_example_mem_imp.c : HAP_mem APIs to control mapping of buffers to user PD heap memory hap_example_perf_imp.c : HAP_perf APIs to measure the execution time without RPC overhead hap_example_power_imp.c : HAP_power APIs to vote for DSP core, bus clock frequencies and voltage corners hap_example_sysmon_cachelock_imp.c : Cache locking manager to lock a section of CDSP L2 cache and release it hap_example_vtcm_mgr_imp.c : VTCM Manager to request or release VTCM memory on CDSP hap_example_unit_test_imp.c : Unit test that validates some of the HAP APIs Building Using the walkthrough script The example comes with a walkthrough script called hap_example_walkthrough.py which builds the code and runs on target. The script must be run using the following command : python hap_example_walkthrough.py -T Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts. Using the make commands To build your code without using the walkthrough script, you will need to build both the Android and Hexagon modules. To build the Android module along with the dependencies, run the following command: make android VERBOSE=1 To build the Hexagon module, run the following command: make hexagon DSP_ARCH=v66 VERBOSE=1 For more information on the build syntax, please refer to the building reference instructions . Running on Target If you want to run your code on target without using the walkthrough script, please use the following steps: Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push the HLOS side hap_example executable and the supporting stub library to the device adb shell mkdir -p /vendor/bin/ adb push android_ReleaseG_aarch64/hap_example /vendor/bin/ adb shell chmod 777 /vendor/bin/hap_example adb push android_ReleaseG_aarch64/ship/libhap_example.so /vendor/lib64/ Push the Hexagon Shared Object to the device's file system adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_ReleaseG_toolv84_v66/ship/libhap_example_skel.so /vendor/lib/rfsa/dsp/sdk Generate a device-specific test signature based on the device's serial number Follow the steps listed in the Use signer.py section of the signing documentation. Note: This step only needs to be done once as the same test signature will enable loading any module. Redirect DSP FARF messages to ADB logcat by creating a farf file adb shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/hap_example.farf\" Please refer to the page on messaging resources which discusses the tools available for logging debug messages from the DSP. Launch a new CLI shell to view the DSP's diagnostic messages using logcat Open a new shell or command window and execute : adb logcat -s adsprpc Execute the hap_example binary as follows : adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/hap_example <0/1/2/3/4/5/6 select HAP API> The command-line argument must be an integer ranging from 0 to 5 and selects the HAP APIs to be demonstrated: 0 - HAP_compute_res.h 1 - HAP_farf.h 2 - HAP_mem.h 3 - HAP_perf.h 4 - HAP_power.h 5 - sysmon_cachelock.h 6 - HAP_vtcm_mgr.h Analyze the output The command window or shell should contain messages returned by the application processor when using the printf command. For example when we run the example on an sm8250 target : adb shell /vendor/bin/hap_example 1 demonstrates FARF run-time logging and the following output is printed in the shell: -----Retrieving VTCM information of CDSP using FastRPC Capability API----- Result of capability query for VTCM_PAGE on CDSP is 262144 bytes Result of capability query for VTCM_COUNT on CDSP is 1 -----------------------HAP API Example-------------------------------- Demonstrating FARF run-time logging hap_example function PASSED Please look at the mini-dm logs or the adb logcat logs for DSP output --------------------------HAP Unit Test------------------------------- hap_unit_test PASSED hap_example PASSED while the FARF logs redirected to the adb logcat are : HAP_utils.c:303:0x1620ea:13: Logging mask set to 31 mod_table.c:541:0x12f0d3:13: open_mod_table_open_dynamic: Module libhap_example_skel.so opened with handle 0xa52093d0 HAP_utils.c:303:0x12f0d3:13: Logging mask set to 0 HAP_utils.c:329:0x12f0d3:13: Logging enabled for file hap_example_farf_runtime.c hap_example_farf_runtime.c:67:0x12f0d3:13: Low FARF message : Run-time logging disabled hap_example_farf_runtime.c:68:0x12f0d3:13: Medium FARF message : Run-time logging disabled hap_example_farf_runtime.c:69:0x12f0d3:13: High FARF message : Run-time logging disabled hap_example_farf_runtime.c:70:0x12f0d3:13: Error FARF message : Run-time logging disabled hap_example_farf_runtime.c:71:0x12f0d3:13: Fatal FARF message : Run-time logging disabled HAP_utils.c:303:0x12f0d3:13: Logging mask set to 31 HAP_utils.c:329:0x12f0d3:13: Logging enabled for file hap_example_farf_runtime.c hap_example_farf_runtime.c:44:0x12f0d3:13: Low FARF message : Run-time logging enabled hap_example_farf_runtime.c:45:0x12f0d3:13: Medium FARF message : Run-time logging enabled hap_example_farf_runtime.c:46:0x12f0d3:13: High FARF message : Run-time logging enabled hap_example_farf_runtime.c:47:0x12f0d3:13: Error FARF message : Run-time logging enabled hap_example_farf_runtime.c:48:0x12f0d3:13: Fatal FARF message : Run-time logging enabled hap_example_farf_runtime.c:50:0x12f0d3:13: Runtime Low FARF message - Run-time logging enabled hap_example_farf_runtime.c:51:0x12f0d3:13: Runtime Medium FARF message - Run-time logging enabled hap_example_farf_runtime.c:52:0x12f0d3:13: Runtime High FARF message - Run-time logging enabled hap_example_farf_runtime.c:53:0x12f0d3:13: Runtime Error FARF message - Run-time logging enabled hap_example_farf_runtime.c:54:0x12f0d3:13: Runtime Fatal FARF message - Run-time logging enabled mod_table.c:719:0x12f0d3:13: open_mod_table_close: unloaded libhap_example_skel.so HAP_utils.c:303:0x1380dd:13: Logging mask set to 31 mod_table.c:541:0x1500e4:13: open_mod_table_open_dynamic: Module libhap_example_skel.so opened with handle 0x4c3093d0 hap_unit_test_imp.c:108:0x1500e4:13: HAP_compute_res_attr_set_vtcm_param : TEST PASSED hap_unit_test_imp.c:109:0x1500e4:13: HAP_compute_res_acquire : TEST PASSED hap_unit_test_imp.c:148:0x1500e4:13: HAP_compute_res_release : TEST PASSED hap_unit_test_imp.c:194:0x1500e4:13: HAP_mmap : TEST PASSED hap_unit_test_imp.c:231:0x1500e4:13: HAP_query_request_VTCM : TEST PASSED hap_unit_test_imp.c:266:0x1500e4:13: HAP_query_release_VTCM : TEST PASSED hap_unit_test_imp.c:304:0x1500e4:13: HAP_query_request_async_VTCM : TEST PASSED hap_unit_test_imp.c:326:0x1500e4:13: hap_example_unit_test PASSED","title":"HAP"},{"location":"examples/hap_example/index.html#hap-example","text":"","title":"HAP example"},{"location":"examples/hap_example/index.html#overview","text":"The hap_example illustrates the usage of the remote APIs API and the following HAP APIs: HAP_compute_res HAP_farf HAP_mem HAP_perf HAP_power sysmon_cachelock HAP_vtcm_mgr","title":"Overview"},{"location":"examples/hap_example/index.html#project-structure","text":"Makefile Root makefile that invokes variant-specific min files to either build the application processor source code or the Hexagon DSP source code. android.min , hexagon.min Contains the make.d directives used to build the application processor and Hexagon DSP source code. inc/hap_example.idl IDL interface that defines the hap_example API. This IDL file is compiled by the QAIC IDL compiler into the following files: hap_example.h : C/C++ header file hap_example_stub.c : Stub source that needs to be built for the HLOS (Android, etc...) hap_example_skel.c : Skel source that needs to be built for the Hexagon DSP src_app/ : Source files for the HLOS executable that offloads the task to DSP hap_example_main.c : Contains main() function of the hap_example that runs on application processor hap_example.c : Invokes call to DSP functions that demonstrate the HAP APIs hap_unit_test.c : Invokes unit test on DSP that validates some of the HAP APIs src_dsp/ : Source files for the Hexagon-side implementation of the hap_example that is compiled into a shared object. Demonstrates usage of the HAP APIs and contains a unit test. hap_example_imp.c : Contains function to open and close handle to a cdsp domain hap_example_compute_res_imp.c : Compute resource manager framework APIs to request and release compute resources on CDSP hap_example_farf_runtime.c : FARF API to control compile-time and run-time logging hap_example_mem_imp.c : HAP_mem APIs to control mapping of buffers to user PD heap memory hap_example_perf_imp.c : HAP_perf APIs to measure the execution time without RPC overhead hap_example_power_imp.c : HAP_power APIs to vote for DSP core, bus clock frequencies and voltage corners hap_example_sysmon_cachelock_imp.c : Cache locking manager to lock a section of CDSP L2 cache and release it hap_example_vtcm_mgr_imp.c : VTCM Manager to request or release VTCM memory on CDSP hap_example_unit_test_imp.c : Unit test that validates some of the HAP APIs","title":"Project structure"},{"location":"examples/hap_example/index.html#building","text":"","title":"Building"},{"location":"examples/hap_example/index.html#using-the-walkthrough-script","text":"The example comes with a walkthrough script called hap_example_walkthrough.py which builds the code and runs on target. The script must be run using the following command : python hap_example_walkthrough.py -T Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts.","title":"Using the walkthrough script"},{"location":"examples/hap_example/index.html#using-the-make-commands","text":"To build your code without using the walkthrough script, you will need to build both the Android and Hexagon modules. To build the Android module along with the dependencies, run the following command: make android VERBOSE=1 To build the Hexagon module, run the following command: make hexagon DSP_ARCH=v66 VERBOSE=1 For more information on the build syntax, please refer to the building reference instructions .","title":"Using the make commands"},{"location":"examples/hap_example/index.html#running-on-target","text":"If you want to run your code on target without using the walkthrough script, please use the following steps: Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push the HLOS side hap_example executable and the supporting stub library to the device adb shell mkdir -p /vendor/bin/ adb push android_ReleaseG_aarch64/hap_example /vendor/bin/ adb shell chmod 777 /vendor/bin/hap_example adb push android_ReleaseG_aarch64/ship/libhap_example.so /vendor/lib64/ Push the Hexagon Shared Object to the device's file system adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_ReleaseG_toolv84_v66/ship/libhap_example_skel.so /vendor/lib/rfsa/dsp/sdk Generate a device-specific test signature based on the device's serial number Follow the steps listed in the Use signer.py section of the signing documentation. Note: This step only needs to be done once as the same test signature will enable loading any module. Redirect DSP FARF messages to ADB logcat by creating a farf file adb shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/hap_example.farf\" Please refer to the page on messaging resources which discusses the tools available for logging debug messages from the DSP. Launch a new CLI shell to view the DSP's diagnostic messages using logcat Open a new shell or command window and execute : adb logcat -s adsprpc Execute the hap_example binary as follows : adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/hap_example <0/1/2/3/4/5/6 select HAP API> The command-line argument must be an integer ranging from 0 to 5 and selects the HAP APIs to be demonstrated: 0 - HAP_compute_res.h 1 - HAP_farf.h 2 - HAP_mem.h 3 - HAP_perf.h 4 - HAP_power.h 5 - sysmon_cachelock.h 6 - HAP_vtcm_mgr.h Analyze the output The command window or shell should contain messages returned by the application processor when using the printf command. For example when we run the example on an sm8250 target : adb shell /vendor/bin/hap_example 1 demonstrates FARF run-time logging and the following output is printed in the shell: -----Retrieving VTCM information of CDSP using FastRPC Capability API----- Result of capability query for VTCM_PAGE on CDSP is 262144 bytes Result of capability query for VTCM_COUNT on CDSP is 1 -----------------------HAP API Example-------------------------------- Demonstrating FARF run-time logging hap_example function PASSED Please look at the mini-dm logs or the adb logcat logs for DSP output --------------------------HAP Unit Test------------------------------- hap_unit_test PASSED hap_example PASSED while the FARF logs redirected to the adb logcat are : HAP_utils.c:303:0x1620ea:13: Logging mask set to 31 mod_table.c:541:0x12f0d3:13: open_mod_table_open_dynamic: Module libhap_example_skel.so opened with handle 0xa52093d0 HAP_utils.c:303:0x12f0d3:13: Logging mask set to 0 HAP_utils.c:329:0x12f0d3:13: Logging enabled for file hap_example_farf_runtime.c hap_example_farf_runtime.c:67:0x12f0d3:13: Low FARF message : Run-time logging disabled hap_example_farf_runtime.c:68:0x12f0d3:13: Medium FARF message : Run-time logging disabled hap_example_farf_runtime.c:69:0x12f0d3:13: High FARF message : Run-time logging disabled hap_example_farf_runtime.c:70:0x12f0d3:13: Error FARF message : Run-time logging disabled hap_example_farf_runtime.c:71:0x12f0d3:13: Fatal FARF message : Run-time logging disabled HAP_utils.c:303:0x12f0d3:13: Logging mask set to 31 HAP_utils.c:329:0x12f0d3:13: Logging enabled for file hap_example_farf_runtime.c hap_example_farf_runtime.c:44:0x12f0d3:13: Low FARF message : Run-time logging enabled hap_example_farf_runtime.c:45:0x12f0d3:13: Medium FARF message : Run-time logging enabled hap_example_farf_runtime.c:46:0x12f0d3:13: High FARF message : Run-time logging enabled hap_example_farf_runtime.c:47:0x12f0d3:13: Error FARF message : Run-time logging enabled hap_example_farf_runtime.c:48:0x12f0d3:13: Fatal FARF message : Run-time logging enabled hap_example_farf_runtime.c:50:0x12f0d3:13: Runtime Low FARF message - Run-time logging enabled hap_example_farf_runtime.c:51:0x12f0d3:13: Runtime Medium FARF message - Run-time logging enabled hap_example_farf_runtime.c:52:0x12f0d3:13: Runtime High FARF message - Run-time logging enabled hap_example_farf_runtime.c:53:0x12f0d3:13: Runtime Error FARF message - Run-time logging enabled hap_example_farf_runtime.c:54:0x12f0d3:13: Runtime Fatal FARF message - Run-time logging enabled mod_table.c:719:0x12f0d3:13: open_mod_table_close: unloaded libhap_example_skel.so HAP_utils.c:303:0x1380dd:13: Logging mask set to 31 mod_table.c:541:0x1500e4:13: open_mod_table_open_dynamic: Module libhap_example_skel.so opened with handle 0x4c3093d0 hap_unit_test_imp.c:108:0x1500e4:13: HAP_compute_res_attr_set_vtcm_param : TEST PASSED hap_unit_test_imp.c:109:0x1500e4:13: HAP_compute_res_acquire : TEST PASSED hap_unit_test_imp.c:148:0x1500e4:13: HAP_compute_res_release : TEST PASSED hap_unit_test_imp.c:194:0x1500e4:13: HAP_mmap : TEST PASSED hap_unit_test_imp.c:231:0x1500e4:13: HAP_query_request_VTCM : TEST PASSED hap_unit_test_imp.c:266:0x1500e4:13: HAP_query_release_VTCM : TEST PASSED hap_unit_test_imp.c:304:0x1500e4:13: HAP_query_request_async_VTCM : TEST PASSED hap_unit_test_imp.c:326:0x1500e4:13: hap_example_unit_test PASSED","title":"Running on Target"},{"location":"examples/lpi_example/index.html","text":"LPI example This example illustrates how to generate a shared object that can be loaded in LPI mode and non-LPI mode. This example should only be used as a reference to create LPI shared objects and run them in audio or sensor PD, but not in dynamic PD. The objective of the LPI mode is to achieve power optimizations for audio/sensors/voice use-cases running on the aDSP and sDSP. In Low Power Island (LPI) mode, shared objects will be loaded into TCM (tightly coupled memory) instead of DDR because DDR access will be disabled. LPI example is only supported on SM8250 and beyond. Instructions All step-by-step instructions for building and loading the LPI shared object on target are captured in the `lpi_walkthrough.py' script. Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts. You can also execute each step manually as follows. Build make hexagon DSP_ARCH=v66 LPI=1 Shared objects that need to be loaded in LPI mode must be created with a separate note section containing text uimg.dl.ver.2.0.0 . This is accomplished by setting the LPI flag ( LPI=1 ) as part of the make command building the DSP shared object. Setting this flag triggers two actions in hexagon.min: Compile src/uimage_dl_v2.c, which defines the note section of the shared .so with the string (uimg.dl.ver.2.0.0). Invoke a custom linker script at link time, src/uimage_v2.lcs, which will add the new note section in the shared object. Do not specify LPI=1 flag to build the shared object for non-LPI mode. Push test-signature file Follow the steps listed in the Use signer.py section of the signing documentation to push test-signature file to device. Note: This step only needs to be done once as the same test signature will enable loading any module. Push required files to device adb root adb remount adb push $(HEXAGON_SDK_ROOT)/libs/run_main_on_hexagon/ship/android_Debug_aarch64/run_main_on_hexagon /vendor/bin adb shell chmod 774 /vendor/bin/run_main_on_hexagon adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk/ adb push $(HEXAGON_SDK_ROOT)/libs/run_main_on_hexagon/ship/hexagon_Debug_dynamic_toolv83_v66/librun_main_on_hexagon_skel.so /vendor/lib/rfsa/dsp/sdk/ adb push $(HEXAGON_SDK_ROOT)examples/lpi_example/hexagon_Debug_dynamic_toolv83_v66/ship/libLPI.so /vendor/lib/rfsa/dsp/sdk/ Direct dsp messages to logcat adb shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/run_main_on_hexagon.farf\" Run example on device adb shell export DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/run_main_on_hexagon 0 libLPI.so The above command loads libLPI.so in the dynamic PD on ADSP using run_main_on_hexagon executable. The first argument of run_main_on_hexagon is domain id that specifies which hexagon DSP to use. 0 in the above command makes run_main_on_hexagon to run on ADSP. Domain id for CDSP is 3 . Fore more details on run_main_on_hexagon, please refer to the Run application on Hexagon Simulator section All the above steps remain the same for loading a non-LPI shared object with the exception of the build command. Confirming usage of the LPI mode Shared object that need to be loaded in LPI mode must be created with a separate note section containing text uimg.dl.ver.2.0.0 . You can confirm whether a shared object is compiled for LPI mode or not by using the command below. readelf --notes <shared_object> If your output is similar to the excerpt below, then the specified shared object is compiled for LPI mode. Notes at offset 0x00001000 with length 0x00000030: Owner Data size Description uimg.dl.ver.2.0.0 0x0000000c Unknown note type: (0x00000000) The GNU/LLVM linker generally uses a linker script that is compiled into the linker executable to control the memory layout of the ELF. It is possible to supply a custom command file or linker script , which in our case will add a new note section in the shared object. src/uimage_v2.lcs is used as the linker script in this example and src/uimage_dl_v2.c is compiled as part of the shared object to put the constant string uimg.dl.ver.2.0.0 in the new section created using uimage_v2.lcs . FastRPC uses this note section uimg.dl.ver.2.0.0 to decide whether to load in TCM (in LPI mode) or in DDR (in non-LPI mode). After running this example, you can see the loaded address of the shared object as below in logcat by running adb logcat -s adsprpc adsprpc : so_source.c:31:0x9b:8: load address of shared object is 0x8bddf000 Below are the TCM ranges for SM8250 and Saipan Target tcm_range Saipan (aDSP) 0x02C00000 - 0x02CD0000 SM8250 (aDSP) 0x02C00000 - 0x02D80000 From the load address in logcat and above TCM ranges you can confirm whether the shared object is loaded in TCM or not. Note : LPI shared objects should only be loaded in audio and sensors PDs. Loading LPI shared objects into dynamic PD is currently not supported.","title":"LPI"},{"location":"examples/lpi_example/index.html#lpi-example","text":"This example illustrates how to generate a shared object that can be loaded in LPI mode and non-LPI mode. This example should only be used as a reference to create LPI shared objects and run them in audio or sensor PD, but not in dynamic PD. The objective of the LPI mode is to achieve power optimizations for audio/sensors/voice use-cases running on the aDSP and sDSP. In Low Power Island (LPI) mode, shared objects will be loaded into TCM (tightly coupled memory) instead of DDR because DDR access will be disabled. LPI example is only supported on SM8250 and beyond.","title":"LPI example"},{"location":"examples/lpi_example/index.html#instructions","text":"All step-by-step instructions for building and loading the LPI shared object on target are captured in the `lpi_walkthrough.py' script. Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts. You can also execute each step manually as follows.","title":"Instructions"},{"location":"examples/lpi_example/index.html#build","text":"make hexagon DSP_ARCH=v66 LPI=1 Shared objects that need to be loaded in LPI mode must be created with a separate note section containing text uimg.dl.ver.2.0.0 . This is accomplished by setting the LPI flag ( LPI=1 ) as part of the make command building the DSP shared object. Setting this flag triggers two actions in hexagon.min: Compile src/uimage_dl_v2.c, which defines the note section of the shared .so with the string (uimg.dl.ver.2.0.0). Invoke a custom linker script at link time, src/uimage_v2.lcs, which will add the new note section in the shared object. Do not specify LPI=1 flag to build the shared object for non-LPI mode.","title":"Build"},{"location":"examples/lpi_example/index.html#push-test-signature-file","text":"Follow the steps listed in the Use signer.py section of the signing documentation to push test-signature file to device. Note: This step only needs to be done once as the same test signature will enable loading any module.","title":"Push test-signature file"},{"location":"examples/lpi_example/index.html#push-required-files-to-device","text":"adb root adb remount adb push $(HEXAGON_SDK_ROOT)/libs/run_main_on_hexagon/ship/android_Debug_aarch64/run_main_on_hexagon /vendor/bin adb shell chmod 774 /vendor/bin/run_main_on_hexagon adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk/ adb push $(HEXAGON_SDK_ROOT)/libs/run_main_on_hexagon/ship/hexagon_Debug_dynamic_toolv83_v66/librun_main_on_hexagon_skel.so /vendor/lib/rfsa/dsp/sdk/ adb push $(HEXAGON_SDK_ROOT)examples/lpi_example/hexagon_Debug_dynamic_toolv83_v66/ship/libLPI.so /vendor/lib/rfsa/dsp/sdk/","title":"Push required files to device"},{"location":"examples/lpi_example/index.html#direct-dsp-messages-to-logcat","text":"adb shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/run_main_on_hexagon.farf\"","title":"Direct dsp messages to logcat"},{"location":"examples/lpi_example/index.html#run-example-on-device","text":"adb shell export DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/run_main_on_hexagon 0 libLPI.so The above command loads libLPI.so in the dynamic PD on ADSP using run_main_on_hexagon executable. The first argument of run_main_on_hexagon is domain id that specifies which hexagon DSP to use. 0 in the above command makes run_main_on_hexagon to run on ADSP. Domain id for CDSP is 3 . Fore more details on run_main_on_hexagon, please refer to the Run application on Hexagon Simulator section All the above steps remain the same for loading a non-LPI shared object with the exception of the build command.","title":"Run example on device"},{"location":"examples/lpi_example/index.html#confirming-usage-of-the-lpi-mode","text":"Shared object that need to be loaded in LPI mode must be created with a separate note section containing text uimg.dl.ver.2.0.0 . You can confirm whether a shared object is compiled for LPI mode or not by using the command below. readelf --notes <shared_object> If your output is similar to the excerpt below, then the specified shared object is compiled for LPI mode. Notes at offset 0x00001000 with length 0x00000030: Owner Data size Description uimg.dl.ver.2.0.0 0x0000000c Unknown note type: (0x00000000) The GNU/LLVM linker generally uses a linker script that is compiled into the linker executable to control the memory layout of the ELF. It is possible to supply a custom command file or linker script , which in our case will add a new note section in the shared object. src/uimage_v2.lcs is used as the linker script in this example and src/uimage_dl_v2.c is compiled as part of the shared object to put the constant string uimg.dl.ver.2.0.0 in the new section created using uimage_v2.lcs . FastRPC uses this note section uimg.dl.ver.2.0.0 to decide whether to load in TCM (in LPI mode) or in DDR (in non-LPI mode). After running this example, you can see the loaded address of the shared object as below in logcat by running adb logcat -s adsprpc adsprpc : so_source.c:31:0x9b:8: load address of shared object is 0x8bddf000 Below are the TCM ranges for SM8250 and Saipan Target tcm_range Saipan (aDSP) 0x02C00000 - 0x02CD0000 SM8250 (aDSP) 0x02C00000 - 0x02D80000 From the load address in logcat and above TCM ranges you can confirm whether the shared object is loaded in TCM or not. Note : LPI shared objects should only be loaded in audio and sensors PDs. Loading LPI shared objects into dynamic PD is currently not supported.","title":"Confirming usage of the LPI mode"},{"location":"examples/multithreading/index.html","text":"Multithreading example Overview The multithreading example illustrates the following features: QuRT APIs: Threads Barriers Mutexes Benefits of using multithreading and L2 prefetching Using the command line or the Eclipse IDE to perform the following steps: Building Simulating Running on target device Debugging on simulator For more information on the usage of QuRT APIs, please refer to the OS section introducing QuRT. Project structure The example demonstrates the usage of QuRT APIs for launching multiple threads, synchronizing threads with mutexes and barriers, and L2 prefetching. Here is the project flow for the multithreading example: Makefile Root makefile that invokes variant-specific min files to either build the application processor source code or the Hexagon DSP source code. hexagon.min , android.min Contain the make.d directives used to build the application processor and Hexagon DSP source code. inc/multithreading.idl IDL interface that defines the multithreading API. This IDL file is compiled by the QAIC IDL compiler into the following files: multithreading.h : C/C++ header file multithreading_stub.c : Stub source that needs to be built for the HLOS (Android, etc...) multithreading_skel.c : Skel source that needs to be built for the Hexagon DSP src/multithreading.c Source for the Android executable that calls the multithreading stub on the HLOS side to offload the compute task onto the DSP. src/multithreading_imp.c Source for the Hexagon side implementation of the multithreading interface and is compiled into a shared object. Building Using the walkthrough script The example comes with a walkthrough script called multithreading_walkthrough.py which builds the code and runs on target. The script must be run using the following command : python multithreading_walkthrough.py -T <Target Name> Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts. Using the make commands To build your code without using the walkthrough script, you will need to build both the Android and Hexagon modules. To build the android module along with the dependencies, run the following command: make android VERBOSE=1 To build the hexagon module and run the example on the simulator, run the following command: make hexagon DSP_ARCH=v66 VERBOSE=1 For more information on the build syntax, please refer to the building reference instructions. . Running on simulator The following command builds the necessary hexagon modules using the Hexagon toolchain 8.4.12 for the v66 architecture and runs the example on the hexagon simulator : make hexagonsim DSP_ARCH=v66 Running on target If you want to run your code on target without using the walkthrough script, please use the following steps: Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push the HLOS side multithreading test executable and supporting multithreading stub library to the device adb shell mkdir -p /vendor/bin/ adb push android_ReleaseG_aarch64/multithreading /vendor/bin/ adb shell chmod 777 /vendor/bin/multithreading adb push android_ReleaseG_aarch64/ship/libmultithreading.so /vendor/lib64/ Push the Hexagon Shared Object to the device's file system adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_ReleaseG_toolv84_v66/ship/libmultithreading_skel.so /vendor/lib/rfsa/dsp/sdk Generate a device-specific test signature based on the device's serial number Follow the steps listed in the Use signer.py section of the signing documentation. Note: This step only needs to be done once as the same test signature will enable loading any module. Redirect DSP FARF messages to ADB logcat by creating a farf file adb shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/multithreading.farf\" Please refer to the page on messaging resources which discusses the tools available for logging debug messages from the DSP. Launch a new CLI shell to view the DSP's diagnostic messages using logcat Open a new shell or command window and execute : adb logcat -s adsprpc Execute the multithreading example binary as follows : adb shell /vendor/bin/multithreading Analyze the output The command window or shell should contain messages returned by the application processor when using the printf command. Retrieving CDSP information using FastRPC Capability API DOMAIN_SUPPORT : 1 UNSIGNED_PD_SUPPORT : 1 Result of capability query for VTCM_PAGE on CDSP is 262144 bytes Result of capability query for VTCM_PAGE on ADSP is 0 bytes Starting multithreading test Test PASSED Please look at the mini-dm logs or the adb logcat logs for DSP output Debugging This section covers the steps to debug the multithreading program. Command-line debugging on the simulator Build the hexagon module using the following command: make hexagon BUILD=Debug DSP_ARCH=v66 This generates the multithreading_q.so file. Define the LLDB_HEXAGON_BOOTER_PATH environment variable : Linux: export LLDB_HEXAGON_BOOTER_PATH=$HEXAGON_SDK_ROOT/rtos/qurt/computev66/sdksim_bin/runelf.pbn Windows: set LLDB_HEXAGON_BOOTER_PATH=%HEXAGON_SDK_ROOT%\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn When hexagon-lldb reaches the simulator launch, it checks for this environment variable and if it exists, treats the runelf.pbn in this path as the main target and picks multithreading_q.so file as its first argument to it. Launch the debugger hexagon-lldb with the run_main_on_hexagon_sim binary along with the arguments as follows: Linux: $HEXAGON_SDK_ROOT/tools/HEXAGON_Tools/8.4.12/Tools/bin/hexagon-lldb $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim -- -mv66 --simulated_returnval --usefs hexagon_Debug_toolv84_v66 --pmu_statsfile hexagon_Debug_toolv84_v66/pmu_stats.txt --cosim_file hexagon_Debug_toolv84_v66/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v66/osam.cfg -- -- $HEXAGON_SDK_ROOT/examples/multithreading/hexagon_Debug_toolv84_v66/multithreading_q.so Windows: %HEXAGON_SDK_ROOT%\\tools\\HEXAGON_Tools\\8.4.12\\Tools\\bin\\hexagon-lldb.exe %HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim -- -mv66 --simulated_returnval --usefs hexagon_Debug_toolv84_v66 --pmu_statsfile hexagon_Debug_toolv84_v66\\pmu_stats.txt --cosim_file hexagon_Debug_toolv84_v66\\q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v66\\osam.cfg -- -- hexagon_Debug_toolv84_v66\\multithreading_q.so The output of the above command must be : Hexagon utilities (pagetable, tlb, pv) loaded Hexagon SDK device_connect command loaded (lldb) target create \"/path/to/SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim\" Current executable set to /path/to/SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim' (hexagon). (lldb) settings set -- target.run-args \"-mv66\" \"--simulated_returnval\" \"--usefs\" \"hexagon_Debug_toolv84_v66\" \"--pmu_statsfile\" \"hexagon_Debug_toolv84_v66/pmu_stats.txt\" \"--cosim_file\" \"hexagon_Debug_toolv84_v66/q6ss.cfg\" \"--l2tcm_base\" \"0xd800\" \"--rtos\" \"hexagon_Debug_toolv84_v66/osam.cfg\" \"--\" \"--\" \"/path/to/SDK_ROOT/examples/multithreading/hexagon_Debug_toolv84_v66/multithreading_q.so\" Breakpoints can be set using the following command : b multithreading_parallel_sum This sets the breakpoint at the entry point of the function multithreading_parallel_sum Use 'r' to start running the program : r The debugger stops the process at the given breakpoint and the output looks as follows : Starting multithreading test Process 1 stopped * thread #16, name = 'ribbon', stop reason = breakpoint 1.1 frame #0: 0xd8044934 multithreading_q.so`multithreading_parallel_sum(h=14593280) at multithreading_imp.c:90:5 87 * We initialize all threads with an equal priority value of: QURT_THREAD_ATTR_PRIORITY_DEFAULT/2 (127) 88 */ 89 -> 90 qurt_thread_attr_init(&attr1); 91 qurt_thread_attr_set_name(&attr1, (char *)\"cntr1\"); 92 qurt_thread_attr_set_stack_addr(&attr1, malloc(1024)); 93 qurt_thread_attr_set_stack_size(&attr1, 1024); You can step into, step over, continue the process, obtain the register information, print the variable values and perform other functions. Some useful debugger commands are: Command Description breakpoint list Lists the breakpoints register read Show general purpose registers for current thread thread list Lists the threads along with the thread ID and name thread backtrace Prints the stack backtrace for current thread frame info List information about the selected frame in current thread up Select stack frame that has called the current stack frame down Select stack frame that is called by the current stack frame frame variable Display a stack frame's arguments and local variables target variable Display global/static variables defined in current source file step Do a source level single step in current thread next Do a source level single step over in current thread si Do an instruction level single step in current thread ni Do an instruction level single step over in current thread finish Step out of current selected frame tlb Display the values for TLB in current thread pagetable Display the values for the pagetable in current thread memory read --size \"n\" \"addr\" --outfile \"file.txt\" Read \"n\" number of bytes from memory starting from the hexadecimal address \"addr\" and save the results to \"file.txt\" disassemble --frame Disassemble current function for the current frame image list Display the main executable and all dependent shared libraries exit To quit the hexagon-lldb debugger Please refer to the Hexagon LLDB Debugger User Guide for more information on hexagon-lldb . Eclipse debugging with the simulator This section explains how to build, run and debug the code on the simulator using the Eclipse IDE. The paths used and the diagrams shown assume the Operating System to be Windows, but similar rules apply for Linux and the paths must be adjusted accordingly. Import the example project to the workspace : To import the multithreading example, right-click in Project Explorer and select : Import -> Hexagon C/C++ -> Import Hexagon Project > Next In the next dialogue box, set the following project properties : Project type : Makefile Project Project Name : multithreading Existing Code Location : \\path\\to\\SDK_ROOT\\examples\\multithreading Click on Finish button to import the multithreading code as a Makefile Project . Build the imported project To build the imported project, right-click on the multithreading project and select : Properties > C/C++ Build In the Builder Settings tab, set the build command : Build command : make BUILD=Debug DSP_ARCH=v66 In the Behavior tab, set the flags : Build : hexagon Clean : hexagon_clean Click the Apply and Close button. To build the project, right-click on the multithreading project and select Build Project . Run the project on simulator To run the project, right-click on the multithreading project and select : Run As > Run Configurations... In the Run Configurations dialog box, select Hexagon C/C++ Application and under the Main tab, set the simulator target : C/C++ Application : \\path\\to\\SDK_ROOT\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn Under the Simulator tab, set the simulator arguments : CPU Architecture : v66 Miscellaneous Flags : --simulated_returnval --usefs hexagon_Debug_toolv84_v66 --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v66\\osam.cfg Under the Arguments tab, set the program arguments : Program arguments : \\path\\to\\SDK_ROOT\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim -- hexagon_Debug_toolv84_v66\\multithreading_q.so To execute the program, click the Run button. Define LLDB_HEXAGON_BOOTER_PATH The LLDB_HEXAGON_BOOTER_PATH environment variable needs to be defined before the start of debugging on the simulator Right-click on the project and select : Properties > C/C++ Build > Environment > Add... In the dialog-box that opens, set the environment variable : Name : LLDB_HEXAGON_BOOTER_PATH Value : \\path\\to\\SDK_ROOT\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn Click Apply and Close to finish setting the LLDB_HEXAGON_BOOTER_PATH environment variable. Debug the project To configure the debug environment, right-click on project and select : Debug As > Debug Configurations... In the Main tab, set : C/C++ Application : \\path\\to\\SDK_ROOT\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim An error message Multiple launchers available - Select one... appears. Please click on Select one... Tick the Use configuration specific settings box and select Launchers : Standard Create LLDB Debug Process Launcher Under the Arguments tab, set the program arguments : Program arguments : --hexagon_Debug_toolv84_v66\\multithreading_q.so Some breakpoints may already be set by default. To read more about breakpoints set by default or how to set them manually, refer to the debug section of the IDE documentation. Under the Debugger tab, set the breakpoint : Stop on startup at : multithreading_parallel_sum This sets the breakpoint at the entry point of the function multithreading_parallel_sum . Click on Apply and Debug to start debugging the program on simulator. Eclipse now asks for a perspective switch prompt to switch to Debug perspective . Select Switch and the executable is launched and the breakpoint is hit at the multithreading_parallel_sum function The debug perspective enables you to perform debugging operations such as step, disassembly, set breakpoints, view/modify variables, and view registers. For example, clicking on the Registers tab of the debug perspective displays the contents of the Hexagon processor registers. If any debug-related information is not displayed, you can display it by choosing Show View from the Window menu.","title":"Multithreading"},{"location":"examples/multithreading/index.html#multithreading-example","text":"","title":"Multithreading example"},{"location":"examples/multithreading/index.html#overview","text":"The multithreading example illustrates the following features: QuRT APIs: Threads Barriers Mutexes Benefits of using multithreading and L2 prefetching Using the command line or the Eclipse IDE to perform the following steps: Building Simulating Running on target device Debugging on simulator For more information on the usage of QuRT APIs, please refer to the OS section introducing QuRT.","title":"Overview"},{"location":"examples/multithreading/index.html#project-structure","text":"The example demonstrates the usage of QuRT APIs for launching multiple threads, synchronizing threads with mutexes and barriers, and L2 prefetching. Here is the project flow for the multithreading example: Makefile Root makefile that invokes variant-specific min files to either build the application processor source code or the Hexagon DSP source code. hexagon.min , android.min Contain the make.d directives used to build the application processor and Hexagon DSP source code. inc/multithreading.idl IDL interface that defines the multithreading API. This IDL file is compiled by the QAIC IDL compiler into the following files: multithreading.h : C/C++ header file multithreading_stub.c : Stub source that needs to be built for the HLOS (Android, etc...) multithreading_skel.c : Skel source that needs to be built for the Hexagon DSP src/multithreading.c Source for the Android executable that calls the multithreading stub on the HLOS side to offload the compute task onto the DSP. src/multithreading_imp.c Source for the Hexagon side implementation of the multithreading interface and is compiled into a shared object.","title":"Project structure"},{"location":"examples/multithreading/index.html#building","text":"","title":"Building"},{"location":"examples/multithreading/index.html#using-the-walkthrough-script","text":"The example comes with a walkthrough script called multithreading_walkthrough.py which builds the code and runs on target. The script must be run using the following command : python multithreading_walkthrough.py -T <Target Name> Please review the generic setup and walkthrough_scripts instructions to learn more about setting up your device and using walkthrough scripts.","title":"Using the walkthrough script"},{"location":"examples/multithreading/index.html#using-the-make-commands","text":"To build your code without using the walkthrough script, you will need to build both the Android and Hexagon modules. To build the android module along with the dependencies, run the following command: make android VERBOSE=1 To build the hexagon module and run the example on the simulator, run the following command: make hexagon DSP_ARCH=v66 VERBOSE=1 For more information on the build syntax, please refer to the building reference instructions. .","title":"Using the make commands"},{"location":"examples/multithreading/index.html#running-on-simulator","text":"The following command builds the necessary hexagon modules using the Hexagon toolchain 8.4.12 for the v66 architecture and runs the example on the hexagon simulator : make hexagonsim DSP_ARCH=v66","title":"Running on simulator"},{"location":"examples/multithreading/index.html#running-on-target","text":"If you want to run your code on target without using the walkthrough script, please use the following steps: Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push the HLOS side multithreading test executable and supporting multithreading stub library to the device adb shell mkdir -p /vendor/bin/ adb push android_ReleaseG_aarch64/multithreading /vendor/bin/ adb shell chmod 777 /vendor/bin/multithreading adb push android_ReleaseG_aarch64/ship/libmultithreading.so /vendor/lib64/ Push the Hexagon Shared Object to the device's file system adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_ReleaseG_toolv84_v66/ship/libmultithreading_skel.so /vendor/lib/rfsa/dsp/sdk Generate a device-specific test signature based on the device's serial number Follow the steps listed in the Use signer.py section of the signing documentation. Note: This step only needs to be done once as the same test signature will enable loading any module. Redirect DSP FARF messages to ADB logcat by creating a farf file adb shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/multithreading.farf\" Please refer to the page on messaging resources which discusses the tools available for logging debug messages from the DSP. Launch a new CLI shell to view the DSP's diagnostic messages using logcat Open a new shell or command window and execute : adb logcat -s adsprpc Execute the multithreading example binary as follows : adb shell /vendor/bin/multithreading Analyze the output The command window or shell should contain messages returned by the application processor when using the printf command. Retrieving CDSP information using FastRPC Capability API DOMAIN_SUPPORT : 1 UNSIGNED_PD_SUPPORT : 1 Result of capability query for VTCM_PAGE on CDSP is 262144 bytes Result of capability query for VTCM_PAGE on ADSP is 0 bytes Starting multithreading test Test PASSED Please look at the mini-dm logs or the adb logcat logs for DSP output","title":"Running on target"},{"location":"examples/multithreading/index.html#debugging","text":"This section covers the steps to debug the multithreading program.","title":"Debugging"},{"location":"examples/multithreading/index.html#command-line-debugging-on-the-simulator","text":"Build the hexagon module using the following command: make hexagon BUILD=Debug DSP_ARCH=v66 This generates the multithreading_q.so file. Define the LLDB_HEXAGON_BOOTER_PATH environment variable : Linux: export LLDB_HEXAGON_BOOTER_PATH=$HEXAGON_SDK_ROOT/rtos/qurt/computev66/sdksim_bin/runelf.pbn Windows: set LLDB_HEXAGON_BOOTER_PATH=%HEXAGON_SDK_ROOT%\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn When hexagon-lldb reaches the simulator launch, it checks for this environment variable and if it exists, treats the runelf.pbn in this path as the main target and picks multithreading_q.so file as its first argument to it. Launch the debugger hexagon-lldb with the run_main_on_hexagon_sim binary along with the arguments as follows: Linux: $HEXAGON_SDK_ROOT/tools/HEXAGON_Tools/8.4.12/Tools/bin/hexagon-lldb $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim -- -mv66 --simulated_returnval --usefs hexagon_Debug_toolv84_v66 --pmu_statsfile hexagon_Debug_toolv84_v66/pmu_stats.txt --cosim_file hexagon_Debug_toolv84_v66/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v66/osam.cfg -- -- $HEXAGON_SDK_ROOT/examples/multithreading/hexagon_Debug_toolv84_v66/multithreading_q.so Windows: %HEXAGON_SDK_ROOT%\\tools\\HEXAGON_Tools\\8.4.12\\Tools\\bin\\hexagon-lldb.exe %HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim -- -mv66 --simulated_returnval --usefs hexagon_Debug_toolv84_v66 --pmu_statsfile hexagon_Debug_toolv84_v66\\pmu_stats.txt --cosim_file hexagon_Debug_toolv84_v66\\q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v66\\osam.cfg -- -- hexagon_Debug_toolv84_v66\\multithreading_q.so The output of the above command must be : Hexagon utilities (pagetable, tlb, pv) loaded Hexagon SDK device_connect command loaded (lldb) target create \"/path/to/SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim\" Current executable set to /path/to/SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim' (hexagon). (lldb) settings set -- target.run-args \"-mv66\" \"--simulated_returnval\" \"--usefs\" \"hexagon_Debug_toolv84_v66\" \"--pmu_statsfile\" \"hexagon_Debug_toolv84_v66/pmu_stats.txt\" \"--cosim_file\" \"hexagon_Debug_toolv84_v66/q6ss.cfg\" \"--l2tcm_base\" \"0xd800\" \"--rtos\" \"hexagon_Debug_toolv84_v66/osam.cfg\" \"--\" \"--\" \"/path/to/SDK_ROOT/examples/multithreading/hexagon_Debug_toolv84_v66/multithreading_q.so\" Breakpoints can be set using the following command : b multithreading_parallel_sum This sets the breakpoint at the entry point of the function multithreading_parallel_sum Use 'r' to start running the program : r The debugger stops the process at the given breakpoint and the output looks as follows : Starting multithreading test Process 1 stopped * thread #16, name = 'ribbon', stop reason = breakpoint 1.1 frame #0: 0xd8044934 multithreading_q.so`multithreading_parallel_sum(h=14593280) at multithreading_imp.c:90:5 87 * We initialize all threads with an equal priority value of: QURT_THREAD_ATTR_PRIORITY_DEFAULT/2 (127) 88 */ 89 -> 90 qurt_thread_attr_init(&attr1); 91 qurt_thread_attr_set_name(&attr1, (char *)\"cntr1\"); 92 qurt_thread_attr_set_stack_addr(&attr1, malloc(1024)); 93 qurt_thread_attr_set_stack_size(&attr1, 1024); You can step into, step over, continue the process, obtain the register information, print the variable values and perform other functions. Some useful debugger commands are: Command Description breakpoint list Lists the breakpoints register read Show general purpose registers for current thread thread list Lists the threads along with the thread ID and name thread backtrace Prints the stack backtrace for current thread frame info List information about the selected frame in current thread up Select stack frame that has called the current stack frame down Select stack frame that is called by the current stack frame frame variable Display a stack frame's arguments and local variables target variable Display global/static variables defined in current source file step Do a source level single step in current thread next Do a source level single step over in current thread si Do an instruction level single step in current thread ni Do an instruction level single step over in current thread finish Step out of current selected frame tlb Display the values for TLB in current thread pagetable Display the values for the pagetable in current thread memory read --size \"n\" \"addr\" --outfile \"file.txt\" Read \"n\" number of bytes from memory starting from the hexadecimal address \"addr\" and save the results to \"file.txt\" disassemble --frame Disassemble current function for the current frame image list Display the main executable and all dependent shared libraries exit To quit the hexagon-lldb debugger Please refer to the Hexagon LLDB Debugger User Guide for more information on hexagon-lldb .","title":"Command-line debugging on the simulator"},{"location":"examples/multithreading/index.html#eclipse-debugging-with-the-simulator","text":"This section explains how to build, run and debug the code on the simulator using the Eclipse IDE. The paths used and the diagrams shown assume the Operating System to be Windows, but similar rules apply for Linux and the paths must be adjusted accordingly.","title":"Eclipse debugging with the simulator"},{"location":"examples/multithreading/index.html#import-the-example-project-to-the-workspace","text":"To import the multithreading example, right-click in Project Explorer and select : Import -> Hexagon C/C++ -> Import Hexagon Project > Next In the next dialogue box, set the following project properties : Project type : Makefile Project Project Name : multithreading Existing Code Location : \\path\\to\\SDK_ROOT\\examples\\multithreading Click on Finish button to import the multithreading code as a Makefile Project .","title":"Import the example project to the workspace :"},{"location":"examples/multithreading/index.html#build-the-imported-project","text":"To build the imported project, right-click on the multithreading project and select : Properties > C/C++ Build In the Builder Settings tab, set the build command : Build command : make BUILD=Debug DSP_ARCH=v66 In the Behavior tab, set the flags : Build : hexagon Clean : hexagon_clean Click the Apply and Close button. To build the project, right-click on the multithreading project and select Build Project .","title":"Build the imported project"},{"location":"examples/multithreading/index.html#run-the-project-on-simulator","text":"To run the project, right-click on the multithreading project and select : Run As > Run Configurations... In the Run Configurations dialog box, select Hexagon C/C++ Application and under the Main tab, set the simulator target : C/C++ Application : \\path\\to\\SDK_ROOT\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn Under the Simulator tab, set the simulator arguments : CPU Architecture : v66 Miscellaneous Flags : --simulated_returnval --usefs hexagon_Debug_toolv84_v66 --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v66\\osam.cfg Under the Arguments tab, set the program arguments : Program arguments : \\path\\to\\SDK_ROOT\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim -- hexagon_Debug_toolv84_v66\\multithreading_q.so To execute the program, click the Run button.","title":"Run the project on simulator"},{"location":"examples/multithreading/index.html#define-lldb_hexagon_booter_path","text":"The LLDB_HEXAGON_BOOTER_PATH environment variable needs to be defined before the start of debugging on the simulator Right-click on the project and select : Properties > C/C++ Build > Environment > Add... In the dialog-box that opens, set the environment variable : Name : LLDB_HEXAGON_BOOTER_PATH Value : \\path\\to\\SDK_ROOT\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn Click Apply and Close to finish setting the LLDB_HEXAGON_BOOTER_PATH environment variable.","title":"Define LLDB_HEXAGON_BOOTER_PATH"},{"location":"examples/multithreading/index.html#debug-the-project","text":"To configure the debug environment, right-click on project and select : Debug As > Debug Configurations... In the Main tab, set : C/C++ Application : \\path\\to\\SDK_ROOT\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim An error message Multiple launchers available - Select one... appears. Please click on Select one... Tick the Use configuration specific settings box and select Launchers : Standard Create LLDB Debug Process Launcher Under the Arguments tab, set the program arguments : Program arguments : --hexagon_Debug_toolv84_v66\\multithreading_q.so Some breakpoints may already be set by default. To read more about breakpoints set by default or how to set them manually, refer to the debug section of the IDE documentation. Under the Debugger tab, set the breakpoint : Stop on startup at : multithreading_parallel_sum This sets the breakpoint at the entry point of the function multithreading_parallel_sum . Click on Apply and Debug to start debugging the program on simulator. Eclipse now asks for a perspective switch prompt to switch to Debug perspective . Select Switch and the executable is launched and the breakpoint is hit at the multithreading_parallel_sum function The debug perspective enables you to perform debugging operations such as step, disassembly, set breakpoints, view/modify variables, and view registers. For example, clicking on the Registers tab of the debug perspective displays the contents of the Hexagon processor registers. If any debug-related information is not displayed, you can display it by choosing Show View from the Window menu.","title":"Debug the project"},{"location":"examples/profiling/index.html","text":"Profiling Example Overview This example illustrates the use of the various profiling tools and APIs available on the Hexagon SDK. This example also measures and reports the FastRPC performance with different buffer sizes and optimizations . This example shows how to use the following: The FastRPC performance tests can be viewed in the shell. These tests print the overhead time for a CPU thread to offload a task to the DSP, wait for the DSP to execute that task, and wake up when the task is complete. The time measurements are meaningless on the simulator. The timer APIs demonstrate the APIs that can be used for accessing cycle and clock counters and use them to time a block of code. These APIs work on both the simulator and device. The time measurements are meaningless on the simulator. The Hexagon Profiler generates function-level profiling reports. This profiler can be used only on the simulator. The Hexagon Trace Analyzer parses trace files generated on device or on simulator and generates various analysis reports. This profiler works only on Linux. The sysMonApp displays various performance-related services on the cDSP, aDSP, and sDSP. This profiler can only be used on device. Prerequisites Before the profiling application is run the following conditions should be met: Make sure the Hexagon SDK is properly setup with the Setup Instructions . Some of the profiling tools discussed in this example are not supported on all devices. Please refer to the feature matrix for more details. The Hexagon Trace Analyzer requires Flamegraph and Catapult . These tools will be downloaded from the internet when the run_profilers.py script is run for the first time. You can download Flamegraph here and Catapult here . Running profiling_walkthrough.py The profiling_walkthrough.py script builds and runs the timer APIs and default FastRPC performance tests on a device. Usage: profiling_walkthrough.py [-p] [-n ITERATIONS] [-i] [-m] [-u] [-q FASTRPC_QOS] [-U UNSIGNED_PD_FLAG] In addition to the options supported by all walkthrough scripts , the profiling_walkthrough.py script supports the following options: Optional arguments: Option Description Default -n ITERATIONS Number of iterations to run. -n 1000 -d DOMAIN Specifies the domain on which to run the example. MDSP and SDSP are not supported. Domain is CDSP by default. 0: ADSP 3: CDSP -d 3 -q FASTRPC_QOS Set the FastRPC QoS latency in micro-seconds. This can improve performance at the expense of some increase in power consumption. QoS is turned ON by default. 0 will turn QoS OFF. Positive numbers set the QoS latency in micro-seconds. -q 300 -y SLEEP_LATENCY Set DSP Sleep Latency. 0 is default and means DCVS V3 vote will not be made. Range of values is from 10 to 65535 microseconds. To remove a previous latency vote, simply vote for a large latency such as 65535 to indicate that there is no specific latency request. -y 0 -U UNSIGNED_PD_FLAG Set the PD to run the example. 1 is default and means run the example on the Unsigned PD. -U 1 -p Run power boost using the request type HAP_power_set_DCVS_v3 with HAP_DCVS_V2_PERFORMANCE_MODE. Further details are provided below . OFF -i Disable ION memory for FastRPC performance tests. ION memory is used by default. This option is relevant only for FUNCTION_NAME default, noop, inbuf, routbuf. Otherwise this option is ignored. ON -m Verify memory operations for FastRPC performance tests by passing a fixed pattern to the DSP function which copies it to the output. The caller then confirms the correct pattern is received. OFF -u Use uncached buffers on the HLOS. OFF For example, to build and run the timer APIs and default FastRPC performance tests on a sm8250 device: python profiling_walkthrough.py -T sm8250 Building Before modifying or understanding the code, we need to make sure we can build the example and generate the binary files from source. You can reproduce the steps of the walkthrough script by building and running the example manually as explained below. Build Android variant as follows: make android BUILD=Debug Build Hexagon variant as follows: make hexagon BUILD=Debug DSP_ARCH=v65 For more information on the build flavors available and build syntax, please refer to the building reference instructions . Running on device Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push Android components as follows: adb wait-for-device shell mkdir -p /vendor/bin adb wait-for-device push android_Debug_aarch64/ship/profiling /vendor/bin adb wait-for-device shell chmod 777 /vendor/bin/profiling adb wait-for-device push android_Debug_aarch64/ship/libprofiling.so /vendor/lib64/ Push Hexagon components as follows: adb wait-for-device shell mkdir -p /vendor/lib/rfsa/dsp/sdk/ adb wait-for-device push hexagon_Debug_toolv84_v66/ship/libprofiling_skel.so /vendor/lib/rfsa/dsp/sdk/ Generate a device-specific test signature based on the device serial number Follow the steps listed in the signing documentation . NOTE: Signing the device needs to be done once as the same test signature will enable loading any module. Direct DSP messages to logcat: adb wait-for-device shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/profiling.farf\" adb wait-for-device Open another window to see aDSP diagnostic messages: Linux: In a separate terminal: adb logcat -s adsprpc Windows: start cmd.exe /c adb logcat -s adsprpc sleep 2 Run Profiling Example: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling Optional Arguments supported by the binary when running the timer APIs and FastRPC performance : Usage: /vendor/bin/profiling [-f FUNCTION_NAME] [-n ITERATIONS] [-s SIZE] [-q FASTRPC_QOS] [-U UNSIGNED_PD_FLAG] [-p] [-i] [-m] [-u] Option Description -f FUNCTION_NAME Specify which function to run. The supported values are as follows: timers to run the timer API examples, default to run a default set of FastRPC performance tests with configurations which are cached, ion set by user, variable size, check_memory off. This option is run if -f is not provided. noop to run the FastRPC performance test with no operation, inbuf to run the FastRPC performance test with input buffer , and routbuf to run the FastRPC performance test with output buffer . -d DOMAIN Specifies the domain on which to run the example. MDSP and SDSP are not supported. Domain is CDSP by default. 0: ADSP 3: CDSP -n ITERATIONS Iterations to run FastRPC performance tests. 1000 is default. This option is relevant only for FUNCTION_NAME noop inbuf, routbuf. Otherwise this option is ignored. -s SIZE Size of data buffer for FastRPC performance tests. If 0 is given the noop function will run. -q QOS_LATENCY Set the FastRPC QoS latency in micro-seconds. Increasing the latency will save power at the expense of low performance. QoS is turned ON by default. 0 will turn QoS OFF. Positive numbers set the QoS latency in micro-seconds (300 is default). -y SLEEP_LATENCY Set DSP Sleep Latency. 0 is default and means DCVS V3 vote will not be made. Range of values is from 10 to 65535 microseconds. To remove a previous latency vote, simply vote for a large latency such as 65535 to indicate that there is no specific latency request. -U UNSIGNED_PD_FLAG Set the PD to run the example. 1 is default and means run the example on the Unsigned PD. -p Run power boost using the request type HAP_power_set_DCVS_v3 with HAP_DCVS_V2_PERFORMANCE_MODE. Further details are provided below -i Disable ION memory for FastRPC performance tests. ION memory is used by default. This option is ignored with the FUNCTION_NAME timers. Otherwise the option is relevant. -m Verify memory operations for FastRPC performance tests by passing a fixed pattern to the DSP function which copies the pattern to the output. The caller then confirms the correct pattern is received. This option is relevant only for FUNCTION_NAME inbuf, routbuf. Otherwise this option is ignored. -u Use Uncached buffers on HLOS for FastRPC performance tests. This option is relevant only for FUNCTION_NAME inbuf, routbuf. Otherwise this option is ignored. Example commands with optional arguments to run the FastRPC tests on a specific buffer: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f inbuf -n 5000 -s 8388608 -p -q 700 The above example command will run the FastRPC performance test on an input buffer of size 8 MB with power boost and FastRPC QoS set to 700 micro-seconds. adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f default The above example command will run the FastRPC performance tests with on cached buffers, with ion disabled, with check memory off. The buffer types and sizes run will be the following: Buffer type Buffer size noop 0K inbuf 32K routbuf 32K inbuf 64K routbuf 64K inbuf 128K routbuf 128K inbuf 1M routbuf 1M inbuf 4M routbuf 4M inbuf 8M routbuf 8M inbuf 16M routbuf 16M The results of the above command is as shown in Profiling summary report . Running on simulator When running the example on the Hexagon simulator , you need to use the --timing option for the cycle count to be accurate. The time measurements will not be meaningful. When running on the simulator, you need to pass both the arguments to the Hexagon simulator and to the test executable. The arguments given to the executable are given in the previous section . The simulator arguments are as follows: Simulator option Description --mv* Simulate for a particular architecture version of Hexagon. E.g. -mv65 . --timing Model processor micro-architecture with cache, multi-threading mode, and processor stalls as part of the simulation. This enables timing so that the simulation more accurately models the operation of the processor hardware. --simulated_returnval Cause the simulator to return a value to its caller indicating the final execution status of the target application. --usefs <path> Cause the simulator to search for required files in the directory with the specified path. --pmu_statsfile <filename> Generate a PMU statistics file with the specified name. See the SysMon Profiler for more information about PMU events. --help Prints available simulator options. The Hexagon Architecture version for the -mv* flag can be selected based on the DSP architecture version from the following: DSP Architecture version Simulator Flag v65 -mv65a_1024 v66 -mv66g_1024 v68 -mv68n_1024 As an example, here is how the profiling_q.so file might be run on the simulator: cd $HEXAGON_SDK_ROOT/examples/profiling/ $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-sim -mv65a_1024 --timing --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg $HEXAGON_SDK_ROOT/rtos/qurt/computev65/sdksim_bin/runelf.pbn -- $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v65/run_main_on_hexagon_sim -- ./hexagon_Debug_toolv84_v65/profiling_q.so -f timers FastRPC performance tests The FastRPC overhead can be calculated for different buffer sizes. Below is the brief description of the tests that can be executed: Function Description noop Makes a FastRPC invocation with no input and output buffers inbuf Makes a FastRPC invocation with one input buffer of varying buffer sizes ranging from 32K to 16M routbuf Makes a FastRPC invocation with one output buffer of varying buffer sizes ranging from 32K to 16M The input buffer and output buffer are defined in the IDL file inc/profiling.idl. To understand the in and rout parameters given in the idl file, please refer to the IDL documentation . Contributing factors to FastRPC performance Various factors impact the performance of FastRPC. Some of the relevant factors are described below. For a detailed explanation of these factors refer to the system performance documentation. The profiling application allows you to explore the impact of some of these factors. ION buffers: ION buffers (available on Android targets) do not require extra copies of buffers when shared between the DSP and the CPU. Non-ION buffers will automatically be copied by FastRPC framework. The impact on performance can be observed in the FastRPC performance tests by using the -i flag to disable ION memory for the FastRPC performance tests. Caches: The FastRPC driver takes care of maintaining cache coherency between CPU and DSP for shared buffers. When the -u flag is given for uncached buffers, RPCMEM_HEAP_UNCACHED is used. You can refer to the RPCMEM API to learn more. Caching should result in better performance, but this depends on the application. FastRPC QoS: FastRPC offers a QoS API to enable users to request for a required FastRPC latency between CPU application making the remote call and beginning of DSP method execution. This latency requested is not guaranteed but the driver tries to meet available options on the target. Refer to remote_handle_control() and remote_handle64_control() defined at remote API . In the FastRPC performance tests, the user can request for latency by providing it with the -q flag. The default latency is set to 300 microseconds. A higher latency increases the FastRPC overhead observed. Sleep latency: The low power sleep modes of the DSP can be entered by setting the DSP sleep latency . The DCVS V3 power voting is done to set the DSP sleep latency. This is supported only on limited devices. The values set for this should be in the range of 10 to 65535 microseconds. Powerboost: If the power boost is set, the following DCVS V3 voting is implemented: HAP_power_request_t request = {0}; request.type = HAP_power_set_DCVS_v3; request.dcvs_v3.set_dcvs_enable = TRUE; request.dcvs_v3.dcvs_enable = TRUE; request.dcvs_v3.dcvs_option = HAP_DCVS_V2_PERFORMANCE_MODE; request.dcvs_v3.set_bus_params = TRUE; request.dcvs_v3.bus_params.min_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.bus_params.max_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.bus_params.target_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.set_core_params = TRUE; request.dcvs_v3.core_params.min_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.core_params.max_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.core_params.target_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.set_sleep_disable = TRUE; request.dcvs_v3.sleep_disable = TRUE; HAP_power_set(NULL, &request); The above settings can be found in the src/rpcperf_imp.c: profiling_powerboost() and profiling_sleep_latency(). To understand the DCVS_v2 and DCVS_v3 APIs, please refer to the HAP_power API documentation . NOTE: Please note the following when running the FastRPC performance tests: There may be variability of timing results between runs The variability of timing results could be attributed due to a few reasons: Workload on the system Some entity in the system may be entering or exiting low power modes. The noop operation is run once before running the FastRPC performance tests on the inbuf and routbuf buffers. This is done so as to ignore the additional overhead of starting a FastRPC session on the DSP. This noop operation is not a part of the average overhead time displayed. The default Linux kernel boot image enables some config items that would impact performance. The Linux kernel boot image should be built with perf defconfig. The average overhead time should be computed over many iterations, to be more accurate. Profiling summary report The FastRPC performance tests described above returns the following summary report on the command line. This report shows the average FastRPC overhead in microseconds when making FastRPC calls per iteration, with variable input and output buffer sizes. NOTE: The average FastRPC overhead per iteration will vary depending on the target and other factors adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f default [Test_Name Buffer_Size] Avg FastRPC overhead per iteration(micro-seconds) [noop 0K] 311us [inbuf 32K] 634us [routbuf 32K] 627us [inbuf 64K] 713us [routbuf 64K] 784us [inbuf 128K] 823us [routbuf 128K] 790us [inbuf 1M] 909us [routbuf 1M] 887us [inbuf 4M] 900us [routbuf 4M] 891us [inbuf 8M] 897us [routbuf 8M] 907us [inbuf 16M] 919us [routbuf 16M] 896us Running run_profilers.py The run_profilers.py script can be used to run the following profiling tools on simulator and on device: Hexagon Profiler Hexagon Trace Analyzer sysMonApp Unlike the profiling_walkthrough.py script , the run_profilers.py script does not automate the steps of building the binaries and running them on to the device. The user needs to ensure that the profiling_walkthrough.py script has been run or the binaries are present on the device. Usage: run_profilers.py -T Target [-M] [-r PROFILER] Required Argument: Option Description -T TARGET Specify the TARGET on which to run the script. Can be any supported device in the feature matrix Optional Arguments: Option Description -M Don't rebuild -r PROFILER Specify which profiler to run. The Supported values are as follows: all (default) to run all the profiling tools that are supported on the selected target, hexagon_profiler to run the Hexagon profiler, hexagon_trace_analyzer to run the Hexagon Trace Analyzer, and sysmon_app to run the sysmon_app The Hexagon profiler needs to be passed the address at which the simulator loaded the shared object. This is accomplished by following the name of the shared object with a : separator and the load address. The $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/generate_config.py script automates these steps by running first the simulator, capturing the shared object load address, and passing that address to the Hexagon profiler, which will generate the HTML file. Similarly, the Hexagon Trace Analyzer needs to be passed the address at which the simulator loaded the shared object in config.py . The $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/generate_config.py script does so automatically. The run_profilers.py script runs consecutively the Hexagon Trace Analyzer on simulator and the device. The run_profilers.py script calls the $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/generate_config.py script and thus runs without requiring any user intervention. You will only need to modify manually the shared object load address if you decide to run the Hexagon Trace Analyzer or the Hexagon profiler outside of these scripts. Example to run the run_profilers.py script on the sm8250: python run_profilers.py -T sm8250 This will run the simulator tests as well as tests on the SM8250 target. If the example is run on Linux, the Hexagon Trace Analyzer will run as well. Cleaning the folder to remove result files: python run_profilers.py -T sm8250 -r clean This will delete the result files generated by the run_profilers.py script Timer APIs Available Timer APIs : Function Description Usage location HAP_perf_get_time_us Gives the current time in microseconds Usage demonstrated in profiling_memcpy_time_us HAP_perf_get_pcycles Gets the number of processor cycles Usage demonstrated in function HAP_perf_get_qtimer_count Needs to be converted to micro seconds. Gets the qtimer from hardware. qtimer count is in terms of hw_ticks of a 19Mhz clock Usage demonstrated in profiling_asm_iterations_time_us() Hexagon Profiler Overview The Hexagon Profiler is a simulator profiling tool that generates cycle analysis reports in json and html files. This profiler displays information about the execution history of a program, which may be used to identify any processor stalls in a program. To profile the example on the simulator with the Hexagon Profiler, do the following: Run the Hexagon example executable profiling_q.so on the simulator using the following simulator flags: --timing --packet_analyze hexagon_profiler.json --uarchtrace uarch.txt (Optional) The above flags generate the json file hexagon_profiler.json . Run the Hexagon Profiler to convert the json profiling file into a human-readable cycle analysis file in HTML format: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-profiler --packet_analyze --elf=$HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim,hexagon_Debug_toolv84_v66/profiling_q.so:0xd8042000 --json=hexagon_profiler.json -o hexagon_profiler.html NOTE: The address 0xd8042000 is referred as vaddr in the console logs and its value is provided below the text including the shared object name, profiling_q.so. Please change this address in the above command for any other address and ensure this address is correct. Refer to the profiling documentation for further information. The files generated are as follows: File name Description hexagon_profiler.json This json file is an intermediate file generated by the Hexagon Simulator. It contains the profiling information that needs to be post-processed by the Hexagon Profiler. hexagon_profiler.html This file contains profiling data generated by the Hexagon Profiler from the json file collected by the Hexagon Simulator. uarch.txt This is the micro-architecture trace file generated by the Hexagon Simulator. It reports major micro-architecture events like cache misses, packets executed, stall cycles, etc. NOTE: The Micro-architecture trace files( uarch.txt ) can generate huge amounts of data. The option -- uarchtrace should be used with filtering to limit the collection of trace file data to relatively small time periods. For further information refer to the simulator documentation . Understanding the results The HTML report file presents an interactive document that allows you to select and display profiling information. The CORE Stalls tab shows a Summary report, Top packets, Top Functions, the Disassembly/Stall Name, etc. In the Top Packets section, you can observe the count and percentage of the profiling packets and functions. Many of the functions shown will be internal to the Simulator. These stats give a good picture of the packets and functions running on the simulator. The other tabs available are HVX Stalls, Events, PMU Events, Derived Stats, Instructions, and Help. These tabs give additional information about the example that has been profiled. Hexagon Trace Analyzer Overview The Hexagon Trace Analyzer is a software analysis tool that processes the Hexagon Embedded Trace Macrocell (ETM) traces generated by the software running on the cDSP or on the Simulator . In the instructions below, we will demonstrate how to generate a trace on the device or on the simulator when running the profiling example, and how to process this trace with the Hexagon Trace Analyzer. Prerequisites The Hexagon Trace Analyzer works only on Linux systems. The Flamegraph and Catapult tools need to be installed to $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/ to view the results. The run_profilers.py script installs these tools to the appropriate location when first run. You will need an internet connection when running the script for the first time in order to download Flamegraph and Catapult. The links to download these two tools are given above Flamegraph and Catapult work only on systems with Python 2. The Hexagon Trace Analyzer requires the module xlsxwriter and sortedcontainers . Refer to $HEXAGON_SDK_ROOT/utils/scripts/python_requirements.txt for more information on the requirements. Profile on device The following command configures and enables tracing on the device, and launches sysMonApp to start collecting the trace: python $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/target_profile_setup.py -T Target -S trace_size -O trace_size is the size of the trace that will be collected. It is expressed in bytes, in hexadecimal format. The trace file is updated in a circular buffer so that its contents will always reflect the most recent events. The default value is 0x4000000 . The -O option retrieves all the device binaries that the Hexagon Trace Analyzer needs to parse any collected trace. Note: The -O option is only needed the first time you run the script on a newly flashed device. It should be omitted thereafter. Run the application adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f timers While your application is running, copy the ETM trace to a .bin file adb wait-for-device shell \"cat /dev/coresight-tmc-etr > /data/local/tmp/trace.bin\" Copy the trace.bin to your local machine adb wait-for-device pull /data/local/tmp/trace.bin trace.bin Run sysMonApp to display the load offsets of all DSP libraries that were recently loaded on device adb wait-for-device shell /data/local/tmp/sysMonApp etmTrace --command dll --q6 cdsp You should see output lines similar to: data.ELF_NAME = libprofiling_skel.so data.LOAD_ADDRESS = 0xe040c000 data.ELF_IDENTIFIER = 0x00014000 data.LOAD_TIMESTAMP = 0x1ea6b0def4 data.UNLOAD_TIMESTAMP = 0x1ea726162d Edit the config.py file by modifying the library names (i.e. ELF_NAME) and load addresses (i.e. LOAD_ADDRESS) values obtained from the etmTrace sysMonApp service that was invoked above. You can refer to the example config file $HEXAGON_SDK_ROOT/examples/profiling/config_example.py . NOTE: Make sure the LLVM_TOOLS_PATH tools version in the config.py file matches the version of the tools present in $HEXAGON_SDK_ROOT/tools/HEXAGON_Tools/ If the same library name occurs multiple times with different load addresses in the report from the sysMonApp etmTrace service, include all these instances in the config.py file If the target is Lahaina, include ver=\"V68\" in the config.py file Run the Hexagon Trace Analyzer to run generate the analysis reports: $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/hexagon-trace-analyzer ./config.py ./HexTA_result ./trace.bin In the command line above config.py is the configuration file you edited HexTA_result is the directory in which the results will be placed trace.bin is the trace file that you retrieved from the device NOTE: If the target is Lahaina, you might see the following or similar logs on the console: [47][ERROR] handleAsid ERROR zero!=1 [190][ERROR] handleProfile ERROR zero!=0 [190][ERROR] Has the correct version of Q6 been specified in config.py ? [684][ERROR] Dropping Negative Gsync in timeline [thread,pcAddr,cycles,Gsync,recordOffset]:[4,4262405372,3,-38,84338] These messages might show up as the ETM trace might have invalid packet headers and some fields in the packets might be corrupted. These errors are not of concern and do not affect the output of the Hexagon Trace Analyzer. Profile on simulator Run the application on the simulator with the following args: --timing --pctrace_nano For example to run on the DSP v66: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-sim --timing --pctrace_nano pctrace.txt -mv66g_1024 --simulated_returnval --usefs $HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66 --pmu_statsfile $HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66/pmu_stats.txt --cosim_file $HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66/q6ss.cfg --l2tcm_base 0xd800 --rtos $HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66/osam.cfg $HEXAGON_SDK_ROOT/rtos/qurt/computev66/sdksim_bin/runelf.pbn -- $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim -- ./$HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66/profiling_q.so -f timers This command will result in generating a trace file pctrace.txt using the Hexagon Simulator when running the profiling_q.so on run_main_on_hexagon_sim executable. For other DSPs the argument -mv66g_1024 can be changed appropriately. Refer to the Running on simulator section above and simulator documentation for further information. NOTE: Make sure in the config_simulator.py script the LLVM_TOOLS_PATH tools version is the same as $HEXAGON_SDK_ROOT/tools/HEXAGON_Tools/8.4.12 in your currently installed Hexagon SDK. Make sure the extension for the trace file for the simulator is .txt. If .bin is used like the trace.bin file pulled from device, the Hexagon Trace Analyzer will not be able to process the traces from simulator. Finally run the Hexagon Trace Analyzer with the config_simulator.py file already present in the example folder : $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/hexagon-trace-analyzer ./config_simulator.py ./result_hexta_simulator ./pctrace.txt In the command line above config_simulator.py is the configuration file result_hexta_simulator is the directory in which the results from simulator will be placed pctrace.txt is the trace file generated when running the application on the simulator Understanding the results The perFunctionStats.xlsx file in the generated result folder has a column \"function\". The column has a drop-down menu allowing you to select the function for which you want to display statistics. The cycleCount column in this file shows the total number of cycles a function has consumed. The same information can also be viewed in flamegraph/globalCycles_icicle. To view the breakdown of cycles per call to this function you can look for the function in flamegraph/globalCycles Run command using the run_profilers.py script: The Hexagon Trace Analyzer works only on Linux systems. For device: python run_profilers.py -T sm8250 -r hexagon_trace_analyzer For simulator: python run_profilers.py -T simulator -r hexagon_trace_analyzer The files generated: File name Description trace.bin This is the binary file pulled from the device flamegraph files Graphical demonstration of the different functions catapult files Graphical demonstration of the different functions xlsx files These files have the time of execution in numerical format The perFunctionStats.xlsx file for the device has a detailed analysis of the different functions running on the device. Many of these functions are marked as hidden_function. These hidden_functions are internal to the device but the user can observe the effects of these functions. To find a particular function to analze, a user can use the drop-down filter for columns to filter the functions. A filtered view of these functions is shown below. The filtered view of the functions shows the relevant functions along with the different cycle count, %load, cycles per packet(cpp), unused bytes, etc. You can make an analysis of the functions that are taking too many cycles using this information. Flamegraph gives a graphical representation of the different profiled functions. The global cycles is one such graphical representation that shows the function call tree with corresponding cycles. The width of each bar is proportional to the number of cycles spent in that task/function and its children. A function can be searched in the top right corner search box. Once found, the function is marked in a shade of purple to highlight it. The function details are showing below the Flamegraph. In the above image profiling_memcpy_time_pcycles is shown as selected. Majority of the functions shown are marked hidden as they are internal to the device. Globalcycles icicles is similar to the global cycles but with an inverted layout. The Global Packets is graphical representation of the packets run for each function. In this representation as well, you can look for a particular function. Once the function is found, it is marked with a shade of purple and its statistics are displayed. Catapult provides another graphical representation of the profiled functions. Catapult provides an interface where you can pan and zoom, in and out of the graphical representation using the zooming options provided at the right side of the interface. To the left of the interface, HWT represents the HW threads. To the extreme right of the interface there are additional tabs that can also be used for analysis. You can search for your function in the top right corner of the page. The statistics of that function appears in the bottom half of the page. On the simulator, the results are quite similar to the results obtained from the device: sysMonApp Overview The sysMonApp is an app that configures and displays multiple performance-related services on the cDSP, aDSP, and sDSP. The sysMonApp can be run as a standalone profiling tool or as an Android Application. In the instructions below we will demonstrate how to use the sysMonApp profiler tools to analyze the profiling example on the device. Profile from the Command Line Copy the sysMonApp to the device adb push $HEXAGON_SDK_ROOT/tools/utils/sysmon/sysMonApp /data/local/tmp/ adb shell chmod 777 /data/local/tmp/sysMonApp Make sure adsprpcd is running: adb shell ps | grep adsprpcd Run the sysMonApp profiler service in user mode on cDSP as follows: adb shell /data/local/tmp/sysMonApp profiler --debugLevel 1 --q6 cdsp Run the profiling example on device: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f inbuf -s 65536 -m') The previous command should have generated a file /sdcard/sysmon_cdsp.bin . This file should be pulled to your local computer as follows: adb pull /sdcard/sysmon_cdsp.bin NOTE: On some devices, the trace file may be generated in the /data folder instead. The sysmon_cdsp.bin file can be parsed with the sysMonApp parser service as shown below. Run the sysMonApp Parser NOTE: The Hexagon SDK contains Windows and Linux versions of the parser. Both versions have the same interface. Below we illustrate how to use the Linux parser. For Windows simply replace parser_linux_v2 with parser_win_v2 in the commands below. The following example command shows how to generate HTML and CSV files in Linux to output directory sysmon_parsed_output . The sysMonApp parser usage can be found here . The Windows tool expects the same arguments: $HEXAGON_SDK_ROOT/tools/utils/sysmon/parser_linux_v2/HTML_Parser/sysmon_parser ./sysmon_cdsp.bin --outdir sysmon_parsed_output Profile using Android Application Install the sysMon DSP profiler application on the device using adb install: adb install -g $HEXAGON_SDK_ROOT/tools/utils/sysmon/sysMon_DSP_Profiler_V2.apk The sysMon DSP Profiler UI for the sysMonApp Android Application provides user flexibility to choose from different modes of profiling. You can select the Mode based on your requirements. The available modes are: Mode Description DSP DCVS User can adjust DSP core and bus clocks dynamically for profiling duration. Default Mode A fixed set of performance metrics will be monitored. Sampling period is either 1 or 50 milli-seconds. User Mode If Default Mode is unchecked, User Mode is enabled. The user can select the desired PMU events to be captured. 8-PMU Mode If both DSP DCVS and Default modes are unchecked, 8-PMU mode is enabled. This allows user to choose whether to configure 4 PMU events or 8 PMU Events. The Configuration Settings button is enabled in user mode and you can select the PMU events to be captured. Further details about the different modes and options can be found in sysMonApp Profiler . Understanding the results The files generated by sysMon Profiler application are the following: File name Description sysmon_cdsp.bin This is the cDSP binary file pulled from the device. sysmon_report directory This directory is generated after running the sysMonApp HTML Parser. It contains HTML and csv files generated by the HTML Parser. This directory is named based on the date the report was generated. The sysMonApp HTML file gives an overall summary of the profiling done using the sysMonApp. Here is an example of the HTML generated from the sysMonApp. To understand the output of these files generated please refer to the sysMonApp profiler documentation","title":"Profiling"},{"location":"examples/profiling/index.html#profiling-example","text":"","title":"Profiling Example"},{"location":"examples/profiling/index.html#overview","text":"This example illustrates the use of the various profiling tools and APIs available on the Hexagon SDK. This example also measures and reports the FastRPC performance with different buffer sizes and optimizations . This example shows how to use the following: The FastRPC performance tests can be viewed in the shell. These tests print the overhead time for a CPU thread to offload a task to the DSP, wait for the DSP to execute that task, and wake up when the task is complete. The time measurements are meaningless on the simulator. The timer APIs demonstrate the APIs that can be used for accessing cycle and clock counters and use them to time a block of code. These APIs work on both the simulator and device. The time measurements are meaningless on the simulator. The Hexagon Profiler generates function-level profiling reports. This profiler can be used only on the simulator. The Hexagon Trace Analyzer parses trace files generated on device or on simulator and generates various analysis reports. This profiler works only on Linux. The sysMonApp displays various performance-related services on the cDSP, aDSP, and sDSP. This profiler can only be used on device.","title":"Overview"},{"location":"examples/profiling/index.html#prerequisites","text":"Before the profiling application is run the following conditions should be met: Make sure the Hexagon SDK is properly setup with the Setup Instructions . Some of the profiling tools discussed in this example are not supported on all devices. Please refer to the feature matrix for more details. The Hexagon Trace Analyzer requires Flamegraph and Catapult . These tools will be downloaded from the internet when the run_profilers.py script is run for the first time. You can download Flamegraph here and Catapult here .","title":"Prerequisites"},{"location":"examples/profiling/index.html#running-profiling_walkthroughpy","text":"The profiling_walkthrough.py script builds and runs the timer APIs and default FastRPC performance tests on a device. Usage: profiling_walkthrough.py [-p] [-n ITERATIONS] [-i] [-m] [-u] [-q FASTRPC_QOS] [-U UNSIGNED_PD_FLAG] In addition to the options supported by all walkthrough scripts , the profiling_walkthrough.py script supports the following options: Optional arguments: Option Description Default -n ITERATIONS Number of iterations to run. -n 1000 -d DOMAIN Specifies the domain on which to run the example. MDSP and SDSP are not supported. Domain is CDSP by default. 0: ADSP 3: CDSP -d 3 -q FASTRPC_QOS Set the FastRPC QoS latency in micro-seconds. This can improve performance at the expense of some increase in power consumption. QoS is turned ON by default. 0 will turn QoS OFF. Positive numbers set the QoS latency in micro-seconds. -q 300 -y SLEEP_LATENCY Set DSP Sleep Latency. 0 is default and means DCVS V3 vote will not be made. Range of values is from 10 to 65535 microseconds. To remove a previous latency vote, simply vote for a large latency such as 65535 to indicate that there is no specific latency request. -y 0 -U UNSIGNED_PD_FLAG Set the PD to run the example. 1 is default and means run the example on the Unsigned PD. -U 1 -p Run power boost using the request type HAP_power_set_DCVS_v3 with HAP_DCVS_V2_PERFORMANCE_MODE. Further details are provided below . OFF -i Disable ION memory for FastRPC performance tests. ION memory is used by default. This option is relevant only for FUNCTION_NAME default, noop, inbuf, routbuf. Otherwise this option is ignored. ON -m Verify memory operations for FastRPC performance tests by passing a fixed pattern to the DSP function which copies it to the output. The caller then confirms the correct pattern is received. OFF -u Use uncached buffers on the HLOS. OFF For example, to build and run the timer APIs and default FastRPC performance tests on a sm8250 device: python profiling_walkthrough.py -T sm8250","title":"Running profiling_walkthrough.py"},{"location":"examples/profiling/index.html#building","text":"Before modifying or understanding the code, we need to make sure we can build the example and generate the binary files from source. You can reproduce the steps of the walkthrough script by building and running the example manually as explained below. Build Android variant as follows: make android BUILD=Debug Build Hexagon variant as follows: make hexagon BUILD=Debug DSP_ARCH=v65 For more information on the build flavors available and build syntax, please refer to the building reference instructions .","title":"Building"},{"location":"examples/profiling/index.html#running-on-device","text":"Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push Android components as follows: adb wait-for-device shell mkdir -p /vendor/bin adb wait-for-device push android_Debug_aarch64/ship/profiling /vendor/bin adb wait-for-device shell chmod 777 /vendor/bin/profiling adb wait-for-device push android_Debug_aarch64/ship/libprofiling.so /vendor/lib64/ Push Hexagon components as follows: adb wait-for-device shell mkdir -p /vendor/lib/rfsa/dsp/sdk/ adb wait-for-device push hexagon_Debug_toolv84_v66/ship/libprofiling_skel.so /vendor/lib/rfsa/dsp/sdk/ Generate a device-specific test signature based on the device serial number Follow the steps listed in the signing documentation . NOTE: Signing the device needs to be done once as the same test signature will enable loading any module. Direct DSP messages to logcat: adb wait-for-device shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/profiling.farf\" adb wait-for-device Open another window to see aDSP diagnostic messages: Linux: In a separate terminal: adb logcat -s adsprpc Windows: start cmd.exe /c adb logcat -s adsprpc sleep 2 Run Profiling Example: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling Optional Arguments supported by the binary when running the timer APIs and FastRPC performance : Usage: /vendor/bin/profiling [-f FUNCTION_NAME] [-n ITERATIONS] [-s SIZE] [-q FASTRPC_QOS] [-U UNSIGNED_PD_FLAG] [-p] [-i] [-m] [-u] Option Description -f FUNCTION_NAME Specify which function to run. The supported values are as follows: timers to run the timer API examples, default to run a default set of FastRPC performance tests with configurations which are cached, ion set by user, variable size, check_memory off. This option is run if -f is not provided. noop to run the FastRPC performance test with no operation, inbuf to run the FastRPC performance test with input buffer , and routbuf to run the FastRPC performance test with output buffer . -d DOMAIN Specifies the domain on which to run the example. MDSP and SDSP are not supported. Domain is CDSP by default. 0: ADSP 3: CDSP -n ITERATIONS Iterations to run FastRPC performance tests. 1000 is default. This option is relevant only for FUNCTION_NAME noop inbuf, routbuf. Otherwise this option is ignored. -s SIZE Size of data buffer for FastRPC performance tests. If 0 is given the noop function will run. -q QOS_LATENCY Set the FastRPC QoS latency in micro-seconds. Increasing the latency will save power at the expense of low performance. QoS is turned ON by default. 0 will turn QoS OFF. Positive numbers set the QoS latency in micro-seconds (300 is default). -y SLEEP_LATENCY Set DSP Sleep Latency. 0 is default and means DCVS V3 vote will not be made. Range of values is from 10 to 65535 microseconds. To remove a previous latency vote, simply vote for a large latency such as 65535 to indicate that there is no specific latency request. -U UNSIGNED_PD_FLAG Set the PD to run the example. 1 is default and means run the example on the Unsigned PD. -p Run power boost using the request type HAP_power_set_DCVS_v3 with HAP_DCVS_V2_PERFORMANCE_MODE. Further details are provided below -i Disable ION memory for FastRPC performance tests. ION memory is used by default. This option is ignored with the FUNCTION_NAME timers. Otherwise the option is relevant. -m Verify memory operations for FastRPC performance tests by passing a fixed pattern to the DSP function which copies the pattern to the output. The caller then confirms the correct pattern is received. This option is relevant only for FUNCTION_NAME inbuf, routbuf. Otherwise this option is ignored. -u Use Uncached buffers on HLOS for FastRPC performance tests. This option is relevant only for FUNCTION_NAME inbuf, routbuf. Otherwise this option is ignored. Example commands with optional arguments to run the FastRPC tests on a specific buffer: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f inbuf -n 5000 -s 8388608 -p -q 700 The above example command will run the FastRPC performance test on an input buffer of size 8 MB with power boost and FastRPC QoS set to 700 micro-seconds. adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f default The above example command will run the FastRPC performance tests with on cached buffers, with ion disabled, with check memory off. The buffer types and sizes run will be the following: Buffer type Buffer size noop 0K inbuf 32K routbuf 32K inbuf 64K routbuf 64K inbuf 128K routbuf 128K inbuf 1M routbuf 1M inbuf 4M routbuf 4M inbuf 8M routbuf 8M inbuf 16M routbuf 16M The results of the above command is as shown in Profiling summary report .","title":"Running on device"},{"location":"examples/profiling/index.html#running-on-simulator","text":"When running the example on the Hexagon simulator , you need to use the --timing option for the cycle count to be accurate. The time measurements will not be meaningful. When running on the simulator, you need to pass both the arguments to the Hexagon simulator and to the test executable. The arguments given to the executable are given in the previous section . The simulator arguments are as follows: Simulator option Description --mv* Simulate for a particular architecture version of Hexagon. E.g. -mv65 . --timing Model processor micro-architecture with cache, multi-threading mode, and processor stalls as part of the simulation. This enables timing so that the simulation more accurately models the operation of the processor hardware. --simulated_returnval Cause the simulator to return a value to its caller indicating the final execution status of the target application. --usefs <path> Cause the simulator to search for required files in the directory with the specified path. --pmu_statsfile <filename> Generate a PMU statistics file with the specified name. See the SysMon Profiler for more information about PMU events. --help Prints available simulator options. The Hexagon Architecture version for the -mv* flag can be selected based on the DSP architecture version from the following: DSP Architecture version Simulator Flag v65 -mv65a_1024 v66 -mv66g_1024 v68 -mv68n_1024 As an example, here is how the profiling_q.so file might be run on the simulator: cd $HEXAGON_SDK_ROOT/examples/profiling/ $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-sim -mv65a_1024 --timing --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg $HEXAGON_SDK_ROOT/rtos/qurt/computev65/sdksim_bin/runelf.pbn -- $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v65/run_main_on_hexagon_sim -- ./hexagon_Debug_toolv84_v65/profiling_q.so -f timers","title":"Running on simulator"},{"location":"examples/profiling/index.html#fastrpc-performance-tests","text":"The FastRPC overhead can be calculated for different buffer sizes. Below is the brief description of the tests that can be executed: Function Description noop Makes a FastRPC invocation with no input and output buffers inbuf Makes a FastRPC invocation with one input buffer of varying buffer sizes ranging from 32K to 16M routbuf Makes a FastRPC invocation with one output buffer of varying buffer sizes ranging from 32K to 16M The input buffer and output buffer are defined in the IDL file inc/profiling.idl. To understand the in and rout parameters given in the idl file, please refer to the IDL documentation .","title":"FastRPC performance tests"},{"location":"examples/profiling/index.html#contributing-factors-to-fastrpc-performance","text":"Various factors impact the performance of FastRPC. Some of the relevant factors are described below. For a detailed explanation of these factors refer to the system performance documentation. The profiling application allows you to explore the impact of some of these factors. ION buffers: ION buffers (available on Android targets) do not require extra copies of buffers when shared between the DSP and the CPU. Non-ION buffers will automatically be copied by FastRPC framework. The impact on performance can be observed in the FastRPC performance tests by using the -i flag to disable ION memory for the FastRPC performance tests. Caches: The FastRPC driver takes care of maintaining cache coherency between CPU and DSP for shared buffers. When the -u flag is given for uncached buffers, RPCMEM_HEAP_UNCACHED is used. You can refer to the RPCMEM API to learn more. Caching should result in better performance, but this depends on the application. FastRPC QoS: FastRPC offers a QoS API to enable users to request for a required FastRPC latency between CPU application making the remote call and beginning of DSP method execution. This latency requested is not guaranteed but the driver tries to meet available options on the target. Refer to remote_handle_control() and remote_handle64_control() defined at remote API . In the FastRPC performance tests, the user can request for latency by providing it with the -q flag. The default latency is set to 300 microseconds. A higher latency increases the FastRPC overhead observed. Sleep latency: The low power sleep modes of the DSP can be entered by setting the DSP sleep latency . The DCVS V3 power voting is done to set the DSP sleep latency. This is supported only on limited devices. The values set for this should be in the range of 10 to 65535 microseconds. Powerboost: If the power boost is set, the following DCVS V3 voting is implemented: HAP_power_request_t request = {0}; request.type = HAP_power_set_DCVS_v3; request.dcvs_v3.set_dcvs_enable = TRUE; request.dcvs_v3.dcvs_enable = TRUE; request.dcvs_v3.dcvs_option = HAP_DCVS_V2_PERFORMANCE_MODE; request.dcvs_v3.set_bus_params = TRUE; request.dcvs_v3.bus_params.min_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.bus_params.max_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.bus_params.target_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.set_core_params = TRUE; request.dcvs_v3.core_params.min_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.core_params.max_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.core_params.target_corner = HAP_DCVS_VCORNER_MAX; request.dcvs_v3.set_sleep_disable = TRUE; request.dcvs_v3.sleep_disable = TRUE; HAP_power_set(NULL, &request); The above settings can be found in the src/rpcperf_imp.c: profiling_powerboost() and profiling_sleep_latency(). To understand the DCVS_v2 and DCVS_v3 APIs, please refer to the HAP_power API documentation . NOTE: Please note the following when running the FastRPC performance tests: There may be variability of timing results between runs The variability of timing results could be attributed due to a few reasons: Workload on the system Some entity in the system may be entering or exiting low power modes. The noop operation is run once before running the FastRPC performance tests on the inbuf and routbuf buffers. This is done so as to ignore the additional overhead of starting a FastRPC session on the DSP. This noop operation is not a part of the average overhead time displayed. The default Linux kernel boot image enables some config items that would impact performance. The Linux kernel boot image should be built with perf defconfig. The average overhead time should be computed over many iterations, to be more accurate.","title":"Contributing factors to FastRPC performance"},{"location":"examples/profiling/index.html#profiling-summary-report","text":"The FastRPC performance tests described above returns the following summary report on the command line. This report shows the average FastRPC overhead in microseconds when making FastRPC calls per iteration, with variable input and output buffer sizes. NOTE: The average FastRPC overhead per iteration will vary depending on the target and other factors adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f default [Test_Name Buffer_Size] Avg FastRPC overhead per iteration(micro-seconds) [noop 0K] 311us [inbuf 32K] 634us [routbuf 32K] 627us [inbuf 64K] 713us [routbuf 64K] 784us [inbuf 128K] 823us [routbuf 128K] 790us [inbuf 1M] 909us [routbuf 1M] 887us [inbuf 4M] 900us [routbuf 4M] 891us [inbuf 8M] 897us [routbuf 8M] 907us [inbuf 16M] 919us [routbuf 16M] 896us","title":"Profiling summary report"},{"location":"examples/profiling/index.html#running-run_profilerspy","text":"The run_profilers.py script can be used to run the following profiling tools on simulator and on device: Hexagon Profiler Hexagon Trace Analyzer sysMonApp Unlike the profiling_walkthrough.py script , the run_profilers.py script does not automate the steps of building the binaries and running them on to the device. The user needs to ensure that the profiling_walkthrough.py script has been run or the binaries are present on the device. Usage: run_profilers.py -T Target [-M] [-r PROFILER] Required Argument: Option Description -T TARGET Specify the TARGET on which to run the script. Can be any supported device in the feature matrix Optional Arguments: Option Description -M Don't rebuild -r PROFILER Specify which profiler to run. The Supported values are as follows: all (default) to run all the profiling tools that are supported on the selected target, hexagon_profiler to run the Hexagon profiler, hexagon_trace_analyzer to run the Hexagon Trace Analyzer, and sysmon_app to run the sysmon_app The Hexagon profiler needs to be passed the address at which the simulator loaded the shared object. This is accomplished by following the name of the shared object with a : separator and the load address. The $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/generate_config.py script automates these steps by running first the simulator, capturing the shared object load address, and passing that address to the Hexagon profiler, which will generate the HTML file. Similarly, the Hexagon Trace Analyzer needs to be passed the address at which the simulator loaded the shared object in config.py . The $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/generate_config.py script does so automatically. The run_profilers.py script runs consecutively the Hexagon Trace Analyzer on simulator and the device. The run_profilers.py script calls the $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/generate_config.py script and thus runs without requiring any user intervention. You will only need to modify manually the shared object load address if you decide to run the Hexagon Trace Analyzer or the Hexagon profiler outside of these scripts. Example to run the run_profilers.py script on the sm8250: python run_profilers.py -T sm8250 This will run the simulator tests as well as tests on the SM8250 target. If the example is run on Linux, the Hexagon Trace Analyzer will run as well. Cleaning the folder to remove result files: python run_profilers.py -T sm8250 -r clean This will delete the result files generated by the run_profilers.py script","title":"Running run_profilers.py"},{"location":"examples/profiling/index.html#timer-apis","text":"Available Timer APIs : Function Description Usage location HAP_perf_get_time_us Gives the current time in microseconds Usage demonstrated in profiling_memcpy_time_us HAP_perf_get_pcycles Gets the number of processor cycles Usage demonstrated in function HAP_perf_get_qtimer_count Needs to be converted to micro seconds. Gets the qtimer from hardware. qtimer count is in terms of hw_ticks of a 19Mhz clock Usage demonstrated in profiling_asm_iterations_time_us()","title":"Timer APIs"},{"location":"examples/profiling/index.html#hexagon-profiler","text":"","title":"Hexagon Profiler"},{"location":"examples/profiling/index.html#overview_1","text":"The Hexagon Profiler is a simulator profiling tool that generates cycle analysis reports in json and html files. This profiler displays information about the execution history of a program, which may be used to identify any processor stalls in a program. To profile the example on the simulator with the Hexagon Profiler, do the following: Run the Hexagon example executable profiling_q.so on the simulator using the following simulator flags: --timing --packet_analyze hexagon_profiler.json --uarchtrace uarch.txt (Optional) The above flags generate the json file hexagon_profiler.json . Run the Hexagon Profiler to convert the json profiling file into a human-readable cycle analysis file in HTML format: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-profiler --packet_analyze --elf=$HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim,hexagon_Debug_toolv84_v66/profiling_q.so:0xd8042000 --json=hexagon_profiler.json -o hexagon_profiler.html NOTE: The address 0xd8042000 is referred as vaddr in the console logs and its value is provided below the text including the shared object name, profiling_q.so. Please change this address in the above command for any other address and ensure this address is correct. Refer to the profiling documentation for further information. The files generated are as follows: File name Description hexagon_profiler.json This json file is an intermediate file generated by the Hexagon Simulator. It contains the profiling information that needs to be post-processed by the Hexagon Profiler. hexagon_profiler.html This file contains profiling data generated by the Hexagon Profiler from the json file collected by the Hexagon Simulator. uarch.txt This is the micro-architecture trace file generated by the Hexagon Simulator. It reports major micro-architecture events like cache misses, packets executed, stall cycles, etc. NOTE: The Micro-architecture trace files( uarch.txt ) can generate huge amounts of data. The option -- uarchtrace should be used with filtering to limit the collection of trace file data to relatively small time periods. For further information refer to the simulator documentation .","title":"Overview"},{"location":"examples/profiling/index.html#understanding-the-results","text":"The HTML report file presents an interactive document that allows you to select and display profiling information. The CORE Stalls tab shows a Summary report, Top packets, Top Functions, the Disassembly/Stall Name, etc. In the Top Packets section, you can observe the count and percentage of the profiling packets and functions. Many of the functions shown will be internal to the Simulator. These stats give a good picture of the packets and functions running on the simulator. The other tabs available are HVX Stalls, Events, PMU Events, Derived Stats, Instructions, and Help. These tabs give additional information about the example that has been profiled.","title":"Understanding the results"},{"location":"examples/profiling/index.html#hexagon-trace-analyzer","text":"","title":"Hexagon Trace Analyzer"},{"location":"examples/profiling/index.html#overview_2","text":"The Hexagon Trace Analyzer is a software analysis tool that processes the Hexagon Embedded Trace Macrocell (ETM) traces generated by the software running on the cDSP or on the Simulator . In the instructions below, we will demonstrate how to generate a trace on the device or on the simulator when running the profiling example, and how to process this trace with the Hexagon Trace Analyzer.","title":"Overview"},{"location":"examples/profiling/index.html#prerequisites_1","text":"The Hexagon Trace Analyzer works only on Linux systems. The Flamegraph and Catapult tools need to be installed to $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/ to view the results. The run_profilers.py script installs these tools to the appropriate location when first run. You will need an internet connection when running the script for the first time in order to download Flamegraph and Catapult. The links to download these two tools are given above Flamegraph and Catapult work only on systems with Python 2. The Hexagon Trace Analyzer requires the module xlsxwriter and sortedcontainers . Refer to $HEXAGON_SDK_ROOT/utils/scripts/python_requirements.txt for more information on the requirements.","title":"Prerequisites"},{"location":"examples/profiling/index.html#profile-on-device","text":"The following command configures and enables tracing on the device, and launches sysMonApp to start collecting the trace: python $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/target_profile_setup.py -T Target -S trace_size -O trace_size is the size of the trace that will be collected. It is expressed in bytes, in hexadecimal format. The trace file is updated in a circular buffer so that its contents will always reflect the most recent events. The default value is 0x4000000 . The -O option retrieves all the device binaries that the Hexagon Trace Analyzer needs to parse any collected trace. Note: The -O option is only needed the first time you run the script on a newly flashed device. It should be omitted thereafter. Run the application adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f timers While your application is running, copy the ETM trace to a .bin file adb wait-for-device shell \"cat /dev/coresight-tmc-etr > /data/local/tmp/trace.bin\" Copy the trace.bin to your local machine adb wait-for-device pull /data/local/tmp/trace.bin trace.bin Run sysMonApp to display the load offsets of all DSP libraries that were recently loaded on device adb wait-for-device shell /data/local/tmp/sysMonApp etmTrace --command dll --q6 cdsp You should see output lines similar to: data.ELF_NAME = libprofiling_skel.so data.LOAD_ADDRESS = 0xe040c000 data.ELF_IDENTIFIER = 0x00014000 data.LOAD_TIMESTAMP = 0x1ea6b0def4 data.UNLOAD_TIMESTAMP = 0x1ea726162d Edit the config.py file by modifying the library names (i.e. ELF_NAME) and load addresses (i.e. LOAD_ADDRESS) values obtained from the etmTrace sysMonApp service that was invoked above. You can refer to the example config file $HEXAGON_SDK_ROOT/examples/profiling/config_example.py . NOTE: Make sure the LLVM_TOOLS_PATH tools version in the config.py file matches the version of the tools present in $HEXAGON_SDK_ROOT/tools/HEXAGON_Tools/ If the same library name occurs multiple times with different load addresses in the report from the sysMonApp etmTrace service, include all these instances in the config.py file If the target is Lahaina, include ver=\"V68\" in the config.py file Run the Hexagon Trace Analyzer to run generate the analysis reports: $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/hexagon-trace-analyzer ./config.py ./HexTA_result ./trace.bin In the command line above config.py is the configuration file you edited HexTA_result is the directory in which the results will be placed trace.bin is the trace file that you retrieved from the device NOTE: If the target is Lahaina, you might see the following or similar logs on the console: [47][ERROR] handleAsid ERROR zero!=1 [190][ERROR] handleProfile ERROR zero!=0 [190][ERROR] Has the correct version of Q6 been specified in config.py ? [684][ERROR] Dropping Negative Gsync in timeline [thread,pcAddr,cycles,Gsync,recordOffset]:[4,4262405372,3,-38,84338] These messages might show up as the ETM trace might have invalid packet headers and some fields in the packets might be corrupted. These errors are not of concern and do not affect the output of the Hexagon Trace Analyzer.","title":"Profile on device"},{"location":"examples/profiling/index.html#profile-on-simulator","text":"Run the application on the simulator with the following args: --timing --pctrace_nano For example to run on the DSP v66: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-sim --timing --pctrace_nano pctrace.txt -mv66g_1024 --simulated_returnval --usefs $HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66 --pmu_statsfile $HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66/pmu_stats.txt --cosim_file $HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66/q6ss.cfg --l2tcm_base 0xd800 --rtos $HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66/osam.cfg $HEXAGON_SDK_ROOT/rtos/qurt/computev66/sdksim_bin/runelf.pbn -- $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/ship/hexagon_toolv84_v66/run_main_on_hexagon_sim -- ./$HEXAGON_SDK_ROOT/examples/profiling/hexagon_Debug_toolv84_v66/profiling_q.so -f timers This command will result in generating a trace file pctrace.txt using the Hexagon Simulator when running the profiling_q.so on run_main_on_hexagon_sim executable. For other DSPs the argument -mv66g_1024 can be changed appropriately. Refer to the Running on simulator section above and simulator documentation for further information. NOTE: Make sure in the config_simulator.py script the LLVM_TOOLS_PATH tools version is the same as $HEXAGON_SDK_ROOT/tools/HEXAGON_Tools/8.4.12 in your currently installed Hexagon SDK. Make sure the extension for the trace file for the simulator is .txt. If .bin is used like the trace.bin file pulled from device, the Hexagon Trace Analyzer will not be able to process the traces from simulator. Finally run the Hexagon Trace Analyzer with the config_simulator.py file already present in the example folder : $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer/hexagon-trace-analyzer ./config_simulator.py ./result_hexta_simulator ./pctrace.txt In the command line above config_simulator.py is the configuration file result_hexta_simulator is the directory in which the results from simulator will be placed pctrace.txt is the trace file generated when running the application on the simulator","title":"Profile on simulator"},{"location":"examples/profiling/index.html#understanding-the-results_1","text":"The perFunctionStats.xlsx file in the generated result folder has a column \"function\". The column has a drop-down menu allowing you to select the function for which you want to display statistics. The cycleCount column in this file shows the total number of cycles a function has consumed. The same information can also be viewed in flamegraph/globalCycles_icicle. To view the breakdown of cycles per call to this function you can look for the function in flamegraph/globalCycles Run command using the run_profilers.py script: The Hexagon Trace Analyzer works only on Linux systems. For device: python run_profilers.py -T sm8250 -r hexagon_trace_analyzer For simulator: python run_profilers.py -T simulator -r hexagon_trace_analyzer The files generated: File name Description trace.bin This is the binary file pulled from the device flamegraph files Graphical demonstration of the different functions catapult files Graphical demonstration of the different functions xlsx files These files have the time of execution in numerical format The perFunctionStats.xlsx file for the device has a detailed analysis of the different functions running on the device. Many of these functions are marked as hidden_function. These hidden_functions are internal to the device but the user can observe the effects of these functions. To find a particular function to analze, a user can use the drop-down filter for columns to filter the functions. A filtered view of these functions is shown below. The filtered view of the functions shows the relevant functions along with the different cycle count, %load, cycles per packet(cpp), unused bytes, etc. You can make an analysis of the functions that are taking too many cycles using this information. Flamegraph gives a graphical representation of the different profiled functions. The global cycles is one such graphical representation that shows the function call tree with corresponding cycles. The width of each bar is proportional to the number of cycles spent in that task/function and its children. A function can be searched in the top right corner search box. Once found, the function is marked in a shade of purple to highlight it. The function details are showing below the Flamegraph. In the above image profiling_memcpy_time_pcycles is shown as selected. Majority of the functions shown are marked hidden as they are internal to the device. Globalcycles icicles is similar to the global cycles but with an inverted layout. The Global Packets is graphical representation of the packets run for each function. In this representation as well, you can look for a particular function. Once the function is found, it is marked with a shade of purple and its statistics are displayed. Catapult provides another graphical representation of the profiled functions. Catapult provides an interface where you can pan and zoom, in and out of the graphical representation using the zooming options provided at the right side of the interface. To the left of the interface, HWT represents the HW threads. To the extreme right of the interface there are additional tabs that can also be used for analysis. You can search for your function in the top right corner of the page. The statistics of that function appears in the bottom half of the page. On the simulator, the results are quite similar to the results obtained from the device:","title":"Understanding the results"},{"location":"examples/profiling/index.html#sysmonapp","text":"","title":"sysMonApp"},{"location":"examples/profiling/index.html#overview_3","text":"The sysMonApp is an app that configures and displays multiple performance-related services on the cDSP, aDSP, and sDSP. The sysMonApp can be run as a standalone profiling tool or as an Android Application. In the instructions below we will demonstrate how to use the sysMonApp profiler tools to analyze the profiling example on the device.","title":"Overview"},{"location":"examples/profiling/index.html#profile-from-the-command-line","text":"Copy the sysMonApp to the device adb push $HEXAGON_SDK_ROOT/tools/utils/sysmon/sysMonApp /data/local/tmp/ adb shell chmod 777 /data/local/tmp/sysMonApp Make sure adsprpcd is running: adb shell ps | grep adsprpcd Run the sysMonApp profiler service in user mode on cDSP as follows: adb shell /data/local/tmp/sysMonApp profiler --debugLevel 1 --q6 cdsp Run the profiling example on device: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f inbuf -s 65536 -m') The previous command should have generated a file /sdcard/sysmon_cdsp.bin . This file should be pulled to your local computer as follows: adb pull /sdcard/sysmon_cdsp.bin NOTE: On some devices, the trace file may be generated in the /data folder instead. The sysmon_cdsp.bin file can be parsed with the sysMonApp parser service as shown below. Run the sysMonApp Parser NOTE: The Hexagon SDK contains Windows and Linux versions of the parser. Both versions have the same interface. Below we illustrate how to use the Linux parser. For Windows simply replace parser_linux_v2 with parser_win_v2 in the commands below. The following example command shows how to generate HTML and CSV files in Linux to output directory sysmon_parsed_output . The sysMonApp parser usage can be found here . The Windows tool expects the same arguments: $HEXAGON_SDK_ROOT/tools/utils/sysmon/parser_linux_v2/HTML_Parser/sysmon_parser ./sysmon_cdsp.bin --outdir sysmon_parsed_output","title":"Profile from the Command Line"},{"location":"examples/profiling/index.html#profile-using-android-application","text":"Install the sysMon DSP profiler application on the device using adb install: adb install -g $HEXAGON_SDK_ROOT/tools/utils/sysmon/sysMon_DSP_Profiler_V2.apk The sysMon DSP Profiler UI for the sysMonApp Android Application provides user flexibility to choose from different modes of profiling. You can select the Mode based on your requirements. The available modes are: Mode Description DSP DCVS User can adjust DSP core and bus clocks dynamically for profiling duration. Default Mode A fixed set of performance metrics will be monitored. Sampling period is either 1 or 50 milli-seconds. User Mode If Default Mode is unchecked, User Mode is enabled. The user can select the desired PMU events to be captured. 8-PMU Mode If both DSP DCVS and Default modes are unchecked, 8-PMU mode is enabled. This allows user to choose whether to configure 4 PMU events or 8 PMU Events. The Configuration Settings button is enabled in user mode and you can select the PMU events to be captured. Further details about the different modes and options can be found in sysMonApp Profiler .","title":"Profile using Android Application"},{"location":"examples/profiling/index.html#understanding-the-results_2","text":"The files generated by sysMon Profiler application are the following: File name Description sysmon_cdsp.bin This is the cDSP binary file pulled from the device. sysmon_report directory This directory is generated after running the sysMonApp HTML Parser. It contains HTML and csv files generated by the HTML Parser. This directory is named based on the date the report was generated. The sysMonApp HTML file gives an overall summary of the profiling done using the sysMonApp. Here is an example of the HTML generated from the sysMonApp. To understand the output of these files generated please refer to the sysMonApp profiler documentation","title":"Understanding the results"},{"location":"examples/qhl/index.html","text":"Qualcomm Hexagon Libraries Example This example illustrates how to use the Qualcomm Hexagon Libraries (QHL) located under $HEXAGON_SDK_ROOT/libs/qhl . Currently, QHL is only supported for V66 and V68 targets. Please refer to the complete documentation on the QHL library for more details on each supported function and the unit test framework provided to test the performance and accuracy of all functions. Overview The example calls a few functions from the qhmath and qhblas libraries and sends their output to the console. The list of functions invoked in this example are listed below: qhmath_abs_af qhmath_abs_ah qhblas_vector_add_af qhblas_vector_add_ah This example uses a different build approach from the other example projects in the SDK: it uses a standalone Makefile and invokes CMake to build the required QHL libs. The standalone Makefile in this project does not have dependencies on the make.d build system. Instructions Walkthrough All step-by-step instructions for building and running the test both on simulator and on target are captured in the qhl_walkthrough.py script. You may run the script directly. To do so, simply run 'python qhl_walkthrough.py'. To see the messages sent to the DSP run 'adb wait-for-device logcat -s adsprpc' in a new window. You can also execute manually each instruction present in the script to reproduce separately the build, run on simulator, and run on target steps. Standalone Makefile Unlike most Hexagon SDK examples, this example uses a standalone Makefile to build the project and its dependencies: no hexagon.min and android.min are provided to enable make.d support. A similar approach may be used for building custom projects that do not come readily equipped with make.d or CMake support. To build the example and its dependencies for a v66 target, simply type the following command: make V=v66 Note: In order to build the QHL libraries, the Makefile invokes a CMake command as part of the command above: cmake_build.bash hexagon DSP_ARCH=v66 from within the QHL library project.","title":"QHL"},{"location":"examples/qhl/index.html#qualcomm-hexagon-libraries-example","text":"This example illustrates how to use the Qualcomm Hexagon Libraries (QHL) located under $HEXAGON_SDK_ROOT/libs/qhl . Currently, QHL is only supported for V66 and V68 targets. Please refer to the complete documentation on the QHL library for more details on each supported function and the unit test framework provided to test the performance and accuracy of all functions.","title":"Qualcomm Hexagon Libraries Example"},{"location":"examples/qhl/index.html#overview","text":"The example calls a few functions from the qhmath and qhblas libraries and sends their output to the console. The list of functions invoked in this example are listed below: qhmath_abs_af qhmath_abs_ah qhblas_vector_add_af qhblas_vector_add_ah This example uses a different build approach from the other example projects in the SDK: it uses a standalone Makefile and invokes CMake to build the required QHL libs. The standalone Makefile in this project does not have dependencies on the make.d build system.","title":"Overview"},{"location":"examples/qhl/index.html#instructions","text":"","title":"Instructions"},{"location":"examples/qhl/index.html#walkthrough","text":"All step-by-step instructions for building and running the test both on simulator and on target are captured in the qhl_walkthrough.py script. You may run the script directly. To do so, simply run 'python qhl_walkthrough.py'. To see the messages sent to the DSP run 'adb wait-for-device logcat -s adsprpc' in a new window. You can also execute manually each instruction present in the script to reproduce separately the build, run on simulator, and run on target steps.","title":"Walkthrough"},{"location":"examples/qhl/index.html#standalone-makefile","text":"Unlike most Hexagon SDK examples, this example uses a standalone Makefile to build the project and its dependencies: no hexagon.min and android.min are provided to enable make.d support. A similar approach may be used for building custom projects that do not come readily equipped with make.d or CMake support. To build the example and its dependencies for a v66 target, simply type the following command: make V=v66 Note: In order to build the QHL libraries, the Makefile invokes a CMake command as part of the command above: cmake_build.bash hexagon DSP_ARCH=v66 from within the QHL library project.","title":"Standalone Makefile"},{"location":"examples/qhl_hvx/index.html","text":"HVX Optimized Qualcomm Hexagon Libraries Example This example illustrates how to use the HVX optimized Qualcomm Hexagon Libraries (QHL HVX) located under $HEXAGON_SDK_ROOT/libs/qhl_hvx. Currently, QHL HVX is only supported for V66 and V68 targets. Please refer to the complete documentation on the QHL HVX library for more details on each supported function and the unit test framework provided to test the performance and accuracy of all functions. Overview The example calls a few functions from the qhblas and qhdsp libraries and sends their output to the console. The list of functions invoked in this example are listed below: qhblas_hvx_vector_add_vb qhblas_hvx_vh_vector_add_vb qhblas_hvx_vector_add_vh qhblas_hvx_vw_vector_add_vh qhblas_hvx_vector_add_vw qhblas_hvx_vector_add_vhf qhblas_hvx_vector_add_vf qhdsp_hvx_vcw_r2dfft_vub qhdsp_hvx_vub_r2difft_vcw This example uses a different build approach from the other example projects in the SDK: it uses a standalone Makefile and invokes CMake to build the required QHL libs. The standalone Makefile in this project does not have dependencies on the make.d build system. Instructions Walkthrough All step-by-step instructions for building and running the test both on simulator and on target are captured in the qhl_hvx_walkthrough.py script. You may run the script directly. To do so, simply run 'python qhl_hvx_walkthrough.py'. To see the messages sent to the DSP run 'adb wait-for-device logcat -s adsprpc' in a new window. You can also execute manually each instruction present in the script to reproduce separately the build, run on simulator, and run on target steps. Standalone Makefile Unlike most Hexagon SDK examples, this example uses a standalone Makefile to build the project and its dependencies: no hexagon.min and android.min are provided to enable make.d support. A similar approach may be used for building custom projects that do not come readily equipped with make.d or CMake support. To build the example and its dependencies for a v66 target, simply type the following command: make V=v66 Note: In order to build the QHL_HVX libraries, the Makefile invokes a CMake command as part of the command above: cmake_build.bash hexagon DSP_ARCH=v66 from within the QHL_HVX library project.","title":"QHL HVX"},{"location":"examples/qhl_hvx/index.html#hvx-optimized-qualcomm-hexagon-libraries-example","text":"This example illustrates how to use the HVX optimized Qualcomm Hexagon Libraries (QHL HVX) located under $HEXAGON_SDK_ROOT/libs/qhl_hvx. Currently, QHL HVX is only supported for V66 and V68 targets. Please refer to the complete documentation on the QHL HVX library for more details on each supported function and the unit test framework provided to test the performance and accuracy of all functions.","title":"HVX Optimized Qualcomm Hexagon Libraries Example"},{"location":"examples/qhl_hvx/index.html#overview","text":"The example calls a few functions from the qhblas and qhdsp libraries and sends their output to the console. The list of functions invoked in this example are listed below: qhblas_hvx_vector_add_vb qhblas_hvx_vh_vector_add_vb qhblas_hvx_vector_add_vh qhblas_hvx_vw_vector_add_vh qhblas_hvx_vector_add_vw qhblas_hvx_vector_add_vhf qhblas_hvx_vector_add_vf qhdsp_hvx_vcw_r2dfft_vub qhdsp_hvx_vub_r2difft_vcw This example uses a different build approach from the other example projects in the SDK: it uses a standalone Makefile and invokes CMake to build the required QHL libs. The standalone Makefile in this project does not have dependencies on the make.d build system.","title":"Overview"},{"location":"examples/qhl_hvx/index.html#instructions","text":"","title":"Instructions"},{"location":"examples/qhl_hvx/index.html#walkthrough","text":"All step-by-step instructions for building and running the test both on simulator and on target are captured in the qhl_hvx_walkthrough.py script. You may run the script directly. To do so, simply run 'python qhl_hvx_walkthrough.py'. To see the messages sent to the DSP run 'adb wait-for-device logcat -s adsprpc' in a new window. You can also execute manually each instruction present in the script to reproduce separately the build, run on simulator, and run on target steps.","title":"Walkthrough"},{"location":"examples/qhl_hvx/index.html#standalone-makefile","text":"Unlike most Hexagon SDK examples, this example uses a standalone Makefile to build the project and its dependencies: no hexagon.min and android.min are provided to enable make.d support. A similar approach may be used for building custom projects that do not come readily equipped with make.d or CMake support. To build the example and its dependencies for a v66 target, simply type the following command: make V=v66 Note: In order to build the QHL_HVX libraries, the Makefile invokes a CMake command as part of the command above: cmake_build.bash hexagon DSP_ARCH=v66 from within the QHL_HVX library project.","title":"Standalone Makefile"},{"location":"examples/qprintf/index.html","text":"Qprintf example This example illustrates how to use the Qualcomm printf library (qprintf) located under $HEXAGON_SDK_ROOT/libs/qprintf . Instructions All step-by-step instructions for building and running the test both on simulator and on target are captured in the qprintf_walkthrough.py script. You may run the script directly. To do so, simply run 'python qprintf_walkthrough.py'. To see the messages sent to the DSP run 'adb wait-for-device logcat -s adsprpc' in a new window. The example demonstrates how the library displays the contents of scalar and vector registers from assembly and provides various options for displaying HVX_Vector contents from C.","title":"QPRINTF"},{"location":"examples/qprintf/index.html#qprintf-example","text":"This example illustrates how to use the Qualcomm printf library (qprintf) located under $HEXAGON_SDK_ROOT/libs/qprintf .","title":"Qprintf example"},{"location":"examples/qprintf/index.html#instructions","text":"All step-by-step instructions for building and running the test both on simulator and on target are captured in the qprintf_walkthrough.py script. You may run the script directly. To do so, simply run 'python qprintf_walkthrough.py'. To see the messages sent to the DSP run 'adb wait-for-device logcat -s adsprpc' in a new window. The example demonstrates how the library displays the contents of scalar and vector registers from assembly and provides various options for displaying HVX_Vector contents from C.","title":"Instructions"},{"location":"reference/faq.html","text":"FAQ Common FAQ How to link c++ libraries statically on Android side? If you are using Android GCC tools, you can link libstdc++ statically by adding the below lines to your android.min file: <application_name>_LIBS += $(ANDROID_TOOLS_DIR)\\platforms\\android-<Android-API-LEVEL>\\arch-arm\\usr\\lib\\libstdc++.a e.g: calculator_LIBS += $(ANDROID_TOOLS_DIR)\\platforms\\android-26\\arch-arm\\usr\\lib\\libstdc++.a If you are using Android CLANG tools, you can link libc++ statically by adding the below lines to your android.min file: <application_name>_LIBS += $(ANDROID_TOOLS_DIR)/sysroot/usr/lib/aarch64-linux-android/libc+_static.a <application_name>_LIBS += $(ANDROID_TOOLS_DIR)/sysroot/usr/lib/aarch64-linux-android/libc+abi.a e.g: calculator_LIBS += $(ANDROID_TOOLS_DIR)/sysroot/usr/lib/aarch64-linux-android/libc+_static.a calculator_LIBS += $(ANDROID_TOOLS_DIR)/sysroot/usr/lib/aarch64-linux-android/libc+abi.a How to run examples on target from Command Line Interface (CLI)? Almost all the SDK examples have a walkthrough script to run the example on the target. This script compiles the example for the HLOS and DSP variants, optionally signs the device, pushes the built libraries to the device and finally, runs the example executable on the device. Refer to this for more information. How to convert an older project to glue-free? With versions earlier than 3.5, the examples contain a glue folder which has <variant>.mak and V_<variant>.min files. When a glue-based example is built for a variant , the rules mentioned in the <variant>.mak> file will be invoked to build the example and its dependencies, using libraries and includes mentioned in V_<variant>.min file. To modify the dependencies in a glue-based project, the glue folder contents needs to be updated which is not practical to do. Glue-free projects were introduced to provide an easy way to change/add dependencies. All the SDK examples are converted to use the glue-free approach. A Glue-free project should have the following files in its root directory: hexagon_deps.min, hexagon.min for building the Hexagon variant, android_deps.min, android.min for building the Android variant, UbuntuARM_deps.min, UbuntuARM.min for building the UbuntuARM variant, and Makefile, which includes above files. A project's dependencies and supported variants must be defined in <>_deps.min files. The dependencies are defined using the variable DEPENDENCIES and their directory location is expressed with <DEPENDENCY NAME>_DIR . Refer to any example in the SDK for more information on the build files. Dynamic Loading What is dynamic loading and how can I use it? Dynamic loading enables to load and run a shared object on DSP without rebuilding the DSP image. Dynamic shared objects are analogous to Linux .so and Windows .dll files. They are implemented as ELF files and exist as files in the HLOS file system which are loaded by the DSP via an inter-processor communication mechanism. Once loaded, all symbols publicly exported by the shared object can be referenced or called. The shared objects are loaded into DSP addressable memory on-demand basis. This helps reduce DSP image size. The creation of shared objects is supported by the Hexagon Tools version 5.0.9 and newer. The document Hexagon Application Binary Interface Specification provides more information about the structure and limitations of dynamic shared objects. The documentation introducing FastRPC also discusses in more details the process of dynamic loading. What is the file format of the dynamic shared object files? The dynamic shared object files are ELFs that adhere to the specification described in the document Hexagon Application Binary Interface Specification which is an addendum to the System V Application Binary Interface as described by http://www.sco.com/developers/gabi/latest/contents.html Do dynamic shared objects support C++? Yes. For C++ support on Android side, refer to calculator_c++_apk example. On hexagon side, refer to calculator C++ example . How do I build/link assembly source code to generate shared object? Shared objects should be built to be position independent and SDK build system takes care of that already. Assembly source files should be added to .ASM_SRCS in hexagon.min files. Please refer to benchmark example in compute/addon. Where to push the executables or shared objects to the target? On a rooted device, push binaries as follows: HLOS Executables * LA: push to /vendor/bin * LE: push to /usr/bin HLOS stub libraries * LA: push to /vendor/lib(64) * LE: push to /usr/lib(64) DSP skeleton libraries * Push to default search paths Note: The loader paths for stub and skeleton libraries are configurable using the environment variables LD_LIRBARY_PATH and [A]DSP_LIBRARY_PATH respectively. For example, use the command below to append /vendor/lib64 to the stub libraries loader path and /vendor/lib/rfsa/dsp/sdk to the DSP libraries loader path before running the calculator executable: adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\\" calculator 0 3 1000 How to run an application on the cDSP in an unrooted device? It is not possible to push executables/libraries in /vendor partition in an unrooted device. Hence, users can run the application from /data partition. In a unrooted device, push the executables to /data/nativetest(64)/vendor , and HLOS Stub and DSP skeleton libraries to any location configured using the environment variables, LD_LIRBARY_PATH and [A]DSP_LIBRARY_PATH , respectively. An application running from this location will also have access to the libraries already residing in /vendor . adb shell export LD_LIBRARY_PATH=/data/nativetest64/vendor:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/data/xxx/yyy\\\" calculator 0 3 1000 Note: With the introduction of treble limitations starting with Android-P, it is not possible to run an application from /data/app . In addition to the above mentioned location, the executable can also be run from the directory /data/local/tmp . An application run from any of these locations will have access to the DSP libraries in the vendor partition as well. How to get the load address and size of the libraries loaded in the simulator? The load address of the library is printed in the console for a simulation. try to load calculator_q.so from file: HIGH:0x1E6:122:search.c .... read headers 0x0 -> d8041000 (0x1000 B): HIGH:0x1E6:539:map_object.c _rtld_map_object_ex: sigverify skipped for ./calculator_q.so, no function specified!: HIGH:0x1E6:598:map_object.c .... mapped [d81ed000 - d8200000] (77824 Bytes): HIGH:0x1E6:729:map_object.c Here, the load address of the library \"calculator_q.so\" is 0xd81ed000 , whereas the size of the library is 77824 bytes. FastRPC How do I know if the remote call completed successfully? If the remote call returns zero then it indicates remote call is succussful. Any other value is a failure, refer to error codes . Also refer to Trouble shooting fastRPC issues if you see any issues. Can I use a buffer in a remote call as both input and output.? Yes, we can use. Refer IDL documentation What is the significance of using ION memory? Refer to rpcmem header file and documentation . What memory can be passed to a FastRPC function? Any memory can be passed. However ION allocated memory is the only memory that will be passed to the DSP directly without a copy. Stack, general heap, and static buffers will be copied. Even though ION memory isn't copied, cache synchronization still has to be performed. This is because DSP and application processor do not share the same cache. Also, these operations are the most significant factor in RPC performance. For more details refer to system performance documents How much stack space is used by FastRPC framework on DSP? FastRPC framework allocates a stack of 16KB per thread on DSP by default. It can be changed using remote_session_control API mentioned in remote.h What is the page granularity of mappings on DSP? The FastRPC framework maps buffers at a page granularity supported by the DSP (4K, 16K, 64K, 256K, 1M). For example, if the client passes a 512K buffer, it gets mapped with two entries of 256K each, correctly aligning the received physical address. The mapping exists only for the duration of that remote invocation. When the DSP returns from the invocation, it unmaps the pages associated with that invocation. The mapping and unmapping happen for each RPC call made. What happens if an application is terminated in the middle of remote invocation? The FastRPC framework keeps track of all resources used for the remote invocation and performs the necessary cleanup such as notifying the DSP processor to kill the threads servicing the remote invocation. The application is blocked from exiting until the clean up is complete so as to guarantee that the buffers passed to the DSP are not in use before the HLOS re-allocates them. Can more than one remote invocation be made at any given point? Yes, there can be multiple threads that can be spawned from the user space application with each making a remote invocation simultaneously. Refer to calculator multi domain example. What cache attribute is used to map the input and output buffers? The input and output buffers are mapped by default with write-back cache attribute (both L1 and L2 cacheable). How to manage the clocks when making a fastrpc call? Refer to the header file HAP_power.h and its documentation . Refer to example in /addons/compute/examples/benchmark HAP_perf_get_pcycles() and HAP_perf_get_time_us() are returning zero when running on hexagon simulator. What should I do? To use HAP_perf_get_pcycles() and HAP_perf_get_time_us() , your simulator test should be running on top of QuRT. All simulator tests run on top of QuRT by default unless NO_QURT_INC variable is defined to 1 in your example makefile as shown in calculator example in examples/calculator/hexagon.min. Please refer to the SDK simulator document page to know more about standalone simulator tests and QuRT-based simulator tests. How to log messages from DSP? Refer to documentation on debug . How can I display fastRPC debug messages? There is an error message logging support for the application processor and the DSP for a FastRPC session between a client application on the application processor and a service running on the DSP. The messages from user space and kernel space on the application processor are logged in logcat and dmesg respectively. The messages from DSP are logged to either mini-DM (please refer mini- DM ) or in logcat (please refer logcat ). To search FastRPC errors from application processor or dsp(adsp/cdsp/mdsp/slpi) on logcat, use following tag names: 'adsprpc' or ' ', e.g : adb logcat -s adsprpc. If the serial number of a device is retrieved by running getserial on ADSP , does it sign only ADSP ? The serial number of the device is not specific to one sub-system and represents the RPC version of all the sub-systems of the device. The serial number returned by running getserial on ADSP is the same as the one returned by CDSP . If the testsig generated with this serial number is pushed to an accessible location on the device, then it is possible to run the application on all the sub-systems. How to resolve queries about QuRT APIs? Refer to this for more information on QuRT.","title":"FAQ"},{"location":"reference/faq.html#faq","text":"","title":"FAQ"},{"location":"reference/faq.html#common-faq","text":"How to link c++ libraries statically on Android side? If you are using Android GCC tools, you can link libstdc++ statically by adding the below lines to your android.min file: <application_name>_LIBS += $(ANDROID_TOOLS_DIR)\\platforms\\android-<Android-API-LEVEL>\\arch-arm\\usr\\lib\\libstdc++.a e.g: calculator_LIBS += $(ANDROID_TOOLS_DIR)\\platforms\\android-26\\arch-arm\\usr\\lib\\libstdc++.a If you are using Android CLANG tools, you can link libc++ statically by adding the below lines to your android.min file: <application_name>_LIBS += $(ANDROID_TOOLS_DIR)/sysroot/usr/lib/aarch64-linux-android/libc+_static.a <application_name>_LIBS += $(ANDROID_TOOLS_DIR)/sysroot/usr/lib/aarch64-linux-android/libc+abi.a e.g: calculator_LIBS += $(ANDROID_TOOLS_DIR)/sysroot/usr/lib/aarch64-linux-android/libc+_static.a calculator_LIBS += $(ANDROID_TOOLS_DIR)/sysroot/usr/lib/aarch64-linux-android/libc+abi.a How to run examples on target from Command Line Interface (CLI)? Almost all the SDK examples have a walkthrough script to run the example on the target. This script compiles the example for the HLOS and DSP variants, optionally signs the device, pushes the built libraries to the device and finally, runs the example executable on the device. Refer to this for more information. How to convert an older project to glue-free? With versions earlier than 3.5, the examples contain a glue folder which has <variant>.mak and V_<variant>.min files. When a glue-based example is built for a variant , the rules mentioned in the <variant>.mak> file will be invoked to build the example and its dependencies, using libraries and includes mentioned in V_<variant>.min file. To modify the dependencies in a glue-based project, the glue folder contents needs to be updated which is not practical to do. Glue-free projects were introduced to provide an easy way to change/add dependencies. All the SDK examples are converted to use the glue-free approach. A Glue-free project should have the following files in its root directory: hexagon_deps.min, hexagon.min for building the Hexagon variant, android_deps.min, android.min for building the Android variant, UbuntuARM_deps.min, UbuntuARM.min for building the UbuntuARM variant, and Makefile, which includes above files. A project's dependencies and supported variants must be defined in <>_deps.min files. The dependencies are defined using the variable DEPENDENCIES and their directory location is expressed with <DEPENDENCY NAME>_DIR . Refer to any example in the SDK for more information on the build files.","title":"Common FAQ"},{"location":"reference/faq.html#dynamic-loading","text":"What is dynamic loading and how can I use it? Dynamic loading enables to load and run a shared object on DSP without rebuilding the DSP image. Dynamic shared objects are analogous to Linux .so and Windows .dll files. They are implemented as ELF files and exist as files in the HLOS file system which are loaded by the DSP via an inter-processor communication mechanism. Once loaded, all symbols publicly exported by the shared object can be referenced or called. The shared objects are loaded into DSP addressable memory on-demand basis. This helps reduce DSP image size. The creation of shared objects is supported by the Hexagon Tools version 5.0.9 and newer. The document Hexagon Application Binary Interface Specification provides more information about the structure and limitations of dynamic shared objects. The documentation introducing FastRPC also discusses in more details the process of dynamic loading. What is the file format of the dynamic shared object files? The dynamic shared object files are ELFs that adhere to the specification described in the document Hexagon Application Binary Interface Specification which is an addendum to the System V Application Binary Interface as described by http://www.sco.com/developers/gabi/latest/contents.html Do dynamic shared objects support C++? Yes. For C++ support on Android side, refer to calculator_c++_apk example. On hexagon side, refer to calculator C++ example . How do I build/link assembly source code to generate shared object? Shared objects should be built to be position independent and SDK build system takes care of that already. Assembly source files should be added to .ASM_SRCS in hexagon.min files. Please refer to benchmark example in compute/addon. Where to push the executables or shared objects to the target? On a rooted device, push binaries as follows: HLOS Executables * LA: push to /vendor/bin * LE: push to /usr/bin HLOS stub libraries * LA: push to /vendor/lib(64) * LE: push to /usr/lib(64) DSP skeleton libraries * Push to default search paths Note: The loader paths for stub and skeleton libraries are configurable using the environment variables LD_LIRBARY_PATH and [A]DSP_LIBRARY_PATH respectively. For example, use the command below to append /vendor/lib64 to the stub libraries loader path and /vendor/lib/rfsa/dsp/sdk to the DSP libraries loader path before running the calculator executable: adb shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\\" calculator 0 3 1000 How to run an application on the cDSP in an unrooted device? It is not possible to push executables/libraries in /vendor partition in an unrooted device. Hence, users can run the application from /data partition. In a unrooted device, push the executables to /data/nativetest(64)/vendor , and HLOS Stub and DSP skeleton libraries to any location configured using the environment variables, LD_LIRBARY_PATH and [A]DSP_LIBRARY_PATH , respectively. An application running from this location will also have access to the libraries already residing in /vendor . adb shell export LD_LIBRARY_PATH=/data/nativetest64/vendor:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/data/xxx/yyy\\\" calculator 0 3 1000 Note: With the introduction of treble limitations starting with Android-P, it is not possible to run an application from /data/app . In addition to the above mentioned location, the executable can also be run from the directory /data/local/tmp . An application run from any of these locations will have access to the DSP libraries in the vendor partition as well. How to get the load address and size of the libraries loaded in the simulator? The load address of the library is printed in the console for a simulation. try to load calculator_q.so from file: HIGH:0x1E6:122:search.c .... read headers 0x0 -> d8041000 (0x1000 B): HIGH:0x1E6:539:map_object.c _rtld_map_object_ex: sigverify skipped for ./calculator_q.so, no function specified!: HIGH:0x1E6:598:map_object.c .... mapped [d81ed000 - d8200000] (77824 Bytes): HIGH:0x1E6:729:map_object.c Here, the load address of the library \"calculator_q.so\" is 0xd81ed000 , whereas the size of the library is 77824 bytes.","title":"Dynamic Loading"},{"location":"reference/faq.html#fastrpc","text":"How do I know if the remote call completed successfully? If the remote call returns zero then it indicates remote call is succussful. Any other value is a failure, refer to error codes . Also refer to Trouble shooting fastRPC issues if you see any issues. Can I use a buffer in a remote call as both input and output.? Yes, we can use. Refer IDL documentation What is the significance of using ION memory? Refer to rpcmem header file and documentation . What memory can be passed to a FastRPC function? Any memory can be passed. However ION allocated memory is the only memory that will be passed to the DSP directly without a copy. Stack, general heap, and static buffers will be copied. Even though ION memory isn't copied, cache synchronization still has to be performed. This is because DSP and application processor do not share the same cache. Also, these operations are the most significant factor in RPC performance. For more details refer to system performance documents How much stack space is used by FastRPC framework on DSP? FastRPC framework allocates a stack of 16KB per thread on DSP by default. It can be changed using remote_session_control API mentioned in remote.h What is the page granularity of mappings on DSP? The FastRPC framework maps buffers at a page granularity supported by the DSP (4K, 16K, 64K, 256K, 1M). For example, if the client passes a 512K buffer, it gets mapped with two entries of 256K each, correctly aligning the received physical address. The mapping exists only for the duration of that remote invocation. When the DSP returns from the invocation, it unmaps the pages associated with that invocation. The mapping and unmapping happen for each RPC call made. What happens if an application is terminated in the middle of remote invocation? The FastRPC framework keeps track of all resources used for the remote invocation and performs the necessary cleanup such as notifying the DSP processor to kill the threads servicing the remote invocation. The application is blocked from exiting until the clean up is complete so as to guarantee that the buffers passed to the DSP are not in use before the HLOS re-allocates them. Can more than one remote invocation be made at any given point? Yes, there can be multiple threads that can be spawned from the user space application with each making a remote invocation simultaneously. Refer to calculator multi domain example. What cache attribute is used to map the input and output buffers? The input and output buffers are mapped by default with write-back cache attribute (both L1 and L2 cacheable). How to manage the clocks when making a fastrpc call? Refer to the header file HAP_power.h and its documentation . Refer to example in /addons/compute/examples/benchmark HAP_perf_get_pcycles() and HAP_perf_get_time_us() are returning zero when running on hexagon simulator. What should I do? To use HAP_perf_get_pcycles() and HAP_perf_get_time_us() , your simulator test should be running on top of QuRT. All simulator tests run on top of QuRT by default unless NO_QURT_INC variable is defined to 1 in your example makefile as shown in calculator example in examples/calculator/hexagon.min. Please refer to the SDK simulator document page to know more about standalone simulator tests and QuRT-based simulator tests. How to log messages from DSP? Refer to documentation on debug . How can I display fastRPC debug messages? There is an error message logging support for the application processor and the DSP for a FastRPC session between a client application on the application processor and a service running on the DSP. The messages from user space and kernel space on the application processor are logged in logcat and dmesg respectively. The messages from DSP are logged to either mini-DM (please refer mini- DM ) or in logcat (please refer logcat ). To search FastRPC errors from application processor or dsp(adsp/cdsp/mdsp/slpi) on logcat, use following tag names: 'adsprpc' or ' ', e.g : adb logcat -s adsprpc. If the serial number of a device is retrieved by running getserial on ADSP , does it sign only ADSP ? The serial number of the device is not specific to one sub-system and represents the RPC version of all the sub-systems of the device. The serial number returned by running getserial on ADSP is the same as the one returned by CDSP . If the testsig generated with this serial number is pushed to an accessible location on the device, then it is possible to run the application on all the sub-systems. How to resolve queries about QuRT APIs? Refer to this for more information on QuRT.","title":"FastRPC"},{"location":"reference/feature_matrix.html","text":"Feature Matrix The Hexagon SDK supports a simulation environment as well as multiple hardware platforms. Not all features are supported on all platforms. The following feature matrix indicates whether a particular feature or example is supported on a particular target. Targets Simulator SM8150 SM6150 SM7150 SM6125 SM8250 Rennell Saipan Kamorta Lahaina Bitra Agatti Mannar strait Cedros Kodiak QCS405 QCS403 QCS610 QCS605 QRB5165 SXR2130 ENEL Operating System AP None LA LA LA LA LA LA LA LA LA LA LA LA LA LA LA LE LE LE LA/LE UBUNTU/LE LE UBUNTU DSP QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT DSPs Supported aDSP Yes (>=v65) Yes (v66) Yes (v66) Yes (v65) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v65) Yes (v66) Yes (v66) No cDSP Yes (>=v65) Yes (v66) Yes (v66) Yes (v65) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v68) Yes (v66) No (v66) Yes (v66) Yes (v66) Yes (v68) Yes (v68) Yes (v66) Yes (v66) Yes (v66) Yes (v65) Yes (v66) Yes (v66) Yes (v66) sDSP No Yes (v66) No No No Yes (v66) No No No Yes (v66) No No No No No No No No No No Yes (v66) Yes (v66) No mDSP No No No No No No No No No No No No No No No No No No No No No No No Tools Version Hexagon Tools N/A 8.2.05 8.2.05 8.2.05 8.2.05 8.3.06 8.2.05 8.3.05 8.2.05 8.4.04 8.3.05 8.2.05 8.2.05 8.3.12 8.4.04 8.4.07 8.2.02 8.2.02 8.2.05 8.1.04 8.3.06 8.3.06 8.4.04 Language C++98/11/14 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes C++17 Yes No No No No No No No No Yes No No No No Yes Yes No No No No No No No Assembly and intrinsics Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Halide Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Debugging LLDB Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No No No No No No logcat No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No printf() Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Profiling SysMon Profiler (UI) No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No Yes No No No No Hexagon Trace Analyzer Yes Yes No No No Yes No Yes No Yes No No No No Yes Yes No No No No No No No SysMonApp (command line interface) No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Hexagon Profiler Yes No No No No No No No No No No No No No No No No No No No No No No Hardware Features Integer/fixed-point HVX Yes cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP Yes Floating-point HVX Yes No No No No No No No No cDSP No No No No Yes Yes No No No No No No No HMX Yes No No No No No No No No cDSP No No No No cDSP cDSP No No No No No No No DCVS v3 No No No No No Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes No No No No Yes Yes No FastRPC Domains No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes CPZ No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP Yes VTCM APIs Yes cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP Yes Cache locking API v2 Yes cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP No No cDSP cDSP cDSP cDSP Yes Unsigned PD No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP No No cDSP cDSP cDSP cDSP Yes Compute resource manager API Yes No No No No cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP No No No cDSP cDSP No IO Coherency No Yes Yes Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No Yes No Yes Yes Yes Max. concurrent FastRPC user PDs on aDSP N/A 1 5 5 2 1 5 4 2 1 4 2 3 3 2 2 4 4 5 1 1 1 N/A Max. concurrent FastRPC user PDs on cDSP N/A 6 4 4 4 6 4 6 4 10 6 N/A 6 6 10 10 4 4 4 7 6 6 6 Max. concurrent FastRPC user PDs on sDSP N/A 4 N/A N/A N/A 4 N/A N/A N/A 4 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 4 4 N/A DSP libraries QHL Yes Yes No No No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No Yes Yes Yes Yes Yes QHL_HVX Yes Yes No No No Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No No No Yes Yes No qprintf Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes DSP worker pool Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Compute Libraries asyncdspq No Yes Yes Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No Yes Yes Yes Yes No fastCV Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes imagedspq No cDSP cDSP cDSP No cDSP cDSP cDSP No cDSP cDSP No No No cDSP cDSP No No cDSP cDSP cDSP cDSP Yes Base SDK examples asyncdspq No Yes Yes Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No Yes Yes Yes Yes No Asynchronous DSP Packet Queue No No No No No No No No No Yes No No No No Yes Yes No No No No No No No Calculator Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Calculator_c++ Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes calculator_c++_apk No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No No No No No No gtest Yes Yes No No No Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No No Yes(Not supported on LE) No No No HAP_example No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes LPI_example Yes No No No No No No No No No No No No No No No No No No No No No No Multithreading Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Profiling Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes QHL No Yes No No No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No No No No Yes Yes QHL_HVX No Yes No No No Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No No No No Yes No Qprintf Yes cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP cDSP cDSP Yes cDSP cDSP Yes Compute examples Benchmark Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Camera CHI No No No No No No No No No Yes No No No No Yes Yes No No No No No No No Camera streamer No Yes No No No Yes No Yes No Yes No No No No Yes No No No No No No No No Corner detect No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No Yes Yes Yes Yes Yes Image DSPQ No Yes Yes Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No Yes Yes Yes Yes Yes UBWC DMA Yes Yes No Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No No No Yes Yes Yes User DMA No No No No No No No No No Yes No No No No Yes Yes No No No No No No No Compute Resource Manager Sample No No No No No No No No No Yes No No No No Yes Yes No No No No No No No .md-sidebar--secondary { display: none; } .md-content__inner md-typeset { max-width: 1200; } .md-content { margin-right: 0; }","title":"Feature matrix"},{"location":"reference/feature_matrix.html#feature-matrix","text":"The Hexagon SDK supports a simulation environment as well as multiple hardware platforms. Not all features are supported on all platforms. The following feature matrix indicates whether a particular feature or example is supported on a particular target. Targets Simulator SM8150 SM6150 SM7150 SM6125 SM8250 Rennell Saipan Kamorta Lahaina Bitra Agatti Mannar strait Cedros Kodiak QCS405 QCS403 QCS610 QCS605 QRB5165 SXR2130 ENEL Operating System AP None LA LA LA LA LA LA LA LA LA LA LA LA LA LA LA LE LE LE LA/LE UBUNTU/LE LE UBUNTU DSP QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT QuRT DSPs Supported aDSP Yes (>=v65) Yes (v66) Yes (v66) Yes (v65) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v65) Yes (v66) Yes (v66) No cDSP Yes (>=v65) Yes (v66) Yes (v66) Yes (v65) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v66) Yes (v68) Yes (v66) No (v66) Yes (v66) Yes (v66) Yes (v68) Yes (v68) Yes (v66) Yes (v66) Yes (v66) Yes (v65) Yes (v66) Yes (v66) Yes (v66) sDSP No Yes (v66) No No No Yes (v66) No No No Yes (v66) No No No No No No No No No No Yes (v66) Yes (v66) No mDSP No No No No No No No No No No No No No No No No No No No No No No No Tools Version Hexagon Tools N/A 8.2.05 8.2.05 8.2.05 8.2.05 8.3.06 8.2.05 8.3.05 8.2.05 8.4.04 8.3.05 8.2.05 8.2.05 8.3.12 8.4.04 8.4.07 8.2.02 8.2.02 8.2.05 8.1.04 8.3.06 8.3.06 8.4.04 Language C++98/11/14 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes C++17 Yes No No No No No No No No Yes No No No No Yes Yes No No No No No No No Assembly and intrinsics Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Halide Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Debugging LLDB Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No No No No No No logcat No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No printf() Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Profiling SysMon Profiler (UI) No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No Yes No No No No Hexagon Trace Analyzer Yes Yes No No No Yes No Yes No Yes No No No No Yes Yes No No No No No No No SysMonApp (command line interface) No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Hexagon Profiler Yes No No No No No No No No No No No No No No No No No No No No No No Hardware Features Integer/fixed-point HVX Yes cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP Yes Floating-point HVX Yes No No No No No No No No cDSP No No No No Yes Yes No No No No No No No HMX Yes No No No No No No No No cDSP No No No No cDSP cDSP No No No No No No No DCVS v3 No No No No No Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes No No No No Yes Yes No FastRPC Domains No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes CPZ No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP Yes VTCM APIs Yes cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP Yes Cache locking API v2 Yes cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP No No cDSP cDSP cDSP cDSP Yes Unsigned PD No cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP No No cDSP cDSP cDSP cDSP Yes Compute resource manager API Yes No No No No cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP No No No cDSP cDSP No IO Coherency No Yes Yes Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No Yes No Yes Yes Yes Max. concurrent FastRPC user PDs on aDSP N/A 1 5 5 2 1 5 4 2 1 4 2 3 3 2 2 4 4 5 1 1 1 N/A Max. concurrent FastRPC user PDs on cDSP N/A 6 4 4 4 6 4 6 4 10 6 N/A 6 6 10 10 4 4 4 7 6 6 6 Max. concurrent FastRPC user PDs on sDSP N/A 4 N/A N/A N/A 4 N/A N/A N/A 4 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 4 4 N/A DSP libraries QHL Yes Yes No No No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No Yes Yes Yes Yes Yes QHL_HVX Yes Yes No No No Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No No No Yes Yes No qprintf Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes DSP worker pool Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Compute Libraries asyncdspq No Yes Yes Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No Yes Yes Yes Yes No fastCV Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes imagedspq No cDSP cDSP cDSP No cDSP cDSP cDSP No cDSP cDSP No No No cDSP cDSP No No cDSP cDSP cDSP cDSP Yes Base SDK examples asyncdspq No Yes Yes Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No Yes Yes Yes Yes No Asynchronous DSP Packet Queue No No No No No No No No No Yes No No No No Yes Yes No No No No No No No Calculator Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Calculator_c++ Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes calculator_c++_apk No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No No No No No No gtest Yes Yes No No No Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No No Yes(Not supported on LE) No No No HAP_example No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes LPI_example Yes No No No No No No No No No No No No No No No No No No No No No No Multithreading Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Profiling Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes QHL No Yes No No No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No No No No Yes Yes QHL_HVX No Yes No No No Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No No No No Yes No Qprintf Yes cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP cDSP No cDSP cDSP cDSP cDSP cDSP cDSP cDSP Yes cDSP cDSP Yes Compute examples Benchmark Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Camera CHI No No No No No No No No No Yes No No No No Yes Yes No No No No No No No Camera streamer No Yes No No No Yes No Yes No Yes No No No No Yes No No No No No No No No Corner detect No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes No No Yes Yes Yes Yes Yes Image DSPQ No Yes Yes Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No Yes Yes Yes Yes Yes UBWC DMA Yes Yes No Yes No Yes Yes Yes No Yes Yes No No No Yes Yes No No No No Yes Yes Yes User DMA No No No No No No No No No Yes No No No No Yes Yes No No No No No No No Compute Resource Manager Sample No No No No No No No No No Yes No No No No Yes Yes No No No No No No No .md-sidebar--secondary { display: none; } .md-content__inner md-typeset { max-width: 1200; } .md-content { margin-right: 0; }","title":"Feature Matrix"},{"location":"reference/idl.html","text":"Interface Description Language (IDL) Introduction to IDL The IDL language is a generic language that describes an interface between two processors. The IDL version supported in the SDK is tuned to describe specifically the interface between the application processor and the Hexagon DSPs communicating using FastRPC . IDL files defining these CPU-DSP interfaces are compiled with the QAIC tool to produce C header files and stub and skeleton source files that are linked into the CPU and DSP modules to enable them to communicate via RPC calls. This document describes the syntax and semantics of IDL. To learn how to build a project that includes IDL files, please refer to the build instructions . For a better understanding of how the CPU and DSP communicate together, please see the RPC documentation. Relationship to OMG IDL The IDL used in the Hexagon SDK is based on the OMG IDL specification but differs in a few respects: All interface methods must have an IDL return a type equivalent to IDL type long. The value returned must be 0 if the method is successful, or an error code on failure. Standard error codes are defined in AEEStdErr.idl which can be found under ${HEXAGON_SDK_ROOT}/incs/stddef . Interfaces may not directly inherit from more than one base interface . The rout parameter modes are used instead of out as explained in the next section . The inrout parameter mode is used instead of inout . The parameter mode inrout supports all the basic data types including string and wstring, and structures containing these data types. inrout also supports sequences but not structures containing sequences. dmahandle is also not supported. The dmahandle type is not present in OMG IDL This type is used to persistently map buffers on the DSP and bypass cache maintenance for these buffers on the APPS. The user can independently map and unmap the file descriptors on the DSP using the HAP_mem APIs . During the remote call, the file descriptors are mapped on IOMMU and get unmapped when the user unregisters the dmahandle explicitly. The advantage of using a dmahandle data type is that users can control the life time of the mapping in DSP. For more information about the dmahandle, please refer to the description of supported IDL types and how they map to C . Note: Cache maintenance for this mapping should be handled by the client. Bounded output parameters OMG IDL supports three parameter attributes or modes that specify the direction the data flows: in (client to server), out (server to client), and inout (both directions). In OMG IDL, the semantics of out and inout is such that the size of a variable-length parameter cannot be known to or bounded by the client at run time. However, standard practice is for all buffers to be bounded by the client. The IDL compiler supports output semantics through the IDL keyword rout , which is the bounded analog of out. The \"r\" in each keyword refers to the UNIX read() system call, where the client provides a buffer and specifies at run time the maximum amount of data to be read into the buffer. For fixed-size types, there is no difference between the traditional out and the new rout , as the size is statically known and therefore needs not be specified by the client. However, for variable-size types, such as sequences and strings, rout implies an upper bound that is passed as an input parameter from client to server. For example, read() could be defined in IDL as follows: typedef sequence<octet> SeqOctet; long read(rout SeqOctet buffer); In IDL, only a single rout parameter is needed, as it implies the client providing to the server the maximum number of octets to return. See further below for details on how this parameter would be mapped to the remote bindings. Note that the traditional OMG IDL out and inout parameter modes are not currently supported by the compiler. NULL Argument Semantics These are the current rules for when NULL is passed as an argument. in parameters: Sequence pointers may be NULL when the associated length is 0. rout parameters: Sequence, string, and wstring pointers may be NULL when the associated length is 0. Return Values must be equivalent type to long, such as AEEResult from AEEStdDef.idl value 0 indicates success a non-zero code indicates a failure. Any data in rout parameters is not propagated back when a non-zero code is returned. Wire Format Limitations in parameters: maximum of 255 in buffers. The total number of buffers in all input arguments combined cannot exceed 255. For example typedef sequence<octet> buf; interface foo { long bar(in sequence<buf> bufs); }; maps to C as struct __seq_unsigned_char { unsigned char* data; int dataLen; }; typedef struct __seq_unsigned_char __seq_unsigned_char; typedef __seq_unsigned_char buf; int foo_bar(buf* bufs, int bufsLen); and it will fail for bufsLen > 254 . rout parameters: maximum of 255 rout buffers in handles: maximum of 15 in dmahandle handles rout handles: maximum of 15 rout dmahandle handles Remote Handles The IDL compiler supports interfaces derived from remote_handle64 which allow the user to maintain a context handle for the opened module. For example, this allows to use a pointer to a local state structure, which holds information between RPC calls. Using a context handle also allows the client to specify which DSP to use at runtime, and, in the case of a crash on the DSP, to enable a restart of the session. interface calculator : remote_handle64 { // Compute a*b, where a and b are both complex long fmult(in float a, in float b, rout float result); }; This is a special interface that tells the compiler to do the following add a remote_handle64 argument as the first argument of every function in the interface For example, with the IDL code above: long calculator_fmult(remote_handle64 h, const float a, const float b, float *result); add an <interface>_open method For example: int calculator_open(const char *uri, remote_handle64 *h); where the uri is a string that allows to specify the domain in which to execute the code. add an <interface>_close method For example: int calculator_close(remote_handle64 h); With remote handles, users can open multiple instances of the handle as is illustrated in the next section . In addition, implementors of DSP modules can allocate a local state structure to return as the handle. If there is no local state to maintain, the handle value can be any arbitrary number. The code example below illustrates both of these approaches: int calculator_open(const char *uri, remote_handle64 *h) { // This value will NOT be returned to the client *h = (remote_handle64)0xdeadc0de; // but this value will be passed to close when the handle is closed // As such, it provides a mechanism to maintain a local structure. For example: // *h = (remote_handle64)malloc(100); return 0; } int calculator_close(remote_handle64 h) { // The value returned by open will be presented to the module on close and any other function assert(h == (remote_handle64)0xdeadc0de); // If instead the remote handle held a valid pointer to a memory allocated in the calculator_open, // this is where we could free the memory. return 0; } Note: The handle value seen by the client on the CPU side is allocated by the framework and not related to the DSP-side handle: * On the server side (DSP), the value assigned to *h in the `open()` implementation will be presented as the first argument to all subsequent function invocations to this interface and with this handle up until the handle is closed. * If the server has instance-specific state to maintain, a common approach consists of allocating the instance structure in the `open()` call, and use the address returned as the server-side handle. * If the server has no instance-specific state to maintain, it is free to return any garbage value that needs not necessarily be unique. * On the client side (CPU), the FastRPC framework will generate and return to the caller of open() a unique handle that is not derived from the server-side handle provided by the server open() implementation: the FastRPC retains a mapping internally from client-side to server-side handles, so that an invocation made on a client-side handle will be received on the server with the corresponding server-side handle, but neither side ever sees the other side's handles. Domains The term domain is used to refer to any of the DSPs that support FastRPC on a given target. Interfaces deriving from remote_handle64 will provide the user with explicit control over the module lifetime. A DSP-side process will be open as long as there is at least one handle reference to it. remote_handle64 h1 = -1, h2 = -1; float c1,c2,result; //open h1, if h1 is the first handle for the default domain it will instantiate a new DSP process assert(0 == calculator_open(calculator_URI, &h1)); //open h2, the process is still open assert(0 == calculator_open(calculator_URI, &h2)); //close h1, since we also opened h2, the process is still open assert(0 == calculator_close(h1)); //call using h1 will fail assert(0 != calculator_fmult(h1, &c1, &c2, &result)); //call using h2 will pass assert(0 == calculator_fmult(h1, &c1, &c2, &result)); //close h2, this is the last handle to the process //it will shutdown the process on the DSP side and free all its resources (void)calculator_close(h2); Domain restart Since users have explicit control over the handle lifetimes they can detect errors and re-open the handle. assert(0 == calculator_open(calculator_URI, &h1)); int nErr=calculator_fmult(h1, &c1, &c2, &result); if (nErr == AEE_ENOSUCH || nErr == AEE_EBADSTATE) { /* AEE_ENOSUCH is returned when Protection Domain Restart (PDR) happens and * AEE_EBADSTATE is returned when PD is exiting or crashing.*/ (void)calculator_close(h1); h1 = -1; // restart the DSP process assert(0 == calculator_open(calculator_URI, &h1)); //try fmult again assert(0 == calculator_fmult(h1, &c1, &c2, &result)); } For a full example illustrating that approach, please refer to the calculator example . Typically once a DSP-side process is killed or crashes, all methods should return AEE_ENOSUCH as defined in ${HEXAGON_SDK_ROOT}/incs/stddef/AEEStdErr.h . Domain Routing Users can explicitly specify the domain on which they need to execute their code. A session is opened on a particular domain by appending a static string representing that domain and defined in $HEXAGON_SDK_ROOT\\incs\\remote.h to the auto-generated interface URI: #include \"remote.h\" ... remote_handle cdsp, adsp; const char *uri_cdsp = calculator_URI CDSP_DOMAIN; const char *uri_adsp = calculator_URI ADSP_DOMAIN; assert(!calculator_open(uri_cdsp, &cdsp)); assert(!calculator_open(uri_adsp, &adsp)); assert(!calculator_sum(cdsp, buf, bufLen, &val)); assert(!calculator_sum(adsp, buf, bufLen, &val)); assert(!calculator_close(cdsp)); // kills the remote PD on aDSP if this is the last handle using the cDSP assert(!calculator_close(adsp)); // kills the remote PD on aDSP if this is the last handle using the aDSP Async FastRPC support Please refer to the Asynchronous FastRPC documentation for more details on how to use an asynchronous FastRPC function. To declare an asynchronous FastRPC function in IDL, use the syntax below: interface lib : remote_handle64 { // synchronous calls long foo(in sequence<octet> inPtr, rout sequence<unsigned long> outPtr, in float alpha); // declare an asynchronous-only RPC async long foo_async(in sequence<octet> inPtr, rout sequence<unsigned long> outPtr, in float alpha); } Corresponding method declaration in header file would have fastrpc_async_descriptor_t as a parameter: Multi-domain version of IDL generated method declarations for sync and async int lib_foo(remote_handle64 h, const unsigned char* inPtr, int inPtrLen, unsigned int* outPtr, int outPtrLen, float alpha); int lib_foo_async(remote_handle64 h, fastrpc_async_descriptor_t* asyncDesc, const unsigned char* inPtr, int inPtrLen, unsigned int* outPtr, int outPtrLen, float alpha); Single-domain version of IDL generated method declarations for sync and async int lib_foo(const unsigned char* inPtr, int inPtrLen, unsigned int* outPtr, int outPtrLen, float alpha); int lib_foo_async(fastrpc_async_descriptor_t* asyncDesc, const unsigned char* inPtr, int inPtrLen, unsigned int* outPtr, int outPtrLen, float alpha); Note: An async method can be forced to execute synchronously by passing a NULL async descriptor. Async stub methods are invoked like synchronous methods but include an additional descriptor as the first argument. Async calls will return 0 on successful submission of the job to DSP. A non-zero return value indicates the failure to make the async RPC call. nErr = lib_foo_async(handle, &desc, inbuf, inbufLen, outbuf, outbufLen, alpha); The skeleton remains identical to the conventional (synchronous) FastRPC skeleton, with the exception that the second argument which follows the remote handle, is the descriptor. Only sequences can be output arguments ( inrout or rout ) when declaring an asynchronous function. All other argument types declared as inrout or rout will cause a compilation error: Primitive type Array Dmahandle Wide string String Enum Struct Note: If an argument of any of these types needs to be specified as an output, it can be declared as a sequence with one element. The sequences created for async methods must be allocated and registered with FastRPC as shared ION buffers . Async call with non-ION buffers for sequences will return AEE_EBADPARM error at runtime. RPC functions that need to be available synchronously and asynchronously must be declared twice in the IDL file, once with and once without the async keyword Note: Clients can also call asynchronous RPC functions synchronously by using a NULL descriptor. Sample IDL A sample IDL file is shown below to illustrate the use of common IDL constructs. #include \"AEEStdDef.idl\" // Needed for 'AEEResult' interface math_example { // This structure is specific to this interface, so we scope it within the // interface to avoid pollution of the global namespace. struct Complex { float real; // Real part float imag; // Imaginary part }; // A vector, consisting of 0 or more Numbers. typedef sequence<Complex> Vector; // Compute a*b, where a and b are both complex AEEResult Mult(in Complex a, in Complex b, rout Complex result); }; This IDL interface will result in the generation of the following C interface: typedef struct math_example_Complex math_example_Complex; struct math_example_Complex { float real; float imag; }; typedef struct _math_example_Vector__seq_math_example_Complex _math_example_Vector__seq_math_example_Complex; typedef _math_example_Vector__seq_math_example_Complex math_example_Vector; struct _math_example_Vector__seq_math_example_Complex { math_example_Complex* data; int dataLen; }; __QAIC_HEADER_EXPORT AEEResult __QAIC_HEADER(math_example_Mult)(const math_example_Complex* a, const math_example_Complex* b, math_example_Complex* result) __QAIC_HEADER_ATTRIBUTE; Note: IDL sequences turn into pointers followed by a length value. The length of a sequence is defined as the number of elements and not the number of bytes in the array. Include directives and code generation Many IDL files include other IDL files in order to make use of types and interfaces declared externally. For example, when defining a Component Services interface in IDL, AEEIQI.idl needs to be included for the definition of IQI , from which all CS interfaces must be derived. However, one important difference between #include in IDL and #include in C/C++ is that in IDL, code is not generated for modules, interfaces, and types included from other IDL files. For example, consider the following IDL: interface foo { /* definition of foo here */ }; interface bar : foo { /* definition of bar here */ }; If this IDL is compiled, the output will contain the appropriate code for both foo and bar. However, suppose the foo definition is moved to foo.idl, and the IDL being compiled is changed as follows: #include \"foo.idl\" interface bar : foo { /* definition of bar here */ }; In this case, only code for bar will be generated. Although the contents of foo.idl are read by the compiler, no code is generated for foo because it is defined in an external (included) IDL file. Instead of generating code for foo, the compiler will translate the #include in the IDL to a #include in the output, with the extension changed from \".idl\" to \".h\". There can be two approaches to include an IDL file in another IDL file: Create an interface in the parent IDL (foo) and inherit that interface in the child IDL (bar). foo.idl interface foo{ long function1(); long function2(in long x); }; bar.idl // foo.idl included outside bar interface #include \"foo.idl\" interface bar: foo{ long baz(); }; Here bar.h will contain bar_function1 and bar_function2 , along with bar_baz This approach gives the flexibility to bundle up some functions together to be inherited for a particular interface. struct , typedef , etc., can be declared outside foo interface but functions must be declared inside foo interface. Multiple interfaces can be declared in foo.idl , but only one interface can be inherited in bar.idl foo.idl interface foo{ long baz1(); }; interface foo1{ long baz2(); }; bar.idl Either this is allowed interface bar : foo { long baz(); }; OR this is allowed interface bar : foo1 { long baz(); }; Note: An IDL having multiple interface declarations (here foo.idl ) cannot be used for shared obejct creation, it can only work as parent IDL. Declaring in foo.idl without any interface and including \"foo.idl\" inside the interface of bar.idl foo.idl #include \"AEEStdDef.idl\" AEEResult func(); bar.idl interface bar{ // foo.idl included inside bar interface #include \"foo.idl\" long baz(); }; Here bar.h will contain bar_func along with bar_baz In this approach interface bar will have access to everything declared in foo.idl . Note: You cannot define an interface in foo.idl in this case. In addition, when an IDL file includes #include \"remote.idl\" and its interface inherits remote_handle64 , the auto-generated header file will contain an interface_open and interface_close function declaration, and all other functions will have a remote_handle64 h as their first argument, as described here Header file generation The header files generated by QAIC are made to resemble hand-written C headers. For each interface name in IDL, for each function name in IDL; functions are generated in header file in the following format: int `interface`_`function`(arg1, arg2, ... argN); Mapping to C This section details the mapping of IDL constructs to C types. Basic built-in types The following table lists the mapping of IDL basic types to C . IDL Type C Type wchar _wchar_t octet unsigned char char char short short unsigned short unsigned short long int unsigned long unsigned int long long int64 unsigned long long uint64 int8 int8 uint8 uint8 int16 int16 uint16 uint16 int32 int32 uint32 uint32 int64 int64 uint64 uint64 int8_t int8_t uint8_t uint8_t int16_t int16_t uint16_t uint16_t int32_t int32_t uint32_t uint32_t int64_t int64_t uint64_t uint64_t float float double double boolean boolean dmahandle int (handle), uint32 (offset), uint32 (length) Definitions for _wchar_t, uint64 and int64 can be found in AEEStdDef.h The dmahandle type takes in three parameters: handle to the buffer, offset into the buffer and size of the buffer allowing to mapping, coherency, and cache operations. Constants Constant declarations in IDL are mapped to #defines in C , with expressions evaluated. Constant declaration in IDL: const short MAX_TRIES = 5 + 10 - 4; Corresponding C macro: #define MAX_TRIES 11 Identifiers Declaring constant in IDL results in declaring a macro in C and C++. For example, the following IDL constant declaration: const short MY_CONSTANT = 3; will result in the following C/C++ code: #define MY_CONSTANT 3 It is recommended that C and C++ keywords not be used as identifiers in IDL. However, if a keyword is used as an identifier, it will be prefixed with _cxx_ in the generated output. For example, the following constant declaration in IDL: const short break = 3; will result in the following C/C++ code: #define _cxx_break 3 Interfaces Types and functions declared within an interface must be scoped within that interface , any such types are prepended with the name of the enclosing interface and an underscore. Any type defined within an interface will be extracted and defined before the corresponding structure in the mapping. IDL declaration interface IFoo { struct inner { / ... / }; long process(in short a); }; Corresponding C prototype typedef struct IFoo_inner { /* ... */ } IFoo_inner; int IFoo_process(short int a); Methods Each method of an interface is mapped as a function. in parameter in parameters are passed by value. These input arguments are mapped as const . All user-defined types (struct, union) are passed as pointers to the defined type. Note that no in pointer may be NULL . An in parameter example is shown below. IDL declaration struct point { short x; float y; }; interface ITest { }; interface IFoo { long process(in short id, in string name, in point origin); }; Corresponding C prototype typedef struct point { short int x; float y; } point; int IFoo_process(short int id, const char* name, const point* origin); rout parameter rout parameters are passed by reference as a pointer. IDL declaration interface IFoo { long process(rout short id, rout string name, rout point origin); }; Corresponding C prototype int IFoo_process(short int* id, char* name, int nameLen, point* origin); inrout parameter inrout parameters are passed by reference as a pointer. This is very similar to rout parameter and an example of an inrout is shown below. IDL declaration interface IFoo { long process(inrout short id, inrout string name, inrout point origin); }; Corresponding C prototype int IFoo_process(short int* id, char* name, int nameLen, point* origin); Structures IDL structures are mapped to C structures, with a typedef to allow the name of the structure to be used as a type. Note that types declared within a structure will have the name of the enclosing structure prepended to their names, as is done with definitions within interfaces. IDL declaration struct extended_point { short x; float y; }; Corresponding C prototype typedef struct extended_point { short int x; float y; } extended_point; Enumerations IDL enumerated types are mapped to C enumerated types, with a typedef to allow the name of the enum to be used as a type. A placeholder enumerator is added to each enum to ensure binary compatibility across compilers. IDL declaration enum color { RED, ORANGE, YELLOW, GREEN, BLUE }; Corresponding C prototype typedef enum color { RED, ORANGE, YELLOW, GREEN, BLUE, _32BIT_PLACEHOLDER_color = 0x7fffffff } color; The starting value for an enum is always 0. Unions Unions are not supported at this time. Arrays IDL fixed-size arrays are mapped to C arrays. IDL declaration struct foo { long sum[2]; }; Corresponding C prototype typedef struct foo { int sum[2]; } foo; Sequences Sequences allow to represent arrays where the length is specified at runtime. For each sequence type sequence<T> , a corresponding structure __seq_T is generated with two members: T* data; int dataLen; The dataLen member specifies the number of elements in the array data (and not the number of bytes). Note that sequence lengths are always in terms of the number of elements in the sequence, not the number of bytes required to store the sequence. Consider the following mapping example for a sequence of long integers. IDL declaration typedef sequence<long> seqlong; Corresponding C prototype struct __seq_int { int* data; int dataLen; }; typedef __seq_int seqlong; This structure is used when constructing sequences of sequence types. Note: Memory buffers exchanged between the CPU and DSP should be declared as ION memory buffers using the RPC MEM APIs . in parameter of a method When a sequence<T> is specified as an in parameter of a method of an interface , the mapping generates two arguments. The first argument is a constant array pointer. It must be valid unless its length is 0, in which case the pointer may be NULL . The second argument specifies the total number of elements of the array and not the array size in bytes. IDL declaration typedef sequence<long> seqlong; interface IFoo { long process(in seqlong sums); }; Corresponding C prototype int IFoo_process( const int* sums, int sumsLen); rout parameter of a method An rout sequence is similar to an in sequence with the exception that the array pointer is not declared as a constant: IDL declaration typedef sequence<long> seqlong; interface IFoo { long process(rout seqlong sums); }; Corresponding C prototype int IFoo_process( int* sums, int sumsLen); inrout parameter of a method An inrout sequence is declared in C in the same way an rout sequence. IDL declaration typedef sequence<long> seqlong; interface IFoo { long process(inrout seqlong sums); }; Corresponding C prototype // see seqlong above int IFoo_process( int* sums, int sumsLen); Member of a structure A sequence may also be declared as a member of a structure: IDL declaration typedef sequence<long> seqlong; struct Atm { seqlong sums; }; Corresponding C prototype typedef struct Atm Atm; struct Atm { int* sums; int sumsLen; }; Within another sequence, or an array Sequences may also be used within another sequence or as part of an array: IDL declaration typedef sequence<long> seqlong; typedef sequence<seqlong> long2d; struct s { seqlong five_sequences[5]; }; Corresponding C prototype struct __seq_int { int* data; int dataLen; }; typedef __seq_int seqlong; struct __seq_seqlong { seqlong* data; int dataLen; }; typedef __seq_seqlong long2d; typedef struct s s; struct s { seqlong five_sequences[5]; }; Strings The IDL string type is mapped as char* , and wstring as _wchar_t* (where _wchar_t is typedef-ed to unsigned short ). When used anywhere other than an in parameter, the pointer is accompanied by a size, which allows the client to specify the number of characters ( char for string , _wchar_t for wstring ) allocated for the string or wstring. This is the length of the buffer in characters, not the length of the string -- since strings are null- terminated in C , the length of the string is computable. All length associated with a string or wstring include the null-terminator. Note: In this section, characters should be interpreted as meaning one-byte chars for string types, and a two-byte _wchar_ts for wstring types. The term \"character\" is not used here in the lexical sense -- when storing text, character set and encoding considerations are left to the application, and it is therefore possible for a lexical character to require more than one IDL character (non-zero byte) to represent it. in parameter of a method string is mapped as const char* and wstring as const _wchar_t* . IDL declaration interface IFoo { long process(in string name); long process_w(in wstring name); }; Corresponding C prototype int IFoo_process(const char* name); int IFoo_process_w(const _wchar_t* name); rout parameter of a method The client must provide a valid buffer, dcl , which can hold up to dclLen characters (including the null terminator). However, when dclLen is 0, dcl may be NULL . On successful return, the returned string dcl will always be null terminated at the dclLen - 1 character. An example of an rout string is shown below. IDL declaration interface IFoo { long process(rout string name); }; Corresponding C prototype int IFoo_process(char* name, int nameLen); inrout parameter of a method This is very similar to rout parameter and an example of an inrout string is shown below. IDL declaration interface IFoo { long process(inrout string name); }; Corresponding C prototype int IFoo_process(char* name, int nameLen); Note: For both types, the length parameters refer to the length of the buffer in characters (one-byte chars for strings, and two-byte _wchar_ts for wstrings), not the length of the string. The lengths are inclusive of a null terminator. Member of a structure Within a structure, a string is mapped as though it were a sequence<char> , and a wstring as though it were a sequence<wchar> . However, as with strings and wstrings, the buffers are always required to be null terminated. The mapping for sequences within structures is detailed in Sequence, part of which is duplicated here for clarity. IDL declaration struct Atm { string ssn; }; Corresponding C prototype typedef struct Atm Atm; struct Atm { char* ssn; int ssnLen; }; The second field ssnLen specifies the total size of the buffer ssn, in characters. Within a sequence When a string or wstring is used within a union or a sequence, it is mapped as a _cstring_t or _wstring_t. Both of these types are structures containing a pointer to a buffer and a buffer length. This structure is the same as the structure that would be generated for a sequence<char> in the case of string, or sequence<wchar> in the case of wstring. See Sequence for details on the structure generated for each sequence. The semantics of the dataLen field are the same as those for a string when it used as the member of a structure; see Member of a structure for details. IDL declaration typedef sequence<string> seqstring; Corresponding C prototype // Note: this struct is only defined // once, at the top of each file struct _cstring_t { char* data; int dataLen; }; struct __seq_string { _cstring_t* data; int dataLen; }; typedef __seq_string seqstring; NULL and empty strings Strings in IDL interfaces are never NULL pointers. Strings in IDL are never absent or omitted by being NULL because they can't be. They either have a value or they are the empty string. An empty string is a valid pointer to a buffer with a single byte of value 0. (\"\" is an empty string) QAIC qaic , Qaic's Another Idl Compiler, is a command-line executable used to implement remote shared objects for the DSP Platform. A shared object is called remote if its methods can be invoked from outside the domain it resides in. The user of a remote object does not need to know where the object is hosted, or what language the object is implemented in. Instead, the user calls methods on a stub object generated by qaic . The stub marshals the input into a shared wire format, and ships the data off to the domain where the object is hosted. The host domain implements a skel object, also generated by qaic , that unmarshals the data, and invokes the requested method on the native object. The FastRPC framework description provides more information on the framework enabling this remote communication on Qualcomm devices. To generate stubs and skels, qaic requires the interface to an object be strictly defined. The syntax for defining an object interface is called IDL and covered in the previous sections. qaic compiles IDL files into headers, stubs, and skels. The generated header can be used to implement the native object, and for users to call methods on the object. The stub and skel are compiled into a shared object. qaic is invoked automatically when building a project. This section discusses how to run and use the compiler separately, including details of the various command-line options. Command-line usage The basic command-line syntax of the tool is: qaic [options] file1.idl [file2.idl ... fileN.idl] Each file specified on the command-line will be compiled by the tool according to the options specified. The available options are: -mdll or --map-dll Generate DLL mapping -o=PATH Use path as the output path. All generated files will be output to the specified path. The default is the current directory ('.'). --cpp or -p=CPP Use CPP as the C preprocessor. The value CPP must name an executable program, and cannot contain any arguments. To pass arguments to the preprocessor, use --arg-cpp ( -pa ). --arg-cpp=ARG or -pa=ARG Pass additional argument arg to the preprocessor. To specify arguments that are themselves options, use the form -pa=ARG (for example, -pa=-E ). Specifying -pa -E will cause the -E to be interpreted as an option to qaic instead of to the preprocessor. Note that for Comment pass-through to work properly, the preprocessor must be set to not strip comments from the source. Typically the flag to do this is -C , making the appropriate argument to qaic -pa=-C . --include-path=PATH or -I=PATH Include path in the search path for included files. May be used multiple times. --indent=WIDTH or -i=WIDTH Use an indentation width of width spaces in the generated code. --warn-undefined or -Wu Issue warning for forward-declared interfaces that are never defined. --define=SYMBOL or -D=SYMBOL Predefine macro for the preprocessor. --header-only or -ho Only generate a header. Stub and skeleton code is not generated if this option is specified. --remoting-only or -ro Only generate stub and skeleton code. The corresponding header is not generated if this option is specified. --parse-only or -s Parse the IDL and perform semantic checking, but do not generate any output. Note that IDL files accepted without errors by the compiler with -s are not guaranteed to work without errors when code generation is enabled. -v Print the version of the compiler. -h Print a brief help message. Usage examples The examples below illustrate typical usage of the IDL compiler. qaic --header-only foo.idl bar.idl The above command compiles foo.idl to the remote header file foo.h, and bar.idl to the remote header file bar.h. No remoting code is generated. qaic foo.idl The above command compiles foo.idl to a remote header file foo.h, along with the following remoting code: File Name Description foo_stub.c C stub implementation foo_skel.c C skeleton implementation foo.h Common header for stub and skel qaic -I../bar -I../far -o out foo.idl The above command compiles foo.idl . It uses ../bar and ../far as the search path for any include files. It uses out as the result directory, and generates out/foo.h , out/foo_stub.c and out/foo_skel.c files. Using other preprocessors By default, qaic uses an internal preprocessor. It may be desirable to use a different preprocessor instead. The Microsoft C preprocessor can be used by having the compiler invoke cl /E /C , which is done with the following command-line. Note that for this to work, cl must be in the PATH . qaic -p=cl -pa=/E -pa=/C file1.idl [file2.idl ... fileN.idl] The ARM C/C++ compiler can also be used to preprocess IDL. Provided armcc is in the PATH , this can be done with the following command-line. qaic -p=armcc -pa=-E -pa=-C file1.idl [file2.idl ... fileN.idl] Note that -pa=-E must be used instead of -pa -E , since in the latter case the -E is interpreted by qaic as being an option to qaic , not to the preprocessor. Error messages Any output printed by the compiler is due to either an error or a warning. Warnings include the text warning: at the beginning of the message, and do not abort code generation. Any message not preceded by warning: is an error, which causes compilation to abort. Both errors and warnings include a reference to the file, line, and position within that line (starting at 0) where the error or warning occurred. Additional details on select errors are given in the following subsections. Identifier 'abc' clashes with an introduced type The OMG IDL specification includes complex scoping rules based not only on where types are defined, but also on where they are used. Specifically, the first component (identifier) of a qualified type name is introduced into the scope where it is used, preventing the use of any identifier with the same name in that scope. Fully-qualified names, which start with ::, are considered to have an empty first component, and thus result in no type introduction. Consider the following example, which illustrates the basic type introduction rules. For full details, see the \"Names and Scoping\" section of OMG IDL Syntax & Semantics. struct Name { string first, last; }; struct Address { string street, city, state, country; }; module M { typedef long Age; }; struct Person { Name name; // invalid IDL: 'name' clashes with 'Name' string address; // OK: 'Address' not introduced in this scope M::Age age; // OK: only 'M', not 'Age', is introduced }; In the Person structure above, the use of the Name type introduces it into the scope of Person, which prevents the member from being called name. The second member, address, is fine because the Address type is not defined within the scope of Person and has not been introduced. The reference to M::Age only causes the first component, M, to be introduced into the scope of Person, thus the age member is also without error. Clashes with introduced types can generally be resolved by changing the qualification to avoid the type introduction. For instance, if in the above example the type of the name member of the Person structure were written ::Name, no type introduction would occur, which would avoid the name clash. Comment pass-through When qaic identifies code comments as a Doxygen comments, or ordinary comments. Doxygen comments are passed through to generated header files. When documenting interface methods, Doxygen comments are generally preferred. Doxygen comments When a method is documented with the Doxygen syntax, qaic will attempt to translate the documentation to the target language in any output files.","title":"IDL documentation"},{"location":"reference/idl.html#interface-description-language-idl","text":"","title":"Interface Description Language (IDL)"},{"location":"reference/idl.html#introduction-to-idl","text":"The IDL language is a generic language that describes an interface between two processors. The IDL version supported in the SDK is tuned to describe specifically the interface between the application processor and the Hexagon DSPs communicating using FastRPC . IDL files defining these CPU-DSP interfaces are compiled with the QAIC tool to produce C header files and stub and skeleton source files that are linked into the CPU and DSP modules to enable them to communicate via RPC calls. This document describes the syntax and semantics of IDL. To learn how to build a project that includes IDL files, please refer to the build instructions . For a better understanding of how the CPU and DSP communicate together, please see the RPC documentation.","title":"Introduction to IDL"},{"location":"reference/idl.html#relationship-to-omg-idl","text":"The IDL used in the Hexagon SDK is based on the OMG IDL specification but differs in a few respects: All interface methods must have an IDL return a type equivalent to IDL type long. The value returned must be 0 if the method is successful, or an error code on failure. Standard error codes are defined in AEEStdErr.idl which can be found under ${HEXAGON_SDK_ROOT}/incs/stddef . Interfaces may not directly inherit from more than one base interface . The rout parameter modes are used instead of out as explained in the next section . The inrout parameter mode is used instead of inout . The parameter mode inrout supports all the basic data types including string and wstring, and structures containing these data types. inrout also supports sequences but not structures containing sequences. dmahandle is also not supported. The dmahandle type is not present in OMG IDL This type is used to persistently map buffers on the DSP and bypass cache maintenance for these buffers on the APPS. The user can independently map and unmap the file descriptors on the DSP using the HAP_mem APIs . During the remote call, the file descriptors are mapped on IOMMU and get unmapped when the user unregisters the dmahandle explicitly. The advantage of using a dmahandle data type is that users can control the life time of the mapping in DSP. For more information about the dmahandle, please refer to the description of supported IDL types and how they map to C . Note: Cache maintenance for this mapping should be handled by the client.","title":"Relationship to OMG IDL"},{"location":"reference/idl.html#bounded-output-parameters","text":"OMG IDL supports three parameter attributes or modes that specify the direction the data flows: in (client to server), out (server to client), and inout (both directions). In OMG IDL, the semantics of out and inout is such that the size of a variable-length parameter cannot be known to or bounded by the client at run time. However, standard practice is for all buffers to be bounded by the client. The IDL compiler supports output semantics through the IDL keyword rout , which is the bounded analog of out. The \"r\" in each keyword refers to the UNIX read() system call, where the client provides a buffer and specifies at run time the maximum amount of data to be read into the buffer. For fixed-size types, there is no difference between the traditional out and the new rout , as the size is statically known and therefore needs not be specified by the client. However, for variable-size types, such as sequences and strings, rout implies an upper bound that is passed as an input parameter from client to server. For example, read() could be defined in IDL as follows: typedef sequence<octet> SeqOctet; long read(rout SeqOctet buffer); In IDL, only a single rout parameter is needed, as it implies the client providing to the server the maximum number of octets to return. See further below for details on how this parameter would be mapped to the remote bindings. Note that the traditional OMG IDL out and inout parameter modes are not currently supported by the compiler.","title":"Bounded output parameters"},{"location":"reference/idl.html#null-argument-semantics","text":"These are the current rules for when NULL is passed as an argument. in parameters: Sequence pointers may be NULL when the associated length is 0. rout parameters: Sequence, string, and wstring pointers may be NULL when the associated length is 0.","title":"NULL Argument Semantics"},{"location":"reference/idl.html#return-values","text":"must be equivalent type to long, such as AEEResult from AEEStdDef.idl value 0 indicates success a non-zero code indicates a failure. Any data in rout parameters is not propagated back when a non-zero code is returned.","title":"Return Values"},{"location":"reference/idl.html#wire-format-limitations","text":"in parameters: maximum of 255 in buffers. The total number of buffers in all input arguments combined cannot exceed 255. For example typedef sequence<octet> buf; interface foo { long bar(in sequence<buf> bufs); }; maps to C as struct __seq_unsigned_char { unsigned char* data; int dataLen; }; typedef struct __seq_unsigned_char __seq_unsigned_char; typedef __seq_unsigned_char buf; int foo_bar(buf* bufs, int bufsLen); and it will fail for bufsLen > 254 . rout parameters: maximum of 255 rout buffers in handles: maximum of 15 in dmahandle handles rout handles: maximum of 15 rout dmahandle handles","title":"Wire Format Limitations"},{"location":"reference/idl.html#remote-handles","text":"The IDL compiler supports interfaces derived from remote_handle64 which allow the user to maintain a context handle for the opened module. For example, this allows to use a pointer to a local state structure, which holds information between RPC calls. Using a context handle also allows the client to specify which DSP to use at runtime, and, in the case of a crash on the DSP, to enable a restart of the session. interface calculator : remote_handle64 { // Compute a*b, where a and b are both complex long fmult(in float a, in float b, rout float result); }; This is a special interface that tells the compiler to do the following add a remote_handle64 argument as the first argument of every function in the interface For example, with the IDL code above: long calculator_fmult(remote_handle64 h, const float a, const float b, float *result); add an <interface>_open method For example: int calculator_open(const char *uri, remote_handle64 *h); where the uri is a string that allows to specify the domain in which to execute the code. add an <interface>_close method For example: int calculator_close(remote_handle64 h); With remote handles, users can open multiple instances of the handle as is illustrated in the next section . In addition, implementors of DSP modules can allocate a local state structure to return as the handle. If there is no local state to maintain, the handle value can be any arbitrary number. The code example below illustrates both of these approaches: int calculator_open(const char *uri, remote_handle64 *h) { // This value will NOT be returned to the client *h = (remote_handle64)0xdeadc0de; // but this value will be passed to close when the handle is closed // As such, it provides a mechanism to maintain a local structure. For example: // *h = (remote_handle64)malloc(100); return 0; } int calculator_close(remote_handle64 h) { // The value returned by open will be presented to the module on close and any other function assert(h == (remote_handle64)0xdeadc0de); // If instead the remote handle held a valid pointer to a memory allocated in the calculator_open, // this is where we could free the memory. return 0; } Note: The handle value seen by the client on the CPU side is allocated by the framework and not related to the DSP-side handle: * On the server side (DSP), the value assigned to *h in the `open()` implementation will be presented as the first argument to all subsequent function invocations to this interface and with this handle up until the handle is closed. * If the server has instance-specific state to maintain, a common approach consists of allocating the instance structure in the `open()` call, and use the address returned as the server-side handle. * If the server has no instance-specific state to maintain, it is free to return any garbage value that needs not necessarily be unique. * On the client side (CPU), the FastRPC framework will generate and return to the caller of open() a unique handle that is not derived from the server-side handle provided by the server open() implementation: the FastRPC retains a mapping internally from client-side to server-side handles, so that an invocation made on a client-side handle will be received on the server with the corresponding server-side handle, but neither side ever sees the other side's handles.","title":"Remote Handles"},{"location":"reference/idl.html#domains","text":"The term domain is used to refer to any of the DSPs that support FastRPC on a given target. Interfaces deriving from remote_handle64 will provide the user with explicit control over the module lifetime. A DSP-side process will be open as long as there is at least one handle reference to it. remote_handle64 h1 = -1, h2 = -1; float c1,c2,result; //open h1, if h1 is the first handle for the default domain it will instantiate a new DSP process assert(0 == calculator_open(calculator_URI, &h1)); //open h2, the process is still open assert(0 == calculator_open(calculator_URI, &h2)); //close h1, since we also opened h2, the process is still open assert(0 == calculator_close(h1)); //call using h1 will fail assert(0 != calculator_fmult(h1, &c1, &c2, &result)); //call using h2 will pass assert(0 == calculator_fmult(h1, &c1, &c2, &result)); //close h2, this is the last handle to the process //it will shutdown the process on the DSP side and free all its resources (void)calculator_close(h2);","title":"Domains"},{"location":"reference/idl.html#domain-restart","text":"Since users have explicit control over the handle lifetimes they can detect errors and re-open the handle. assert(0 == calculator_open(calculator_URI, &h1)); int nErr=calculator_fmult(h1, &c1, &c2, &result); if (nErr == AEE_ENOSUCH || nErr == AEE_EBADSTATE) { /* AEE_ENOSUCH is returned when Protection Domain Restart (PDR) happens and * AEE_EBADSTATE is returned when PD is exiting or crashing.*/ (void)calculator_close(h1); h1 = -1; // restart the DSP process assert(0 == calculator_open(calculator_URI, &h1)); //try fmult again assert(0 == calculator_fmult(h1, &c1, &c2, &result)); } For a full example illustrating that approach, please refer to the calculator example . Typically once a DSP-side process is killed or crashes, all methods should return AEE_ENOSUCH as defined in ${HEXAGON_SDK_ROOT}/incs/stddef/AEEStdErr.h .","title":"Domain restart"},{"location":"reference/idl.html#domain-routing","text":"Users can explicitly specify the domain on which they need to execute their code. A session is opened on a particular domain by appending a static string representing that domain and defined in $HEXAGON_SDK_ROOT\\incs\\remote.h to the auto-generated interface URI: #include \"remote.h\" ... remote_handle cdsp, adsp; const char *uri_cdsp = calculator_URI CDSP_DOMAIN; const char *uri_adsp = calculator_URI ADSP_DOMAIN; assert(!calculator_open(uri_cdsp, &cdsp)); assert(!calculator_open(uri_adsp, &adsp)); assert(!calculator_sum(cdsp, buf, bufLen, &val)); assert(!calculator_sum(adsp, buf, bufLen, &val)); assert(!calculator_close(cdsp)); // kills the remote PD on aDSP if this is the last handle using the cDSP assert(!calculator_close(adsp)); // kills the remote PD on aDSP if this is the last handle using the aDSP","title":"Domain Routing"},{"location":"reference/idl.html#async-fastrpc-support","text":"Please refer to the Asynchronous FastRPC documentation for more details on how to use an asynchronous FastRPC function. To declare an asynchronous FastRPC function in IDL, use the syntax below: interface lib : remote_handle64 { // synchronous calls long foo(in sequence<octet> inPtr, rout sequence<unsigned long> outPtr, in float alpha); // declare an asynchronous-only RPC async long foo_async(in sequence<octet> inPtr, rout sequence<unsigned long> outPtr, in float alpha); } Corresponding method declaration in header file would have fastrpc_async_descriptor_t as a parameter: Multi-domain version of IDL generated method declarations for sync and async int lib_foo(remote_handle64 h, const unsigned char* inPtr, int inPtrLen, unsigned int* outPtr, int outPtrLen, float alpha); int lib_foo_async(remote_handle64 h, fastrpc_async_descriptor_t* asyncDesc, const unsigned char* inPtr, int inPtrLen, unsigned int* outPtr, int outPtrLen, float alpha); Single-domain version of IDL generated method declarations for sync and async int lib_foo(const unsigned char* inPtr, int inPtrLen, unsigned int* outPtr, int outPtrLen, float alpha); int lib_foo_async(fastrpc_async_descriptor_t* asyncDesc, const unsigned char* inPtr, int inPtrLen, unsigned int* outPtr, int outPtrLen, float alpha); Note: An async method can be forced to execute synchronously by passing a NULL async descriptor. Async stub methods are invoked like synchronous methods but include an additional descriptor as the first argument. Async calls will return 0 on successful submission of the job to DSP. A non-zero return value indicates the failure to make the async RPC call. nErr = lib_foo_async(handle, &desc, inbuf, inbufLen, outbuf, outbufLen, alpha); The skeleton remains identical to the conventional (synchronous) FastRPC skeleton, with the exception that the second argument which follows the remote handle, is the descriptor. Only sequences can be output arguments ( inrout or rout ) when declaring an asynchronous function. All other argument types declared as inrout or rout will cause a compilation error: Primitive type Array Dmahandle Wide string String Enum Struct Note: If an argument of any of these types needs to be specified as an output, it can be declared as a sequence with one element. The sequences created for async methods must be allocated and registered with FastRPC as shared ION buffers . Async call with non-ION buffers for sequences will return AEE_EBADPARM error at runtime. RPC functions that need to be available synchronously and asynchronously must be declared twice in the IDL file, once with and once without the async keyword Note: Clients can also call asynchronous RPC functions synchronously by using a NULL descriptor.","title":"Async FastRPC support"},{"location":"reference/idl.html#sample-idl","text":"A sample IDL file is shown below to illustrate the use of common IDL constructs. #include \"AEEStdDef.idl\" // Needed for 'AEEResult' interface math_example { // This structure is specific to this interface, so we scope it within the // interface to avoid pollution of the global namespace. struct Complex { float real; // Real part float imag; // Imaginary part }; // A vector, consisting of 0 or more Numbers. typedef sequence<Complex> Vector; // Compute a*b, where a and b are both complex AEEResult Mult(in Complex a, in Complex b, rout Complex result); }; This IDL interface will result in the generation of the following C interface: typedef struct math_example_Complex math_example_Complex; struct math_example_Complex { float real; float imag; }; typedef struct _math_example_Vector__seq_math_example_Complex _math_example_Vector__seq_math_example_Complex; typedef _math_example_Vector__seq_math_example_Complex math_example_Vector; struct _math_example_Vector__seq_math_example_Complex { math_example_Complex* data; int dataLen; }; __QAIC_HEADER_EXPORT AEEResult __QAIC_HEADER(math_example_Mult)(const math_example_Complex* a, const math_example_Complex* b, math_example_Complex* result) __QAIC_HEADER_ATTRIBUTE; Note: IDL sequences turn into pointers followed by a length value. The length of a sequence is defined as the number of elements and not the number of bytes in the array.","title":"Sample IDL"},{"location":"reference/idl.html#include-directives-and-code-generation","text":"Many IDL files include other IDL files in order to make use of types and interfaces declared externally. For example, when defining a Component Services interface in IDL, AEEIQI.idl needs to be included for the definition of IQI , from which all CS interfaces must be derived. However, one important difference between #include in IDL and #include in C/C++ is that in IDL, code is not generated for modules, interfaces, and types included from other IDL files. For example, consider the following IDL: interface foo { /* definition of foo here */ }; interface bar : foo { /* definition of bar here */ }; If this IDL is compiled, the output will contain the appropriate code for both foo and bar. However, suppose the foo definition is moved to foo.idl, and the IDL being compiled is changed as follows: #include \"foo.idl\" interface bar : foo { /* definition of bar here */ }; In this case, only code for bar will be generated. Although the contents of foo.idl are read by the compiler, no code is generated for foo because it is defined in an external (included) IDL file. Instead of generating code for foo, the compiler will translate the #include in the IDL to a #include in the output, with the extension changed from \".idl\" to \".h\". There can be two approaches to include an IDL file in another IDL file: Create an interface in the parent IDL (foo) and inherit that interface in the child IDL (bar). foo.idl interface foo{ long function1(); long function2(in long x); }; bar.idl // foo.idl included outside bar interface #include \"foo.idl\" interface bar: foo{ long baz(); }; Here bar.h will contain bar_function1 and bar_function2 , along with bar_baz This approach gives the flexibility to bundle up some functions together to be inherited for a particular interface. struct , typedef , etc., can be declared outside foo interface but functions must be declared inside foo interface. Multiple interfaces can be declared in foo.idl , but only one interface can be inherited in bar.idl foo.idl interface foo{ long baz1(); }; interface foo1{ long baz2(); }; bar.idl Either this is allowed interface bar : foo { long baz(); }; OR this is allowed interface bar : foo1 { long baz(); }; Note: An IDL having multiple interface declarations (here foo.idl ) cannot be used for shared obejct creation, it can only work as parent IDL. Declaring in foo.idl without any interface and including \"foo.idl\" inside the interface of bar.idl foo.idl #include \"AEEStdDef.idl\" AEEResult func(); bar.idl interface bar{ // foo.idl included inside bar interface #include \"foo.idl\" long baz(); }; Here bar.h will contain bar_func along with bar_baz In this approach interface bar will have access to everything declared in foo.idl . Note: You cannot define an interface in foo.idl in this case. In addition, when an IDL file includes #include \"remote.idl\" and its interface inherits remote_handle64 , the auto-generated header file will contain an interface_open and interface_close function declaration, and all other functions will have a remote_handle64 h as their first argument, as described here","title":"Include directives and code generation"},{"location":"reference/idl.html#header-file-generation","text":"The header files generated by QAIC are made to resemble hand-written C headers. For each interface name in IDL, for each function name in IDL; functions are generated in header file in the following format: int `interface`_`function`(arg1, arg2, ... argN);","title":"Header file generation"},{"location":"reference/idl.html#mapping-to-c","text":"This section details the mapping of IDL constructs to C types.","title":"Mapping to C"},{"location":"reference/idl.html#basic-built-in-types","text":"The following table lists the mapping of IDL basic types to C . IDL Type C Type wchar _wchar_t octet unsigned char char char short short unsigned short unsigned short long int unsigned long unsigned int long long int64 unsigned long long uint64 int8 int8 uint8 uint8 int16 int16 uint16 uint16 int32 int32 uint32 uint32 int64 int64 uint64 uint64 int8_t int8_t uint8_t uint8_t int16_t int16_t uint16_t uint16_t int32_t int32_t uint32_t uint32_t int64_t int64_t uint64_t uint64_t float float double double boolean boolean dmahandle int (handle), uint32 (offset), uint32 (length) Definitions for _wchar_t, uint64 and int64 can be found in AEEStdDef.h The dmahandle type takes in three parameters: handle to the buffer, offset into the buffer and size of the buffer allowing to mapping, coherency, and cache operations.","title":"Basic built-in types"},{"location":"reference/idl.html#constants","text":"Constant declarations in IDL are mapped to #defines in C , with expressions evaluated. Constant declaration in IDL: const short MAX_TRIES = 5 + 10 - 4; Corresponding C macro: #define MAX_TRIES 11","title":"Constants"},{"location":"reference/idl.html#identifiers","text":"Declaring constant in IDL results in declaring a macro in C and C++. For example, the following IDL constant declaration: const short MY_CONSTANT = 3; will result in the following C/C++ code: #define MY_CONSTANT 3 It is recommended that C and C++ keywords not be used as identifiers in IDL. However, if a keyword is used as an identifier, it will be prefixed with _cxx_ in the generated output. For example, the following constant declaration in IDL: const short break = 3; will result in the following C/C++ code: #define _cxx_break 3","title":"Identifiers"},{"location":"reference/idl.html#interfaces","text":"Types and functions declared within an interface must be scoped within that interface , any such types are prepended with the name of the enclosing interface and an underscore. Any type defined within an interface will be extracted and defined before the corresponding structure in the mapping. IDL declaration interface IFoo { struct inner { / ... / }; long process(in short a); }; Corresponding C prototype typedef struct IFoo_inner { /* ... */ } IFoo_inner; int IFoo_process(short int a);","title":"Interfaces"},{"location":"reference/idl.html#methods","text":"Each method of an interface is mapped as a function.","title":"Methods"},{"location":"reference/idl.html#in-parameter","text":"in parameters are passed by value. These input arguments are mapped as const . All user-defined types (struct, union) are passed as pointers to the defined type. Note that no in pointer may be NULL . An in parameter example is shown below. IDL declaration struct point { short x; float y; }; interface ITest { }; interface IFoo { long process(in short id, in string name, in point origin); }; Corresponding C prototype typedef struct point { short int x; float y; } point; int IFoo_process(short int id, const char* name, const point* origin);","title":"in parameter"},{"location":"reference/idl.html#rout-parameter","text":"rout parameters are passed by reference as a pointer. IDL declaration interface IFoo { long process(rout short id, rout string name, rout point origin); }; Corresponding C prototype int IFoo_process(short int* id, char* name, int nameLen, point* origin);","title":"rout parameter"},{"location":"reference/idl.html#inrout-parameter","text":"inrout parameters are passed by reference as a pointer. This is very similar to rout parameter and an example of an inrout is shown below. IDL declaration interface IFoo { long process(inrout short id, inrout string name, inrout point origin); }; Corresponding C prototype int IFoo_process(short int* id, char* name, int nameLen, point* origin);","title":"inrout parameter"},{"location":"reference/idl.html#structures","text":"IDL structures are mapped to C structures, with a typedef to allow the name of the structure to be used as a type. Note that types declared within a structure will have the name of the enclosing structure prepended to their names, as is done with definitions within interfaces. IDL declaration struct extended_point { short x; float y; }; Corresponding C prototype typedef struct extended_point { short int x; float y; } extended_point;","title":"Structures"},{"location":"reference/idl.html#enumerations","text":"IDL enumerated types are mapped to C enumerated types, with a typedef to allow the name of the enum to be used as a type. A placeholder enumerator is added to each enum to ensure binary compatibility across compilers. IDL declaration enum color { RED, ORANGE, YELLOW, GREEN, BLUE }; Corresponding C prototype typedef enum color { RED, ORANGE, YELLOW, GREEN, BLUE, _32BIT_PLACEHOLDER_color = 0x7fffffff } color; The starting value for an enum is always 0.","title":"Enumerations"},{"location":"reference/idl.html#unions","text":"Unions are not supported at this time.","title":"Unions"},{"location":"reference/idl.html#arrays","text":"IDL fixed-size arrays are mapped to C arrays. IDL declaration struct foo { long sum[2]; }; Corresponding C prototype typedef struct foo { int sum[2]; } foo;","title":"Arrays"},{"location":"reference/idl.html#sequences","text":"Sequences allow to represent arrays where the length is specified at runtime. For each sequence type sequence<T> , a corresponding structure __seq_T is generated with two members: T* data; int dataLen; The dataLen member specifies the number of elements in the array data (and not the number of bytes). Note that sequence lengths are always in terms of the number of elements in the sequence, not the number of bytes required to store the sequence. Consider the following mapping example for a sequence of long integers. IDL declaration typedef sequence<long> seqlong; Corresponding C prototype struct __seq_int { int* data; int dataLen; }; typedef __seq_int seqlong; This structure is used when constructing sequences of sequence types. Note: Memory buffers exchanged between the CPU and DSP should be declared as ION memory buffers using the RPC MEM APIs .","title":"Sequences"},{"location":"reference/idl.html#in-parameter-of-a-method","text":"When a sequence<T> is specified as an in parameter of a method of an interface , the mapping generates two arguments. The first argument is a constant array pointer. It must be valid unless its length is 0, in which case the pointer may be NULL . The second argument specifies the total number of elements of the array and not the array size in bytes. IDL declaration typedef sequence<long> seqlong; interface IFoo { long process(in seqlong sums); }; Corresponding C prototype int IFoo_process( const int* sums, int sumsLen);","title":"in parameter of a method"},{"location":"reference/idl.html#rout-parameter-of-a-method","text":"An rout sequence is similar to an in sequence with the exception that the array pointer is not declared as a constant: IDL declaration typedef sequence<long> seqlong; interface IFoo { long process(rout seqlong sums); }; Corresponding C prototype int IFoo_process( int* sums, int sumsLen);","title":"rout parameter of a method"},{"location":"reference/idl.html#inrout-parameter-of-a-method","text":"An inrout sequence is declared in C in the same way an rout sequence. IDL declaration typedef sequence<long> seqlong; interface IFoo { long process(inrout seqlong sums); }; Corresponding C prototype // see seqlong above int IFoo_process( int* sums, int sumsLen);","title":"inrout parameter of a method"},{"location":"reference/idl.html#member-of-a-structure","text":"A sequence may also be declared as a member of a structure: IDL declaration typedef sequence<long> seqlong; struct Atm { seqlong sums; }; Corresponding C prototype typedef struct Atm Atm; struct Atm { int* sums; int sumsLen; };","title":"Member of a structure"},{"location":"reference/idl.html#within-another-sequence-or-an-array","text":"Sequences may also be used within another sequence or as part of an array: IDL declaration typedef sequence<long> seqlong; typedef sequence<seqlong> long2d; struct s { seqlong five_sequences[5]; }; Corresponding C prototype struct __seq_int { int* data; int dataLen; }; typedef __seq_int seqlong; struct __seq_seqlong { seqlong* data; int dataLen; }; typedef __seq_seqlong long2d; typedef struct s s; struct s { seqlong five_sequences[5]; };","title":"Within another sequence, or an array"},{"location":"reference/idl.html#strings","text":"The IDL string type is mapped as char* , and wstring as _wchar_t* (where _wchar_t is typedef-ed to unsigned short ). When used anywhere other than an in parameter, the pointer is accompanied by a size, which allows the client to specify the number of characters ( char for string , _wchar_t for wstring ) allocated for the string or wstring. This is the length of the buffer in characters, not the length of the string -- since strings are null- terminated in C , the length of the string is computable. All length associated with a string or wstring include the null-terminator. Note: In this section, characters should be interpreted as meaning one-byte chars for string types, and a two-byte _wchar_ts for wstring types. The term \"character\" is not used here in the lexical sense -- when storing text, character set and encoding considerations are left to the application, and it is therefore possible for a lexical character to require more than one IDL character (non-zero byte) to represent it.","title":"Strings"},{"location":"reference/idl.html#in-parameter-of-a-method_1","text":"string is mapped as const char* and wstring as const _wchar_t* . IDL declaration interface IFoo { long process(in string name); long process_w(in wstring name); }; Corresponding C prototype int IFoo_process(const char* name); int IFoo_process_w(const _wchar_t* name);","title":"in parameter of a method"},{"location":"reference/idl.html#rout-parameter-of-a-method_1","text":"The client must provide a valid buffer, dcl , which can hold up to dclLen characters (including the null terminator). However, when dclLen is 0, dcl may be NULL . On successful return, the returned string dcl will always be null terminated at the dclLen - 1 character. An example of an rout string is shown below. IDL declaration interface IFoo { long process(rout string name); }; Corresponding C prototype int IFoo_process(char* name, int nameLen);","title":"rout parameter of a method"},{"location":"reference/idl.html#inrout-parameter-of-a-method_1","text":"This is very similar to rout parameter and an example of an inrout string is shown below. IDL declaration interface IFoo { long process(inrout string name); }; Corresponding C prototype int IFoo_process(char* name, int nameLen); Note: For both types, the length parameters refer to the length of the buffer in characters (one-byte chars for strings, and two-byte _wchar_ts for wstrings), not the length of the string. The lengths are inclusive of a null terminator.","title":"inrout parameter of a method"},{"location":"reference/idl.html#member-of-a-structure_1","text":"Within a structure, a string is mapped as though it were a sequence<char> , and a wstring as though it were a sequence<wchar> . However, as with strings and wstrings, the buffers are always required to be null terminated. The mapping for sequences within structures is detailed in Sequence, part of which is duplicated here for clarity. IDL declaration struct Atm { string ssn; }; Corresponding C prototype typedef struct Atm Atm; struct Atm { char* ssn; int ssnLen; }; The second field ssnLen specifies the total size of the buffer ssn, in characters.","title":"Member of a structure"},{"location":"reference/idl.html#within-a-sequence","text":"When a string or wstring is used within a union or a sequence, it is mapped as a _cstring_t or _wstring_t. Both of these types are structures containing a pointer to a buffer and a buffer length. This structure is the same as the structure that would be generated for a sequence<char> in the case of string, or sequence<wchar> in the case of wstring. See Sequence for details on the structure generated for each sequence. The semantics of the dataLen field are the same as those for a string when it used as the member of a structure; see Member of a structure for details. IDL declaration typedef sequence<string> seqstring; Corresponding C prototype // Note: this struct is only defined // once, at the top of each file struct _cstring_t { char* data; int dataLen; }; struct __seq_string { _cstring_t* data; int dataLen; }; typedef __seq_string seqstring;","title":"Within a sequence"},{"location":"reference/idl.html#null-and-empty-strings","text":"Strings in IDL interfaces are never NULL pointers. Strings in IDL are never absent or omitted by being NULL because they can't be. They either have a value or they are the empty string. An empty string is a valid pointer to a buffer with a single byte of value 0. (\"\" is an empty string)","title":"NULL and empty strings"},{"location":"reference/idl.html#qaic","text":"qaic , Qaic's Another Idl Compiler, is a command-line executable used to implement remote shared objects for the DSP Platform. A shared object is called remote if its methods can be invoked from outside the domain it resides in. The user of a remote object does not need to know where the object is hosted, or what language the object is implemented in. Instead, the user calls methods on a stub object generated by qaic . The stub marshals the input into a shared wire format, and ships the data off to the domain where the object is hosted. The host domain implements a skel object, also generated by qaic , that unmarshals the data, and invokes the requested method on the native object. The FastRPC framework description provides more information on the framework enabling this remote communication on Qualcomm devices. To generate stubs and skels, qaic requires the interface to an object be strictly defined. The syntax for defining an object interface is called IDL and covered in the previous sections. qaic compiles IDL files into headers, stubs, and skels. The generated header can be used to implement the native object, and for users to call methods on the object. The stub and skel are compiled into a shared object. qaic is invoked automatically when building a project. This section discusses how to run and use the compiler separately, including details of the various command-line options.","title":"QAIC"},{"location":"reference/idl.html#command-line-usage","text":"The basic command-line syntax of the tool is: qaic [options] file1.idl [file2.idl ... fileN.idl] Each file specified on the command-line will be compiled by the tool according to the options specified. The available options are: -mdll or --map-dll Generate DLL mapping -o=PATH Use path as the output path. All generated files will be output to the specified path. The default is the current directory ('.'). --cpp or -p=CPP Use CPP as the C preprocessor. The value CPP must name an executable program, and cannot contain any arguments. To pass arguments to the preprocessor, use --arg-cpp ( -pa ). --arg-cpp=ARG or -pa=ARG Pass additional argument arg to the preprocessor. To specify arguments that are themselves options, use the form -pa=ARG (for example, -pa=-E ). Specifying -pa -E will cause the -E to be interpreted as an option to qaic instead of to the preprocessor. Note that for Comment pass-through to work properly, the preprocessor must be set to not strip comments from the source. Typically the flag to do this is -C , making the appropriate argument to qaic -pa=-C . --include-path=PATH or -I=PATH Include path in the search path for included files. May be used multiple times. --indent=WIDTH or -i=WIDTH Use an indentation width of width spaces in the generated code. --warn-undefined or -Wu Issue warning for forward-declared interfaces that are never defined. --define=SYMBOL or -D=SYMBOL Predefine macro for the preprocessor. --header-only or -ho Only generate a header. Stub and skeleton code is not generated if this option is specified. --remoting-only or -ro Only generate stub and skeleton code. The corresponding header is not generated if this option is specified. --parse-only or -s Parse the IDL and perform semantic checking, but do not generate any output. Note that IDL files accepted without errors by the compiler with -s are not guaranteed to work without errors when code generation is enabled. -v Print the version of the compiler. -h Print a brief help message.","title":"Command-line usage"},{"location":"reference/idl.html#usage-examples","text":"The examples below illustrate typical usage of the IDL compiler. qaic --header-only foo.idl bar.idl The above command compiles foo.idl to the remote header file foo.h, and bar.idl to the remote header file bar.h. No remoting code is generated. qaic foo.idl The above command compiles foo.idl to a remote header file foo.h, along with the following remoting code: File Name Description foo_stub.c C stub implementation foo_skel.c C skeleton implementation foo.h Common header for stub and skel qaic -I../bar -I../far -o out foo.idl The above command compiles foo.idl . It uses ../bar and ../far as the search path for any include files. It uses out as the result directory, and generates out/foo.h , out/foo_stub.c and out/foo_skel.c files.","title":"Usage examples"},{"location":"reference/idl.html#using-other-preprocessors","text":"By default, qaic uses an internal preprocessor. It may be desirable to use a different preprocessor instead. The Microsoft C preprocessor can be used by having the compiler invoke cl /E /C , which is done with the following command-line. Note that for this to work, cl must be in the PATH . qaic -p=cl -pa=/E -pa=/C file1.idl [file2.idl ... fileN.idl] The ARM C/C++ compiler can also be used to preprocess IDL. Provided armcc is in the PATH , this can be done with the following command-line. qaic -p=armcc -pa=-E -pa=-C file1.idl [file2.idl ... fileN.idl] Note that -pa=-E must be used instead of -pa -E , since in the latter case the -E is interpreted by qaic as being an option to qaic , not to the preprocessor.","title":"Using other preprocessors"},{"location":"reference/idl.html#error-messages","text":"Any output printed by the compiler is due to either an error or a warning. Warnings include the text warning: at the beginning of the message, and do not abort code generation. Any message not preceded by warning: is an error, which causes compilation to abort. Both errors and warnings include a reference to the file, line, and position within that line (starting at 0) where the error or warning occurred. Additional details on select errors are given in the following subsections.","title":"Error messages"},{"location":"reference/idl.html#identifier-abc-clashes-with-an-introduced-type","text":"The OMG IDL specification includes complex scoping rules based not only on where types are defined, but also on where they are used. Specifically, the first component (identifier) of a qualified type name is introduced into the scope where it is used, preventing the use of any identifier with the same name in that scope. Fully-qualified names, which start with ::, are considered to have an empty first component, and thus result in no type introduction. Consider the following example, which illustrates the basic type introduction rules. For full details, see the \"Names and Scoping\" section of OMG IDL Syntax & Semantics. struct Name { string first, last; }; struct Address { string street, city, state, country; }; module M { typedef long Age; }; struct Person { Name name; // invalid IDL: 'name' clashes with 'Name' string address; // OK: 'Address' not introduced in this scope M::Age age; // OK: only 'M', not 'Age', is introduced }; In the Person structure above, the use of the Name type introduces it into the scope of Person, which prevents the member from being called name. The second member, address, is fine because the Address type is not defined within the scope of Person and has not been introduced. The reference to M::Age only causes the first component, M, to be introduced into the scope of Person, thus the age member is also without error. Clashes with introduced types can generally be resolved by changing the qualification to avoid the type introduction. For instance, if in the above example the type of the name member of the Person structure were written ::Name, no type introduction would occur, which would avoid the name clash.","title":"Identifier 'abc' clashes with an introduced type"},{"location":"reference/idl.html#comment-pass-through","text":"When qaic identifies code comments as a Doxygen comments, or ordinary comments. Doxygen comments are passed through to generated header files. When documenting interface methods, Doxygen comments are generally preferred.","title":"Comment pass-through"},{"location":"reference/idl.html#doxygen-comments","text":"When a method is documented with the Doxygen syntax, qaic will attempt to translate the documentation to the target language in any output files.","title":"Doxygen comments"},{"location":"reference/manuals.html","text":"Reference manuals Development tools The Hexagon tools provide the means to compile, simulate, debug, and analyze code. For complete documentation on each of these tools, see one of the following manuals: Manual Description Hexagon Programming Software Development Tools Hexagon software development tools overview Hexagon LLVM C/C++ Compiler User Guide Hexagon compilers clang and clang++ Hexagon Simulator User Guide Hexagon simulator hexagon-sim Hexagon LLDB Debugger User Guide Hexagon debugger hexagon-lldb Hexagon gprof Profiler User Guide Hexagon function profiler hexagon-gprof Hexagon Profiler User Guide Hexagon system profiler hexagon-profiler Hexagon Code Coverage Profiler User Guide Hexagon code coverage profiler hexagon-coverage Hexagon Resource Analyzer User Guide Memory resource analyzer hexagon-analyzer-backend Hexagon C TRACE32 User Guide Hexagon TRACE32 Hexagon Utilities User Guide Hexagon utilities The Hexagon utilities include the following tools: Assembler hexagon-llvm-mc : Translate assembly source files into object files Linker hexagon-link : Combine object files into executable Archiver hexagon-ar : Create, modify, and extract files from archives Object file symbol lister hexagon-nm : List symbols from object files Object file copier hexagon-elfcopy : Copy and translate object files Object file viewer hexagon-llvm-objdump : Display contents of object files Archive indexer hexagon-ranlib : Generate index to archive contents Object file section size lister hexagon-size : List section sizes and total size Object file string lister hexagon-strings : List strings from object files Object file stripper hexagon-strip : Remove symbols from object file C++ filter hexagon-c++filt : Filter used to demangle encoded C++ symbols Address converter hexagon-addr2line : Convert addresses to file and line ELF file viewer hexagon-readelf : Display contents of ELF format files Programming languages Assembly and intrinsics For the best performance, write time-critical functions using direct DSP instructions. Do this by either writing assembly code or using intrinsics within C/C++ code. The DSP optimization guidelines page provides tips on how to write efficient code with intrinsics or assembly language. For complete documentation on assembly instructions and intrinsics, see one of the programming reference manuals: Hexagon V65 Programmer's Reference Manual User Guide Hexagon V65 HVX Programmer's Reference Manual User Guide Hexagon V66 Programmer's Reference Manual User Guide Hexagon V66 HVX Programmer's Reference Manual User Guide Hexagon V68 Programmer's Reference Manual User Guide Hexagon V68 HVX Programmer's Reference Manual User Guide C/C++ The DSP code is usually written in C or C++. Such code does not require any change to run on the DSP: Qualcomm\u2019s compiler relies on LLVM and currently supports the following versions of the C and C++ standards: C language: K&R C, ANSI C89, ISO C90, ISO C94 (C89+AMD1), ISO C99 (+TC1, TC2, TC3), ISO C11 C++ language: C++98, and with some minor limitations, C++11, C++14, and C++17 For complete documentation on C/C++ support, see the Hexagon LLVM C/C++ Compiler . The C/C++ compiler is HVX-aware when using the -fvectorize option. For more vectorization options, consider using Halide, writing code with Intrinsics, or using assembly. Halide Halide is a dialect of C++ that allows you to exploit the Hexagon Vector eXtensions (HVX) of the Qualcomm DSP from C. For more information on Halide, see the Halide for HVX user guide . IDL The Interface Description Language (IDL) defines the interface that the CPU and DSP use to communicate in C/C++ using RPC . For complete documentation on IDL, see the IDL reference documentation . DSP software libraries Several DSP software libraries provide optimized implementations of a number of DSP routines. For complete information on these libraries, see the following documents: Manual Description Qualcomm Hexagon Libraries (QHL) Math, BLAS, and DSP libraries optimized for Hexagon (scalar and HVX) Hexagon C Library User Guide Common C libraries Hexagon C++ Library Supplement A guide for comparing and selecting libstdc++ and libc++ libraries Hexagon QPRINTF Library A library extending printf support to HVX registers and assembly Others Following are other reference contents available within the Hexagon SDK: Manual Description SM8250 cDSP architecture overview A set of slides providing an architecture overview of the SM8250 cDSP System Software APIs APIs for system-level libraries managing DSP resources Qurt User Guide Qualcomm real-time operating system Hexagon Application Binary Interface (ABI) User Guide Application Binary Interface (ABI)","title":"Reference manuals"},{"location":"reference/manuals.html#reference-manuals","text":"","title":"Reference manuals"},{"location":"reference/manuals.html#development-tools","text":"The Hexagon tools provide the means to compile, simulate, debug, and analyze code. For complete documentation on each of these tools, see one of the following manuals: Manual Description Hexagon Programming Software Development Tools Hexagon software development tools overview Hexagon LLVM C/C++ Compiler User Guide Hexagon compilers clang and clang++ Hexagon Simulator User Guide Hexagon simulator hexagon-sim Hexagon LLDB Debugger User Guide Hexagon debugger hexagon-lldb Hexagon gprof Profiler User Guide Hexagon function profiler hexagon-gprof Hexagon Profiler User Guide Hexagon system profiler hexagon-profiler Hexagon Code Coverage Profiler User Guide Hexagon code coverage profiler hexagon-coverage Hexagon Resource Analyzer User Guide Memory resource analyzer hexagon-analyzer-backend Hexagon C TRACE32 User Guide Hexagon TRACE32 Hexagon Utilities User Guide Hexagon utilities The Hexagon utilities include the following tools: Assembler hexagon-llvm-mc : Translate assembly source files into object files Linker hexagon-link : Combine object files into executable Archiver hexagon-ar : Create, modify, and extract files from archives Object file symbol lister hexagon-nm : List symbols from object files Object file copier hexagon-elfcopy : Copy and translate object files Object file viewer hexagon-llvm-objdump : Display contents of object files Archive indexer hexagon-ranlib : Generate index to archive contents Object file section size lister hexagon-size : List section sizes and total size Object file string lister hexagon-strings : List strings from object files Object file stripper hexagon-strip : Remove symbols from object file C++ filter hexagon-c++filt : Filter used to demangle encoded C++ symbols Address converter hexagon-addr2line : Convert addresses to file and line ELF file viewer hexagon-readelf : Display contents of ELF format files","title":"Development tools"},{"location":"reference/manuals.html#programming-languages","text":"","title":"Programming languages"},{"location":"reference/manuals.html#assembly-and-intrinsics","text":"For the best performance, write time-critical functions using direct DSP instructions. Do this by either writing assembly code or using intrinsics within C/C++ code. The DSP optimization guidelines page provides tips on how to write efficient code with intrinsics or assembly language. For complete documentation on assembly instructions and intrinsics, see one of the programming reference manuals: Hexagon V65 Programmer's Reference Manual User Guide Hexagon V65 HVX Programmer's Reference Manual User Guide Hexagon V66 Programmer's Reference Manual User Guide Hexagon V66 HVX Programmer's Reference Manual User Guide Hexagon V68 Programmer's Reference Manual User Guide Hexagon V68 HVX Programmer's Reference Manual User Guide","title":"Assembly and intrinsics"},{"location":"reference/manuals.html#cc","text":"The DSP code is usually written in C or C++. Such code does not require any change to run on the DSP: Qualcomm\u2019s compiler relies on LLVM and currently supports the following versions of the C and C++ standards: C language: K&R C, ANSI C89, ISO C90, ISO C94 (C89+AMD1), ISO C99 (+TC1, TC2, TC3), ISO C11 C++ language: C++98, and with some minor limitations, C++11, C++14, and C++17 For complete documentation on C/C++ support, see the Hexagon LLVM C/C++ Compiler . The C/C++ compiler is HVX-aware when using the -fvectorize option. For more vectorization options, consider using Halide, writing code with Intrinsics, or using assembly.","title":"C/C++"},{"location":"reference/manuals.html#halide","text":"Halide is a dialect of C++ that allows you to exploit the Hexagon Vector eXtensions (HVX) of the Qualcomm DSP from C. For more information on Halide, see the Halide for HVX user guide .","title":"Halide"},{"location":"reference/manuals.html#idl","text":"The Interface Description Language (IDL) defines the interface that the CPU and DSP use to communicate in C/C++ using RPC . For complete documentation on IDL, see the IDL reference documentation .","title":"IDL"},{"location":"reference/manuals.html#dsp-software-libraries","text":"Several DSP software libraries provide optimized implementations of a number of DSP routines. For complete information on these libraries, see the following documents: Manual Description Qualcomm Hexagon Libraries (QHL) Math, BLAS, and DSP libraries optimized for Hexagon (scalar and HVX) Hexagon C Library User Guide Common C libraries Hexagon C++ Library Supplement A guide for comparing and selecting libstdc++ and libc++ libraries Hexagon QPRINTF Library A library extending printf support to HVX registers and assembly","title":"DSP software libraries"},{"location":"reference/manuals.html#others","text":"Following are other reference contents available within the Hexagon SDK: Manual Description SM8250 cDSP architecture overview A set of slides providing an architecture overview of the SM8250 cDSP System Software APIs APIs for system-level libraries managing DSP resources Qurt User Guide Qualcomm real-time operating system Hexagon Application Binary Interface (ABI) User Guide Application Binary Interface (ABI)","title":"Others"},{"location":"reference/releases.html","text":"Release notes 4.4.0 Tools Incorporates Hexagon tools release 8.4.12 Incorporates Halide tools release 2.4 Incorporates QuRT release VU_CORE_KERNEL_QURT04.01.16.07.02 Resolves compilation error with QAIC compiler when /// is used at the end of an interface in IDL file Fixes KW errors and known MISRA issues in QAIC compiler Enables setup of multiple targets for Hexagon-Trace-Analyzer Combines enable_trace.sh and one_time_setup.sh scripts into target_profile_setup.py for HexTA setup Automates generation of HexTA configuration files in run_profilers.py script Introduces Async mode support for target debugging Introduces remote debugger support on Strait Fixes the QHCG tool output Q-format issue in HVX code generation for exp2 function Introduces HVX instruction visualization tool to utils Improvements made to the installer's reliablility Hexagon IDE Simplifies the IDE GUI for new project creation Introduces CMake-based build support for new project creation Adds import project support for CMake- and make.d-based projects Target support Introduces support for QCS605 , ENEL and Strait targets Libraries Introduces static compilation in Worker pool library Updates to QHL HVX library: Adds BLAS operations for Cholesky decomposition and Matrix transpose for QF32 Adds math operations for square root and inverse for QF32 Adds benchmarks for modexp and clipping functions Updates IPC header files remote_default.h and AEEStdErr.h Removes FastCV libraries and recommends to use dynamic loading of FastCV library available on the device Examples Adds CMake-based build support for all examples and libraries with the exception of qfe_library Runs compute examples by default on unsigned PD Adds multi-domains support for QoS in profiling example Updates to HAP example: Replaces HAP power DCVS v2 with DCVS v3 APIs Adds usage of HAP_compute_res_attr_set_release_callback API Introduces support for non-NULL context Fixes calculator_c++ example and adds usage of each function in the example walkthrough Fixes known unsigned PD issue in asyncdspq_example Updates CMakeLists.txt of calculator_c++_apk example Enhances the \u201cgtest\u201d example to use \u201cgoogletest\u201d framework APIs from Hexagon too Modifies asyncdspq_example , corner_detect and camera_chi to use FastCV APIs by dynamic loading of library on the device Renames output binary names built with CMake to make them same as that of Make.d Build system Enhances CMake to link pre-built libraries if available Enhances CMake build system to support APK building from Android Studio Introduces ship folder with CMake to save the built binaries Documentation Documents Async FastRPC APIs Updates remote header file and documentation Updates the benchmark example documentation on the recommended approach to run C++ examples on DSP Updates the corner_detect example documentation to demonstrate the recommended approach to use FastCV library APIs Updates QHL and QHL_HVX documentation describing the naming convention used for APIs Other Moves Hexagon debugger utils from tools to utils/debugger Moves debug scripts to utils/debugger 4.3.0 Tools Incorporates Hexagon tools release 8.4.11 Incorporates Halide tools release 2.3.03 Incorporates QuRT release 04.01.09 Fixes the boundary error in QHCG tool and error reports generated by tool Introduces new log filtering options to mini-dm Introduces remote debugger support for Kodiak target Fixes stability issue with assembly debugging Enables the support for debugging PD dumps with FRAMEKEY-enabled builds Resolves issues related to target variable command in lldb Resolves the issue of not hitting breakpoint for callback functions Adds -o image search-path add . when debugging examples on simulator Adds googletest framework library for robust unit testing Target support Introduces support for Kodiak target Libraries Enhances run_main_on_hexagon to accept executable names other than \"run_main_on_hexagon_sim\" Refactors remote.h to remote_default.h Introduces getopt_custom library for custom implementation of getopt and getopt_long Adds support for custom stack size of worker threads for worker pool library QHL Adds IEEE f32 <-> IEEE f16 conversion in HVX instructions Updates the qhmath_hvx accuracy and performance reports with qhcg tool fixes Adds support for Vector -> Vector operations for the remaining QHL_HVX math and trigonometric functions: \"sqrt\", \"exp\", \"log\", \"pow\", \"fast_exp\", \"fast_log\", \"fast_inverse_sqrt\", \"floor\", \"ceil\", \"round\", \"trunc\" Adds \u201cqhblas_hvx_matrix_matrix_mpy_aqf32\u201d, \u201cqhblas_hvx_matrix_vector_mpy_aqf32\u201d, \u201cqhblas_hvx_qf32_vector_dot_aqf32\u201d, \u201cqhblas_hvx_vector_add_aqf32\u201d, \u201cqhblas_hvx_vector_scaling_aqf32\u201d library functions and their test cases Examples Runs examples by default on the Unsigned PD Adds additional -U flag for supporting running on Unsigned PD or Signed PD Creates new file rpcperf.h in profiling example Adds support of older tools for runprofilers.py script Fixes SSR/PDR handling code in calculator example Enhances HAP_example to demonstrate the usage of \"dmahandle\u201d and \u201cfastrpc mmap\u201d Enhances profiling example to demonstrate the usage of HAP_send_early_signal API Adds gtest example to demonstrate the usage of \u201cgoogletest\u201d framework Build system Modularizes CMake build system for better user experience Introduces CMake support for all base SDK and compute addon examples Removes redundancies from example .min files with the introduction of cpu.min Modifies build/cmake/cmake_configure.bash to use os-release instead of lsb_release fixes bug of running simulator multiple times with make hexagonsim Documentation Updates the cmake documentation as per the new changes and additions Adds the QHL library performance details for Lahaina Adds QAIC user guide in the IDL documentation Adds include directives and code generation information in the IDL documentation Limitations Unsigned PD for asyncdspq example is not supported for Kona, Saipan, Bitra, QRB5165 and SXR2130 Other Introduces debug_exceptions.py for debugging PD exceptions Introduces stubbed implementation of HAP_setFARFRuntimeLoggingParams for simulator Updates profiling numbers for Lahaina in documentation Adds cmake support to use any custom toolchain for cross compilation Includes utility source files for memory and timing related APIs abstracting the OS-specific information from example source files Enhances setup_sdk_env script to throw error if python, make and QAIC are not installed 4.2.0 Tools Incorporates Hexagon tools release 8.4.09 Incorporates Halide tools release 2.3.01 Fixes bug with adding line breakpoint in Hexagon IDE Adds IDE support to set breakpoints automatically on all the IDL functions of project being debugged Adds versioning support to QAIC compiler Removes unnecessary GUI options in IDE Fixes bug with writing to a memory block in memory view of IDE Removes SysmonParser (deprecated) and updates the HTML parser (sysmon_parser) with STID and Marker profiling support. Enhances mini-dm to connect to device using usb bus number Target support Introduces support for Cedros, Mannar, QRB5165 (LE) and SXR2130 targets Libraries Removes the dspCV library, use worker pool library and HAP_power_APIs instead Adds QFE (Qfloat emulation Library) for emulating qf16/32 formats on x86 using libnative Extends qhl_hvx acos, atan and tanh input range by linear interpolation beyond the existing input range Extends qhl_hvx sinh and cosh input range by exponential approximation for intervals beyond existing input range Examples Introduces UserDMA example in the compute addon Adds DSP Packet Queue support in benchmark example Adds simulator support for hap_example Adds inlane_gather operation to benchmark example to compare HVX gather performance across targets Adds self-minimum operation to benchmark example to get the minimum value and the index of its first occurrence over a 1D source signal Modifies benchmark gather kernel VTCM size requirement Build system Adds support for custom build output folder using BUILD_OUTPUT_DIR option Includes Qurt header files and libraries by default Any new example created using 4.2.0 SDK will be a QuRT-based example by default unless the user sets NO_QURT_INC to 1 in the hexagon.min file This change breaks the backward compatibility of audio addon 1.0.x with 4.2.0 SDK, so please use audio addon with version 1.1.x or later when using 4.2.0 SDK Other Adds new API (HAP_mem_available_stack()) to get the stack size (in bytes) available for current thread Fixes simulator crash with compute_resource_acquire APIs on v65 Updates documentation contents 4.1.0 Tools Incorporates Hexagon Tools release 8.4.05 Incorporates HALIDE Tools release 2.3 Updates mini-dm allowing all level logs from sensors subsystems Introduces PD Dumps for debugging dynamic PDs on the DSP Target support Introduces support for Bitra and Agatti targets Libraries Incorporates QuRT release 04.00.18.07.01 Introduces Python version 3.7.8 Adds Asynchronous DSP Packet Queue (dspqueue) support to the Image DSPQ library Updates Worker pool library to support Static worker pool Removes the QFXP library Adds V65 architecture support to QHL library Updates to QHL HVX library: Fixes overflow issues in floating point power and exponent functions Adds conversion routines for {qf32, qf16} to {int8, uint8, int16, uint16, int32, uint32, qf16, qf32, float, __fp16} Adds 1-D block FIR filter function for qf32, int16 and int32 data formats Renames the library functions to follow the naming convention below: Changes v to a for all QHL HVX functions processing arrays Functions operating on HVX vectors will have v in the name Functions operating on scalar data will not have any qualifier: v or `a\u2019 Build system Adds build support for Hexagon Tools version 8.2.x Note: QuRT-based simulator tests are not supported with 8.2.x tools Cleans the CMake helper function runHexagonSim to support run_main_on_hexagon and standard simulator examples Adds new make.d commands support for dependent libraries in the base SDK and compute add-on Adds support to load custom cosims onto the simulator Removes wrapping of _stack_chk_fail symbol in the build system to avoid undefined PLT symbol _wrap__stack_chk_fail error on target during dynamic loading Fixes parallel build issues with -jx option Examples Updates calculator_c++_apk allowing unsigned offload to the cDSP Updates capi_v2_gain_cpp as a CPP-based on-target audio example Updates corner_detect and image_dspq to use FastRPC domains Updates benchmark : Adds demonstration of the Compute resource manager API Adds VTCM inlane gather, matrix transpose and vector float 1-D block FIR filter functions Updates profiling : Adds support for sleep latency Enables option to run the example on ADSP Adds runtime test for HVX support on devices Uses run_main_on_hexagon to load and call the shared objects Introduces dspqueue , a new example for Asynchronous DSP Packet Queue API Adds new APIs for persistent FastRPC memory mapping Documentation Merges qprintf documentation into Hexagon SDK HTML documentation Adds HAP_ps documentation Fixes broken links in documentation Documents approach for capturing the load address of a shared object when running on simulator Adds optimization guidelines for making efficient HVX memory accesses Documents new example for Asynchronous DSP Packet Queue (\"dspqueue\") API Adds DCVS duty cycling policy under DCVS_v2/v3 DCVS policies Other Cleans up setup_sdk_env.source script to not throw a warning when 32-bit compatibility libraries are not installed on Ubuntu Updates Compute resource manager API: Register release callback, cached mode for VTCM allocation, application identifier for VTCM partition selection, minimum VTCM size selection Adds HMX as a resource. HMX lock and unlock API Adds VTCM query API with page layout information Adds simulator support Updates UI, command line sysMon profiler and HTML based parser with Lahaina support 4.0.0 Tools Incorporates Hexagon Tools release 8.4.04 Incorporates HALIDE Tools release 2.2.06 Incorporates mini-dm version 3.2 Deprecates support of Hexagon Tools older than 8.3.x Introduces QHCG , a tool generating optimized HVX code for polynomial approximations of arbitrary function Renames testsig.py to signer.py enhancing functionality to sign shared objects and the device Updates run_main_on_hexagon to handle the registered TLS destructors and Exit Handlers in the user module Target support Introduces support for Lahaina, Kamorta and QRB5165 targets Libraries Incorporates QuRT release 04.00.09.07 Removes support for machine learning and Hexagon NN Refer to the QNN SDK for machine learning support Introduces a new worker pool library and modifies all SDK examples using dspCV library to use the new worker pool library Allows Hexagon SDK clients to directly use rpcmem APIs from FastRPC libraries available on target Adds unit tests and scripts to test, profile all the functions of the QHL and QHL HVX Build system Introduces simplified make commands Enhances CMake build support to enable target-based compilation and simplified usage Ships prebuilt binaries along with source for SDK libraries to allow the same SDK to be used by multiple users Ships prebuilt binaries only for one variant ( ReleaseG or Release ) depending on the library Makes dynamic_ optional in the build command string and removes it from the generated folder names All SDK examples are compiled for the dynamic variant by default Examples Cleans up and introduces new SDK examples Adds profiling example illustrating all techniques available to profile code Consolidates qurt_multithread , qurt_mutexes and qurt_thread_t1 into one example multithreading Adds lpi_example to demonstrate how to build a shared object for LPI and non-LPI mode Adds hap_example to show the usage of system libraries Adds qhl_hvx example to showcase the usage of QHL HVX , which includes Math, DSP and Image processing libraries optimized for HVX. Adds camera_chi example in the compute add-on. This example hooks up any DSP post processing algorithm into camera pipeline Removes benchmark example and renames benchmark_v65 as benchmark in the compute add-on Consolidates calculator_c++ and calculator_c++14 into calculator_c++ Consolidates calculator and calculator_multi_domains into calculator Uses FastRPC domains for all base SDK examples Renames cornerApp to corner_detect in the compute add-on Consolidates histogram into benchmark example in the compute add-on Renames calculator_c++_app as calculator_c++_apk and makes it compatible with Android R Removes the examples below (most of which are included in the compute add-on benchmark example) conv3x3a16_v60 conv3x3a32_v60 dilate3x3_v60 dilate5x5_v60 downscaleBy2 epsilon_v60 fast9 gaussian7x7 sigma3x3_v60 sobel3x3_v60 rpcperf calculator_multi_legacy Other First release using Qualcomm Package Manager(QPM) Introduces add-on support Uses QPM's add-on feature to install user-selectable components (for example Audio add-on, Minimal Android-NDK, Full Android-NDK, Python, CMake) Includes compute add-on by default Introduces support for V68 Removes support for older architectures v55, v60 and v62 Updates SDK directory structure to reflect base SDK and add-on model Moves Audio content to $(HEXAGON_SDK_ROOT)/addons/audio Moves Compute content to $(HEXAGON_SDK_ROOT)/addons/compute Moves QuRT libraries/headers to $(HEXAGON_SDK_ROOT)/rtos Moves FastRPC-related content to $(HEXAGON_SDK_ROOT)/ipc/fastrpc Fixes multiple bugs Re-architects the Hexagon SDK documentation","title":"About"},{"location":"reference/releases.html#release-notes","text":"","title":"Release notes"},{"location":"reference/releases.html#440","text":"","title":"4.4.0"},{"location":"reference/releases.html#tools","text":"Incorporates Hexagon tools release 8.4.12 Incorporates Halide tools release 2.4 Incorporates QuRT release VU_CORE_KERNEL_QURT04.01.16.07.02 Resolves compilation error with QAIC compiler when /// is used at the end of an interface in IDL file Fixes KW errors and known MISRA issues in QAIC compiler Enables setup of multiple targets for Hexagon-Trace-Analyzer Combines enable_trace.sh and one_time_setup.sh scripts into target_profile_setup.py for HexTA setup Automates generation of HexTA configuration files in run_profilers.py script Introduces Async mode support for target debugging Introduces remote debugger support on Strait Fixes the QHCG tool output Q-format issue in HVX code generation for exp2 function Introduces HVX instruction visualization tool to utils Improvements made to the installer's reliablility","title":"Tools"},{"location":"reference/releases.html#hexagon-ide","text":"Simplifies the IDE GUI for new project creation Introduces CMake-based build support for new project creation Adds import project support for CMake- and make.d-based projects","title":"Hexagon IDE"},{"location":"reference/releases.html#target-support","text":"Introduces support for QCS605 , ENEL and Strait targets","title":"Target support"},{"location":"reference/releases.html#libraries","text":"Introduces static compilation in Worker pool library Updates to QHL HVX library: Adds BLAS operations for Cholesky decomposition and Matrix transpose for QF32 Adds math operations for square root and inverse for QF32 Adds benchmarks for modexp and clipping functions Updates IPC header files remote_default.h and AEEStdErr.h Removes FastCV libraries and recommends to use dynamic loading of FastCV library available on the device","title":"Libraries"},{"location":"reference/releases.html#examples","text":"Adds CMake-based build support for all examples and libraries with the exception of qfe_library Runs compute examples by default on unsigned PD Adds multi-domains support for QoS in profiling example Updates to HAP example: Replaces HAP power DCVS v2 with DCVS v3 APIs Adds usage of HAP_compute_res_attr_set_release_callback API Introduces support for non-NULL context Fixes calculator_c++ example and adds usage of each function in the example walkthrough Fixes known unsigned PD issue in asyncdspq_example Updates CMakeLists.txt of calculator_c++_apk example Enhances the \u201cgtest\u201d example to use \u201cgoogletest\u201d framework APIs from Hexagon too Modifies asyncdspq_example , corner_detect and camera_chi to use FastCV APIs by dynamic loading of library on the device Renames output binary names built with CMake to make them same as that of Make.d","title":"Examples"},{"location":"reference/releases.html#build-system","text":"Enhances CMake to link pre-built libraries if available Enhances CMake build system to support APK building from Android Studio Introduces ship folder with CMake to save the built binaries","title":"Build system"},{"location":"reference/releases.html#documentation","text":"Documents Async FastRPC APIs Updates remote header file and documentation Updates the benchmark example documentation on the recommended approach to run C++ examples on DSP Updates the corner_detect example documentation to demonstrate the recommended approach to use FastCV library APIs Updates QHL and QHL_HVX documentation describing the naming convention used for APIs","title":"Documentation"},{"location":"reference/releases.html#other","text":"Moves Hexagon debugger utils from tools to utils/debugger Moves debug scripts to utils/debugger","title":"Other"},{"location":"reference/releases.html#430","text":"","title":"4.3.0"},{"location":"reference/releases.html#tools_1","text":"Incorporates Hexagon tools release 8.4.11 Incorporates Halide tools release 2.3.03 Incorporates QuRT release 04.01.09 Fixes the boundary error in QHCG tool and error reports generated by tool Introduces new log filtering options to mini-dm Introduces remote debugger support for Kodiak target Fixes stability issue with assembly debugging Enables the support for debugging PD dumps with FRAMEKEY-enabled builds Resolves issues related to target variable command in lldb Resolves the issue of not hitting breakpoint for callback functions Adds -o image search-path add . when debugging examples on simulator Adds googletest framework library for robust unit testing","title":"Tools"},{"location":"reference/releases.html#target-support_1","text":"Introduces support for Kodiak target","title":"Target support"},{"location":"reference/releases.html#libraries_1","text":"Enhances run_main_on_hexagon to accept executable names other than \"run_main_on_hexagon_sim\" Refactors remote.h to remote_default.h Introduces getopt_custom library for custom implementation of getopt and getopt_long Adds support for custom stack size of worker threads for worker pool library QHL Adds IEEE f32 <-> IEEE f16 conversion in HVX instructions Updates the qhmath_hvx accuracy and performance reports with qhcg tool fixes Adds support for Vector -> Vector operations for the remaining QHL_HVX math and trigonometric functions: \"sqrt\", \"exp\", \"log\", \"pow\", \"fast_exp\", \"fast_log\", \"fast_inverse_sqrt\", \"floor\", \"ceil\", \"round\", \"trunc\" Adds \u201cqhblas_hvx_matrix_matrix_mpy_aqf32\u201d, \u201cqhblas_hvx_matrix_vector_mpy_aqf32\u201d, \u201cqhblas_hvx_qf32_vector_dot_aqf32\u201d, \u201cqhblas_hvx_vector_add_aqf32\u201d, \u201cqhblas_hvx_vector_scaling_aqf32\u201d library functions and their test cases","title":"Libraries"},{"location":"reference/releases.html#examples_1","text":"Runs examples by default on the Unsigned PD Adds additional -U flag for supporting running on Unsigned PD or Signed PD Creates new file rpcperf.h in profiling example Adds support of older tools for runprofilers.py script Fixes SSR/PDR handling code in calculator example Enhances HAP_example to demonstrate the usage of \"dmahandle\u201d and \u201cfastrpc mmap\u201d Enhances profiling example to demonstrate the usage of HAP_send_early_signal API Adds gtest example to demonstrate the usage of \u201cgoogletest\u201d framework","title":"Examples"},{"location":"reference/releases.html#build-system_1","text":"Modularizes CMake build system for better user experience Introduces CMake support for all base SDK and compute addon examples Removes redundancies from example .min files with the introduction of cpu.min Modifies build/cmake/cmake_configure.bash to use os-release instead of lsb_release fixes bug of running simulator multiple times with make hexagonsim","title":"Build system"},{"location":"reference/releases.html#documentation_1","text":"Updates the cmake documentation as per the new changes and additions Adds the QHL library performance details for Lahaina Adds QAIC user guide in the IDL documentation Adds include directives and code generation information in the IDL documentation","title":"Documentation"},{"location":"reference/releases.html#limitations","text":"Unsigned PD for asyncdspq example is not supported for Kona, Saipan, Bitra, QRB5165 and SXR2130","title":"Limitations"},{"location":"reference/releases.html#other_1","text":"Introduces debug_exceptions.py for debugging PD exceptions Introduces stubbed implementation of HAP_setFARFRuntimeLoggingParams for simulator Updates profiling numbers for Lahaina in documentation Adds cmake support to use any custom toolchain for cross compilation Includes utility source files for memory and timing related APIs abstracting the OS-specific information from example source files Enhances setup_sdk_env script to throw error if python, make and QAIC are not installed","title":"Other"},{"location":"reference/releases.html#420","text":"","title":"4.2.0"},{"location":"reference/releases.html#tools_2","text":"Incorporates Hexagon tools release 8.4.09 Incorporates Halide tools release 2.3.01 Fixes bug with adding line breakpoint in Hexagon IDE Adds IDE support to set breakpoints automatically on all the IDL functions of project being debugged Adds versioning support to QAIC compiler Removes unnecessary GUI options in IDE Fixes bug with writing to a memory block in memory view of IDE Removes SysmonParser (deprecated) and updates the HTML parser (sysmon_parser) with STID and Marker profiling support. Enhances mini-dm to connect to device using usb bus number","title":"Tools"},{"location":"reference/releases.html#target-support_2","text":"Introduces support for Cedros, Mannar, QRB5165 (LE) and SXR2130 targets","title":"Target support"},{"location":"reference/releases.html#libraries_2","text":"Removes the dspCV library, use worker pool library and HAP_power_APIs instead Adds QFE (Qfloat emulation Library) for emulating qf16/32 formats on x86 using libnative Extends qhl_hvx acos, atan and tanh input range by linear interpolation beyond the existing input range Extends qhl_hvx sinh and cosh input range by exponential approximation for intervals beyond existing input range","title":"Libraries"},{"location":"reference/releases.html#examples_2","text":"Introduces UserDMA example in the compute addon Adds DSP Packet Queue support in benchmark example Adds simulator support for hap_example Adds inlane_gather operation to benchmark example to compare HVX gather performance across targets Adds self-minimum operation to benchmark example to get the minimum value and the index of its first occurrence over a 1D source signal Modifies benchmark gather kernel VTCM size requirement","title":"Examples"},{"location":"reference/releases.html#build-system_2","text":"Adds support for custom build output folder using BUILD_OUTPUT_DIR option Includes Qurt header files and libraries by default Any new example created using 4.2.0 SDK will be a QuRT-based example by default unless the user sets NO_QURT_INC to 1 in the hexagon.min file This change breaks the backward compatibility of audio addon 1.0.x with 4.2.0 SDK, so please use audio addon with version 1.1.x or later when using 4.2.0 SDK","title":"Build system"},{"location":"reference/releases.html#other_2","text":"Adds new API (HAP_mem_available_stack()) to get the stack size (in bytes) available for current thread Fixes simulator crash with compute_resource_acquire APIs on v65 Updates documentation contents","title":"Other"},{"location":"reference/releases.html#410","text":"","title":"4.1.0"},{"location":"reference/releases.html#tools_3","text":"Incorporates Hexagon Tools release 8.4.05 Incorporates HALIDE Tools release 2.3 Updates mini-dm allowing all level logs from sensors subsystems Introduces PD Dumps for debugging dynamic PDs on the DSP","title":"Tools"},{"location":"reference/releases.html#target-support_3","text":"Introduces support for Bitra and Agatti targets","title":"Target support"},{"location":"reference/releases.html#libraries_3","text":"Incorporates QuRT release 04.00.18.07.01 Introduces Python version 3.7.8 Adds Asynchronous DSP Packet Queue (dspqueue) support to the Image DSPQ library Updates Worker pool library to support Static worker pool Removes the QFXP library Adds V65 architecture support to QHL library Updates to QHL HVX library: Fixes overflow issues in floating point power and exponent functions Adds conversion routines for {qf32, qf16} to {int8, uint8, int16, uint16, int32, uint32, qf16, qf32, float, __fp16} Adds 1-D block FIR filter function for qf32, int16 and int32 data formats Renames the library functions to follow the naming convention below: Changes v to a for all QHL HVX functions processing arrays Functions operating on HVX vectors will have v in the name Functions operating on scalar data will not have any qualifier: v or `a\u2019","title":"Libraries"},{"location":"reference/releases.html#build-system_3","text":"Adds build support for Hexagon Tools version 8.2.x Note: QuRT-based simulator tests are not supported with 8.2.x tools Cleans the CMake helper function runHexagonSim to support run_main_on_hexagon and standard simulator examples Adds new make.d commands support for dependent libraries in the base SDK and compute add-on Adds support to load custom cosims onto the simulator Removes wrapping of _stack_chk_fail symbol in the build system to avoid undefined PLT symbol _wrap__stack_chk_fail error on target during dynamic loading Fixes parallel build issues with -jx option","title":"Build system"},{"location":"reference/releases.html#examples_3","text":"Updates calculator_c++_apk allowing unsigned offload to the cDSP Updates capi_v2_gain_cpp as a CPP-based on-target audio example Updates corner_detect and image_dspq to use FastRPC domains Updates benchmark : Adds demonstration of the Compute resource manager API Adds VTCM inlane gather, matrix transpose and vector float 1-D block FIR filter functions Updates profiling : Adds support for sleep latency Enables option to run the example on ADSP Adds runtime test for HVX support on devices Uses run_main_on_hexagon to load and call the shared objects Introduces dspqueue , a new example for Asynchronous DSP Packet Queue API Adds new APIs for persistent FastRPC memory mapping","title":"Examples"},{"location":"reference/releases.html#documentation_2","text":"Merges qprintf documentation into Hexagon SDK HTML documentation Adds HAP_ps documentation Fixes broken links in documentation Documents approach for capturing the load address of a shared object when running on simulator Adds optimization guidelines for making efficient HVX memory accesses Documents new example for Asynchronous DSP Packet Queue (\"dspqueue\") API Adds DCVS duty cycling policy under DCVS_v2/v3 DCVS policies","title":"Documentation"},{"location":"reference/releases.html#other_3","text":"Cleans up setup_sdk_env.source script to not throw a warning when 32-bit compatibility libraries are not installed on Ubuntu Updates Compute resource manager API: Register release callback, cached mode for VTCM allocation, application identifier for VTCM partition selection, minimum VTCM size selection Adds HMX as a resource. HMX lock and unlock API Adds VTCM query API with page layout information Adds simulator support Updates UI, command line sysMon profiler and HTML based parser with Lahaina support","title":"Other"},{"location":"reference/releases.html#400","text":"","title":"4.0.0"},{"location":"reference/releases.html#tools_4","text":"Incorporates Hexagon Tools release 8.4.04 Incorporates HALIDE Tools release 2.2.06 Incorporates mini-dm version 3.2 Deprecates support of Hexagon Tools older than 8.3.x Introduces QHCG , a tool generating optimized HVX code for polynomial approximations of arbitrary function Renames testsig.py to signer.py enhancing functionality to sign shared objects and the device Updates run_main_on_hexagon to handle the registered TLS destructors and Exit Handlers in the user module","title":"Tools"},{"location":"reference/releases.html#target-support_4","text":"Introduces support for Lahaina, Kamorta and QRB5165 targets","title":"Target support"},{"location":"reference/releases.html#libraries_4","text":"Incorporates QuRT release 04.00.09.07 Removes support for machine learning and Hexagon NN Refer to the QNN SDK for machine learning support Introduces a new worker pool library and modifies all SDK examples using dspCV library to use the new worker pool library Allows Hexagon SDK clients to directly use rpcmem APIs from FastRPC libraries available on target Adds unit tests and scripts to test, profile all the functions of the QHL and QHL HVX","title":"Libraries"},{"location":"reference/releases.html#build-system_4","text":"Introduces simplified make commands Enhances CMake build support to enable target-based compilation and simplified usage Ships prebuilt binaries along with source for SDK libraries to allow the same SDK to be used by multiple users Ships prebuilt binaries only for one variant ( ReleaseG or Release ) depending on the library Makes dynamic_ optional in the build command string and removes it from the generated folder names All SDK examples are compiled for the dynamic variant by default","title":"Build system"},{"location":"reference/releases.html#examples_4","text":"Cleans up and introduces new SDK examples Adds profiling example illustrating all techniques available to profile code Consolidates qurt_multithread , qurt_mutexes and qurt_thread_t1 into one example multithreading Adds lpi_example to demonstrate how to build a shared object for LPI and non-LPI mode Adds hap_example to show the usage of system libraries Adds qhl_hvx example to showcase the usage of QHL HVX , which includes Math, DSP and Image processing libraries optimized for HVX. Adds camera_chi example in the compute add-on. This example hooks up any DSP post processing algorithm into camera pipeline Removes benchmark example and renames benchmark_v65 as benchmark in the compute add-on Consolidates calculator_c++ and calculator_c++14 into calculator_c++ Consolidates calculator and calculator_multi_domains into calculator Uses FastRPC domains for all base SDK examples Renames cornerApp to corner_detect in the compute add-on Consolidates histogram into benchmark example in the compute add-on Renames calculator_c++_app as calculator_c++_apk and makes it compatible with Android R Removes the examples below (most of which are included in the compute add-on benchmark example) conv3x3a16_v60 conv3x3a32_v60 dilate3x3_v60 dilate5x5_v60 downscaleBy2 epsilon_v60 fast9 gaussian7x7 sigma3x3_v60 sobel3x3_v60 rpcperf calculator_multi_legacy","title":"Examples"},{"location":"reference/releases.html#other_4","text":"First release using Qualcomm Package Manager(QPM) Introduces add-on support Uses QPM's add-on feature to install user-selectable components (for example Audio add-on, Minimal Android-NDK, Full Android-NDK, Python, CMake) Includes compute add-on by default Introduces support for V68 Removes support for older architectures v55, v60 and v62 Updates SDK directory structure to reflect base SDK and add-on model Moves Audio content to $(HEXAGON_SDK_ROOT)/addons/audio Moves Compute content to $(HEXAGON_SDK_ROOT)/addons/compute Moves QuRT libraries/headers to $(HEXAGON_SDK_ROOT)/rtos Moves FastRPC-related content to $(HEXAGON_SDK_ROOT)/ipc/fastrpc Fixes multiple bugs Re-architects the Hexagon SDK documentation","title":"Other"},{"location":"reference/troubleshooting.html","text":"Troubleshooting Common issues This page provides inputs to resolve common issues seen during developement. make is pulling the Hexagon Tools from the wrong location The environment variable \"HEXAGON_TOOLS_ROOT\" points to the default location for Hexagon Tools which is set to ${HEXAGON_SDK_ROOT}/tools/HEXAGON_Tools/ You can override the default tools location by setting the environment variable HEXAGON_TOOLS_ROOT to the desired version of the Hexagon Tools. How to fix undefined PLT symbol errors seen during execution on the Hexagon simulator or the device? While running the application, this error is seen when any of the loaded libraries are unable to find the definition of the symbol in the set of loaded libraries, i.e the skeleton libraries and their dependencies. Hence, ensure that the symbol is defined in one of the libraries given as input to the linker before pushing the libraries to the target. To find out the symbols in a library, use Hexagon Tool hexagon-llvm-readelf as follows: %DEFAULT_HEXAGON_TOOLS_ROOT%/Tools/bin/hexagon-llvm-readelf -a <library_path> Look out for the symbol in this output to figure out if the symbol is defined or not ( UND ). Use the linker option -u or -extern-list to provide the symbol name as argument in the linker command to explicitly export the symbol in the output library. Refer to linker doc for complete details. Why is the Hexagon simulator hexagon-sim crashing? If you work with Windows, please make sure NOT to have the Hexagon tools as part of the path specified by the PATH environment variable. What should I do if I see the following error while building my application? dlerror undefined symbol __gxx_personality_v0 in ./<user_lib>.so) The symbol __gxx_personality_v0 is part of libc++ and libc++abi libraries. The error is seen when these libraries are improperly linked with the user library. The libc++ standard library is distributed as two distinct libraries, libc++ and libc++abi. When -stdlib=libc++ is used, the compiler/linker is configured to link these two libraries automatically. If -nostdlib argument is used to disable the automatic inclusion of libc++, then the C++ code will require linkage with both libc++ and libc++abi. Hence, the issue can be resolved by linking both libc++ and libc++abi libraries with the user library. What should I do if I see the following error while running my application? Fatal: Current link configuration does not support relocation type `R_HEX_32_6_X' for symbol `_ZSt4clog' referred from C:/Qualcomm/Hexagon_Tools/8.3.04/Tools/target/hexagon/lib/v66/G0/libstdc++.a(iostream.o)[.text] , recompile with -fPIC hexagon-clang.exe: error: hexagon-link command failed with exit code 1 (use -v to see invocation) This error is seen if libstdc++ is linked statically with the user library compiled as a shared object. The library libstdc++ residing in the pic folder has to be linked with the user shared object. For example, libstdc++ is to be picked from the directory %DEFAULT_HEXAGON_TOOLS_ROOT%/Tools/target/hexagon/lib/v66/G0/pic. Why is dynamic loading failing? This happens usually when the shared object is not found in the search path specified with [A]DSP_LIBRARY_PATH. Refer to Remote file system . To know about DSP messages during dynamic loading failure, refer to Trouble shooting fastRPC issues section Why does my FastRPC call not return? Capture and analyze the DSP logs with mini-dm or logcat . If you do not see any logs from your own application, do you see log messages suggesting that your device is not signed , that your shared object cannot be found, or that no shared object is implementing the DSP function you are trying to call? What should I do if I notice high latency for RPC calls? Follow these steps to handle high FastRPC latency: The primary boot image adds more than 1 ms overhead to a FastRPC call by enabling extra logging in kernel modules. For best performance, use a secondary boot image by either working with a production device or asking your CE on how to flash a secondary boot image. Always use ION buffers in any remote call. Buffers allocated in heap or stack would add an overhead due to extra memory copy. For more details refer RPCMEM APIs Ensure voting for appropriate/max clocks on ARM & DSP as shown in the profiling benchmark. Skip first RPC call overhead. First call includes session open latency. Find average, minimum and maximum overhead over N iterations Enable FastRPC QoS using the remote APIs Using incorrect array size with FastRPC When passing an array to the DSP using FastRPC, the array size must be specified as the number of elements in the array, not as its size in bytes. Missing the 128-byte compilation flag If you compile a function to be run with the 128-byte HVX mode, you must explicitly set the compilation flag. -mhvx-length=128B Using incorrect memory alignment for scalar accesses in C programming, make sure you consistently use the correct data type for all memory allocations and memory de-references. in Assembly programming, make sure you use the correct mem instruction (e.g. memb, memh, memw, memd) for the desired data width, and that the addresses that you are using, matches the same alignment. Compiling for the wrong target Ensure your compilation flag is compatible with the architecture you are targeting. For example, do not run code compiled for V68 on an SM8250 device. Making out-of-bound memory accesses Load and stores to unmapped/invalid addresses cause TLB-miss exceptions. Note that L2 prefetching also triggers a TLB-miss exception if the start address is unmapped/invalid in the calling PD. However, if a prefetch crosses into an invalid page, the remainder is simply dropped without failure. Missing a function implementation A missing function implementation does not result in a compilation error if the function is declared as extern but it results in a runtime dynamic linking error. Inspect the DSP logs for the presence of an undefined PLT symbol i.e. funtion_name in your_library.so message. Also, remember that C++ mangles function names, so to access a function written in C or assembly from a C++ file, the function must be defined as extern \u201cC\u201d. Using an incorrect function declaration If you modify the parameters of an implementation, remember to modify the parameters of its declaration. Otherwise, your code might still compile but you end up passing incorrect parameters to a function and get an unexpected behavior. Modifying callee-saved registers The calling conventions specify which registers are the responsibility of the calling function to save, and which registers are the responsibility of the callee function to save. This is described in the Hexagon Application Binary Interface User_Guide . When writing assembly code, it is important to save all callee-saved registers if you modify them in your implementation. These registers are R16-R27 and R29-R31. The following example shows how you do so with registers R16-R19, for example: { ALLOCFRAME(#2*8) // enough space for 4 32-bit registers // Note: needs to be a multiple of 8 } { memd(r29 + #0) = r17:16 memd(r29 + #8) = r19:18 } { <your code here may use and modify registers r16-r19 in addition to any caller-saved register> } { r17:16 = memd(r29 + #0) r19:18 = memd(r29 + #8) } { DEALLOC_RETURN } Failing to make assembly code position-independent Code can compile properly when testing for the simulator, which builds a static library, but fail to compile when building a dynamic library. This might be caused by setting a register to an immediate address: test_array: .byte 0,1,2,3 ... r1 = #test_array // absolute addressing will only compile properly when building a static library r1 = ADD(PC,##test_array@PCREL) // PC-relative addressing is needed when building a dynamic library Running out of memory on the stack FastRPC created user thread stacks are 16 KB currently. Do not use excessive local variables, and be especially careful with HVX intrinsics, which might spill onto the stack and quickly consume large amount of stack memory. When is make: Command not found error seen? If you have already run the Hexagon SDK setup script , this error is seen when gow is not installed along with SDK installation. On Windows and after running the Hexagon SDK setup script, run the command which make in the console. If the Hexagon SDK is properly setup, this should point to the installed gow location. which make C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0\\tools\\utils\\gow-0.8.0\\bin\\make.EXE Is it possible to run an application from /data/app instead of /vendor/bin ? With the introduction of treble limitations starting with Android-P, it is not possible to run an application from /data/app . However, users can run the application from /data/nativetest/vendor or /data/nativetest64/vendor , depending on if it is a 32-bit or a 64-bit application. The executable can also be run from the location /data/local/tmp . An application run from any of these locations will have access to the libraries in the vendor partition. What to do to resolve the following loader issue on the Hexagon simulator? _rtld_map_object_ex: cannot open libc++.so.1, errno 2 (no such file or directory) This issue could be seen when the hexagon simulator executable is linked with the libraries libc++.so and libc++abi.so. These libraries have symbolic links as follows: libc++.so -> libc++.so.1 -> libc++.so.1.0 libc++abi.so -> libc++abi.so.1 -> libc++abi.so.1.0 When the simulator loads the executable, it also tries to load the dependent c++ libraries and looks for their symbolic links during this process. These symbolic links have to be available in the current directory to be available to be loaded by the simulator. The solution to this issue is illustrated in hexagon.min as part of the calculator C++ example. What to do when I see the following error message when running mini-dm? Device open failed with error -3 This issue is seen if your device is not added to the udev rules. Please add your device to the udev rules . Only a user with sudo privilege can make changes to the udev rules. Alternatively, you can run mini-dm with sudo. What to do when building C++ examples and the build fails when linking pthread.h? fatal error: 'pthread.h' file not found include <pthread.h> error generated. The header file pthread.h is included with the QuRT libraries. This issue is seen when a C++ based library is built as a standalone library without linking to QuRT. Refer to the calculator_c++ example for building the library with pthread.h from QuRT. Alternatively, if you do not want to link with QuRT, you can pass a compiler flag specifying a C++ standard, that has no build dependency on pthread.h such as C++03. The compiler flag can be passed to the hexagon.min file as follows: CXX_FLAGS += -std=c++03 For CMake, the compiler flag can be passed to the CMakeLists.txt file as follows: set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++03\") Eclipse IDE Why do errors appear in the C/C++ editor even though the code looks OK? There are three possible reasons for this: The text strings for the project name, workspace name, or SDK root may contain space characters. Remove any spaces that appear in these strings and try again. Indexing of the project may not have occurred properly. To fix this, right-click on the project and choose Index -> Rebuild . Why does the build log state \"No such file or directory\" even though the file is available and accessible to a newly-created template project? A possible reason for this error could be the presence of space characters in the text strings specifying the project path or Hexagon SDK Root location. Remove any spaces that appear in these strings and try again. Why do I get build errors in a project after integrating a library? This can happen if the pathname of the integrated library is incorrect: Check if you integrated the library using the same configuration it was built with. For example, if a library is built using the ReleaseG configuration, you cannot integrate the library using the Debug configuration. Check the linker settings in the build configuration to see if the library that was integrated is actually present. While debugging or running a project, why aren't certain views visible? Choose Show View from the Window menu and select the required view. If the view you want is not listed, select Other and then choose it from there. When trying to start the debugger, why do I get the error \"IDE taking too long to connect\"? This happens when the application cannot attach to the debugger because the debug port is already being used. This can be resolved by changing the port number. Make sure that the ports in the application code match what is specified in the IDE configuration. Refer to target debugging using eclipse for more details Why doesn't the debugger not break at a breakpoint? Switch to the Debug perspective to see the execution stopped at the breakpoint. Trouble shooting fastRPC issues If you run into errors when offloading an application onto a Hexagon DSP, you should be gathering the logs generated by the DSP for more information. The rest of the page discusses some of the most common errors that you may encounter. If the error you run into is not discussed on this page, please refer to $HEXAGON_SDK_ROOT/incs/stddef/AEEStdErr.h for a complete list of error codes. Analyzing errors Operation not permitted Error <0xffffffff>: apps_dev_init failed. <domain 3>, errno Operation not permitted This error is seen when FastRPC fails to open a session with DSP or fails to create a corresponding user process on DSP. Check whether the DSP is up and running by referring to Checking DSP State section . If DSP is not up and running, then please contact Qualcomm customer engineering(CE) team. Check whether the file fastrpc_shell_<0/3> loaded on target is same as the one in the DSP image. You can check time stamps of these files. Location on the target for Android P onwards: /vendor/dsp/<adsp/cdsp> Location in the DSP image: <adsp/cdsp>_proc/build/ms/dynamic_modules/<chip_number>.<adsp/cdsp>.prod If timestamp doesn't match, then flash aDSP/cDSP image again properly. If you do not have access to the DSP image,contact Qualcomm customer engineering (CE) team. Check DSP logs for any errors during user PD creation. PD creation can fail because of file read failures on HLOS. Check if the user process has the required SELinux policy enabled. Check for the Access Vector Cache(avc) denial errors in kernel logs. Here is an example of denial errors you might see when attempting to access the /dsp folder: dmesg: avc: denied { open } for pid=10222 comm='XXXXXXXXX' path='/dsp/fastrpc_shell_0' dev='sdd7' ino=26 scontext=u:r:xxxxxxxx:s0 tcontext=u:object_r:unlabeled:s0 tclass=file permissive=1 SEPF_SECMOBILE_7.0_0002 If above errors are found, check whether Selinux policy is defined properly for your process. For example: For cameraserver process to be able to access a particular directory, the following entry should be there in \" /device/qcom/sepolicy/vendor/common/cameraserver.te\" file: r_dir_file(cameraserver, adsprpcd_file); (#allow cameraserver to access/dsp) Sigverify failure The following messages indicate a signature verification failure error: CDSP: signature verify start failed for <xxxx>_skel.so This error is seen when a test signature testsig-<xxxx>.so for the target is not found in the designated location on the target. This file is needed to run unsigned dynamic shared libraries on the DSP outside an unsigned protection domain . Please review the documentation about signing and check the following: Check whether testsig- .so is present on the paths specified by the [A]DSP_LIBRARY_PATH environment variable If testsig- .so is present on the DSP library path, then check whether the serial number for the device and the generated testsig match. If you do not want to use a test signature, please read more about the signing process to understand how to properly sign a single library object or run within an unsigned PD environment. Exception in user process on DSP and the user process is killed (Error: 39/0x27) Error 27: remote handle invoke failed. <domain 3, handle 3, sc 4020200, pra 0x72b7cc13f8> AEE_ENOSUCH = 39 (0x27) Further FastRPC calls will return the same error code 39. Clients are expected to handle this error code and restart the session as discussed here . You may also want to collect a crash dump to understand why this error occurred. Please refer to the debugging instructions for more information. Alternatively, you may want to use the remote debugger and step through your code to understand where your error occurs. This error may also be seen if you are using a shared object that is built for an architecture incompatible with the target on which your application is running. Call stack is printed when crash happens and user can try fixing the issue . The table below shows the latest Hexagon version supported by each target: Target Hexagon Version on CDSP SM8150 V66 SM8250 V66 Lahaina V68 DSP Restart Error A remote procedure call returns AEE_ECONNRESET when an SSR occurred and triggers a restart of the DSP. The user then needs to wait for the DSP to complete its restart and reconnect to all the HLOS services before relaunching their own session. This approach is illustrated in calculator_test.c as part of the calculator example. User Process failed to load the shared object on DSP Error 80000406: remote handle open domain failed.<domain 2, name adspmsgd_adsp>, dlerror cannot open <xxxxx>_skel.so AEE_EUNABLETOLOAD This error is seen when the process failed to load the shared object on DSP. Ensure the shared object files are present on target in the directories defined by [A]DSP_LIBRARY_PATH environment variable. Make sure that either testsig is present on the target or the library and all its dependencies are signed. Check the DSP logs logs for any dlopen errors. Most common reason for dlopen errors is Undefined PLT symbol dlopen error:<file:///libsysmondomain_skel.so?sysmondomain_skel_handle_invoke&_modver=1.0&_dom=sdsp> undefined symbol PLT This error comes from DSP, when a symbol is used in shared object but not exposed in FastRPC shell or not found in any of the dynamically linked libraries. Ensure SELinux policy is properly defined so that the process has access to load the shared object Error: 80000402. Failure due to not enough memory in FastRPC Heap Error 80000402: remote handle invoke failed. <domain 3, handle b89040f0, sc 3020200, pra 0x85205698> AEE_ENOMEMORY = 0x80000402 This error indicates that there is not enough memory in FastRPC Heap to allocate memory for stack allocation and thread creation in the userPD on DSP. You can fix this error, by modifying the stack size using remote_session_control API Error due to mismatch in stub & skel Error: 8000040e CDSP: Error:8000040e open_mod_table_handle_invoke failed for <handle 7d203f50 sc 8010000 0714> mod_table.c AEE_EBADPARM = 0x8000040e These errors can be seen: when the parameters passed to the DSP function do not match those declared in its IDL definition. when the stub and the skel are not compiled against the same version of the IDL file. Hence make sure that: The parameters match the function signature The right version of the shared objects for DSP and application are pushed onto the device at the right location specified Fopen failure Error 45: fopen failed for <xxxxx>_skel.so (No such file or directory) If you see an fopen failure message in the DSP logs, then follow the recommendations below: Check that the file \" _skel.so\" is present on target in the directories defined by [A]DSP_LIBRARY_PATH environment variable Check whether Selinux policy is defined properly for your process as mentioned above. For example: For cameraserver process to be able to access a particular Untrusted application trying to offload to a signedPD on the DSP: Error AEE_ECONNREFUSED (0x72) logcat: `Error 0x72: apps_dev_init: untrusted app trying to offload to signed remote process (errno 111, Connection refused). Try offloading to unsigned-PD using remote_session_control` dmesg: `Error: adsprpc (3400): calculator_mult: fastrpc_init_create_dynamic_process: untrusted app trying to offload to signed remote process` If calling \" _open\" to spawn a new process on cDSP and obtain its handle fails with the error above, then request for unsigned-PD offload using the remote_session_control API and retry the handle open call. The code below demonstrates that approach: ``` int err = 0; char *cDSPURI = calculator_URI CDSP_DOMAIN; remote_handle64 handle = 0; /* Try to open a signed PD on cDSP */ err = calculator_open(cDSPURI, &handle); if (err == AEE_ECONNREFUSED) { /* * Untrusted app has been denied offloading to a signed PD. * Request for unsigned-PD offload. */ struct remote_rpc_control_unsigned_module data = {1, CDSP_DOMAIN_ID}; err = remote_session_control(DSPRPC_CONTROL_UNSIGNED_MODULE, (void*)&data, sizeof(data)); if (err) { // unsigned-PD offload request failed printf(\"Error 0x%x: unsigned-PD offload request using remote_session_control failed\\n\", err); goto bail; } // Retry handle open err = calculator_open(cDSPURI, &handle); } ``` If your application cannot run as unsigned, work with your OEM to whitelist your application instead. Below error messages that are seen during process exit can be ignored. 01-11 00:44:33.431 32220 32220 I : [Test Case] TestRandomGraph/SingleOperationTest.L2_POOL_2D_V1_2/49 BEGIN 01-11 00:44:33.587 949 31916 D : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:931: Error 0xffffffff: remote_handle_invoke failed for handle 0x3, method 4 on domain 3 (sc 0x4020200) 01-11 00:44:33.587 949 31916 E : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:244:listener protocol failure ffffffff 01-11 00:44:33.587 949 31916 D : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:931: Error 0x8000040d: remote_handle_invoke failed for handle 0x3, method 4 on domain 3 (sc 0x4020200) 01-11 00:44:33.587 949 31916 E : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:251::error: -2147482611: 0 == (nErr = __QAIC_HEADER(adsp_listener_next2)( ctx, nErr, 0, 0, &ctx, &handle, &sc, inBufs, inBufsLen, &inBufsLenReq)) 01-11 00:44:33.587 949 31916 E : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:333:Error 0x8000040d: listener2 thread exited 01-11 00:44:33.588 949 31916 D : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:947: Error 0x8000040d: remote_handle64_invoke failed for handle 0x8905cc90, method 3 on domain 3 (sc 0x3000000) 01-11 00:44:33.589 949 31917 E : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/log_config.c:322:Received exit. Checking DSP state You can check for state of DSP with the help of below commands: Find the proper subsystem node for ADSP: adb shell cat /sys/bus/msm_subsys/devices/subsys<1/2/3/4>/name should show \"adsp/cdsp\". Then run adb shell cat /sys/bus/msm_subsys/devices/subsys<1/2/3/4>/state to find whether it is \"ONLINE\".","title":"Trouble Shooting"},{"location":"reference/troubleshooting.html#troubleshooting","text":"","title":"Troubleshooting"},{"location":"reference/troubleshooting.html#common-issues","text":"This page provides inputs to resolve common issues seen during developement. make is pulling the Hexagon Tools from the wrong location The environment variable \"HEXAGON_TOOLS_ROOT\" points to the default location for Hexagon Tools which is set to ${HEXAGON_SDK_ROOT}/tools/HEXAGON_Tools/ You can override the default tools location by setting the environment variable HEXAGON_TOOLS_ROOT to the desired version of the Hexagon Tools. How to fix undefined PLT symbol errors seen during execution on the Hexagon simulator or the device? While running the application, this error is seen when any of the loaded libraries are unable to find the definition of the symbol in the set of loaded libraries, i.e the skeleton libraries and their dependencies. Hence, ensure that the symbol is defined in one of the libraries given as input to the linker before pushing the libraries to the target. To find out the symbols in a library, use Hexagon Tool hexagon-llvm-readelf as follows: %DEFAULT_HEXAGON_TOOLS_ROOT%/Tools/bin/hexagon-llvm-readelf -a <library_path> Look out for the symbol in this output to figure out if the symbol is defined or not ( UND ). Use the linker option -u or -extern-list to provide the symbol name as argument in the linker command to explicitly export the symbol in the output library. Refer to linker doc for complete details. Why is the Hexagon simulator hexagon-sim crashing? If you work with Windows, please make sure NOT to have the Hexagon tools as part of the path specified by the PATH environment variable. What should I do if I see the following error while building my application? dlerror undefined symbol __gxx_personality_v0 in ./<user_lib>.so) The symbol __gxx_personality_v0 is part of libc++ and libc++abi libraries. The error is seen when these libraries are improperly linked with the user library. The libc++ standard library is distributed as two distinct libraries, libc++ and libc++abi. When -stdlib=libc++ is used, the compiler/linker is configured to link these two libraries automatically. If -nostdlib argument is used to disable the automatic inclusion of libc++, then the C++ code will require linkage with both libc++ and libc++abi. Hence, the issue can be resolved by linking both libc++ and libc++abi libraries with the user library. What should I do if I see the following error while running my application? Fatal: Current link configuration does not support relocation type `R_HEX_32_6_X' for symbol `_ZSt4clog' referred from C:/Qualcomm/Hexagon_Tools/8.3.04/Tools/target/hexagon/lib/v66/G0/libstdc++.a(iostream.o)[.text] , recompile with -fPIC hexagon-clang.exe: error: hexagon-link command failed with exit code 1 (use -v to see invocation) This error is seen if libstdc++ is linked statically with the user library compiled as a shared object. The library libstdc++ residing in the pic folder has to be linked with the user shared object. For example, libstdc++ is to be picked from the directory %DEFAULT_HEXAGON_TOOLS_ROOT%/Tools/target/hexagon/lib/v66/G0/pic. Why is dynamic loading failing? This happens usually when the shared object is not found in the search path specified with [A]DSP_LIBRARY_PATH. Refer to Remote file system . To know about DSP messages during dynamic loading failure, refer to Trouble shooting fastRPC issues section Why does my FastRPC call not return? Capture and analyze the DSP logs with mini-dm or logcat . If you do not see any logs from your own application, do you see log messages suggesting that your device is not signed , that your shared object cannot be found, or that no shared object is implementing the DSP function you are trying to call? What should I do if I notice high latency for RPC calls? Follow these steps to handle high FastRPC latency: The primary boot image adds more than 1 ms overhead to a FastRPC call by enabling extra logging in kernel modules. For best performance, use a secondary boot image by either working with a production device or asking your CE on how to flash a secondary boot image. Always use ION buffers in any remote call. Buffers allocated in heap or stack would add an overhead due to extra memory copy. For more details refer RPCMEM APIs Ensure voting for appropriate/max clocks on ARM & DSP as shown in the profiling benchmark. Skip first RPC call overhead. First call includes session open latency. Find average, minimum and maximum overhead over N iterations Enable FastRPC QoS using the remote APIs Using incorrect array size with FastRPC When passing an array to the DSP using FastRPC, the array size must be specified as the number of elements in the array, not as its size in bytes. Missing the 128-byte compilation flag If you compile a function to be run with the 128-byte HVX mode, you must explicitly set the compilation flag. -mhvx-length=128B Using incorrect memory alignment for scalar accesses in C programming, make sure you consistently use the correct data type for all memory allocations and memory de-references. in Assembly programming, make sure you use the correct mem instruction (e.g. memb, memh, memw, memd) for the desired data width, and that the addresses that you are using, matches the same alignment. Compiling for the wrong target Ensure your compilation flag is compatible with the architecture you are targeting. For example, do not run code compiled for V68 on an SM8250 device. Making out-of-bound memory accesses Load and stores to unmapped/invalid addresses cause TLB-miss exceptions. Note that L2 prefetching also triggers a TLB-miss exception if the start address is unmapped/invalid in the calling PD. However, if a prefetch crosses into an invalid page, the remainder is simply dropped without failure. Missing a function implementation A missing function implementation does not result in a compilation error if the function is declared as extern but it results in a runtime dynamic linking error. Inspect the DSP logs for the presence of an undefined PLT symbol i.e. funtion_name in your_library.so message. Also, remember that C++ mangles function names, so to access a function written in C or assembly from a C++ file, the function must be defined as extern \u201cC\u201d. Using an incorrect function declaration If you modify the parameters of an implementation, remember to modify the parameters of its declaration. Otherwise, your code might still compile but you end up passing incorrect parameters to a function and get an unexpected behavior. Modifying callee-saved registers The calling conventions specify which registers are the responsibility of the calling function to save, and which registers are the responsibility of the callee function to save. This is described in the Hexagon Application Binary Interface User_Guide . When writing assembly code, it is important to save all callee-saved registers if you modify them in your implementation. These registers are R16-R27 and R29-R31. The following example shows how you do so with registers R16-R19, for example: { ALLOCFRAME(#2*8) // enough space for 4 32-bit registers // Note: needs to be a multiple of 8 } { memd(r29 + #0) = r17:16 memd(r29 + #8) = r19:18 } { <your code here may use and modify registers r16-r19 in addition to any caller-saved register> } { r17:16 = memd(r29 + #0) r19:18 = memd(r29 + #8) } { DEALLOC_RETURN } Failing to make assembly code position-independent Code can compile properly when testing for the simulator, which builds a static library, but fail to compile when building a dynamic library. This might be caused by setting a register to an immediate address: test_array: .byte 0,1,2,3 ... r1 = #test_array // absolute addressing will only compile properly when building a static library r1 = ADD(PC,##test_array@PCREL) // PC-relative addressing is needed when building a dynamic library Running out of memory on the stack FastRPC created user thread stacks are 16 KB currently. Do not use excessive local variables, and be especially careful with HVX intrinsics, which might spill onto the stack and quickly consume large amount of stack memory. When is make: Command not found error seen? If you have already run the Hexagon SDK setup script , this error is seen when gow is not installed along with SDK installation. On Windows and after running the Hexagon SDK setup script, run the command which make in the console. If the Hexagon SDK is properly setup, this should point to the installed gow location. which make C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0\\tools\\utils\\gow-0.8.0\\bin\\make.EXE Is it possible to run an application from /data/app instead of /vendor/bin ? With the introduction of treble limitations starting with Android-P, it is not possible to run an application from /data/app . However, users can run the application from /data/nativetest/vendor or /data/nativetest64/vendor , depending on if it is a 32-bit or a 64-bit application. The executable can also be run from the location /data/local/tmp . An application run from any of these locations will have access to the libraries in the vendor partition. What to do to resolve the following loader issue on the Hexagon simulator? _rtld_map_object_ex: cannot open libc++.so.1, errno 2 (no such file or directory) This issue could be seen when the hexagon simulator executable is linked with the libraries libc++.so and libc++abi.so. These libraries have symbolic links as follows: libc++.so -> libc++.so.1 -> libc++.so.1.0 libc++abi.so -> libc++abi.so.1 -> libc++abi.so.1.0 When the simulator loads the executable, it also tries to load the dependent c++ libraries and looks for their symbolic links during this process. These symbolic links have to be available in the current directory to be available to be loaded by the simulator. The solution to this issue is illustrated in hexagon.min as part of the calculator C++ example. What to do when I see the following error message when running mini-dm? Device open failed with error -3 This issue is seen if your device is not added to the udev rules. Please add your device to the udev rules . Only a user with sudo privilege can make changes to the udev rules. Alternatively, you can run mini-dm with sudo. What to do when building C++ examples and the build fails when linking pthread.h? fatal error: 'pthread.h' file not found include <pthread.h> error generated. The header file pthread.h is included with the QuRT libraries. This issue is seen when a C++ based library is built as a standalone library without linking to QuRT. Refer to the calculator_c++ example for building the library with pthread.h from QuRT. Alternatively, if you do not want to link with QuRT, you can pass a compiler flag specifying a C++ standard, that has no build dependency on pthread.h such as C++03. The compiler flag can be passed to the hexagon.min file as follows: CXX_FLAGS += -std=c++03 For CMake, the compiler flag can be passed to the CMakeLists.txt file as follows: set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++03\")","title":"Common issues"},{"location":"reference/troubleshooting.html#eclipse-ide","text":"Why do errors appear in the C/C++ editor even though the code looks OK? There are three possible reasons for this: The text strings for the project name, workspace name, or SDK root may contain space characters. Remove any spaces that appear in these strings and try again. Indexing of the project may not have occurred properly. To fix this, right-click on the project and choose Index -> Rebuild . Why does the build log state \"No such file or directory\" even though the file is available and accessible to a newly-created template project? A possible reason for this error could be the presence of space characters in the text strings specifying the project path or Hexagon SDK Root location. Remove any spaces that appear in these strings and try again. Why do I get build errors in a project after integrating a library? This can happen if the pathname of the integrated library is incorrect: Check if you integrated the library using the same configuration it was built with. For example, if a library is built using the ReleaseG configuration, you cannot integrate the library using the Debug configuration. Check the linker settings in the build configuration to see if the library that was integrated is actually present. While debugging or running a project, why aren't certain views visible? Choose Show View from the Window menu and select the required view. If the view you want is not listed, select Other and then choose it from there. When trying to start the debugger, why do I get the error \"IDE taking too long to connect\"? This happens when the application cannot attach to the debugger because the debug port is already being used. This can be resolved by changing the port number. Make sure that the ports in the application code match what is specified in the IDE configuration. Refer to target debugging using eclipse for more details Why doesn't the debugger not break at a breakpoint? Switch to the Debug perspective to see the execution stopped at the breakpoint.","title":"Eclipse IDE"},{"location":"reference/troubleshooting.html#trouble-shooting-fastrpc-issues","text":"If you run into errors when offloading an application onto a Hexagon DSP, you should be gathering the logs generated by the DSP for more information. The rest of the page discusses some of the most common errors that you may encounter. If the error you run into is not discussed on this page, please refer to $HEXAGON_SDK_ROOT/incs/stddef/AEEStdErr.h for a complete list of error codes.","title":"Trouble shooting fastRPC issues"},{"location":"reference/troubleshooting.html#analyzing-errors","text":"Operation not permitted Error <0xffffffff>: apps_dev_init failed. <domain 3>, errno Operation not permitted This error is seen when FastRPC fails to open a session with DSP or fails to create a corresponding user process on DSP. Check whether the DSP is up and running by referring to Checking DSP State section . If DSP is not up and running, then please contact Qualcomm customer engineering(CE) team. Check whether the file fastrpc_shell_<0/3> loaded on target is same as the one in the DSP image. You can check time stamps of these files. Location on the target for Android P onwards: /vendor/dsp/<adsp/cdsp> Location in the DSP image: <adsp/cdsp>_proc/build/ms/dynamic_modules/<chip_number>.<adsp/cdsp>.prod If timestamp doesn't match, then flash aDSP/cDSP image again properly. If you do not have access to the DSP image,contact Qualcomm customer engineering (CE) team. Check DSP logs for any errors during user PD creation. PD creation can fail because of file read failures on HLOS. Check if the user process has the required SELinux policy enabled. Check for the Access Vector Cache(avc) denial errors in kernel logs. Here is an example of denial errors you might see when attempting to access the /dsp folder: dmesg: avc: denied { open } for pid=10222 comm='XXXXXXXXX' path='/dsp/fastrpc_shell_0' dev='sdd7' ino=26 scontext=u:r:xxxxxxxx:s0 tcontext=u:object_r:unlabeled:s0 tclass=file permissive=1 SEPF_SECMOBILE_7.0_0002 If above errors are found, check whether Selinux policy is defined properly for your process. For example: For cameraserver process to be able to access a particular directory, the following entry should be there in \" /device/qcom/sepolicy/vendor/common/cameraserver.te\" file: r_dir_file(cameraserver, adsprpcd_file); (#allow cameraserver to access/dsp) Sigverify failure The following messages indicate a signature verification failure error: CDSP: signature verify start failed for <xxxx>_skel.so This error is seen when a test signature testsig-<xxxx>.so for the target is not found in the designated location on the target. This file is needed to run unsigned dynamic shared libraries on the DSP outside an unsigned protection domain . Please review the documentation about signing and check the following: Check whether testsig- .so is present on the paths specified by the [A]DSP_LIBRARY_PATH environment variable If testsig- .so is present on the DSP library path, then check whether the serial number for the device and the generated testsig match. If you do not want to use a test signature, please read more about the signing process to understand how to properly sign a single library object or run within an unsigned PD environment. Exception in user process on DSP and the user process is killed (Error: 39/0x27) Error 27: remote handle invoke failed. <domain 3, handle 3, sc 4020200, pra 0x72b7cc13f8> AEE_ENOSUCH = 39 (0x27) Further FastRPC calls will return the same error code 39. Clients are expected to handle this error code and restart the session as discussed here . You may also want to collect a crash dump to understand why this error occurred. Please refer to the debugging instructions for more information. Alternatively, you may want to use the remote debugger and step through your code to understand where your error occurs. This error may also be seen if you are using a shared object that is built for an architecture incompatible with the target on which your application is running. Call stack is printed when crash happens and user can try fixing the issue . The table below shows the latest Hexagon version supported by each target: Target Hexagon Version on CDSP SM8150 V66 SM8250 V66 Lahaina V68 DSP Restart Error A remote procedure call returns AEE_ECONNRESET when an SSR occurred and triggers a restart of the DSP. The user then needs to wait for the DSP to complete its restart and reconnect to all the HLOS services before relaunching their own session. This approach is illustrated in calculator_test.c as part of the calculator example. User Process failed to load the shared object on DSP Error 80000406: remote handle open domain failed.<domain 2, name adspmsgd_adsp>, dlerror cannot open <xxxxx>_skel.so AEE_EUNABLETOLOAD This error is seen when the process failed to load the shared object on DSP. Ensure the shared object files are present on target in the directories defined by [A]DSP_LIBRARY_PATH environment variable. Make sure that either testsig is present on the target or the library and all its dependencies are signed. Check the DSP logs logs for any dlopen errors. Most common reason for dlopen errors is Undefined PLT symbol dlopen error:<file:///libsysmondomain_skel.so?sysmondomain_skel_handle_invoke&_modver=1.0&_dom=sdsp> undefined symbol PLT This error comes from DSP, when a symbol is used in shared object but not exposed in FastRPC shell or not found in any of the dynamically linked libraries. Ensure SELinux policy is properly defined so that the process has access to load the shared object Error: 80000402. Failure due to not enough memory in FastRPC Heap Error 80000402: remote handle invoke failed. <domain 3, handle b89040f0, sc 3020200, pra 0x85205698> AEE_ENOMEMORY = 0x80000402 This error indicates that there is not enough memory in FastRPC Heap to allocate memory for stack allocation and thread creation in the userPD on DSP. You can fix this error, by modifying the stack size using remote_session_control API Error due to mismatch in stub & skel Error: 8000040e CDSP: Error:8000040e open_mod_table_handle_invoke failed for <handle 7d203f50 sc 8010000 0714> mod_table.c AEE_EBADPARM = 0x8000040e These errors can be seen: when the parameters passed to the DSP function do not match those declared in its IDL definition. when the stub and the skel are not compiled against the same version of the IDL file. Hence make sure that: The parameters match the function signature The right version of the shared objects for DSP and application are pushed onto the device at the right location specified Fopen failure Error 45: fopen failed for <xxxxx>_skel.so (No such file or directory) If you see an fopen failure message in the DSP logs, then follow the recommendations below: Check that the file \" _skel.so\" is present on target in the directories defined by [A]DSP_LIBRARY_PATH environment variable Check whether Selinux policy is defined properly for your process as mentioned above. For example: For cameraserver process to be able to access a particular Untrusted application trying to offload to a signedPD on the DSP: Error AEE_ECONNREFUSED (0x72) logcat: `Error 0x72: apps_dev_init: untrusted app trying to offload to signed remote process (errno 111, Connection refused). Try offloading to unsigned-PD using remote_session_control` dmesg: `Error: adsprpc (3400): calculator_mult: fastrpc_init_create_dynamic_process: untrusted app trying to offload to signed remote process` If calling \" _open\" to spawn a new process on cDSP and obtain its handle fails with the error above, then request for unsigned-PD offload using the remote_session_control API and retry the handle open call. The code below demonstrates that approach: ``` int err = 0; char *cDSPURI = calculator_URI CDSP_DOMAIN; remote_handle64 handle = 0; /* Try to open a signed PD on cDSP */ err = calculator_open(cDSPURI, &handle); if (err == AEE_ECONNREFUSED) { /* * Untrusted app has been denied offloading to a signed PD. * Request for unsigned-PD offload. */ struct remote_rpc_control_unsigned_module data = {1, CDSP_DOMAIN_ID}; err = remote_session_control(DSPRPC_CONTROL_UNSIGNED_MODULE, (void*)&data, sizeof(data)); if (err) { // unsigned-PD offload request failed printf(\"Error 0x%x: unsigned-PD offload request using remote_session_control failed\\n\", err); goto bail; } // Retry handle open err = calculator_open(cDSPURI, &handle); } ``` If your application cannot run as unsigned, work with your OEM to whitelist your application instead. Below error messages that are seen during process exit can be ignored. 01-11 00:44:33.431 32220 32220 I : [Test Case] TestRandomGraph/SingleOperationTest.L2_POOL_2D_V1_2/49 BEGIN 01-11 00:44:33.587 949 31916 D : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:931: Error 0xffffffff: remote_handle_invoke failed for handle 0x3, method 4 on domain 3 (sc 0x4020200) 01-11 00:44:33.587 949 31916 E : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:244:listener protocol failure ffffffff 01-11 00:44:33.587 949 31916 D : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:931: Error 0x8000040d: remote_handle_invoke failed for handle 0x3, method 4 on domain 3 (sc 0x4020200) 01-11 00:44:33.587 949 31916 E : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:251::error: -2147482611: 0 == (nErr = __QAIC_HEADER(adsp_listener_next2)( ctx, nErr, 0, 0, &ctx, &handle, &sc, inBufs, inBufsLen, &inBufsLenReq)) 01-11 00:44:33.587 949 31916 E : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:333:Error 0x8000040d: listener2 thread exited 01-11 00:44:33.588 949 31916 D : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:947: Error 0x8000040d: remote_handle64_invoke failed for handle 0x8905cc90, method 3 on domain 3 (sc 0x3000000) 01-11 00:44:33.589 949 31917 E : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/log_config.c:322:Received exit.","title":"Analyzing errors"},{"location":"reference/troubleshooting.html#checking-dsp-state","text":"You can check for state of DSP with the help of below commands: Find the proper subsystem node for ADSP: adb shell cat /sys/bus/msm_subsys/devices/subsys<1/2/3/4>/name should show \"adsp/cdsp\". Then run adb shell cat /sys/bus/msm_subsys/devices/subsys<1/2/3/4>/state to find whether it is \"ONLINE\".","title":"Checking DSP state"},{"location":"software/system_integration.html","text":"System integration Roles and responsibilities This section discusses the different participants in the Hexagon software ecosystem, their roles and responsibilities. This SDK is primarily targeted towards independent developers and OEM/integrator partners . System integrators \"System integrators\" in this document refers to organizations building and integrating complete software packages for Qualcomm Snapdragon SoC based devices. In the mobile phone ecosystem the device manufacturer typically acts as the system integrator; in other cases the integrator may be a third-party solution provider or a subsystem vendor. Regardless, the system integrator has capabilities and responsibilities beyond independent developers, and is ultimately responsible for the software running on their product. As part of building the application CPU software image, system integrators can manage which applications can run on the DSPs and the resource access privileges: Starting with Lahaina, integrators can mark selected cDSP client applications or system services as privileged clients. See Marking clients as privileged for more details. Starting with Lahaina, integrators can create additional VTCM partitions for use by privileged clients in specific use cases. See VTCM partitions for more details. Before partitioning VTCM, integrators should ensure that the functional and performance requirements of their applications and dependent libraries are met. They can work with Qualcomm Customer Engineering(CE) to determine an appropriate configuration for their device. System integrators can sign dynamically-loaded DSP modules for running in Signed PDs . See Device signing for more details. However, to improve product security, all DSP applications should run as unsigned PDs unless they require services not available in them. System integrators can whitelist clients to enable them to create Signed PDs on the cDSP in Lahaina and later products. Starting with Lahaina, other clients can only create Unsigned PDs . System integrators can build and integrate software running in CPZ PDs or a secure PD System integrators must ensure application priority levels are set appropriately: Critical system services typically need higher priority than integrated applications, which may also be given higher priority than downloaded third-party applications. Note that prior to Lahaina, Signed PDs have access to a higher priority range than unsigned PDs . Starting with Lahaina, higher priorities are available to privileged clients , regardless of whether they use signed or unsigned PDs. OEM/integrator partners \"OEM/integrator partners\" in this document refers to organizations developing custom DSP software to be integrated by a device manufacturer or a System integrator . A common example would be an independent company developing camera algorithms that a phone OEM integrates into their camera application. Such developers can work as independent developers to develop their software using stand-alone test applications, but will need to work with their system integrator partners to determine how the software is ultimately integrated in the product. Since system integrators have the ability to sign dynamically loadable modules, software designed for integration directly into a product can run in a signed PD . Developers can install test signatures on test devices to facilitate testing their software in signed PDs before integration to the product. Independent developers \"Independent developers\" or ISVs (Independent Software Vendors) in this document refers to people or organizations developing installable applications that have a DSP component. This includes applications deployed on mobile devices via application stores, internal applications within organizations, and possibly other deployment models in other markets. Independent developers are not expected to require direct support from system integrators or Qualcomm. Independent developers can use the SDK to develop and test software running on the Hexagon Compute DSP in Unsigned PDs . They can distribute their DSP modules as a part of their application and call directly to the DSP module using FastRPC calls from native code. Such applications always act as unprivileged clients on Lahaina and later products that make such distinction. Independent developers cannot easily sign modules for loading into signed PDs on production devices. Doing so requires working with system integrators to sign the modules or install the appropriate credentials on devices at build time. Developers can however install test signatures on test devices to facilitate testing code in signed PDs. Independent developers may also achieve DSP acceleration through existing CPU software libraries and frameworks that internally offload to the system's DSPs, such as SNPE, QNN, FastCV, etc. Such usage is outside of the scope of this SDK. Privileged and unprivileged clients Starting with Lahaina, CPU-side client processes for the cDSP are categorized as privileged and unprivileged clients. Privileged clients can have access to higher priority levels and resources unprivileged clients do not have. This mechanism is designed to ensure system services and critical pre-installed applications can access the cDSP with priority over installable third-party applications while still keeping the cDSP open for all clients. The distinction between privileged and unprivileged clients currently only applies to the Compute DSP (cDSP). Other DSPs are not widely open for installable applications. Marking clients as privileged System services System services are identified as privileged based on the Group ID (GID) of the process. The GID used may vary between products. The current GIDs are: Lahaina: 2908 On Android, System integrators can set system services as privileged by adding the correct group (here oem_2908 ) to the service's .rc file. For example, vppservice is configured as privileged as follows (assuming it needs also to be in the camera group): service vendor.vppservice /vendor/bin/vppservice class hal user media group camera oem_2908 See $ANDROID_BUILD_TOP/system/core/init/README.md in the Android source tree for a discussion on the Android Init language and .rc files. Built-in applications System integrators can configure built-in device applications as privileged clients by using the com.qualcomm.permission.qti.FASTRPCPRIVILEGE permission in the application's AndroidManifest XML file: <uses-permission android:name=\"com.qualcomm.permission.qti.FASTRPCPRIVILEGE\"/> ... <permission android:name=\"com.qualcomm.permission.qti.FASTRPCPRIVILEGE\" android:protectionLevel=\"signatureOrSystem\" /> Test applications Test applications can also use the setgid() system call to change their group. The process must call setgid(2908) before opening a session on the DSP and making any FastRPC calls, including memory mapping operations. This can be useful for testing code intended for privileged clients before system integration. Calling setgid() requires root privileges and is not available for regular applications on production devices. Priority levels The system caps DSP thread priority levels for unprivileged processes. This ensures critical system services can use the DSP at a higher priority than installable applications. The priority ranges may be changed, but are currently: Unprivileged clients: 64 through 254 Privileged clients: 1 through 254 Note that QuRT, the DSP RTOS, uses lower priority values for higher priorities. In other words, a thread at priority 1 has higher execution priority than one at priority 2. The priority limits apply both to the priority used for FastRPC threads (set with the FASTRPC_THREAD_PARAMS or FASTRPC_RELATIVE_THREAD_PRIORITY session control requests) and threads created locally on the DSP. The process can query its thread priority ceiling using the HAP_get_thread_priority_ceiling() API defined in HAP_ps.h . Thread priorities are also used as the resource reservation priority with the Compute Resource Manager . Prior to Lahaina priority limits are based on whether the process is running as a signed or unsigned PD. See Unsigned PD services and limitations . VTCM partitions Lahaina introduces the ability to create multiple partitions within the CDSP's VTCM. Multiple partitions are intended for device-specific use cases such as camera streaming which may require guaranteed parallel VTCM allocations to succeed while other applications use VTCM; most applications should continue to use the default partition, and many devices will ship with a single default VTCM partition only. Some VTCM partitions in the system can be restricted to privileged clients only. All clients can access regular non-privileged partitions. Note that some CDSP libraries, notably neural network runtimes, may run at reduced performance levels or have functional limitations if they do not get access to the whole VTCM in the system. System integrators should discuss their partitioning requirements and plans with Qualcomm CE as part of designing their products. System integrators configure VTCM partitions in the Linux Kernel Device Tree under the RPMSG section of the appropriate DSP: qcom,msm_cdsprm_rpmsg { qcom,msm_cdsp_rm { qcom,vtcm-partition-info = <0 2048 0x1>, <1 1024 0x2>, <2 512 0x4>, <3 512 0x4>; qcom,vtcm-partition-map = <0 0>, <1 0>, <2 1>, <30 2>, <31 3>; }; }; The configuration has two parts: qcom,vtcm-partition-info specifies the partitions, their sizes and flags in <index size_in_KB flags> format. index Partitions must be defined with a linear partition index starting with 0 till (Number of VTCM partitions - 1). VTCM memory will be partitioned in the order provided (0 being the first partition). size_in_KB Size of each partition should be a multiple of 256KB. Given 256KB is the minimum VTCM allocation size, 256K, 1M, 4M are supported page sizes. Specifying a 3MB partition will allow maximum of 1MB page (3x). Similarly, a 512KB partition will be of 256KB pages (2x). flags Flags can be used to set some partitions as privileged, i.e. only available to privileged clients. Currently 0x1 (PRIMARY), 0x2 (SECONDARY) and 0x4 (PRIVILEGED) are the supported flags per partition (only one per partition). PRIMARY and SECONDARY partitions are available to all the clients while the PRIMARY partition is used by default. Partition selection is controlled by the vtcm-partition-map information. There must be only one PRIMARY partition. qcom,vtcm-partition-map maps application type identifiers to partitions using <Application_ID partition_ID> . Clients use application type IDs to request non-default partitions. The application identifier is specified as a value [0...31] in the device tree. Client applications use the HAP_compute_res_attr_set_app_type() API to select a non-default VTCM partition for their allocation. The application type ID must match the values configured in the device tree. Developers wishing to use non-default VTCM partitions must work with their System integrator partners to determine appropriate application types to use. VTCM partitioning can be temporarily disabled at runtime. This will treat the entire VTCM as a single default partition, and may be useful for testing purposes or to implement device-wide \"performance mode\" settings. There are two options available: debugfs node: /d/compute/vtcm_partition_state A read would return '0' if VTCM partitioning is disabled, '1' if enabled. Write '0' to disable partition. Write '1' to switch back to the default partition table specified in the device tree. Kernel API: int cdsprm_compute_vtcm_set_partition_map(unsigned int b_vtcm_partitioning); b_vtcm_partitioning = 0 disables partition if already enabled via device tree configuration. b_vtcm_partitioning = 1 resets the configuration to the default specified in device tree. Returns 0 on success. Available via include/linux/soc/qcom/cdsprm.h List QCOM_CDSP_RM as a dependency for using the API. Access will be restricted to only dynamically loadable kernel modules when QCOM_CDSP_RM is compiled as a module (default). Signed and unsigned PDs Most Hexagon DSPs in the system require code running on the DSP to be cryptographically signed. This includes libraries built with the Hexagon SDK and installed to the device at runtime, such as most examples shipped in the SDK. To avoid having to separately sign software for the Hexagon DSP, the Compute DSP (cDSP) supports dedicated sandboxed Unsigned PDs that can load and execute unsigned code. Unsigned PDs Unsigned PDs are sandboxed DSP processes used to offload computation workloads to the cDSP. From a system security point of view they are considered extensions to their CPU client processes: They operate in memory provided by the client process and do not have access to drivers or other resources the client could not access directly. Since code running in unsigned PDs does not need to be signed, it can be easily shipped with installable applications - application developers not working directly with OEMs or system integrators should design their DSP software to run in unsigned PDs on the cDSP. Unsigned PD support Unsigned PDs are only supported on the cDSP and not available on other DSPs. To check if a DSP supports Unsigned PD, perform a capability query using the remote APIs using DSP attribute UNSIGNED_PD_SUPPORT . Note that all devices supported by this SDK support unsigned PDs on the cDSP. Other system DSPs only support Signed PDs and are not available for third-party application use. Unsigned PD services and limitations To allow running unsigned code, unsigned PDs are more tightly sandboxed than regular PDs and have fewer services available. The intent is to support all system services required by compute offload, and while the list can change in the future the current list includes: Inter-processor communication with FastRPC Thread creation and thread services - mutexes, semaphores, signals, etc, including lock/wait operations with timeouts. Memory allocation, mapping, and memory management Full DSP instruction set, including HVX and HMX Cache management operations Clock and power management VTCM allocation and usage The most notable omissions are drivers such as UBWC/DMA and the camera streamer and access to cache locking and timer APIs. Prior to Lahaina unsigned PD thread and resource management priorities are limited to a pre-defined range (64-254). For Lahaina and later products priorities are determined by the client privilege level instead; see section Priority levels above. Additionally, unsigned PDs can create a maximum of 128 threads. Activate Unsigned PD All DSP processes are started as signed PDs by default. To request an unsigned PD instead, clients must use the remote_session_control API as follows before starting to use the DSP: #pragma weak remote_session_control int unsigned_pd = 0; if ( remote_session_control ) { struct remote_rpc_control_unsigned_module data; data.enable = 1; data.domain = CDSP_DOMAIN_ID; err = remote_session_control(DSPRPC_CONTROL_UNSIGNED_MODULE, (void*)&data, sizeof(data)); } The calculator example illustrates how to run code in an unsigned PD using this approach. The client's DSP process is instantiated as a signed or unsigned PD when it is first launched and cannot be changed. Because of this the call to remote_session_control must be made before using the DSP - before making any FastRPC calls, mapping memory to the DSP, etc. Signed PDs Signed PDs execute as regular DSP processes and can have access to drivers and resources not available to Unsigned PDs. They are intended to be used as extensions to the underlying operating system and frameworks, and are isolated from their CPU-side client processes. Only signed libraries can be loaded to signed PDs. On the cDSP Signed PDs are used for system services such as camera processing: The camera streamer cannot be accessed from Unsigned PDs. Other DSPs in the system only support signed PDs, and all code must be signed before it can be loaded on them. Most Hexagon system libraries are shipped as signed so that they can be loaded to both signed and unsigned PDs. Lahaina and later products restrict launching signed PDs to specified whitelisted applications only. To avoid issues, all clients should use unsigned PDs for their cDSP offload if possible. All code loaded into Signed PDs must be signed unless the device has a test signature installed. See Device signing for discussion on how to install test signatures on test devices. For production devices System integrators can hash libraries as part of the image build process or create and install new Trusted Code Groups (TCGs) used to sign modules for multiple builds. System integrators should contact Qualcomm Customer Engineering for documentation and support on signing software for production devices. CPZ Content Protection Zone (CPZ) PDs are sandboxed user processes that can be used to post-process DRM-protected video content. Only System integrators can develop or integrate software that runs in CPZ PDs, and CPZ processing can only be used as part of the system video processing framework. For more details contact Qualcomm Customer Engineering. Secure PD The cDSP supports a sandboxed Secure PD that can be used to improve the performance of secure use cases such as face authentication by offloading algorithms from secure execution environments on the application CPU to the cDSP. Only System integrators can develop or integrate software that runs in the secure PD. For more details contact Qualcomm Customer Engineering. DSP rebuilding/hashing System integrators can rebuild the DSP image. This may be necessary to add or modify code running in static processes, or to change certain configuration settings. This SDK focuses on developing dynamically loadable DSP modules that can be installed separately, and rebuilding the image is outside of the scope. System integrators should work with their Qualcomm Customer Engineering contacts to get more information. As part of the image build process system integrators can statically hash dynamically loadable modules so that they can be loaded in Signed PDs for the corresponding DSP build.","title":"System integration"},{"location":"software/system_integration.html#system-integration","text":"","title":"System integration"},{"location":"software/system_integration.html#roles-and-responsibilities","text":"This section discusses the different participants in the Hexagon software ecosystem, their roles and responsibilities. This SDK is primarily targeted towards independent developers and OEM/integrator partners .","title":"Roles and responsibilities"},{"location":"software/system_integration.html#system-integrators","text":"\"System integrators\" in this document refers to organizations building and integrating complete software packages for Qualcomm Snapdragon SoC based devices. In the mobile phone ecosystem the device manufacturer typically acts as the system integrator; in other cases the integrator may be a third-party solution provider or a subsystem vendor. Regardless, the system integrator has capabilities and responsibilities beyond independent developers, and is ultimately responsible for the software running on their product. As part of building the application CPU software image, system integrators can manage which applications can run on the DSPs and the resource access privileges: Starting with Lahaina, integrators can mark selected cDSP client applications or system services as privileged clients. See Marking clients as privileged for more details. Starting with Lahaina, integrators can create additional VTCM partitions for use by privileged clients in specific use cases. See VTCM partitions for more details. Before partitioning VTCM, integrators should ensure that the functional and performance requirements of their applications and dependent libraries are met. They can work with Qualcomm Customer Engineering(CE) to determine an appropriate configuration for their device. System integrators can sign dynamically-loaded DSP modules for running in Signed PDs . See Device signing for more details. However, to improve product security, all DSP applications should run as unsigned PDs unless they require services not available in them. System integrators can whitelist clients to enable them to create Signed PDs on the cDSP in Lahaina and later products. Starting with Lahaina, other clients can only create Unsigned PDs . System integrators can build and integrate software running in CPZ PDs or a secure PD System integrators must ensure application priority levels are set appropriately: Critical system services typically need higher priority than integrated applications, which may also be given higher priority than downloaded third-party applications. Note that prior to Lahaina, Signed PDs have access to a higher priority range than unsigned PDs . Starting with Lahaina, higher priorities are available to privileged clients , regardless of whether they use signed or unsigned PDs.","title":"System integrators"},{"location":"software/system_integration.html#oemintegrator-partners","text":"\"OEM/integrator partners\" in this document refers to organizations developing custom DSP software to be integrated by a device manufacturer or a System integrator . A common example would be an independent company developing camera algorithms that a phone OEM integrates into their camera application. Such developers can work as independent developers to develop their software using stand-alone test applications, but will need to work with their system integrator partners to determine how the software is ultimately integrated in the product. Since system integrators have the ability to sign dynamically loadable modules, software designed for integration directly into a product can run in a signed PD . Developers can install test signatures on test devices to facilitate testing their software in signed PDs before integration to the product.","title":"OEM/integrator partners"},{"location":"software/system_integration.html#independent-developers","text":"\"Independent developers\" or ISVs (Independent Software Vendors) in this document refers to people or organizations developing installable applications that have a DSP component. This includes applications deployed on mobile devices via application stores, internal applications within organizations, and possibly other deployment models in other markets. Independent developers are not expected to require direct support from system integrators or Qualcomm. Independent developers can use the SDK to develop and test software running on the Hexagon Compute DSP in Unsigned PDs . They can distribute their DSP modules as a part of their application and call directly to the DSP module using FastRPC calls from native code. Such applications always act as unprivileged clients on Lahaina and later products that make such distinction. Independent developers cannot easily sign modules for loading into signed PDs on production devices. Doing so requires working with system integrators to sign the modules or install the appropriate credentials on devices at build time. Developers can however install test signatures on test devices to facilitate testing code in signed PDs. Independent developers may also achieve DSP acceleration through existing CPU software libraries and frameworks that internally offload to the system's DSPs, such as SNPE, QNN, FastCV, etc. Such usage is outside of the scope of this SDK.","title":"Independent developers"},{"location":"software/system_integration.html#privileged-and-unprivileged-clients","text":"Starting with Lahaina, CPU-side client processes for the cDSP are categorized as privileged and unprivileged clients. Privileged clients can have access to higher priority levels and resources unprivileged clients do not have. This mechanism is designed to ensure system services and critical pre-installed applications can access the cDSP with priority over installable third-party applications while still keeping the cDSP open for all clients. The distinction between privileged and unprivileged clients currently only applies to the Compute DSP (cDSP). Other DSPs are not widely open for installable applications.","title":"Privileged and unprivileged clients"},{"location":"software/system_integration.html#marking-clients-as-privileged","text":"","title":"Marking clients as privileged"},{"location":"software/system_integration.html#system-services","text":"System services are identified as privileged based on the Group ID (GID) of the process. The GID used may vary between products. The current GIDs are: Lahaina: 2908 On Android, System integrators can set system services as privileged by adding the correct group (here oem_2908 ) to the service's .rc file. For example, vppservice is configured as privileged as follows (assuming it needs also to be in the camera group): service vendor.vppservice /vendor/bin/vppservice class hal user media group camera oem_2908 See $ANDROID_BUILD_TOP/system/core/init/README.md in the Android source tree for a discussion on the Android Init language and .rc files.","title":"System services"},{"location":"software/system_integration.html#built-in-applications","text":"System integrators can configure built-in device applications as privileged clients by using the com.qualcomm.permission.qti.FASTRPCPRIVILEGE permission in the application's AndroidManifest XML file: <uses-permission android:name=\"com.qualcomm.permission.qti.FASTRPCPRIVILEGE\"/> ... <permission android:name=\"com.qualcomm.permission.qti.FASTRPCPRIVILEGE\" android:protectionLevel=\"signatureOrSystem\" />","title":"Built-in applications"},{"location":"software/system_integration.html#test-applications","text":"Test applications can also use the setgid() system call to change their group. The process must call setgid(2908) before opening a session on the DSP and making any FastRPC calls, including memory mapping operations. This can be useful for testing code intended for privileged clients before system integration. Calling setgid() requires root privileges and is not available for regular applications on production devices.","title":"Test applications"},{"location":"software/system_integration.html#priority-levels","text":"The system caps DSP thread priority levels for unprivileged processes. This ensures critical system services can use the DSP at a higher priority than installable applications. The priority ranges may be changed, but are currently: Unprivileged clients: 64 through 254 Privileged clients: 1 through 254 Note that QuRT, the DSP RTOS, uses lower priority values for higher priorities. In other words, a thread at priority 1 has higher execution priority than one at priority 2. The priority limits apply both to the priority used for FastRPC threads (set with the FASTRPC_THREAD_PARAMS or FASTRPC_RELATIVE_THREAD_PRIORITY session control requests) and threads created locally on the DSP. The process can query its thread priority ceiling using the HAP_get_thread_priority_ceiling() API defined in HAP_ps.h . Thread priorities are also used as the resource reservation priority with the Compute Resource Manager . Prior to Lahaina priority limits are based on whether the process is running as a signed or unsigned PD. See Unsigned PD services and limitations .","title":"Priority levels"},{"location":"software/system_integration.html#vtcm-partitions","text":"Lahaina introduces the ability to create multiple partitions within the CDSP's VTCM. Multiple partitions are intended for device-specific use cases such as camera streaming which may require guaranteed parallel VTCM allocations to succeed while other applications use VTCM; most applications should continue to use the default partition, and many devices will ship with a single default VTCM partition only. Some VTCM partitions in the system can be restricted to privileged clients only. All clients can access regular non-privileged partitions. Note that some CDSP libraries, notably neural network runtimes, may run at reduced performance levels or have functional limitations if they do not get access to the whole VTCM in the system. System integrators should discuss their partitioning requirements and plans with Qualcomm CE as part of designing their products. System integrators configure VTCM partitions in the Linux Kernel Device Tree under the RPMSG section of the appropriate DSP: qcom,msm_cdsprm_rpmsg { qcom,msm_cdsp_rm { qcom,vtcm-partition-info = <0 2048 0x1>, <1 1024 0x2>, <2 512 0x4>, <3 512 0x4>; qcom,vtcm-partition-map = <0 0>, <1 0>, <2 1>, <30 2>, <31 3>; }; }; The configuration has two parts: qcom,vtcm-partition-info specifies the partitions, their sizes and flags in <index size_in_KB flags> format. index Partitions must be defined with a linear partition index starting with 0 till (Number of VTCM partitions - 1). VTCM memory will be partitioned in the order provided (0 being the first partition). size_in_KB Size of each partition should be a multiple of 256KB. Given 256KB is the minimum VTCM allocation size, 256K, 1M, 4M are supported page sizes. Specifying a 3MB partition will allow maximum of 1MB page (3x). Similarly, a 512KB partition will be of 256KB pages (2x). flags Flags can be used to set some partitions as privileged, i.e. only available to privileged clients. Currently 0x1 (PRIMARY), 0x2 (SECONDARY) and 0x4 (PRIVILEGED) are the supported flags per partition (only one per partition). PRIMARY and SECONDARY partitions are available to all the clients while the PRIMARY partition is used by default. Partition selection is controlled by the vtcm-partition-map information. There must be only one PRIMARY partition. qcom,vtcm-partition-map maps application type identifiers to partitions using <Application_ID partition_ID> . Clients use application type IDs to request non-default partitions. The application identifier is specified as a value [0...31] in the device tree. Client applications use the HAP_compute_res_attr_set_app_type() API to select a non-default VTCM partition for their allocation. The application type ID must match the values configured in the device tree. Developers wishing to use non-default VTCM partitions must work with their System integrator partners to determine appropriate application types to use. VTCM partitioning can be temporarily disabled at runtime. This will treat the entire VTCM as a single default partition, and may be useful for testing purposes or to implement device-wide \"performance mode\" settings. There are two options available: debugfs node: /d/compute/vtcm_partition_state A read would return '0' if VTCM partitioning is disabled, '1' if enabled. Write '0' to disable partition. Write '1' to switch back to the default partition table specified in the device tree. Kernel API: int cdsprm_compute_vtcm_set_partition_map(unsigned int b_vtcm_partitioning); b_vtcm_partitioning = 0 disables partition if already enabled via device tree configuration. b_vtcm_partitioning = 1 resets the configuration to the default specified in device tree. Returns 0 on success. Available via include/linux/soc/qcom/cdsprm.h List QCOM_CDSP_RM as a dependency for using the API. Access will be restricted to only dynamically loadable kernel modules when QCOM_CDSP_RM is compiled as a module (default).","title":"VTCM partitions"},{"location":"software/system_integration.html#signed-and-unsigned-pds","text":"Most Hexagon DSPs in the system require code running on the DSP to be cryptographically signed. This includes libraries built with the Hexagon SDK and installed to the device at runtime, such as most examples shipped in the SDK. To avoid having to separately sign software for the Hexagon DSP, the Compute DSP (cDSP) supports dedicated sandboxed Unsigned PDs that can load and execute unsigned code.","title":"Signed and unsigned PDs"},{"location":"software/system_integration.html#unsigned-pds","text":"Unsigned PDs are sandboxed DSP processes used to offload computation workloads to the cDSP. From a system security point of view they are considered extensions to their CPU client processes: They operate in memory provided by the client process and do not have access to drivers or other resources the client could not access directly. Since code running in unsigned PDs does not need to be signed, it can be easily shipped with installable applications - application developers not working directly with OEMs or system integrators should design their DSP software to run in unsigned PDs on the cDSP.","title":"Unsigned PDs"},{"location":"software/system_integration.html#unsigned-pd-support","text":"Unsigned PDs are only supported on the cDSP and not available on other DSPs. To check if a DSP supports Unsigned PD, perform a capability query using the remote APIs using DSP attribute UNSIGNED_PD_SUPPORT . Note that all devices supported by this SDK support unsigned PDs on the cDSP. Other system DSPs only support Signed PDs and are not available for third-party application use.","title":"Unsigned PD support"},{"location":"software/system_integration.html#unsigned-pd-services-and-limitations","text":"To allow running unsigned code, unsigned PDs are more tightly sandboxed than regular PDs and have fewer services available. The intent is to support all system services required by compute offload, and while the list can change in the future the current list includes: Inter-processor communication with FastRPC Thread creation and thread services - mutexes, semaphores, signals, etc, including lock/wait operations with timeouts. Memory allocation, mapping, and memory management Full DSP instruction set, including HVX and HMX Cache management operations Clock and power management VTCM allocation and usage The most notable omissions are drivers such as UBWC/DMA and the camera streamer and access to cache locking and timer APIs. Prior to Lahaina unsigned PD thread and resource management priorities are limited to a pre-defined range (64-254). For Lahaina and later products priorities are determined by the client privilege level instead; see section Priority levels above. Additionally, unsigned PDs can create a maximum of 128 threads.","title":"Unsigned PD services and limitations"},{"location":"software/system_integration.html#activate-unsigned-pd","text":"All DSP processes are started as signed PDs by default. To request an unsigned PD instead, clients must use the remote_session_control API as follows before starting to use the DSP: #pragma weak remote_session_control int unsigned_pd = 0; if ( remote_session_control ) { struct remote_rpc_control_unsigned_module data; data.enable = 1; data.domain = CDSP_DOMAIN_ID; err = remote_session_control(DSPRPC_CONTROL_UNSIGNED_MODULE, (void*)&data, sizeof(data)); } The calculator example illustrates how to run code in an unsigned PD using this approach. The client's DSP process is instantiated as a signed or unsigned PD when it is first launched and cannot be changed. Because of this the call to remote_session_control must be made before using the DSP - before making any FastRPC calls, mapping memory to the DSP, etc.","title":"Activate Unsigned PD"},{"location":"software/system_integration.html#signed-pds","text":"Signed PDs execute as regular DSP processes and can have access to drivers and resources not available to Unsigned PDs. They are intended to be used as extensions to the underlying operating system and frameworks, and are isolated from their CPU-side client processes. Only signed libraries can be loaded to signed PDs. On the cDSP Signed PDs are used for system services such as camera processing: The camera streamer cannot be accessed from Unsigned PDs. Other DSPs in the system only support signed PDs, and all code must be signed before it can be loaded on them. Most Hexagon system libraries are shipped as signed so that they can be loaded to both signed and unsigned PDs. Lahaina and later products restrict launching signed PDs to specified whitelisted applications only. To avoid issues, all clients should use unsigned PDs for their cDSP offload if possible. All code loaded into Signed PDs must be signed unless the device has a test signature installed. See Device signing for discussion on how to install test signatures on test devices. For production devices System integrators can hash libraries as part of the image build process or create and install new Trusted Code Groups (TCGs) used to sign modules for multiple builds. System integrators should contact Qualcomm Customer Engineering for documentation and support on signing software for production devices.","title":"Signed PDs"},{"location":"software/system_integration.html#cpz","text":"Content Protection Zone (CPZ) PDs are sandboxed user processes that can be used to post-process DRM-protected video content. Only System integrators can develop or integrate software that runs in CPZ PDs, and CPZ processing can only be used as part of the system video processing framework. For more details contact Qualcomm Customer Engineering.","title":"CPZ"},{"location":"software/system_integration.html#secure-pd","text":"The cDSP supports a sandboxed Secure PD that can be used to improve the performance of secure use cases such as face authentication by offloading algorithms from secure execution environments on the application CPU to the cDSP. Only System integrators can develop or integrate software that runs in the secure PD. For more details contact Qualcomm Customer Engineering.","title":"Secure PD"},{"location":"software/system_integration.html#dsp-rebuildinghashing","text":"System integrators can rebuild the DSP image. This may be necessary to add or modify code running in static processes, or to change certain configuration settings. This SDK focuses on developing dynamically loadable DSP modules that can be installed separately, and rebuilding the image is outside of the scope. System integrators should work with their Qualcomm Customer Engineering contacts to get more information. As part of the image build process system integrators can statically hash dynamically loadable modules so that they can be loaded in Signed PDs for the corresponding DSP build.","title":"DSP rebuilding/hashing"},{"location":"software/hexagon_libraries/index.html","text":"Hexagon Libraries The Hexagon SDK includes several libraries that are accessible directly from the DSPs. These libraries are intended to assist with code development by providing debugging capabilities or optimized kernels. Libraries Summary Example Qualcomm Hexagon Libraries (QHL) Set of libraries optimized for scalar Hexagon DSP qhl Qualcomm Hexagon Libraries for HVX (QHL HVX) Set of libraries optimized for HVX qhl_hvx Qualcomm printf library (qprintf) printf library extension for HVX and assembly qprintf","title":"Hexagon libraries"},{"location":"software/hexagon_libraries/index.html#hexagon-libraries","text":"The Hexagon SDK includes several libraries that are accessible directly from the DSPs. These libraries are intended to assist with code development by providing debugging capabilities or optimized kernels. Libraries Summary Example Qualcomm Hexagon Libraries (QHL) Set of libraries optimized for scalar Hexagon DSP qhl Qualcomm Hexagon Libraries for HVX (QHL HVX) Set of libraries optimized for HVX qhl_hvx Qualcomm printf library (qprintf) printf library extension for HVX and assembly qprintf","title":"Hexagon Libraries"},{"location":"software/ipc/dspqueue.html","text":"Asynchronous DSP Packet Queue Introduction The Asynchronous DSP Packet Queue ( dspqueue ) is a queue-based asynchronous inter-processor communication API for communicating between the main Application CPU and the Hexagon DSP. It provides a low-overhead communication API for clients that can queue up multiple requests on the DSP and are looking for a low-level non-IDL-based communication primitives. The Asynchronous DSP Packet Queue API is defined and documented in dspqueue.h . It is supported on Lahaina and later targets on the Compute DSP or NSP; see the feature matrix . The dspqueue API provides clients with a number of benefits over simple FastRPC calls: Clients can queue multiple requests on the DSP without waiting for a response. This helps keep the DSP occupied and can improve throughput regardless of communication latency Messages in queue packets are opaque byte buffers; clients are free to write or reuse custom serialization without using an IDL compiler for their messages. Clients can directly control when cache maintenance operations take place. The Asynchronous DSP Packet Queue API is not necessarily a better solution for all clients. In particular it has some disadvantages that can affect many applications: Clients must handle marshaling their messages themselves and ensure they trigger cache maintenance operations as needed. FastRPC handles this automatically on the clients' behalf. dspqueue does not necessarily improve latency for single-shot operations. Clients will only see throughput improvements if they can asynchronously queue multiple requests on the DSP. The framework does not handle in-band buffer mappings; all buffers used in dspqueue messages must be first mapped to the DSP's MMU and SMMU. See RPC documentation for a further discussion on shared memory mapping. dspqueue API Basic Usage Flow The Asynchronous DSP Packet Queue is accessed through a simple C API as documented in dspqueue API documentation . The sequence diagram below illustrates a basic usage flow from a client's perspective: The client creates a new queue ( dspqueue_create() ), exports it for use on the DSP ( dspqueue_export() ), and passes the ID to the DSP. On the DSP the client uses the ID to open a local handle to the queue ( dspqueue_import() ). The host CPU client writes messages to the queue ( dspqueue_write() ). The write calls return immediately as long as there is space in the queue, in parallel to delivering the packet to the DSP. The DSP client reads messages from the queue ( dspqueue_read() ). If the queue is empty, the call will block until a packet is available, but if there is already a packet in the queue it returns immediately. The DSP client can similarly send packets to the CPU client. Creating Queues Clients create queues with dspqueue_create() on the host CPU. Some of the key arguments are: req_queue_size : CPU to DSP request queue size in bytes. Typical values are around 4kB depending on packet sizes used; the queue should be large enough to hold several packets. resp_queue_size : DSP to CPU queue size in bytes. Similar to the request queue, typical values are around 4kB. packet_callback : Pointer to a packet callback function. See section Callbacks for more details. See the API definition for more details. One client can create multiple queues and use them in parallel. The system has an upper limit of queues a single process can use, but it is typically high - 64 in the initial implementation, possibly higher in future products. Reading and Writing Packets Both CPU and DSP clients can read and write packets from/to a queue with dspqueue_read() , dspqueue_write() , and their variants. There are two basic variants of the functions: dspqueue_read_noblock() / dspqueue_write_noblock() : Non-blocking read/write operations. These functions will return immediately with an error code ( AEE_EWOULDBLOCK ) if there is no packet to read or not enough space to write. dspqueue_read() / dspqueue_write() : Blocking operations. These functions will wait until a packet is available or there is sufficient space in the queue. Both functions take an optional timeout parameter and will return with an error code ( AEE_EEXPIRED ) if the timeout expires before the operation could succeed. The non-blocking variants are typically used in a packet callback function (see section Callbacks ) to read packets. In other cases the blocking variants are usually appropriate. Each packet consists of the same three basic components which are also visible as arguments to all read/write operations: Flags : Information about packet contents. Mostly used internally within the framework; most clients can currently ignore packet flags. See enum dspqueue_packet_flags . Buffer references : References to pre-mapped buffers, such as input and output buffers for an operation described in the packet. The framework can populate buffer address information and perform cache maintenance operations based on buffer references. See section Buffer References for more information. Message : Opaque message, constructed and interpreted by the client. The dspqueue framework simply passes the message from the sender to the recipient. Typically the message contains information on what operation to perform on the buffers attached to the packet, arguments, and other data passed by value. Message should be kept relatively small for best performance, typically around 8-256 bytes. The framework currently limits the maximum message size to 64kB. For read operations the client must provide adequate space for buffer references and the message. For example, with dspqueue_read_noblock() : AEEResult dspqueue_read_noblock(dspqueue_t queue, uint32_t *flags, uint32_t max_buffers, uint32_t *num_buffers, struct dspqueue_buffer *buffers, uint32_t max_message_length, uint32_t *message_length, uint8_t *message); max_buffers must be equal to or higher than the number of buffer references in the packet, buffers must point to an array of struct dspqueue_buffer of the correct size, max_message_length must be equal to or higher than the length of the message in the packet in bytes, and message must point to a byte array of max_message_length . If any of the buffers is too small, the call will fail with AEE_EBUFFERTOOSMALL . Buffer References and Cache Maintenance Each Asynchronous DSP Packet Queue packet can contain one or more buffer references. Buffer references typically refer to input and output buffers used in an operation described in the packet. Clients can use buffer references to get information about buffer mappings without having to track it themselves and to get the dspqueue framework to perform cache maintenance operations. All buffers must be previously mapped to the DSP with fastrpc_mmap() and must be shareable ION buffers. For example, this code snippet allocates and maps a buffer using the RPCMEM library (error handling omitted): uint64_t remote_addr; void *buf = rpcmem_alloc(RPCMEM_HEAP_ID_SYSTEM, RPCMEM_DEFAULT_FLAGS, BUFFER_SIZE); int fd = rpcmem_to_fd(buf); fastrpc_mmap(CDSP_DOMAIN_ID, fd, buf, 0, BUFFER_SIZE, FASTRPC_MAP_FD); See the dspqueue Example Application for a more comprehensive example on allocating and deallocating buffers. Each buffer reference in a dspqueue packet has five main properties, corresponding to fields in struct dspqueue_buffer : fd : Buffer file descriptor. The buffer must be mapped to the DSP using the same FD. size : Buffer size in bytes. Set to zero to use the whole buffer when writing a packet; in this case the framework will populate the field in received packets. offset : Offset in bytes within the allocated buffer. Set to zero to use the whole buffer when writing a packet. A non-zero size and offset can be used to refer to a subsection of a buffer. ptr : Buffer virtual address in the current process; NULL if a valid mapping is not available. Populated by the framework in received packets, can be left empty in packets being sent. flags : Buffer flags, including cache maintenance operations. See enum #dspqueue_buffer_flags . Buffer flags come in two categories: Changes to buffer reference count and buffer cache maintenance operations: DSPQUEUE_BUFFER_FLAG_REF and DSPQUEUE_BUFFER_FLAG_DEREF are used to add or remove a reference to a buffer. Using them is not mandatory, but can help debugging if a client accidentally attempts to unmap a buffer that is still being used. DSPQUEUE_BUFFER_(FLUSH|INVALIDATE)_(SENDER|RECIPIENT) are used to instruct the dspqueue framework to perform cache maintenance on the buffer as part of processing the packet. \"Sender\" and \"recipient\" refer to the sender and recipient of the specific packet (CPU or DSP depending on communication direction); \"flush\" and \"invalidate\" specify the type of cache maintenance to perform (flush/clean or invalidate). On devices with I/O coherency support the framework will automatically skip cache maintenance operations where possible, so clients should always specify the operations needed assuming CPU and DSP caches are not coherent. As a rule, buffers must be flushed before any data written to them is visible to the other processor, and must be invalidated before new data can be read. For a typical scenario where the host CPU sends requests to the DSP for processing, the following flags are appropriate: Packet Buffer Flags CPU->DSP Request Input buffer DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER | DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT CPU->DSP Request Output buffer DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER DSP->CPU Response Input buffer DSPQUEUE_BUFFER_FLAG_DEREF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER DSP->CPU Response Output buffer DSPQUEUE_BUFFER_FLAG_DEREF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER | DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT Note that flushing an input buffer on the \"sender\" side is not necessary if the application can guarantee the processor has not written to the buffer and the caches do not contain dirty cache lines for it. Performing unnecessary cache maintenance operations is always safe but on some platforms may have a performance penalty. The dspqueue Example Application illustrates how to use buffer references, request cache maintenance, and use framework-provided buffer information to access buffer contents. Callbacks The Asynchronous DSP Packet Queue framework provides clients with two types of callbacks: Error callbacks: Called when the framework encounters an unrecoverable error. Such errors are currently only raised on the host CPU when its corresponding DSP process crashes. The only way to recover is to tear down and restart all DSP sessions. Packet callbacks: Called when one or more packets may be available to read from a queue. Most clients use a packet callback to read packets from a queue instead of blocking read operations. The callback is not called for every packet, but instead the client should read all packets available in the queue before returning from the callback or otherwise ensure all packets will be read. A typical packet callback function has the following structure: static void packet_callback ( dspqueue_t queue , AEEResult error , void * context ) { while ( 1 ) { //... AEEResult err = dspqueue_read_noblock ( queue , & flags , MAX_BUFFERS , & num_bufs , & bufs , sizeof ( msg ), & msg_len , & msg ); if ( err == AEE_EWOULDBLOCK ) break ; // No more packets until next callback if ( err != 0 ) // Handle error // Process packet } } See the dspqueue Example Application for a more complete packet callback example for both the host CPU and the DSP. Performance Considerations Using the Asynchronous DSP Packet Queue can yield better performance than regular synchronous FastRPC calls, but it is not automatically more efficient. This section discusses some performance considerations to get the most out of the dspqueue API. Queue Depth The Asynchronous DSP Packet Queue is not designed to improve end-to-end latency for individual requests. Instead, it lets clients queue up multiple requests on the DSP, keeping the DSP occupied and increasing overall throughput. This is only possible if the queues are large enough to hold several packets, and the client application queues enough work. Queues should typically be sized to fit multiple pieces of work. The exact number depends on the workload, but at a minimum there should be always be a few requests queued, enough to occupy the DSP for at least 5-20 milliseconds. If the DSP can drain the request queue it will end up having to wait the host CPU for more work, reducing the benefit from using the packet queue. To determine a suitable queue size, consider the size of each packet and the number of packets that should be queued. Each packet consists of a 8-byte header, 24 bytes of data for each buffer reference, plus a message. Each packet is further 8-byte aligned. Assuming four buffer references and a 64-byte message, each packet would be: 8 + 4*24 + 64 = 168 bytes A 4kB queue would be enough to store around 24 packets, which should be sufficient for most use cases. Packet queues are allocated dynamically from regular system memory, making them fairly inexpensive so allocating a larger queue (16-64kB) would be a reasonable decision. Early Wakeup While the Asynchronous DSP Packet Queue is not designed to reduce single-shot latency, it provides a way to reduce latencies by sending \"early wakeup packets\". Host CPU power management and interrupt handling is a large contributor to end-to-end latency, and clients can reduce the impact by sending an early wakeup packet from the DSP to the CPU a short time before processing is complete. This starts waking up the CPU and executing some of the interrupt handling code while the DSP finishes processing in parallel. Clients can send an early wakeup packet by calling AEEResult dspqueue_write_early_wakeup_noblock ( dspqueue_t queue , uint32_t wakeup_delay , uint32_t packet_flags ); wakeup_delay should be set to the time (in microseconds) until the actual response packet is available if known; the framework can use this information to optimize waiting for the final packet. packet_flags should match the flags for the actual response packet if known. Choosing how early to send an early wakeup packet is critical for good performance. After handling the wakeup packet the dspqueue framework will wait for the final packet in a tight loop for a period of time - this can consume power, so long waits should be avoided. On the other hand sending the wakeup packet too late reduces the benefit seen. To avoid consuming excess power, the framework limits the wait time after an early wakeup packet based on the wakeup_delay parameter and internal limits. If the final packet does not arrive within the expected time window the framework will revert back to a normal lower-power higher-latency wait. Developers can use two statistics from the framework to tune early wakeup signaling: dspqueue_get_stat ( queue , DSPQUEUE_STAT_EARLY_WAKEUP_WAIT_TIME , & val ); dspqueue_get_stat ( queue , DSPQUEUE_STAT_EARLY_WAKEUP_MISSES , & val ); DSPQUEUE_STAT_EARLY_WAKEUP_WAIT_TIME returns the total time spent waiting for the final packet. This time should be minimized to avoid spending extra processor cycles and power waiting for packets. If the time is high the early wakeup packets are being sent too early. The target should be no more than a few microseconds per packet on average. DSPQUEUE_STAT_EARLY_WAKEUP_MISSES counts the number of times the framework did not receive the final packet within a time window and considered it a miss. If this counter is larger than zero early wakeup packets are being sent too early or the wakeup_delay value is too low. Clients should tune any early wakeup signaling to minimize the end-to-end latency seen while minimizing wakeup wait times. Such tuning is highly workload and product specific. Early wakeup signaling is also available for packets sent from the host CPU to the DSP but is typically not useful in that direction. Limits The current Asynchronous DSP Packet Queue implementation has a number of built-in limits. These limits are chosen so that they are unlikely to impact real-world clients and may be increased or removed in future implementations: Number of queues: 64 queues per process Queue size: 16 MB. Note that using large packet sizes (multiple kilobytes or more) that would require using large queues is likely to yield poor performance. Message size: 64 kB per packet Number of buffer references: 64 per packet","title":"Asynchronous DSP Packet Queue"},{"location":"software/ipc/dspqueue.html#asynchronous-dsp-packet-queue","text":"","title":"Asynchronous DSP Packet Queue"},{"location":"software/ipc/dspqueue.html#intro","text":"The Asynchronous DSP Packet Queue ( dspqueue ) is a queue-based asynchronous inter-processor communication API for communicating between the main Application CPU and the Hexagon DSP. It provides a low-overhead communication API for clients that can queue up multiple requests on the DSP and are looking for a low-level non-IDL-based communication primitives. The Asynchronous DSP Packet Queue API is defined and documented in dspqueue.h . It is supported on Lahaina and later targets on the Compute DSP or NSP; see the feature matrix . The dspqueue API provides clients with a number of benefits over simple FastRPC calls: Clients can queue multiple requests on the DSP without waiting for a response. This helps keep the DSP occupied and can improve throughput regardless of communication latency Messages in queue packets are opaque byte buffers; clients are free to write or reuse custom serialization without using an IDL compiler for their messages. Clients can directly control when cache maintenance operations take place. The Asynchronous DSP Packet Queue API is not necessarily a better solution for all clients. In particular it has some disadvantages that can affect many applications: Clients must handle marshaling their messages themselves and ensure they trigger cache maintenance operations as needed. FastRPC handles this automatically on the clients' behalf. dspqueue does not necessarily improve latency for single-shot operations. Clients will only see throughput improvements if they can asynchronously queue multiple requests on the DSP. The framework does not handle in-band buffer mappings; all buffers used in dspqueue messages must be first mapped to the DSP's MMU and SMMU. See RPC documentation for a further discussion on shared memory mapping.","title":"Introduction"},{"location":"software/ipc/dspqueue.html#api","text":"","title":"dspqueue API"},{"location":"software/ipc/dspqueue.html#flow","text":"The Asynchronous DSP Packet Queue is accessed through a simple C API as documented in dspqueue API documentation . The sequence diagram below illustrates a basic usage flow from a client's perspective: The client creates a new queue ( dspqueue_create() ), exports it for use on the DSP ( dspqueue_export() ), and passes the ID to the DSP. On the DSP the client uses the ID to open a local handle to the queue ( dspqueue_import() ). The host CPU client writes messages to the queue ( dspqueue_write() ). The write calls return immediately as long as there is space in the queue, in parallel to delivering the packet to the DSP. The DSP client reads messages from the queue ( dspqueue_read() ). If the queue is empty, the call will block until a packet is available, but if there is already a packet in the queue it returns immediately. The DSP client can similarly send packets to the CPU client.","title":"Basic Usage Flow"},{"location":"software/ipc/dspqueue.html#creating","text":"Clients create queues with dspqueue_create() on the host CPU. Some of the key arguments are: req_queue_size : CPU to DSP request queue size in bytes. Typical values are around 4kB depending on packet sizes used; the queue should be large enough to hold several packets. resp_queue_size : DSP to CPU queue size in bytes. Similar to the request queue, typical values are around 4kB. packet_callback : Pointer to a packet callback function. See section Callbacks for more details. See the API definition for more details. One client can create multiple queues and use them in parallel. The system has an upper limit of queues a single process can use, but it is typically high - 64 in the initial implementation, possibly higher in future products.","title":"Creating Queues"},{"location":"software/ipc/dspqueue.html#packets","text":"Both CPU and DSP clients can read and write packets from/to a queue with dspqueue_read() , dspqueue_write() , and their variants. There are two basic variants of the functions: dspqueue_read_noblock() / dspqueue_write_noblock() : Non-blocking read/write operations. These functions will return immediately with an error code ( AEE_EWOULDBLOCK ) if there is no packet to read or not enough space to write. dspqueue_read() / dspqueue_write() : Blocking operations. These functions will wait until a packet is available or there is sufficient space in the queue. Both functions take an optional timeout parameter and will return with an error code ( AEE_EEXPIRED ) if the timeout expires before the operation could succeed. The non-blocking variants are typically used in a packet callback function (see section Callbacks ) to read packets. In other cases the blocking variants are usually appropriate. Each packet consists of the same three basic components which are also visible as arguments to all read/write operations: Flags : Information about packet contents. Mostly used internally within the framework; most clients can currently ignore packet flags. See enum dspqueue_packet_flags . Buffer references : References to pre-mapped buffers, such as input and output buffers for an operation described in the packet. The framework can populate buffer address information and perform cache maintenance operations based on buffer references. See section Buffer References for more information. Message : Opaque message, constructed and interpreted by the client. The dspqueue framework simply passes the message from the sender to the recipient. Typically the message contains information on what operation to perform on the buffers attached to the packet, arguments, and other data passed by value. Message should be kept relatively small for best performance, typically around 8-256 bytes. The framework currently limits the maximum message size to 64kB. For read operations the client must provide adequate space for buffer references and the message. For example, with dspqueue_read_noblock() : AEEResult dspqueue_read_noblock(dspqueue_t queue, uint32_t *flags, uint32_t max_buffers, uint32_t *num_buffers, struct dspqueue_buffer *buffers, uint32_t max_message_length, uint32_t *message_length, uint8_t *message); max_buffers must be equal to or higher than the number of buffer references in the packet, buffers must point to an array of struct dspqueue_buffer of the correct size, max_message_length must be equal to or higher than the length of the message in the packet in bytes, and message must point to a byte array of max_message_length . If any of the buffers is too small, the call will fail with AEE_EBUFFERTOOSMALL .","title":"Reading and Writing Packets"},{"location":"software/ipc/dspqueue.html#buffers","text":"Each Asynchronous DSP Packet Queue packet can contain one or more buffer references. Buffer references typically refer to input and output buffers used in an operation described in the packet. Clients can use buffer references to get information about buffer mappings without having to track it themselves and to get the dspqueue framework to perform cache maintenance operations. All buffers must be previously mapped to the DSP with fastrpc_mmap() and must be shareable ION buffers. For example, this code snippet allocates and maps a buffer using the RPCMEM library (error handling omitted): uint64_t remote_addr; void *buf = rpcmem_alloc(RPCMEM_HEAP_ID_SYSTEM, RPCMEM_DEFAULT_FLAGS, BUFFER_SIZE); int fd = rpcmem_to_fd(buf); fastrpc_mmap(CDSP_DOMAIN_ID, fd, buf, 0, BUFFER_SIZE, FASTRPC_MAP_FD); See the dspqueue Example Application for a more comprehensive example on allocating and deallocating buffers. Each buffer reference in a dspqueue packet has five main properties, corresponding to fields in struct dspqueue_buffer : fd : Buffer file descriptor. The buffer must be mapped to the DSP using the same FD. size : Buffer size in bytes. Set to zero to use the whole buffer when writing a packet; in this case the framework will populate the field in received packets. offset : Offset in bytes within the allocated buffer. Set to zero to use the whole buffer when writing a packet. A non-zero size and offset can be used to refer to a subsection of a buffer. ptr : Buffer virtual address in the current process; NULL if a valid mapping is not available. Populated by the framework in received packets, can be left empty in packets being sent. flags : Buffer flags, including cache maintenance operations. See enum #dspqueue_buffer_flags . Buffer flags come in two categories: Changes to buffer reference count and buffer cache maintenance operations: DSPQUEUE_BUFFER_FLAG_REF and DSPQUEUE_BUFFER_FLAG_DEREF are used to add or remove a reference to a buffer. Using them is not mandatory, but can help debugging if a client accidentally attempts to unmap a buffer that is still being used. DSPQUEUE_BUFFER_(FLUSH|INVALIDATE)_(SENDER|RECIPIENT) are used to instruct the dspqueue framework to perform cache maintenance on the buffer as part of processing the packet. \"Sender\" and \"recipient\" refer to the sender and recipient of the specific packet (CPU or DSP depending on communication direction); \"flush\" and \"invalidate\" specify the type of cache maintenance to perform (flush/clean or invalidate). On devices with I/O coherency support the framework will automatically skip cache maintenance operations where possible, so clients should always specify the operations needed assuming CPU and DSP caches are not coherent. As a rule, buffers must be flushed before any data written to them is visible to the other processor, and must be invalidated before new data can be read. For a typical scenario where the host CPU sends requests to the DSP for processing, the following flags are appropriate: Packet Buffer Flags CPU->DSP Request Input buffer DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER | DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT CPU->DSP Request Output buffer DSPQUEUE_BUFFER_FLAG_REF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER DSP->CPU Response Input buffer DSPQUEUE_BUFFER_FLAG_DEREF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER DSP->CPU Response Output buffer DSPQUEUE_BUFFER_FLAG_DEREF | DSPQUEUE_BUFFER_FLAG_FLUSH_SENDER | DSPQUEUE_BUFFER_FLAG_INVALIDATE_RECIPIENT Note that flushing an input buffer on the \"sender\" side is not necessary if the application can guarantee the processor has not written to the buffer and the caches do not contain dirty cache lines for it. Performing unnecessary cache maintenance operations is always safe but on some platforms may have a performance penalty. The dspqueue Example Application illustrates how to use buffer references, request cache maintenance, and use framework-provided buffer information to access buffer contents.","title":"Buffer References and Cache Maintenance"},{"location":"software/ipc/dspqueue.html#callbacks","text":"The Asynchronous DSP Packet Queue framework provides clients with two types of callbacks: Error callbacks: Called when the framework encounters an unrecoverable error. Such errors are currently only raised on the host CPU when its corresponding DSP process crashes. The only way to recover is to tear down and restart all DSP sessions. Packet callbacks: Called when one or more packets may be available to read from a queue. Most clients use a packet callback to read packets from a queue instead of blocking read operations. The callback is not called for every packet, but instead the client should read all packets available in the queue before returning from the callback or otherwise ensure all packets will be read. A typical packet callback function has the following structure: static void packet_callback ( dspqueue_t queue , AEEResult error , void * context ) { while ( 1 ) { //... AEEResult err = dspqueue_read_noblock ( queue , & flags , MAX_BUFFERS , & num_bufs , & bufs , sizeof ( msg ), & msg_len , & msg ); if ( err == AEE_EWOULDBLOCK ) break ; // No more packets until next callback if ( err != 0 ) // Handle error // Process packet } } See the dspqueue Example Application for a more complete packet callback example for both the host CPU and the DSP.","title":"Callbacks"},{"location":"software/ipc/dspqueue.html#perf","text":"Using the Asynchronous DSP Packet Queue can yield better performance than regular synchronous FastRPC calls, but it is not automatically more efficient. This section discusses some performance considerations to get the most out of the dspqueue API.","title":"Performance Considerations"},{"location":"software/ipc/dspqueue.html#queuedepth","text":"The Asynchronous DSP Packet Queue is not designed to improve end-to-end latency for individual requests. Instead, it lets clients queue up multiple requests on the DSP, keeping the DSP occupied and increasing overall throughput. This is only possible if the queues are large enough to hold several packets, and the client application queues enough work. Queues should typically be sized to fit multiple pieces of work. The exact number depends on the workload, but at a minimum there should be always be a few requests queued, enough to occupy the DSP for at least 5-20 milliseconds. If the DSP can drain the request queue it will end up having to wait the host CPU for more work, reducing the benefit from using the packet queue. To determine a suitable queue size, consider the size of each packet and the number of packets that should be queued. Each packet consists of a 8-byte header, 24 bytes of data for each buffer reference, plus a message. Each packet is further 8-byte aligned. Assuming four buffer references and a 64-byte message, each packet would be: 8 + 4*24 + 64 = 168 bytes A 4kB queue would be enough to store around 24 packets, which should be sufficient for most use cases. Packet queues are allocated dynamically from regular system memory, making them fairly inexpensive so allocating a larger queue (16-64kB) would be a reasonable decision.","title":"Queue Depth"},{"location":"software/ipc/dspqueue.html#early-wakeup","text":"While the Asynchronous DSP Packet Queue is not designed to reduce single-shot latency, it provides a way to reduce latencies by sending \"early wakeup packets\". Host CPU power management and interrupt handling is a large contributor to end-to-end latency, and clients can reduce the impact by sending an early wakeup packet from the DSP to the CPU a short time before processing is complete. This starts waking up the CPU and executing some of the interrupt handling code while the DSP finishes processing in parallel. Clients can send an early wakeup packet by calling AEEResult dspqueue_write_early_wakeup_noblock ( dspqueue_t queue , uint32_t wakeup_delay , uint32_t packet_flags ); wakeup_delay should be set to the time (in microseconds) until the actual response packet is available if known; the framework can use this information to optimize waiting for the final packet. packet_flags should match the flags for the actual response packet if known. Choosing how early to send an early wakeup packet is critical for good performance. After handling the wakeup packet the dspqueue framework will wait for the final packet in a tight loop for a period of time - this can consume power, so long waits should be avoided. On the other hand sending the wakeup packet too late reduces the benefit seen. To avoid consuming excess power, the framework limits the wait time after an early wakeup packet based on the wakeup_delay parameter and internal limits. If the final packet does not arrive within the expected time window the framework will revert back to a normal lower-power higher-latency wait. Developers can use two statistics from the framework to tune early wakeup signaling: dspqueue_get_stat ( queue , DSPQUEUE_STAT_EARLY_WAKEUP_WAIT_TIME , & val ); dspqueue_get_stat ( queue , DSPQUEUE_STAT_EARLY_WAKEUP_MISSES , & val ); DSPQUEUE_STAT_EARLY_WAKEUP_WAIT_TIME returns the total time spent waiting for the final packet. This time should be minimized to avoid spending extra processor cycles and power waiting for packets. If the time is high the early wakeup packets are being sent too early. The target should be no more than a few microseconds per packet on average. DSPQUEUE_STAT_EARLY_WAKEUP_MISSES counts the number of times the framework did not receive the final packet within a time window and considered it a miss. If this counter is larger than zero early wakeup packets are being sent too early or the wakeup_delay value is too low. Clients should tune any early wakeup signaling to minimize the end-to-end latency seen while minimizing wakeup wait times. Such tuning is highly workload and product specific. Early wakeup signaling is also available for packets sent from the host CPU to the DSP but is typically not useful in that direction.","title":"Early Wakeup"},{"location":"software/ipc/dspqueue.html#limits","text":"The current Asynchronous DSP Packet Queue implementation has a number of built-in limits. These limits are chosen so that they are unlikely to impact real-world clients and may be increased or removed in future implementations: Number of queues: 64 queues per process Queue size: 16 MB. Note that using large packet sizes (multiple kilobytes or more) that would require using large queues is likely to yield poor performance. Message size: 64 kB per packet Number of buffer references: 64 per packet","title":"Limits"},{"location":"software/ipc/rpc.html","text":"RPC A Remote Procedure Call (RPC) allows a computer program calling a procedure to execute in another remote processor, while hiding the details of the remote interaction. FastRPC is the RPC mechanism used to enable remote function calls between the CPU and DSP. Customers with algorithms that benefit from being executed on the DSP can use the FastRPC framework to offload large processing tasks onto the DSP. The DSP can then leverage its internal processing resources, such as HVX, to execute the tasks in a more compute- and power-efficient way than the CPU. FastRPC interfaces are defined in an IDL file, and they are compiled using the QAIC compiler to generate header files and stub and skel code. The header files and stub should be built and linked into the CPU executable while the header files and skel should be built and linked into the DSP library. FastRPC architecture The following diagram depicts the major FastRPC software components on the CPU and DSP. Definition of the terms in the diagram: Term Description Application User mode process that initiates the remote invocation Stub Auto-generated code that takes care of marshaling parameters and runs on the CPU FastRPC user driver on CPU User mode library that is used by the stub code to do remote invocations FastRPC Kernel Driver Receives the remote invocations from the client, queues them up with the FastRPC DSP driver, and then waits for the response after signaling the remote side FastRPC DSP Driver Dequeues the messages sent by the FastRPC kernel driver and dispatches them for processing FastRPC user driver on DSP User mode code that includes a shell executable to run in the user protection domain (PD) on the DSP and complete the remote invocations to the skel library Skel Auto-generated code that un-marshals parameters and invokes the user-defined implementation of the function that runs on the DSP User PD User protection domain on the DSP that provides the environment to run the user code FastRPC workflow The FastRPC framework is a typical proxy pattern. The interface object stub and the implementation skeleton objects are on different processors. FastRPC clients are directly exposed to the stub object, and the skeleton object is called by the FastRPC framework on the DSP. The FastRPC framework consists of the following components. Workflow: The CPU process calls the stub version of the function. The stub code converts the function call to an RPC message. The stub code internally invokes the FastRPC framework on the CPU to queue the converted message. The FastRPC framework on the CPU sends the queued message to the FastRPC DSP framework on the DSP. The FastRPC DSP framework on the DSP dispatches the call to the relevant skeleton code. The skeleton code un-marshals the parameters and calls the method implementation. The skeleton code waits for the implementation to finish processing, and, in turn, marshals the return value and any other output arguments into the return message. The skeleton code calls the FastRPC DSP framework to queue the return message to be transmitted to the CPU. The FastRPC DSP framework on the DSP sends the return message back to the FastRPC framework on the CPU. The FastRPC framework identifies the waiting stub code and dispatches the return value. The stub code un-marshals the return message and sends it to the calling User mode process. Android software components The FastRPC framework consists of several libraries in the system and vendor partitions of the Android image. Software component Description /system/vendor/lib/lib*rpc.so or /vendor/lib/lib*rpc.so , where * is adsp , cdsp , or sdsp Shared object library to be linked with the user-space vendor application that is invoking the remote procedure call. This library interfaces with the kernel driver to initiate the remote invocation to the aDSP, cDSP, or sDSP. /system/lib/lib*rpc_system.so , where * is adsp , cdsp , or sdsp Shared object library that is to be linked with the user-space system application that is invoking the remote procedure call. This library interfaces with the kernel driver to initiate the remote invocation to the aDSP, cDSP, or sDSP. This library is applicable for system applications on Android P and onwards. NOTE: The /system/vendor path is only supported on Android O. DSP protection domains Because the DSP is a real-time processor whose stability critically affects the overall user experience, different PDs exist in the DSP software architecture. These PDs ensure the stability of the kernel software and the safety of Qualcomm proprietary hardware information. There are various protection domains in the DSP. Kernel: Access to all memory of all PDs Guest OS: Access to the memory of its own PD, the memory of the user PD, and some system registers User: Access only to the memory of its own PD DSP system libraries make system calls to the Guest OS or Kernel as appropriate to access operating system services. FastRPC client programs run in user PDs. Dynamic vs static PD User PDs can be created statically at boot time, but they are more often created dynamically at run time by CPU applications that are to offload modules to the DSP. Both static and dynamic PDs support dynamic loading of shared objects. Static PD Static PDs are created on DSPs to support specific use cases like Audio and Sensors. The static PDs allow the dynamic loading of shared objects with the help of a daemon running on the CPU. For more details, see Dynamic loading . Dynamic user PD Every CPU user process that uses a DSP via FastRPC will have a corresponding dynamic user PD on the DSP. The FastRPC Kernel driver manages the life cycle of the dynamic user PD. That is, if the CPU user process exits, the corresponding user PD on the DSP is cleaned up as well. The DSP supports different types of dynamic user PD environments, including signed and unsigned PDs. Each DSP can only support a limited number of concurrent dynamic user PDs. This limit depends on the hardware configuration for each supported chip and DSP; see the feature matrix . Signed and unsigned PDs DSPs support different types of dynamic execution environments (dynamic PDs) including signed and unsigned PDs. Signed PDs are available on all the DSPs, and they require that the modules (shared objects) being loaded in the PD are signed with a digital signature. This signature must be verified at the time of loading the shared objects in the PD. For details on signing a shared object, see Signing . On the other hand, unsigned PDs are only supported in the cDSP, and they allow the DSP modules to load without any digital signatures. An unsigned PD is a sandboxed low-rights process that allows the signature-free modules to run on the cDSP. In the event of a compromise, access to full system functionality and data is prevented by the sandbox. Unsigned PDs are designed to support general compute applications and have limited access to underlying drivers (see the following sections for available services and limitations). The available/unavailable services in the unsigned PD may change in the future, depending on continuing security reviews. Unsigned PD support To check whether a device supports unsigned PDs, perform a capability query using the DSP attribute, UNSIGNED_PD_SUPPORT . For an example on how to make this query, see the calculator example . Unsigned PD available services Thread creation and thread services HVX contexts Clock frequency controls VTCM Cache operations Map HLOS memory allocated by the corresponding HLOS application Unsigned PD limitations Access to limited drivers: UBWCDMA and Camera Streamer are not available to unsigned PDs QuRT timer APIs are not available. Applications can use qurt_signal_wait_timed() , qurt_thread_sleep() and related APIs instead. See the QuRT user guide for more information. No access to L2 cache locking APIs Thread limitations Thread priority ceiling (highest priority for any unsigned PD thread): 64. Note that this limitation is no longer applicable on Lahaina and later targets. Instead the priority limit applies to unprivileged clients , whether they use signed or unsigned PDs; privileged clients have no such limit. Maximum number of threads allowed per unsigned PD: 128 NOTE: These limitations might change in the future. Request signature-free offload To request a signature-free dynamic module offload, clients make the request as follows: #pragma weak remote_session_control if (remote_session_control) { struct remote_rpc_control_unsigned_module data; data.enable = 1; data.domain = CDSP_DOMAIN_ID; remote_session_control(DSPRPC_CONTROL_UNSIGNED_MODULE, (void*)&data, sizeof(data)); } This request must be made before calling any other FastRPC function. A success message from the remote_session_control() function allows the client to offload the dynamic shared object to the cDSP without signing. IDL Compiler Interfaces for the DSP platform and all FastRPC programs are described in a language called IDL. IDL allows interface authors to expose only what that object does, but not where it resides or the programming language in which it is implemented. IDL provides flexibility of software implementation while maintaining a consistent interface for the software module. Following is a typical IDL header file. #include \"AEEStdDef.idl\" #include \"remote.idl\" interface calculator : remote_handle64 { long sum(in sequence<long> vec, rout long long res); long max(in sequence<long> vec, rout long res); }; When using the function parameters: Indicate input parameters as in . Indicate parameters to be modified as output as rout . For more information on the concept and use of the IDL compiler, see the IDL reference page . For information on how to define the interface, generate the stub and skel, and link and test the application, see the calculator example . Multi-domain Snapdragon products include multiple Hexagon DSPs (for example, cDSP, aDSP, mDSP, or sDSP). On many targets, more than one of these domains is available to FastRPC CPU user processes. A domain is a remote environment (DSP) for loading and executing code. Linking the CPU user process to the FastRPC user library for a specific DSP (like libcdsprpc.so for the cDSP) allows the CPU user process to offload to the cDSP only. To offload modules to multiple domains from the same CPU user process, use the multi-domain feature. The multi-domain framework provides the following benefits over the single domain framework: The CPU user process does not need to choose which DSP to use at link time. Instead, it can link to libadsprpc.so and specify the domain later at runtime. The CPU user process can query the target for its domain capabilities and select the domain with the required capabilities. The CPU user process can open multiple concurrent sessions on different DSPs. The multi-domain session is handle-based, allowing the CPU user process to restart a crashed DSP session by closing the handle using interface_close () and reopening a new handle. Upon closing the handle, the framework calls the user-written deinitialization function, which allows you to clean up any resource being used. (This process is referred to as session restart or PD restart.) The handle for a session is passed to each interface API and can easily be associated to user-defined context stored in the DSP memory. This allows an application to easily access data that is persistent across FastRPC calls. To see how to use multi-domain feature, see the calculator example . FastRPC threads and processes A separate user PD is created on the DSP for each HLOS user process that runs on the CPU and establishes a FastRPC session with the DSP. Each user process or thread on the CPU has a corresponding user PD or thread on the DSP. The user PD on DSP is created when the device is opened on the CPU, and it is destroyed when the device is closed on the CPU. A shell executable is loaded on the DSP when a user PD on the DSP is spawned. When an RPC message is invoked from a CPU thread and no corresponding thread exists on the DSP, the required thread is created on the DSP. The threads are destroyed when the corresponding CPU thread exits. The CPU user process can use the remote_session_control API that allows clients to configure the stack size and priority of all threads in their user PD on the DSP. Clients can use the following API and data structure (exposed in the remote.h header) to configure their DSP thread parameters: struct remote_rpc_thread_params { int domain; int prio; int stack_size; }; Handling exceptions SSR The DSP subsystem enters a SubSystem Restart (SSR) state and FastRPC returns AEE_ECONNRESET when a non-recoverable exception occurs: in a kernel of the DSP subsystem in a critical process. A process can be spawned as critical by setting a debug attribute as follows from the terminal window each time before launching the application: adb shell setprop vendor.fastrpc.process.attrs 1 PD restart The following PDs have the capability to be restarted without disrupting other user PDs running on the same subsystem. The new PD will be spawned for the same HLOS process that was used before. Static PDs on the audio and sensor domains, which all support Protection Domain Restart (PDR) Dynamic user PDs on the cDSP, which all have their own user space handles When a user PD on the cDSP incurs a non-recoverable exception, FastRPC returns either AEE_ENOSUCH or AEE_EBADSTATE + DSP_OFFSET . These error codes allow the client to take an appropriate action. An example of code catching these errors and restarting the session is illustrated in calculator_test.c as part of the calculator example. Restrictions This API must be called before making any other RPC call from your CPU process: this call must be the first call from your application. The thread stack size is to be between 16 KB and 8 MB. The DSP thread priority must be between 1 and 254 (with 1 being the highest). We do not recommend using priority 255, because you be preempted by interrupts even if other threads are idle. 255 is basically treated as the idle priority. On SM8250 devices, the unsigned PDs are restricted to a maximum thread priority of 64. On SM8350 and later devices, all unprivileged PDs are restricted to a maximum thread priority of 64. For details, see system integration page . Currently, we do not support individual thread configuration. That is, if you update the thread priority and stack size once, the same parameters will be used for all the FastRPC threads in that DSP user PD. After the first RPC call is made, the FastRPC thread parameters cannot be updated again, and they will persist for the lifetime of the DSP process. You can always spawn worker threads anytime from the DSP with different stack sizes and priorities. Example Set the priority to 100 and the stack size to 4 MB on the cDSP: #include \"remote.h\" ... main() { struct remote_rpc_thread_params th_data; th_data.domain = CDSP_DOMAIN_ID; th_data.stack_size = 4*1024*1024; th_data.prio = 100; nErr = remote_session_control(FASTRPC_THREAD_PARAMS, (void*)&th_data, sizeof(th_data)); if (nErr) { printf(\"ERROR 0x%x: remote_session_control failed to set thread params\\n\", nErr); } else { printf(\" - Successfully set CDSP user process thread parameter(s)\\n\"); } .... my_first_RPC_call(); } Status notifications of DSP User process In targets after LAHAINA, the CPU user process can use the remote_session_control API that allows clients to register for the status notifications of the DSP User process. Clients can use this API and following data structure (exposed in the remote.h header) to register for status notifications of DSP user process. typedef struct remote_rpc_notif_register { void *context; int domain; fastrpc_notif_fn_t notifier_fn; } remote_rpc_notif_register_t; Following notifications are supported for DSP User process. // DSP User PD status notification flags typedef enum remote_rpc_status_flags { /* DSP user process is up */ FASTRPC_USER_PD_UP = 0, /* DSP user process exited */ FASTRPC_USER_PD_EXIT = 1, /* DSP user process forcefully killed. Happens when DSP resources needs to be freed. */ FASTRPC_USER_PD_FORCE_KILL = 2, /* Exception in the user process of DSP. */ FASTRPC_USER_PD_EXCEPTION = 3, /* Subsystem restart of the DSP, where user process is running. */ FASTRPC_DSP_SSR = 4, } remote_rpc_status_flags_t; Example Register for Status notifications of DSP User process. #include \"remote.h\" ... int fastrpc_notif_dsp(void *context, int domain, int session, remote_rpc_status_flags_t status) { ... } ... main() { struct remote_rpc_notif_register status_notif; status_notif.context = (void *)<NOTIF_CONTEXT>; status_notif.domain = domain; status_notif.notifier_fn = fastrpc_notif_dsp; nErr = remote_session_control(FASTRPC_REGISTER_STATUS_NOTIFICATIONS, (void*)&status_notif, sizeof(status_notif)); if (nErr) { printf(\"ERROR 0x%x: remote_session_control failed to register for status notifications of DSP process\\n\", nErr); } else { printf(\" - Successfully registered for status notifications of DSP process \\n\"); } .... my_RPC_call(); } Asynchronous FastRPC FastRPC is a remote procedure call framework for CPU clients to efficiently offload tasks to the DSP. FastRPC is synchronous by default: the caller thread on the CPU is blocked until the remote method execution on the DSP completes and returns with a result. This prevents the same client thread from doing additional processing while remote call in progress. Asynchronous FastRPC allows the caller to continue while the remote call is still ongoing. An asynchronous FastRPC call is queued to the DSP thread while control returns immediately to the calling thread with a job ID. The caller gets notified later when the call completes. The caller must process the notification and release the job. Refer to example in $HEXAGON_SDK_ROOT/addons/compute/docs/docs/examples/benchmark/README.md, to understand how to use async support. IDL async support Please refer to the IDL documentation for more details on how to declare an asynchronous FastRPC function. Async Descriptor A FastRPC async descriptor is made of a of notification type, job ID and callback: struct fastrpc_async_descriptor { enum fastrpc_async_notify_type type; /**< asynchronous notification type */ fastrpc_async_jobid jobid; /**< jobid returned in async remote invocation call */ union { struct fastrpc_async_callback cb; /**< call back function filled by user */ } } fastrpc_async_descriptor_t; fastrpc_async_notify_type is used to notify the caller of an async FastRPC call completion, notifications are one of the following types: Callback notification User poll for job completion No notification required fastrpc_async_notify_type is defined in remote.h : enum fastrpc_async_notify_type{ FASTRPC_ASYNC_NO_SYNC = 0, /**< No notification asynchronous call*/ FASTRPC_ASYNC_CALLBACK, /**< asynchronous call with response with call back */ FASTRPC_ASYNC_POLL, /**< asynchronous call with polling */ FASTRPC_ASYNC_TYPE_MAX, /**< reserved */ }; A client making an asynchronous FastRPC is responsible for initializing the async descriptor with the required type of notification and pass it as an argument to the stub method. An async call will return 0 on successful submission of the job to the DSP and update jobid field in the async descriptor. Async call will return non-zero value (error code) in case of failure to submit the job. fastrpc_async_callback is made of a function pointer to the callback function and a context pointer: typedef struct fastrpc_async_callback { void (*fn)(fastrpc_async_jobid jobid, void* context, int result);/**< call back function */ void *context; /**< unique context filled by user*/ }fastrpc_async_callback_t; Notification declaration Callback notification: User needs to put the callback function in the descriptor struct fastrpc_async_descriptor desc; desc.type = FASTRPC_ASYNC_CALLBACK; desc.cb.fn= fn; desc.cb.context = ptr; Async call completion is notified to user with a callback function shared in the descriptor. User shall process the notification and release the jobid. Release function can be called within a callback function also. One worker thread is used for notifying all async jobs. Hence it is recommended to keep processing inside a callback function to a minimum. void foo_async_callback(fastrpc_async_jobid job_id, void* context, int result) { /* Callback function for jobid */ /* Release jobid */ } User poll for job completion: User needs to use FASTRPC_ASYNC_POLL for notify_type struct fastrpc_async_descriptor desc; desc.type = FASTRPC_ASYNC_POLL; The FastRPC library does not notify call completion to user for the jobs submitted with FASTRPC_ASYNC_POLL type. Instead, the user needs to poll for the status of the job using fastrpc_async_get_status function and release jobid after its use. Blocking poll and non-blocking poll with timeout are supported. The fastrpc_async_get_status function returns 0 on completion of the job. Actual return value (success/error/return value) of the remote call will be updated in &result if the job status is completed. /* Get current status of an async call with jobid. * * @param jobid, jobid returned in descriptor during successfull async call * @param timeout_us, timeout in micro seconds * timeout = 0, returns immediately with status/result * timeout > 0, wait for completion with timeout in micro-sec * timeout < 0. wait indefinitely. Block thread until job completion. * @param result, integer pointer for the result of the job * 0 on success * error code on failure * @retval, 0 on job completion and result of job is part of @param result * AEE_EBUSY, if job status is pending and is not returned from the DSP * AEE_EBADPARM, if job id is invalid * AEE_EFAILED, FastRPC internal error */ int fastrpc_async_get_status(fastrpc_async_jobid jobid, int timeout_us, int *result); No notification required: User needs to use FASTRPC_ASYNC_NO_SYNC for notify_type struct fastrpc_async_descriptor desc; desc.type = FASTRPC_ASYNC_NO_SYNC; In this mode, no notification are sent to user after completion of the job with the result of either success or failure. The FastRPC library will ignore the result and automatically release the jobid after completion. User is not required to book keep any of the data associated with this type of job after successful submission. Release Async JobID Client shall release jobid after receiving call completion notification with user poll or callback. The release allows the framework to clean internal states associated with the async jobid. AEE_PENDING will be returned if the release function called before the job completion. /* Release Async job. Release async job after receiving status either through callback/poll * * @param jobid, jobid returned during Async job submission. * @retval, 0 on success * AEE_EBUSY, if job status is pending and is not yet returned from the DSP * AEE_EBADPARM, if job id is invalid */ int fastrpc_release_async_job(fastrpc_async_jobid jobid); NOTE: This function can be called within the callback function also Remote File system The DSP does not have its own file system, so it uses the CPU remote file system to read shared object files. Prebuilt shared objects and the FastRPC shell are present in the remote file system. Any client libraries that are to be loaded in the DSP user PD are also read by the FastRPC framework from the remote file system. On Android builds, the remote file system is implicitly implemented by the user mode library: libadsprpc.so or libcdsprpc.so . It uses the calling process's context to open files. Search path on remote file system On Android, the default file system search directory, PATH , is /vendor/lib/rfsa/dsp;/vendor/dsp . The optional environment variable, DSP_LIBRARY_PATH , can be used to prepend a user-specified path to the default file system search directory. It contains a list of directories to search when dlopen(\"library\") is called. If multiple versions of a file are found in the DSP search path, it will choose the one found earliest in the search path. The FastRPC framework first searches in the domain of the path, and then it searches in the path directly, for all the paths that are part of DSP_LIBRARY_PATH . If any shared objects are specific to a particular domain, they can be put under the domain directory of the search path. See examples below. The SM8250 and later targets support the optional DSP_LIBRARY_PATH environment variable. If DSP_LIBRARY_PATH is not defined, the FastRPC framework looks for the ADSP_LIBRARY_PATH environment variable. For older targets, it checks only for the ADSP_LIBRARY_PATH environment variable. The ADSP_LIBRARY_PATH environment variable replaces the default path, while DSP_LIBRARY_PATH prepends to the default search path. For more information on DSP_LIBRARY_PATH , see the next section. Using DSP_LIBRARY_PATH DSP_LIBRARY_PATH is a ; delimited variable used for specifying search paths on the remote file system. Use the following instructions to set the DSP_LIBRARY_PATH : adb shell To set up a single directory for DSP_LIBRARY_PATH : export DSP_LIBRARY_PATH = \u201cfoo\u201d To set up multiple directories for DSP_LIBRARY_PATH : export DSP_LIBRARY_PATH = \u201cfoo; bar\u201d To set up multiple directories for DSP_LIBRARY_PATH , including the current process directory: export DSP_LIBRARY_PATH = \u201c; foo; bar\u201d NOTE: The client application can call setenv() to set this variable from within the program as well. Default value When DSP_LIBRARY_PATH is not set, a default value for the module directory is used. This value is specific to the CPU OS. On Android, the path is /vendor/lib/rfsa/dsp;/vendor/dsp . On Windows, the path is c:\\Program Files\\Qualcomm\\RFSA\\aDSP . On LE the, path is /dsp;/usr/lib/rfsa/adsp . Usage examples Assumptions: The CPU user process is using the cDSP domain The default DSP_LIBRARY_PATH is used The following paths will be searched for the shared objects in this order: /vendor/lib/rfsa/dsp/cdsp/ /vendor/lib/rfsa/dsp/ /vendor/dsp/cdsp/ /vendor/dsp/ Assumptions: The CPU user process is using the cDSP domain DSP_LIBRARY_PATH = \u201cfoo;bar\u201d The following paths will be searched for the shared objects in this order: foo/cdsp/ foo/ bar/cdsp/ bar/ /vendor/lib/rfsa/dsp/cdsp/ /vendor/lib/rfsa/dsp/ /vendor/dsp/cdsp/ /vendor/dsp/ Assumptions: The CPU user process is using the cDSP domain DSP_LIBRARY_PATH = \u201c;foo;bar\u201d The following paths will be searched for the shared objects in this order: ./cdsp/ ./ foo/cdsp/ foo/ bar/cdsp/ bar/ /vendor/lib/rfsa/dsp/cdsp/ /vendor/lib/rfsa/dsp/ /vendor/dsp/cdsp/ /vendor/dsp/ Dynamic loading The Hexagon SDK provides the tools and services to create and execute custom code on the DSP via dynamic shared objects. Dynamic shared objects allow for customization of the DSP image at runtime without the need to rebuild the DSP image. They also allow for DSP code to be added or removed based on runtime needs. Dynamic shared objects are analogous to Linux SO and Windows DLL files. They are implemented as ELF files, and in the CPU file system, they exist as files that are loaded by the DSP via an interprocessor communication mechanism. Once loaded, all symbols publicly exported by the shared object can be referenced or called. The creation of shared objects is supported by the Hexagon Tools. For more information about the structure and limitations of dynamic shared objects, see the Hexagon Application Binary Interface (ABI) User Guide . Dynamic loading is supported as follows: Within FastRPC invocation The FastRPC framework dynamically loads the necessary skel libraries and their dependencies from the remote file system when the first method is called from each interface. FastRPC will also create a new dynamic user PD on the DSP during the first invocation from an HLOS user process. Outside FastRPC invocation Static code on the DSP can use dlopen() to initiate shared object loading. In this case, vendor-side daemons are provided with the Android image to support the dynamic loading for specific domains. This mechanism is used by the Audio and Sensor static code. The /vendor/bin/adsprpcd (for Audio) and /vendor/bin/sdsprpcd (for Sensor) daemons on the CPU must be up and running to support dynamic loading in the static audio and sensor environment on the DSP. Memory management This section discusses the Hexagon DSP memory management model for FastRPC-based applications: the types of memory available, memory management hardware, and memory performance implications that you must be aware of. Hardware overview A typical Qualcomm Snapdragon SoC includes several processors and other hardware cores, all with access to the same external system memory. All such processors and cores, including the Hexagon DSP, access memory via one or more memory management units (MMUs). These MMUs control which parts of the memory the different cores and software running on them can access. The following diagram illustrates a simple example of the main CPU, a Hexagon DSP, and a third hardware core. The Hexagon DSP, when implemented as a compute or application DSP, uses two different MMUs: An internal MMU inside the DSP, managed by the DSP operating system An external System MMU (SMMU) between the DSP and system buses and the external memory, managed by the application processor (CPU) All accesses to external memory and hardware registers are via the SMMU. However, accesses to internal DSP memories (such as VTCM) and DSP subsystem registers are not via the SMMU. By default, each processor and hardware core is isolated to its own memory space, and cannot access memory belonging to other cores. It is possible however to allocate and map shared memory to share data efficiently between multiple cores. For the Hexagon DSP this can be done with FastRPC APIs as discussed below . MMUs and address spaces The following diagram represents the three different address spaces on the Hexagon DSP that you should know about, and the MMUs used to translate between them. Each process on the DSP runs in its own Virtual Address space (DSP VA). The DSP VA is managed by QuRT, the DSP RTOS , ensuring that different processes are isolated from each other and from the operating system itself. The Hexagon DSP uses a 32-bit virtual address space, limiting each process to 4 GB of address space. If a process attempts to access memory that is not mapped to the DSP MMU, the DSP-side process is killed with a page fault. Memory accesses going out from the Hexagon DSP towards the system are in the DSP Physical Address space (DSP PA). This space is not the same as the overall SoC physical address space; rather, DSP PAs are used as input addresses to the SMMU, which translates them to system physical addresses. The SMMU is used for two purposes: to control what memory the DSP can access, and to give the DSP a contiguous view of physically non-contiguous buffers. The SMMU is managed by the main application CPU. If the DSP attempts to access memory that is not mapped to the SMMU, the access results in an SMMU page fault handled by the CPU, which typically results in a system crash. Finally, memory accesses going out from the SMMU to the system buses and external memory are in the System Physical Address space (System PA). This address space is global across the entire SoC. FastRPC memory management As discussed above , each FastRPC client process runs its DSP code in a separate dynamic user PD. Each such PD is a separate process with its own virtual address space, separate from other DSP processes and its CPU-side counterpart. The user PD starts with some local memory available, and it can get access to more memory in two ways: Heap allocations Standard C/C++ heap allocation operations (such as malloc() and operator new ) on the DSP use a custom heap manager that requests more memory from the CPU as needed. This memory is automatically mapped to the process's address space, but it is not accessible to the CPU-side client. Shared buffers Client applications can allocate shared memory buffers and map them to the DSP either temporarily for the duration of an RPC call, or persistently with explicit map/unmap calls. Using shared buffers lets the CPU-side client application and its DSP counterpart share data efficiently without copying between the processors. For more information, see the allocate memory section . The FastRPC framework maps memory to the DSP MMU and the SMMU as needed. By default, FastRPC copies all arguments passed in a function call from CPU-accessible memory to DSP-accessible memory as part of the call. For small amounts of data, this is not a concern, but for large input/output buffers such as camera pictures or video frames, the copy can take a significant amount of time. To avoid copies, FastRPC clients should use shared ION buffers for all large input/output data buffers. Allocate memory for shared buffers On Android and other supported Linux platforms, shared memory buffers must be allocated with the ION allocator. The easiest approach is to use the RPCMEM library ; it automatically uses the correct ION APIs and registers buffers for FastRPC. If using RPCMEM is not possible, for example, when the buffers are allocated by a different framework, clients can use the remote_register_buf() function that is defined as part of the remote API : remote_register_buf(buffer, size, fd); // register remote_register_buf(buffer, size, -1); // unregister Transient shared buffers FastRPC automatically recognizes shared buffers in function calls based on their address. Buffers allocated with the RPCMEM library are automatically used as shared buffers; other ION buffers must first be registered using remote_register_buf() , as discussed above. The buffer address and size passed to the remote_register_buf() call must match the values passed to the RPC call where they are used, otherwise the buffer will not be recognized. FastRPC automatically maps shared buffers used as function arguments to the SMMU and DSP MMU for the duration of the call, and it passes the resulting DSP VA as an argument to the DSP-side function. Once the function call returns, FastRPC automatically unmaps the buffers. As a result, the pointers will not be valid on the DSP side after the call returns, and the same buffer might receive a different address during subsequent calls. FastRPC also handles cache maintenance operations automatically for shared buffers used as RPC function call parameters. Persistent shared buffers using dmahandle objects Shared buffers mapped automatically during FastRPC calls are only mapped for the duration of the call. Clients that need to map a buffer persistently across function calls can pass buffers to the DSP as dmahandle objects and manage their mapping lifetimes manually. These dmahandle -based persistent shared buffers must be allocated from ION similarly to regular transient shared buffers, but they are passed to the DSP with a separate function call: interface example { AEEResult map_buffer(in dmahandle buffer); AEEResult unmap_buffer(); }; //... buffer = rpcmem_alloc(RPCMEM_HEAP_ID_SYSTEM, RPCMEM_DEFAULT_FLAGS, size); fd = rpcmem_to_fd(buffer); example_map_buffer(fd, 0, size); // ... Use buffer ... example_unmap_buffer(); NOTE: A dmahandle parameter in the IDL interface is converted into three parameters in the generated C interface. For details, see the IDL reference page . On the DSP side, the implementation can use HAP_mmap() to map the buffer to the process, and HAP_munmap() to unmap the buffer once done. Both functions are documented as part of HAP memory management APIs : AEEResult example_map_buffer(int bufferfd, uint32 bufferoffset, uint32 bufferlen) { //... buffer = HAP_mmap(NULL, bufferlen, HAP_PROT_READ|HAP_PROT_WRITE, 0, bufferfd, 0); } AEEResult example_unmap_buffer(void) { //... HAP_munmap(buffer, size); } If the buffer is used to share data between the CPU and the DSP between map/unmap operations, the application must also perform cache maintenance operations manually: On the DSP side, use the qurt_mem_cache_clean() function to flush or invalidate the cache as needed. For details, see the QuRT documentation . On the CPU side, cache maintenance is not required on platforms that support I/O coherency (to determine whether I/O coherency is supported, see the feature matrix . On other platforms, standard Arm cache maintenance operations might be available, but we do not recommend using persistent shared buffers for sharing data between the CPU and DSP without I/O coherency support. See the fcvqueue library in $HEXAGON_SDK_ROOT/examples/asyncdspq_sample/ for an example on how to use dmahandle objects to persistently map buffers to the DSP and how perform cache maintenance operations on the DSP. Persistent shared buffers using fastrpc_mmap Shared buffers mapped automatically during FastRPC calls are only mapped for the duration of the call. Clients that need to map a buffer persistently across function calls can map them using fastrpc_mmap() and manage the lifetime of the mapping manually. The fastrpc_mmap() API provides similar options as dmahandle type of object discussed in the previous section. It provides a unified interface with control flags for explicitly mapping a buffer to the DSP. The fastrpc_mmap() API is available from Lahaina and later targets. It is recommended to use over dmahandle objects when available. Refer to fastrpc_mmap API for more information. On the CPU side, an application allocates an ION memory and maps it to the remote process on the DSP as follows. #include \"remote.h\" buffer = rpcmem_alloc(RPCMEM_HEAP_ID_SYSTEM, RPCMEM_DEFAULT_FLAGS, size); fd = rpcmem_to_fd(buffer); nErr = fastrpc_mmap(domain, fd, buffer, 0, size, FASTRPC_MAP_FD); // ... share fd with DSP for accessing the same buffer on DSP ... nErr = fastrpc_munmap(domain, fd, buffer, size); On the DSP side, the implementation can use HAP_mmap_get() to pass the buffer file descriptor in order to increment the reference to the buffer mapping and get the virtual address of the buffer, and HAP_mmap_put() to release the reference on the buffer once done. Both functions are documented as part of HAP memory management APIs : //... nErr = HAP_mmap_get(fd, &buffer, NULL); //... use buffer and release reference once done ... nErr = HAP_mmap_put(fd); If the buffer is used to share data between the CPU and the DSP between map/unmap operations, the application must also perform cache maintenance operations manually: On the DSP side, use the qurt_mem_cache_clean() function to flush or invalidate the cache as needed. For details, see the QuRT documentation . On the CPU side, cache maintenance is not required on platforms that support I/O coherency. To determine whether I/O coherency is supported, see the feature matrix . On other platforms, standard Arm cache maintenance operations might be available, but we do not recommend using persistent shared buffers for sharing data between the CPU and DSP without I/O coherency support. Performance implications TLB pressure The Hexagon DSP uses a software-managed Translation Lookaside Buffer (TLB) instead of a full hardware MMU, making TLB misses more expensive than many other processors. Because the number of TLB entries is limited in the TLB table, the TLB table can quickly fill up. From then on, each addition into the TLB table must be proceeded by the eviction of an existing TLB entry. This is referred to as TLB pressure . To reduce the TLB pressure, the Hexagon TLB accommodates various sizes of TLB entries, for example, 4k, 16k, 64k, 256k, 1 MB, 4 MB, 16 MB. Therefore, to optimize data usage for large buffers, it is important that these large buffers use a minimum number of TLB entries. Do this by allocating large contiguous buffers rather than many individual smaller ones, and maintain good data locality within those buffers. Even buffers that are non-contiguous in physical memory are visible to the DSP as contiguous through the SMMU. Shared buffers vs copying data By default, FastRPC copies all arguments passed in a function call from CPU-accessible memory to DSP-accessible memory as part of the function call. For large buffers, this can take a significant amount of time. To avoid copies, client applications should use shared ION buffers for all large input/output buffers, as discussed above in Allocating memory for shared buffers and Transient shared buffers . FastRPC debugging For generic instructions on how to debug an Android application making FastRPC calls to the DSP, see the Debug section. When you run into specific issues, you can also see the FastRPC debugging FAQs . FastRPC performance The latency of a FastRPC synchronous call is the amount of time spent from when the CPU thread initiates a call to the DSP until it can resume its operation, less the amount of time spent by the DSP to execute the task itself. Under optimized conditions, the FastRPC round-trip average latency is on the order of 100 to 700 microseconds on the targets with hardware IO coherency support. It is recommended to measure average FastRPC latency over multiple RPC calls instead of one call for consistent results as it depends on variable latencies like CPU wakeup and scheduler delays. The FastRPC latency is measured on Lahaina chipset for different workloads on the DSP are published in the tables below. Because CPU wake up delay is one of the significant contributors to FastRPC latency, performance is measured with and without FastRPC QoS (PM_QOS) mode with the DSP running at TURBO clocks. To understand the latencies and potential optimizations to FastRPC performance, see the discussion on the system-level optimizations section. The FastRPC latency for remote calls with no processing (NOP loopback) on DSP is profiled using the profiling application and published in the table below. The table captures the numbers collected from running the two ADB commands below. Chipset: Lahaina # FastRPC QoS on, ION buffers, IO coherent, DSP_clocks=TURBO adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f default -n 1000 -p -q 300 # FastRPC QoS off, ION buffers, IO coherent, DSP_clocks=TURBO adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f default -n 1000 -p -q 0 Buffer type Buffer size FastRPC latency for NOP loopback with QoS off (microseconds) FastRPC latency for NOP loopback with QoS on (microseconds) noop 0K 55 54 inbuf 32K 93 109 routbuf 32K 93 110 inbuf 64K 91 108 routbuf 64K 92 108 inbuf 128K 97 109 routbuf 128K 96 109 inbuf 1M 175 138 routbuf 1M 174 138 inbuf 4M 174 139 routbuf 4M 174 139 inbuf 8M 179 140 routbuf 8M 180 141 inbuf 16M 176 153 routbuf 16M 171 153 The FastRPC performance published in the table below is a sample measurement of some realistic DSP workloads, performed on the benchmark application example available in the compute add-on. Such performance may vary somewhat across devices, software builds, and trials, due to numerous factors. Chipset: Lahaina # With FastRPC QoS enabled, ION buffers, IO coherent, DSP_clocks=TURBO adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/benchmark -o /data/local/benchmark.csv -f epsilon -P 6 -L 1000 -l 1 -s -q # With FastRPC QoS disabled (default mode), ION buffers, IO coherent, DSP_clocks=TURBO adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/benchmark -o /data/local/benchmark.csv -f epsilon -P 6 -L 1000 -l 1 -s Benchmark Function Image size DSP skel method execution time (microseconds) FastRPC latency with QoS OFF (microseconds) FastRPC latency with QoS ON (microseconds) Epsilon 1920x1080 949 301 265 Bilateral 1920x1080 13259 303 275 Fast9 1920x1080 4232 189 177 Integrate 1920x1080 615 339 303 Dilate3x3 1920x1080 228 300 271 Dilate5x5 1920x1080 175 203 190 Conv3x3 1920x1080 237 297 284 Gaussian7x7 1920x1080 217 305 272 Sobel3x3 1920x1080 174 304 269 FFT 1024x1024 2767 309 270 Scatter 1920x1080 223 406 383 Gather 1920x1080 268 263 244 Histogram 1920x1080 128 274 256","title":"RPC"},{"location":"software/ipc/rpc.html#rpc","text":"A Remote Procedure Call (RPC) allows a computer program calling a procedure to execute in another remote processor, while hiding the details of the remote interaction. FastRPC is the RPC mechanism used to enable remote function calls between the CPU and DSP. Customers with algorithms that benefit from being executed on the DSP can use the FastRPC framework to offload large processing tasks onto the DSP. The DSP can then leverage its internal processing resources, such as HVX, to execute the tasks in a more compute- and power-efficient way than the CPU. FastRPC interfaces are defined in an IDL file, and they are compiled using the QAIC compiler to generate header files and stub and skel code. The header files and stub should be built and linked into the CPU executable while the header files and skel should be built and linked into the DSP library.","title":"RPC"},{"location":"software/ipc/rpc.html#fastrpc-architecture","text":"The following diagram depicts the major FastRPC software components on the CPU and DSP. Definition of the terms in the diagram: Term Description Application User mode process that initiates the remote invocation Stub Auto-generated code that takes care of marshaling parameters and runs on the CPU FastRPC user driver on CPU User mode library that is used by the stub code to do remote invocations FastRPC Kernel Driver Receives the remote invocations from the client, queues them up with the FastRPC DSP driver, and then waits for the response after signaling the remote side FastRPC DSP Driver Dequeues the messages sent by the FastRPC kernel driver and dispatches them for processing FastRPC user driver on DSP User mode code that includes a shell executable to run in the user protection domain (PD) on the DSP and complete the remote invocations to the skel library Skel Auto-generated code that un-marshals parameters and invokes the user-defined implementation of the function that runs on the DSP User PD User protection domain on the DSP that provides the environment to run the user code","title":"FastRPC architecture"},{"location":"software/ipc/rpc.html#fastrpc-workflow","text":"The FastRPC framework is a typical proxy pattern. The interface object stub and the implementation skeleton objects are on different processors. FastRPC clients are directly exposed to the stub object, and the skeleton object is called by the FastRPC framework on the DSP. The FastRPC framework consists of the following components. Workflow: The CPU process calls the stub version of the function. The stub code converts the function call to an RPC message. The stub code internally invokes the FastRPC framework on the CPU to queue the converted message. The FastRPC framework on the CPU sends the queued message to the FastRPC DSP framework on the DSP. The FastRPC DSP framework on the DSP dispatches the call to the relevant skeleton code. The skeleton code un-marshals the parameters and calls the method implementation. The skeleton code waits for the implementation to finish processing, and, in turn, marshals the return value and any other output arguments into the return message. The skeleton code calls the FastRPC DSP framework to queue the return message to be transmitted to the CPU. The FastRPC DSP framework on the DSP sends the return message back to the FastRPC framework on the CPU. The FastRPC framework identifies the waiting stub code and dispatches the return value. The stub code un-marshals the return message and sends it to the calling User mode process.","title":"FastRPC workflow"},{"location":"software/ipc/rpc.html#android-software-components","text":"The FastRPC framework consists of several libraries in the system and vendor partitions of the Android image. Software component Description /system/vendor/lib/lib*rpc.so or /vendor/lib/lib*rpc.so , where * is adsp , cdsp , or sdsp Shared object library to be linked with the user-space vendor application that is invoking the remote procedure call. This library interfaces with the kernel driver to initiate the remote invocation to the aDSP, cDSP, or sDSP. /system/lib/lib*rpc_system.so , where * is adsp , cdsp , or sdsp Shared object library that is to be linked with the user-space system application that is invoking the remote procedure call. This library interfaces with the kernel driver to initiate the remote invocation to the aDSP, cDSP, or sDSP. This library is applicable for system applications on Android P and onwards. NOTE: The /system/vendor path is only supported on Android O.","title":"Android software components"},{"location":"software/ipc/rpc.html#dsp-protection-domains","text":"Because the DSP is a real-time processor whose stability critically affects the overall user experience, different PDs exist in the DSP software architecture. These PDs ensure the stability of the kernel software and the safety of Qualcomm proprietary hardware information. There are various protection domains in the DSP. Kernel: Access to all memory of all PDs Guest OS: Access to the memory of its own PD, the memory of the user PD, and some system registers User: Access only to the memory of its own PD DSP system libraries make system calls to the Guest OS or Kernel as appropriate to access operating system services. FastRPC client programs run in user PDs.","title":"DSP protection domains"},{"location":"software/ipc/rpc.html#dynamic-vs-static-pd","text":"User PDs can be created statically at boot time, but they are more often created dynamically at run time by CPU applications that are to offload modules to the DSP. Both static and dynamic PDs support dynamic loading of shared objects.","title":"Dynamic vs static PD"},{"location":"software/ipc/rpc.html#static-pd","text":"Static PDs are created on DSPs to support specific use cases like Audio and Sensors. The static PDs allow the dynamic loading of shared objects with the help of a daemon running on the CPU. For more details, see Dynamic loading .","title":"Static PD"},{"location":"software/ipc/rpc.html#dynamic-user-pd","text":"Every CPU user process that uses a DSP via FastRPC will have a corresponding dynamic user PD on the DSP. The FastRPC Kernel driver manages the life cycle of the dynamic user PD. That is, if the CPU user process exits, the corresponding user PD on the DSP is cleaned up as well. The DSP supports different types of dynamic user PD environments, including signed and unsigned PDs. Each DSP can only support a limited number of concurrent dynamic user PDs. This limit depends on the hardware configuration for each supported chip and DSP; see the feature matrix .","title":"Dynamic user PD"},{"location":"software/ipc/rpc.html#signed-and-unsigned-pds","text":"DSPs support different types of dynamic execution environments (dynamic PDs) including signed and unsigned PDs. Signed PDs are available on all the DSPs, and they require that the modules (shared objects) being loaded in the PD are signed with a digital signature. This signature must be verified at the time of loading the shared objects in the PD. For details on signing a shared object, see Signing . On the other hand, unsigned PDs are only supported in the cDSP, and they allow the DSP modules to load without any digital signatures. An unsigned PD is a sandboxed low-rights process that allows the signature-free modules to run on the cDSP. In the event of a compromise, access to full system functionality and data is prevented by the sandbox. Unsigned PDs are designed to support general compute applications and have limited access to underlying drivers (see the following sections for available services and limitations). The available/unavailable services in the unsigned PD may change in the future, depending on continuing security reviews.","title":"Signed and unsigned PDs"},{"location":"software/ipc/rpc.html#unsigned-pd-support","text":"To check whether a device supports unsigned PDs, perform a capability query using the DSP attribute, UNSIGNED_PD_SUPPORT . For an example on how to make this query, see the calculator example .","title":"Unsigned PD support"},{"location":"software/ipc/rpc.html#unsigned-pd-available-services","text":"Thread creation and thread services HVX contexts Clock frequency controls VTCM Cache operations Map HLOS memory allocated by the corresponding HLOS application","title":"Unsigned PD available services"},{"location":"software/ipc/rpc.html#unsigned-pd-limitations","text":"Access to limited drivers: UBWCDMA and Camera Streamer are not available to unsigned PDs QuRT timer APIs are not available. Applications can use qurt_signal_wait_timed() , qurt_thread_sleep() and related APIs instead. See the QuRT user guide for more information. No access to L2 cache locking APIs Thread limitations Thread priority ceiling (highest priority for any unsigned PD thread): 64. Note that this limitation is no longer applicable on Lahaina and later targets. Instead the priority limit applies to unprivileged clients , whether they use signed or unsigned PDs; privileged clients have no such limit. Maximum number of threads allowed per unsigned PD: 128 NOTE: These limitations might change in the future.","title":"Unsigned PD limitations"},{"location":"software/ipc/rpc.html#request-signature-free-offload","text":"To request a signature-free dynamic module offload, clients make the request as follows: #pragma weak remote_session_control if (remote_session_control) { struct remote_rpc_control_unsigned_module data; data.enable = 1; data.domain = CDSP_DOMAIN_ID; remote_session_control(DSPRPC_CONTROL_UNSIGNED_MODULE, (void*)&data, sizeof(data)); } This request must be made before calling any other FastRPC function. A success message from the remote_session_control() function allows the client to offload the dynamic shared object to the cDSP without signing.","title":"Request signature-free offload"},{"location":"software/ipc/rpc.html#idl-compiler","text":"Interfaces for the DSP platform and all FastRPC programs are described in a language called IDL. IDL allows interface authors to expose only what that object does, but not where it resides or the programming language in which it is implemented. IDL provides flexibility of software implementation while maintaining a consistent interface for the software module. Following is a typical IDL header file. #include \"AEEStdDef.idl\" #include \"remote.idl\" interface calculator : remote_handle64 { long sum(in sequence<long> vec, rout long long res); long max(in sequence<long> vec, rout long res); }; When using the function parameters: Indicate input parameters as in . Indicate parameters to be modified as output as rout . For more information on the concept and use of the IDL compiler, see the IDL reference page . For information on how to define the interface, generate the stub and skel, and link and test the application, see the calculator example .","title":"IDL Compiler"},{"location":"software/ipc/rpc.html#multi-domain","text":"Snapdragon products include multiple Hexagon DSPs (for example, cDSP, aDSP, mDSP, or sDSP). On many targets, more than one of these domains is available to FastRPC CPU user processes. A domain is a remote environment (DSP) for loading and executing code. Linking the CPU user process to the FastRPC user library for a specific DSP (like libcdsprpc.so for the cDSP) allows the CPU user process to offload to the cDSP only. To offload modules to multiple domains from the same CPU user process, use the multi-domain feature. The multi-domain framework provides the following benefits over the single domain framework: The CPU user process does not need to choose which DSP to use at link time. Instead, it can link to libadsprpc.so and specify the domain later at runtime. The CPU user process can query the target for its domain capabilities and select the domain with the required capabilities. The CPU user process can open multiple concurrent sessions on different DSPs. The multi-domain session is handle-based, allowing the CPU user process to restart a crashed DSP session by closing the handle using interface_close () and reopening a new handle. Upon closing the handle, the framework calls the user-written deinitialization function, which allows you to clean up any resource being used. (This process is referred to as session restart or PD restart.) The handle for a session is passed to each interface API and can easily be associated to user-defined context stored in the DSP memory. This allows an application to easily access data that is persistent across FastRPC calls. To see how to use multi-domain feature, see the calculator example .","title":"Multi-domain"},{"location":"software/ipc/rpc.html#fastrpc-threads-and-processes","text":"A separate user PD is created on the DSP for each HLOS user process that runs on the CPU and establishes a FastRPC session with the DSP. Each user process or thread on the CPU has a corresponding user PD or thread on the DSP. The user PD on DSP is created when the device is opened on the CPU, and it is destroyed when the device is closed on the CPU. A shell executable is loaded on the DSP when a user PD on the DSP is spawned. When an RPC message is invoked from a CPU thread and no corresponding thread exists on the DSP, the required thread is created on the DSP. The threads are destroyed when the corresponding CPU thread exits. The CPU user process can use the remote_session_control API that allows clients to configure the stack size and priority of all threads in their user PD on the DSP. Clients can use the following API and data structure (exposed in the remote.h header) to configure their DSP thread parameters: struct remote_rpc_thread_params { int domain; int prio; int stack_size; };","title":"FastRPC threads and processes"},{"location":"software/ipc/rpc.html#handling-exceptions","text":"","title":"Handling exceptions"},{"location":"software/ipc/rpc.html#ssr","text":"The DSP subsystem enters a SubSystem Restart (SSR) state and FastRPC returns AEE_ECONNRESET when a non-recoverable exception occurs: in a kernel of the DSP subsystem in a critical process. A process can be spawned as critical by setting a debug attribute as follows from the terminal window each time before launching the application: adb shell setprop vendor.fastrpc.process.attrs 1","title":"SSR"},{"location":"software/ipc/rpc.html#pd-restart","text":"The following PDs have the capability to be restarted without disrupting other user PDs running on the same subsystem. The new PD will be spawned for the same HLOS process that was used before. Static PDs on the audio and sensor domains, which all support Protection Domain Restart (PDR) Dynamic user PDs on the cDSP, which all have their own user space handles When a user PD on the cDSP incurs a non-recoverable exception, FastRPC returns either AEE_ENOSUCH or AEE_EBADSTATE + DSP_OFFSET . These error codes allow the client to take an appropriate action. An example of code catching these errors and restarting the session is illustrated in calculator_test.c as part of the calculator example.","title":"PD restart"},{"location":"software/ipc/rpc.html#restrictions","text":"This API must be called before making any other RPC call from your CPU process: this call must be the first call from your application. The thread stack size is to be between 16 KB and 8 MB. The DSP thread priority must be between 1 and 254 (with 1 being the highest). We do not recommend using priority 255, because you be preempted by interrupts even if other threads are idle. 255 is basically treated as the idle priority. On SM8250 devices, the unsigned PDs are restricted to a maximum thread priority of 64. On SM8350 and later devices, all unprivileged PDs are restricted to a maximum thread priority of 64. For details, see system integration page . Currently, we do not support individual thread configuration. That is, if you update the thread priority and stack size once, the same parameters will be used for all the FastRPC threads in that DSP user PD. After the first RPC call is made, the FastRPC thread parameters cannot be updated again, and they will persist for the lifetime of the DSP process. You can always spawn worker threads anytime from the DSP with different stack sizes and priorities.","title":"Restrictions"},{"location":"software/ipc/rpc.html#example","text":"Set the priority to 100 and the stack size to 4 MB on the cDSP: #include \"remote.h\" ... main() { struct remote_rpc_thread_params th_data; th_data.domain = CDSP_DOMAIN_ID; th_data.stack_size = 4*1024*1024; th_data.prio = 100; nErr = remote_session_control(FASTRPC_THREAD_PARAMS, (void*)&th_data, sizeof(th_data)); if (nErr) { printf(\"ERROR 0x%x: remote_session_control failed to set thread params\\n\", nErr); } else { printf(\" - Successfully set CDSP user process thread parameter(s)\\n\"); } .... my_first_RPC_call(); }","title":"Example"},{"location":"software/ipc/rpc.html#status-notifications-of-dsp-user-process","text":"In targets after LAHAINA, the CPU user process can use the remote_session_control API that allows clients to register for the status notifications of the DSP User process. Clients can use this API and following data structure (exposed in the remote.h header) to register for status notifications of DSP user process. typedef struct remote_rpc_notif_register { void *context; int domain; fastrpc_notif_fn_t notifier_fn; } remote_rpc_notif_register_t; Following notifications are supported for DSP User process. // DSP User PD status notification flags typedef enum remote_rpc_status_flags { /* DSP user process is up */ FASTRPC_USER_PD_UP = 0, /* DSP user process exited */ FASTRPC_USER_PD_EXIT = 1, /* DSP user process forcefully killed. Happens when DSP resources needs to be freed. */ FASTRPC_USER_PD_FORCE_KILL = 2, /* Exception in the user process of DSP. */ FASTRPC_USER_PD_EXCEPTION = 3, /* Subsystem restart of the DSP, where user process is running. */ FASTRPC_DSP_SSR = 4, } remote_rpc_status_flags_t;","title":"Status notifications of DSP User process"},{"location":"software/ipc/rpc.html#example_1","text":"Register for Status notifications of DSP User process. #include \"remote.h\" ... int fastrpc_notif_dsp(void *context, int domain, int session, remote_rpc_status_flags_t status) { ... } ... main() { struct remote_rpc_notif_register status_notif; status_notif.context = (void *)<NOTIF_CONTEXT>; status_notif.domain = domain; status_notif.notifier_fn = fastrpc_notif_dsp; nErr = remote_session_control(FASTRPC_REGISTER_STATUS_NOTIFICATIONS, (void*)&status_notif, sizeof(status_notif)); if (nErr) { printf(\"ERROR 0x%x: remote_session_control failed to register for status notifications of DSP process\\n\", nErr); } else { printf(\" - Successfully registered for status notifications of DSP process \\n\"); } .... my_RPC_call(); }","title":"Example"},{"location":"software/ipc/rpc.html#asynchronous-fastrpc","text":"FastRPC is a remote procedure call framework for CPU clients to efficiently offload tasks to the DSP. FastRPC is synchronous by default: the caller thread on the CPU is blocked until the remote method execution on the DSP completes and returns with a result. This prevents the same client thread from doing additional processing while remote call in progress. Asynchronous FastRPC allows the caller to continue while the remote call is still ongoing. An asynchronous FastRPC call is queued to the DSP thread while control returns immediately to the calling thread with a job ID. The caller gets notified later when the call completes. The caller must process the notification and release the job. Refer to example in $HEXAGON_SDK_ROOT/addons/compute/docs/docs/examples/benchmark/README.md, to understand how to use async support.","title":"Asynchronous FastRPC"},{"location":"software/ipc/rpc.html#idl-async-support","text":"Please refer to the IDL documentation for more details on how to declare an asynchronous FastRPC function.","title":"IDL async support"},{"location":"software/ipc/rpc.html#async-descriptor","text":"A FastRPC async descriptor is made of a of notification type, job ID and callback: struct fastrpc_async_descriptor { enum fastrpc_async_notify_type type; /**< asynchronous notification type */ fastrpc_async_jobid jobid; /**< jobid returned in async remote invocation call */ union { struct fastrpc_async_callback cb; /**< call back function filled by user */ } } fastrpc_async_descriptor_t; fastrpc_async_notify_type is used to notify the caller of an async FastRPC call completion, notifications are one of the following types: Callback notification User poll for job completion No notification required fastrpc_async_notify_type is defined in remote.h : enum fastrpc_async_notify_type{ FASTRPC_ASYNC_NO_SYNC = 0, /**< No notification asynchronous call*/ FASTRPC_ASYNC_CALLBACK, /**< asynchronous call with response with call back */ FASTRPC_ASYNC_POLL, /**< asynchronous call with polling */ FASTRPC_ASYNC_TYPE_MAX, /**< reserved */ }; A client making an asynchronous FastRPC is responsible for initializing the async descriptor with the required type of notification and pass it as an argument to the stub method. An async call will return 0 on successful submission of the job to the DSP and update jobid field in the async descriptor. Async call will return non-zero value (error code) in case of failure to submit the job. fastrpc_async_callback is made of a function pointer to the callback function and a context pointer: typedef struct fastrpc_async_callback { void (*fn)(fastrpc_async_jobid jobid, void* context, int result);/**< call back function */ void *context; /**< unique context filled by user*/ }fastrpc_async_callback_t;","title":"Async Descriptor"},{"location":"software/ipc/rpc.html#notification-declaration","text":"Callback notification: User needs to put the callback function in the descriptor struct fastrpc_async_descriptor desc; desc.type = FASTRPC_ASYNC_CALLBACK; desc.cb.fn= fn; desc.cb.context = ptr; Async call completion is notified to user with a callback function shared in the descriptor. User shall process the notification and release the jobid. Release function can be called within a callback function also. One worker thread is used for notifying all async jobs. Hence it is recommended to keep processing inside a callback function to a minimum. void foo_async_callback(fastrpc_async_jobid job_id, void* context, int result) { /* Callback function for jobid */ /* Release jobid */ } User poll for job completion: User needs to use FASTRPC_ASYNC_POLL for notify_type struct fastrpc_async_descriptor desc; desc.type = FASTRPC_ASYNC_POLL; The FastRPC library does not notify call completion to user for the jobs submitted with FASTRPC_ASYNC_POLL type. Instead, the user needs to poll for the status of the job using fastrpc_async_get_status function and release jobid after its use. Blocking poll and non-blocking poll with timeout are supported. The fastrpc_async_get_status function returns 0 on completion of the job. Actual return value (success/error/return value) of the remote call will be updated in &result if the job status is completed. /* Get current status of an async call with jobid. * * @param jobid, jobid returned in descriptor during successfull async call * @param timeout_us, timeout in micro seconds * timeout = 0, returns immediately with status/result * timeout > 0, wait for completion with timeout in micro-sec * timeout < 0. wait indefinitely. Block thread until job completion. * @param result, integer pointer for the result of the job * 0 on success * error code on failure * @retval, 0 on job completion and result of job is part of @param result * AEE_EBUSY, if job status is pending and is not returned from the DSP * AEE_EBADPARM, if job id is invalid * AEE_EFAILED, FastRPC internal error */ int fastrpc_async_get_status(fastrpc_async_jobid jobid, int timeout_us, int *result); No notification required: User needs to use FASTRPC_ASYNC_NO_SYNC for notify_type struct fastrpc_async_descriptor desc; desc.type = FASTRPC_ASYNC_NO_SYNC; In this mode, no notification are sent to user after completion of the job with the result of either success or failure. The FastRPC library will ignore the result and automatically release the jobid after completion. User is not required to book keep any of the data associated with this type of job after successful submission.","title":"Notification declaration"},{"location":"software/ipc/rpc.html#release-async-jobid","text":"Client shall release jobid after receiving call completion notification with user poll or callback. The release allows the framework to clean internal states associated with the async jobid. AEE_PENDING will be returned if the release function called before the job completion. /* Release Async job. Release async job after receiving status either through callback/poll * * @param jobid, jobid returned during Async job submission. * @retval, 0 on success * AEE_EBUSY, if job status is pending and is not yet returned from the DSP * AEE_EBADPARM, if job id is invalid */ int fastrpc_release_async_job(fastrpc_async_jobid jobid); NOTE: This function can be called within the callback function also","title":"Release Async JobID"},{"location":"software/ipc/rpc.html#remote-file-system","text":"The DSP does not have its own file system, so it uses the CPU remote file system to read shared object files. Prebuilt shared objects and the FastRPC shell are present in the remote file system. Any client libraries that are to be loaded in the DSP user PD are also read by the FastRPC framework from the remote file system. On Android builds, the remote file system is implicitly implemented by the user mode library: libadsprpc.so or libcdsprpc.so . It uses the calling process's context to open files.","title":"Remote File system"},{"location":"software/ipc/rpc.html#search-path-on-remote-file-system","text":"On Android, the default file system search directory, PATH , is /vendor/lib/rfsa/dsp;/vendor/dsp . The optional environment variable, DSP_LIBRARY_PATH , can be used to prepend a user-specified path to the default file system search directory. It contains a list of directories to search when dlopen(\"library\") is called. If multiple versions of a file are found in the DSP search path, it will choose the one found earliest in the search path. The FastRPC framework first searches in the domain of the path, and then it searches in the path directly, for all the paths that are part of DSP_LIBRARY_PATH . If any shared objects are specific to a particular domain, they can be put under the domain directory of the search path. See examples below. The SM8250 and later targets support the optional DSP_LIBRARY_PATH environment variable. If DSP_LIBRARY_PATH is not defined, the FastRPC framework looks for the ADSP_LIBRARY_PATH environment variable. For older targets, it checks only for the ADSP_LIBRARY_PATH environment variable. The ADSP_LIBRARY_PATH environment variable replaces the default path, while DSP_LIBRARY_PATH prepends to the default search path. For more information on DSP_LIBRARY_PATH , see the next section.","title":"Search path on remote file system"},{"location":"software/ipc/rpc.html#using-dsp_library_path","text":"DSP_LIBRARY_PATH is a ; delimited variable used for specifying search paths on the remote file system. Use the following instructions to set the DSP_LIBRARY_PATH : adb shell To set up a single directory for DSP_LIBRARY_PATH : export DSP_LIBRARY_PATH = \u201cfoo\u201d To set up multiple directories for DSP_LIBRARY_PATH : export DSP_LIBRARY_PATH = \u201cfoo; bar\u201d To set up multiple directories for DSP_LIBRARY_PATH , including the current process directory: export DSP_LIBRARY_PATH = \u201c; foo; bar\u201d NOTE: The client application can call setenv() to set this variable from within the program as well.","title":"Using DSP_LIBRARY_PATH"},{"location":"software/ipc/rpc.html#default-value","text":"When DSP_LIBRARY_PATH is not set, a default value for the module directory is used. This value is specific to the CPU OS. On Android, the path is /vendor/lib/rfsa/dsp;/vendor/dsp . On Windows, the path is c:\\Program Files\\Qualcomm\\RFSA\\aDSP . On LE the, path is /dsp;/usr/lib/rfsa/adsp .","title":"Default value"},{"location":"software/ipc/rpc.html#usage-examples","text":"Assumptions: The CPU user process is using the cDSP domain The default DSP_LIBRARY_PATH is used The following paths will be searched for the shared objects in this order: /vendor/lib/rfsa/dsp/cdsp/ /vendor/lib/rfsa/dsp/ /vendor/dsp/cdsp/ /vendor/dsp/ Assumptions: The CPU user process is using the cDSP domain DSP_LIBRARY_PATH = \u201cfoo;bar\u201d The following paths will be searched for the shared objects in this order: foo/cdsp/ foo/ bar/cdsp/ bar/ /vendor/lib/rfsa/dsp/cdsp/ /vendor/lib/rfsa/dsp/ /vendor/dsp/cdsp/ /vendor/dsp/ Assumptions: The CPU user process is using the cDSP domain DSP_LIBRARY_PATH = \u201c;foo;bar\u201d The following paths will be searched for the shared objects in this order: ./cdsp/ ./ foo/cdsp/ foo/ bar/cdsp/ bar/ /vendor/lib/rfsa/dsp/cdsp/ /vendor/lib/rfsa/dsp/ /vendor/dsp/cdsp/ /vendor/dsp/","title":"Usage examples"},{"location":"software/ipc/rpc.html#dynamic-loading","text":"The Hexagon SDK provides the tools and services to create and execute custom code on the DSP via dynamic shared objects. Dynamic shared objects allow for customization of the DSP image at runtime without the need to rebuild the DSP image. They also allow for DSP code to be added or removed based on runtime needs. Dynamic shared objects are analogous to Linux SO and Windows DLL files. They are implemented as ELF files, and in the CPU file system, they exist as files that are loaded by the DSP via an interprocessor communication mechanism. Once loaded, all symbols publicly exported by the shared object can be referenced or called. The creation of shared objects is supported by the Hexagon Tools. For more information about the structure and limitations of dynamic shared objects, see the Hexagon Application Binary Interface (ABI) User Guide . Dynamic loading is supported as follows: Within FastRPC invocation The FastRPC framework dynamically loads the necessary skel libraries and their dependencies from the remote file system when the first method is called from each interface. FastRPC will also create a new dynamic user PD on the DSP during the first invocation from an HLOS user process. Outside FastRPC invocation Static code on the DSP can use dlopen() to initiate shared object loading. In this case, vendor-side daemons are provided with the Android image to support the dynamic loading for specific domains. This mechanism is used by the Audio and Sensor static code. The /vendor/bin/adsprpcd (for Audio) and /vendor/bin/sdsprpcd (for Sensor) daemons on the CPU must be up and running to support dynamic loading in the static audio and sensor environment on the DSP.","title":"Dynamic loading"},{"location":"software/ipc/rpc.html#memory-management","text":"This section discusses the Hexagon DSP memory management model for FastRPC-based applications: the types of memory available, memory management hardware, and memory performance implications that you must be aware of.","title":"Memory management"},{"location":"software/ipc/rpc.html#hardware-overview","text":"A typical Qualcomm Snapdragon SoC includes several processors and other hardware cores, all with access to the same external system memory. All such processors and cores, including the Hexagon DSP, access memory via one or more memory management units (MMUs). These MMUs control which parts of the memory the different cores and software running on them can access. The following diagram illustrates a simple example of the main CPU, a Hexagon DSP, and a third hardware core. The Hexagon DSP, when implemented as a compute or application DSP, uses two different MMUs: An internal MMU inside the DSP, managed by the DSP operating system An external System MMU (SMMU) between the DSP and system buses and the external memory, managed by the application processor (CPU) All accesses to external memory and hardware registers are via the SMMU. However, accesses to internal DSP memories (such as VTCM) and DSP subsystem registers are not via the SMMU. By default, each processor and hardware core is isolated to its own memory space, and cannot access memory belonging to other cores. It is possible however to allocate and map shared memory to share data efficiently between multiple cores. For the Hexagon DSP this can be done with FastRPC APIs as discussed below .","title":"Hardware overview"},{"location":"software/ipc/rpc.html#mmus-and-address-spaces","text":"The following diagram represents the three different address spaces on the Hexagon DSP that you should know about, and the MMUs used to translate between them. Each process on the DSP runs in its own Virtual Address space (DSP VA). The DSP VA is managed by QuRT, the DSP RTOS , ensuring that different processes are isolated from each other and from the operating system itself. The Hexagon DSP uses a 32-bit virtual address space, limiting each process to 4 GB of address space. If a process attempts to access memory that is not mapped to the DSP MMU, the DSP-side process is killed with a page fault. Memory accesses going out from the Hexagon DSP towards the system are in the DSP Physical Address space (DSP PA). This space is not the same as the overall SoC physical address space; rather, DSP PAs are used as input addresses to the SMMU, which translates them to system physical addresses. The SMMU is used for two purposes: to control what memory the DSP can access, and to give the DSP a contiguous view of physically non-contiguous buffers. The SMMU is managed by the main application CPU. If the DSP attempts to access memory that is not mapped to the SMMU, the access results in an SMMU page fault handled by the CPU, which typically results in a system crash. Finally, memory accesses going out from the SMMU to the system buses and external memory are in the System Physical Address space (System PA). This address space is global across the entire SoC.","title":"MMUs and address spaces"},{"location":"software/ipc/rpc.html#fastrpc-memory-management","text":"As discussed above , each FastRPC client process runs its DSP code in a separate dynamic user PD. Each such PD is a separate process with its own virtual address space, separate from other DSP processes and its CPU-side counterpart. The user PD starts with some local memory available, and it can get access to more memory in two ways: Heap allocations Standard C/C++ heap allocation operations (such as malloc() and operator new ) on the DSP use a custom heap manager that requests more memory from the CPU as needed. This memory is automatically mapped to the process's address space, but it is not accessible to the CPU-side client. Shared buffers Client applications can allocate shared memory buffers and map them to the DSP either temporarily for the duration of an RPC call, or persistently with explicit map/unmap calls. Using shared buffers lets the CPU-side client application and its DSP counterpart share data efficiently without copying between the processors. For more information, see the allocate memory section . The FastRPC framework maps memory to the DSP MMU and the SMMU as needed. By default, FastRPC copies all arguments passed in a function call from CPU-accessible memory to DSP-accessible memory as part of the call. For small amounts of data, this is not a concern, but for large input/output buffers such as camera pictures or video frames, the copy can take a significant amount of time. To avoid copies, FastRPC clients should use shared ION buffers for all large input/output data buffers.","title":"FastRPC memory management"},{"location":"software/ipc/rpc.html#allocate-memory-for-shared-buffers","text":"On Android and other supported Linux platforms, shared memory buffers must be allocated with the ION allocator. The easiest approach is to use the RPCMEM library ; it automatically uses the correct ION APIs and registers buffers for FastRPC. If using RPCMEM is not possible, for example, when the buffers are allocated by a different framework, clients can use the remote_register_buf() function that is defined as part of the remote API : remote_register_buf(buffer, size, fd); // register remote_register_buf(buffer, size, -1); // unregister","title":"Allocate memory for shared buffers"},{"location":"software/ipc/rpc.html#transient-shared-buffers","text":"FastRPC automatically recognizes shared buffers in function calls based on their address. Buffers allocated with the RPCMEM library are automatically used as shared buffers; other ION buffers must first be registered using remote_register_buf() , as discussed above. The buffer address and size passed to the remote_register_buf() call must match the values passed to the RPC call where they are used, otherwise the buffer will not be recognized. FastRPC automatically maps shared buffers used as function arguments to the SMMU and DSP MMU for the duration of the call, and it passes the resulting DSP VA as an argument to the DSP-side function. Once the function call returns, FastRPC automatically unmaps the buffers. As a result, the pointers will not be valid on the DSP side after the call returns, and the same buffer might receive a different address during subsequent calls. FastRPC also handles cache maintenance operations automatically for shared buffers used as RPC function call parameters.","title":"Transient shared buffers"},{"location":"software/ipc/rpc.html#persistent-shared-buffers-using-dmahandle-objects","text":"Shared buffers mapped automatically during FastRPC calls are only mapped for the duration of the call. Clients that need to map a buffer persistently across function calls can pass buffers to the DSP as dmahandle objects and manage their mapping lifetimes manually. These dmahandle -based persistent shared buffers must be allocated from ION similarly to regular transient shared buffers, but they are passed to the DSP with a separate function call: interface example { AEEResult map_buffer(in dmahandle buffer); AEEResult unmap_buffer(); }; //... buffer = rpcmem_alloc(RPCMEM_HEAP_ID_SYSTEM, RPCMEM_DEFAULT_FLAGS, size); fd = rpcmem_to_fd(buffer); example_map_buffer(fd, 0, size); // ... Use buffer ... example_unmap_buffer(); NOTE: A dmahandle parameter in the IDL interface is converted into three parameters in the generated C interface. For details, see the IDL reference page . On the DSP side, the implementation can use HAP_mmap() to map the buffer to the process, and HAP_munmap() to unmap the buffer once done. Both functions are documented as part of HAP memory management APIs : AEEResult example_map_buffer(int bufferfd, uint32 bufferoffset, uint32 bufferlen) { //... buffer = HAP_mmap(NULL, bufferlen, HAP_PROT_READ|HAP_PROT_WRITE, 0, bufferfd, 0); } AEEResult example_unmap_buffer(void) { //... HAP_munmap(buffer, size); } If the buffer is used to share data between the CPU and the DSP between map/unmap operations, the application must also perform cache maintenance operations manually: On the DSP side, use the qurt_mem_cache_clean() function to flush or invalidate the cache as needed. For details, see the QuRT documentation . On the CPU side, cache maintenance is not required on platforms that support I/O coherency (to determine whether I/O coherency is supported, see the feature matrix . On other platforms, standard Arm cache maintenance operations might be available, but we do not recommend using persistent shared buffers for sharing data between the CPU and DSP without I/O coherency support. See the fcvqueue library in $HEXAGON_SDK_ROOT/examples/asyncdspq_sample/ for an example on how to use dmahandle objects to persistently map buffers to the DSP and how perform cache maintenance operations on the DSP.","title":"Persistent shared buffers using dmahandle objects"},{"location":"software/ipc/rpc.html#persistent-shared-buffers-using-fastrpc_mmap","text":"Shared buffers mapped automatically during FastRPC calls are only mapped for the duration of the call. Clients that need to map a buffer persistently across function calls can map them using fastrpc_mmap() and manage the lifetime of the mapping manually. The fastrpc_mmap() API provides similar options as dmahandle type of object discussed in the previous section. It provides a unified interface with control flags for explicitly mapping a buffer to the DSP. The fastrpc_mmap() API is available from Lahaina and later targets. It is recommended to use over dmahandle objects when available. Refer to fastrpc_mmap API for more information. On the CPU side, an application allocates an ION memory and maps it to the remote process on the DSP as follows. #include \"remote.h\" buffer = rpcmem_alloc(RPCMEM_HEAP_ID_SYSTEM, RPCMEM_DEFAULT_FLAGS, size); fd = rpcmem_to_fd(buffer); nErr = fastrpc_mmap(domain, fd, buffer, 0, size, FASTRPC_MAP_FD); // ... share fd with DSP for accessing the same buffer on DSP ... nErr = fastrpc_munmap(domain, fd, buffer, size); On the DSP side, the implementation can use HAP_mmap_get() to pass the buffer file descriptor in order to increment the reference to the buffer mapping and get the virtual address of the buffer, and HAP_mmap_put() to release the reference on the buffer once done. Both functions are documented as part of HAP memory management APIs : //... nErr = HAP_mmap_get(fd, &buffer, NULL); //... use buffer and release reference once done ... nErr = HAP_mmap_put(fd); If the buffer is used to share data between the CPU and the DSP between map/unmap operations, the application must also perform cache maintenance operations manually: On the DSP side, use the qurt_mem_cache_clean() function to flush or invalidate the cache as needed. For details, see the QuRT documentation . On the CPU side, cache maintenance is not required on platforms that support I/O coherency. To determine whether I/O coherency is supported, see the feature matrix . On other platforms, standard Arm cache maintenance operations might be available, but we do not recommend using persistent shared buffers for sharing data between the CPU and DSP without I/O coherency support.","title":"Persistent shared buffers using fastrpc_mmap"},{"location":"software/ipc/rpc.html#performance-implications","text":"","title":"Performance implications"},{"location":"software/ipc/rpc.html#tlb-pressure","text":"The Hexagon DSP uses a software-managed Translation Lookaside Buffer (TLB) instead of a full hardware MMU, making TLB misses more expensive than many other processors. Because the number of TLB entries is limited in the TLB table, the TLB table can quickly fill up. From then on, each addition into the TLB table must be proceeded by the eviction of an existing TLB entry. This is referred to as TLB pressure . To reduce the TLB pressure, the Hexagon TLB accommodates various sizes of TLB entries, for example, 4k, 16k, 64k, 256k, 1 MB, 4 MB, 16 MB. Therefore, to optimize data usage for large buffers, it is important that these large buffers use a minimum number of TLB entries. Do this by allocating large contiguous buffers rather than many individual smaller ones, and maintain good data locality within those buffers. Even buffers that are non-contiguous in physical memory are visible to the DSP as contiguous through the SMMU.","title":"TLB pressure"},{"location":"software/ipc/rpc.html#shared-buffers-vs-copying-data","text":"By default, FastRPC copies all arguments passed in a function call from CPU-accessible memory to DSP-accessible memory as part of the function call. For large buffers, this can take a significant amount of time. To avoid copies, client applications should use shared ION buffers for all large input/output buffers, as discussed above in Allocating memory for shared buffers and Transient shared buffers .","title":"Shared buffers vs copying data"},{"location":"software/ipc/rpc.html#fastrpc-debugging","text":"For generic instructions on how to debug an Android application making FastRPC calls to the DSP, see the Debug section. When you run into specific issues, you can also see the FastRPC debugging FAQs .","title":"FastRPC debugging"},{"location":"software/ipc/rpc.html#fastrpc-performance","text":"The latency of a FastRPC synchronous call is the amount of time spent from when the CPU thread initiates a call to the DSP until it can resume its operation, less the amount of time spent by the DSP to execute the task itself. Under optimized conditions, the FastRPC round-trip average latency is on the order of 100 to 700 microseconds on the targets with hardware IO coherency support. It is recommended to measure average FastRPC latency over multiple RPC calls instead of one call for consistent results as it depends on variable latencies like CPU wakeup and scheduler delays. The FastRPC latency is measured on Lahaina chipset for different workloads on the DSP are published in the tables below. Because CPU wake up delay is one of the significant contributors to FastRPC latency, performance is measured with and without FastRPC QoS (PM_QOS) mode with the DSP running at TURBO clocks. To understand the latencies and potential optimizations to FastRPC performance, see the discussion on the system-level optimizations section. The FastRPC latency for remote calls with no processing (NOP loopback) on DSP is profiled using the profiling application and published in the table below. The table captures the numbers collected from running the two ADB commands below. Chipset: Lahaina # FastRPC QoS on, ION buffers, IO coherent, DSP_clocks=TURBO adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f default -n 1000 -p -q 300 # FastRPC QoS off, ION buffers, IO coherent, DSP_clocks=TURBO adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/profiling -f default -n 1000 -p -q 0 Buffer type Buffer size FastRPC latency for NOP loopback with QoS off (microseconds) FastRPC latency for NOP loopback with QoS on (microseconds) noop 0K 55 54 inbuf 32K 93 109 routbuf 32K 93 110 inbuf 64K 91 108 routbuf 64K 92 108 inbuf 128K 97 109 routbuf 128K 96 109 inbuf 1M 175 138 routbuf 1M 174 138 inbuf 4M 174 139 routbuf 4M 174 139 inbuf 8M 179 140 routbuf 8M 180 141 inbuf 16M 176 153 routbuf 16M 171 153 The FastRPC performance published in the table below is a sample measurement of some realistic DSP workloads, performed on the benchmark application example available in the compute add-on. Such performance may vary somewhat across devices, software builds, and trials, due to numerous factors. Chipset: Lahaina # With FastRPC QoS enabled, ION buffers, IO coherent, DSP_clocks=TURBO adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/benchmark -o /data/local/benchmark.csv -f epsilon -P 6 -L 1000 -l 1 -s -q # With FastRPC QoS disabled (default mode), ION buffers, IO coherent, DSP_clocks=TURBO adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH DSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin/benchmark -o /data/local/benchmark.csv -f epsilon -P 6 -L 1000 -l 1 -s Benchmark Function Image size DSP skel method execution time (microseconds) FastRPC latency with QoS OFF (microseconds) FastRPC latency with QoS ON (microseconds) Epsilon 1920x1080 949 301 265 Bilateral 1920x1080 13259 303 275 Fast9 1920x1080 4232 189 177 Integrate 1920x1080 615 339 303 Dilate3x3 1920x1080 228 300 271 Dilate5x5 1920x1080 175 203 190 Conv3x3 1920x1080 237 297 284 Gaussian7x7 1920x1080 217 305 272 Sobel3x3 1920x1080 174 304 269 FFT 1024x1024 2767 309 270 Scatter 1920x1080 223 406 383 Gather 1920x1080 268 263 244 Histogram 1920x1080 128 274 256","title":"FastRPC performance"},{"location":"software/os/os_support_cpu.html","text":"CPU OS Build Support The Hexagon SDK provides support to build use cases for both Android and Linux--Linux Embedded (LE), Linaro, Ubuntu and OpenWRT--HLOS running on the application processor. The SDK includes libraries for both Android and Linux variants: Libraries for Android variants are located in folders starting with Android (e.g., Android_Debug ) Libraries for all supported Linux variants (LE, Linaro, Ubuntu and OpenWRT) are located in folders starting with Ubuntu (e.g., Ubuntu_Debug ) For example, see $HEXAGON_SDK_ROOT/ipc/fastrpc/remote/ship to view the libraries available for various OS configurations. In the remainder of this page, all Linux variants are referred to as Ubuntu. Cross-compilation tools for Linux are not provided with the Hexagon SDK. This page explains how to download, install, and use these tools to build examples for Ubuntu variants. Android executables The focus of this page is to provide support for building SDK examples for Ubuntu variants, because the Hexagon SDK already supports Android natively. For simple step-by-step instructions to build and run an Android executable, see the calculator example instructions . For an example on how to build an Android APK, please refer to the calculator C++ APK example . Ubuntu executables This section discusses the steps to cross-compile and build Ubuntu binaries for hard-float ARM ABI or soft-float ARM ABI, using a Windows or Linux machine on which the Hexagon SDK is installed. Linaro support The following instructions are for using the gcc-linaro-4.9 tools with which all Hexagon SDK examples are tested. Download the correct version of the tools based on the specified configuration. Configuration Hard-float ABI Soft-float ABI Windows tools to build 64-bit Linaro i686-mingw32_aarch64-linux-gnu N.A. Windows tools to build 32-bit Linaro i686-mingw32_arm-linux-gnueabihf i686-mingw32_arm-linux-gnueabi Linux tools to build 64-bit Linaro x86_64_aarch64-linux-gnu N.A. Linux tools to build 32-bit Linaro x86_64_arm-linux-gnueabihf x86_64_arm-linux-gnueabi Extract the tools under the $HEXAGON_SDK_ROOT/tools/ folder. Rename the tools folder as Linaro for all 32-bit Linaro flavors, or Linaro64 for all 64-bit Linaro flavors Build your projects with the following commands: Linaro flavor Hard-float ABI Soft-float ABI 64-bit Linaro make tree V=UbuntuARM_Debug_aarch64 make tree V=UbuntuARM_Debug_aarch64 SOFT_FLOAT=1 32-bit Linaro make tree V=UbuntuARM_Debug make tree V=UbuntuARM_Debug SOFT_FLOAT=1 NOTE: If you get the following error while building: Download libwinpthread-1.dll from this location , add the local path to this DLL into your SYSTEM PATH, and try again. LE and Ubuntu support For LE and Ubuntu support, follow the same procedure as described for Linaro, with the appropriate tools. Snap support A Snap is a package that holds the application along with its dependencies. It can run across Linux distributions without any modifications. Snapcraft is a framework that is used to build Snap. Users can build their own Snap using Snapcraft and publish them to the Snap store. The Snap store provides a place to upload snaps, and for users to browse and install the software they want. Follow these steps to build snaps from Hexagon SDK. OpenWRT support Windows machine Building OpenWRT binaries is not supported on Windows. Linux machine To build 64-bit or 32-bit OpenWRT executables: Follow these instructions to download and build the OpenWRT tools. Copy the tools folder, toolchain-<architecture> from <QSDK_BUILD_ROOT>/qsdk/staging_dir/ to {HEXAGON_SDK_ROOT}/tools/ . Rename the tools folder as OpenWRT for 32-bit OpenWRT flavors, or OpenWRT64 for 64-bit OpenWRT flavors. For 64-bit Ubuntu binaries, rename the toolchain-<architecture> folder to OpenWRT64 . Then try building with make tree V=UbuntuARM_Debug_aarch64 OPENWRT=1 . For 32-bit Ubuntu binaries, rename the toolchain-<architecture> folder to OpenWRT . Then try building with make tree V=UbuntuARM_Debug OPENWRT=1 . Build your projects with make tree V=UbuntuARM_Debug OPENWRT=1 for 32-bit flavors, and make tree V=UbuntuARM_Debug_aarch64 OPENWRT=1 for 64-bit flavors Run the project After the Ubuntu binaries are generated, follow the same steps as illustrated in the calculator example for signing, loading, and running the project.","title":"CPU OS build support"},{"location":"software/os/os_support_cpu.html#cpu-os-build-support","text":"The Hexagon SDK provides support to build use cases for both Android and Linux--Linux Embedded (LE), Linaro, Ubuntu and OpenWRT--HLOS running on the application processor. The SDK includes libraries for both Android and Linux variants: Libraries for Android variants are located in folders starting with Android (e.g., Android_Debug ) Libraries for all supported Linux variants (LE, Linaro, Ubuntu and OpenWRT) are located in folders starting with Ubuntu (e.g., Ubuntu_Debug ) For example, see $HEXAGON_SDK_ROOT/ipc/fastrpc/remote/ship to view the libraries available for various OS configurations. In the remainder of this page, all Linux variants are referred to as Ubuntu. Cross-compilation tools for Linux are not provided with the Hexagon SDK. This page explains how to download, install, and use these tools to build examples for Ubuntu variants.","title":"CPU OS Build Support"},{"location":"software/os/os_support_cpu.html#android-executables","text":"The focus of this page is to provide support for building SDK examples for Ubuntu variants, because the Hexagon SDK already supports Android natively. For simple step-by-step instructions to build and run an Android executable, see the calculator example instructions . For an example on how to build an Android APK, please refer to the calculator C++ APK example .","title":"Android executables"},{"location":"software/os/os_support_cpu.html#ubuntu-executables","text":"This section discusses the steps to cross-compile and build Ubuntu binaries for hard-float ARM ABI or soft-float ARM ABI, using a Windows or Linux machine on which the Hexagon SDK is installed.","title":"Ubuntu executables"},{"location":"software/os/os_support_cpu.html#linaro-support","text":"The following instructions are for using the gcc-linaro-4.9 tools with which all Hexagon SDK examples are tested. Download the correct version of the tools based on the specified configuration. Configuration Hard-float ABI Soft-float ABI Windows tools to build 64-bit Linaro i686-mingw32_aarch64-linux-gnu N.A. Windows tools to build 32-bit Linaro i686-mingw32_arm-linux-gnueabihf i686-mingw32_arm-linux-gnueabi Linux tools to build 64-bit Linaro x86_64_aarch64-linux-gnu N.A. Linux tools to build 32-bit Linaro x86_64_arm-linux-gnueabihf x86_64_arm-linux-gnueabi Extract the tools under the $HEXAGON_SDK_ROOT/tools/ folder. Rename the tools folder as Linaro for all 32-bit Linaro flavors, or Linaro64 for all 64-bit Linaro flavors Build your projects with the following commands: Linaro flavor Hard-float ABI Soft-float ABI 64-bit Linaro make tree V=UbuntuARM_Debug_aarch64 make tree V=UbuntuARM_Debug_aarch64 SOFT_FLOAT=1 32-bit Linaro make tree V=UbuntuARM_Debug make tree V=UbuntuARM_Debug SOFT_FLOAT=1 NOTE: If you get the following error while building: Download libwinpthread-1.dll from this location , add the local path to this DLL into your SYSTEM PATH, and try again.","title":"Linaro support"},{"location":"software/os/os_support_cpu.html#le-and-ubuntu-support","text":"For LE and Ubuntu support, follow the same procedure as described for Linaro, with the appropriate tools.","title":"LE and Ubuntu support"},{"location":"software/os/os_support_cpu.html#snap-support","text":"A Snap is a package that holds the application along with its dependencies. It can run across Linux distributions without any modifications. Snapcraft is a framework that is used to build Snap. Users can build their own Snap using Snapcraft and publish them to the Snap store. The Snap store provides a place to upload snaps, and for users to browse and install the software they want. Follow these steps to build snaps from Hexagon SDK.","title":"Snap support"},{"location":"software/os/os_support_cpu.html#openwrt-support","text":"","title":"OpenWRT support"},{"location":"software/os/os_support_cpu.html#windows-machine","text":"Building OpenWRT binaries is not supported on Windows.","title":"Windows machine"},{"location":"software/os/os_support_cpu.html#linux-machine","text":"To build 64-bit or 32-bit OpenWRT executables: Follow these instructions to download and build the OpenWRT tools. Copy the tools folder, toolchain-<architecture> from <QSDK_BUILD_ROOT>/qsdk/staging_dir/ to {HEXAGON_SDK_ROOT}/tools/ . Rename the tools folder as OpenWRT for 32-bit OpenWRT flavors, or OpenWRT64 for 64-bit OpenWRT flavors. For 64-bit Ubuntu binaries, rename the toolchain-<architecture> folder to OpenWRT64 . Then try building with make tree V=UbuntuARM_Debug_aarch64 OPENWRT=1 . For 32-bit Ubuntu binaries, rename the toolchain-<architecture> folder to OpenWRT . Then try building with make tree V=UbuntuARM_Debug OPENWRT=1 . Build your projects with make tree V=UbuntuARM_Debug OPENWRT=1 for 32-bit flavors, and make tree V=UbuntuARM_Debug_aarch64 OPENWRT=1 for 64-bit flavors","title":"Linux machine"},{"location":"software/os/os_support_cpu.html#run-the-project","text":"After the Ubuntu binaries are generated, follow the same steps as illustrated in the calculator example for signing, loading, and running the project.","title":"Run the project"},{"location":"software/os/os_support_dsp.html","text":"DSP OS and resource management QuRT Following is a brief overview of Qualcomm Hexagon RTOS (QuRT) and some usage examples involving QuRT API calls. For more details on QuRT, see the QuRT user guide . QuRT is a simple real-time operating system (RTOS) that runs on the DSP and supports multithreading, thread communication and synchronization, interrupt handling, and memory management. The DSP can execute a \ufb01xed number of threads simultaneously. These threads are referred as hardware threads. For example, the cDSP of the SM8150 and SM8250 devices includes four hardware threads, while Lahaina's cDSP includes six hardware threads. These hardware threads might need to share resources. For example, the six hardware threads in Lahaina share four HVX contexts, one HMX context, and the memory subsystem. QuRT provides support for sharing these hardware resources to run a large number of software threads with different requirements. Specifically, QuRT supports real-time priority-based preemptive multithreading: Multithreading means that multiple software threads can execute at the same time in a user program. QuRT initially assigns a single thread of execution to a user executable, and the program can then create additional threads. Priority-based means that each thread is assigned a priority level. The priority determines which threads have execution priority. Preemptive means that a thread can be preempted (the processor is taken away)when a higher priority thread is ready to execute. Real-time means the operating system can perform operations within fixed time constraints. QuRT consists of the a kernel and a user library: The kernel provides a minimal set of operating system facilities, including thread creation, scheduling, and blocking. It also performs basic memory management. The QURT user library provides an API to the kernel operations and some additional library functions to aid in programming. Thread management QuRT supports real-time priority-based preemptive multithreading, with no form of time-slicing for same-priority threads. For an example that demonstrates QuRT thread APIs, see multithreading . Thread attributes and contexts QURT provides the qurt_thread_create() function to create a thread with specified attributes. Threads have two kinds of attributes: Static attributes, which cannot be changed after a thread is created. Static attributes are set before a thread is created (using the qurt_thread_attr_init() and qurt_thread_attr_set() functions) and when a thread is created (by directly passing the attributes as arguments to qurt_thread_create() ). Dynamic attributes, which can be changed after the thread is created. Dynamic attributes are set after a thread is created using one of the qurt_thread_set_*() functions such as qurt_thread_set_priority() . Thread priority is an example of an attribute that allows both static and dynamic configurations. It is important to set the initial thread priority statically to avoid its being given a very low default priority by QuRT. Because the DSP is a multi-user environment, it can be difficult to know the appropriate thread priority for a new thread being spawned, so the general recommendation is to query the priority of the thread doing the spawning, and either use the same, or slightly above or below, based on the relative priority of the threads under the user's control. A thread can also dynamically change its priority any time. All threads are initialized to the Ready state. During system startup, the scheduler selects the highest-priority threads for execution and changes their thread state to Running. The action of suspending one thread and resuming another is called a context switch . The following operations can cause a context switch: Creating or exiting a thread Changing a thread priority Waiting on or releasing a mutex or semaphore Waiting on or resuming from a signal, barrier, or condition variable Reading or writing from a pipe Servicing an Interrupt Thread synchronization mechanisms Mutex Threads use mutexes to synchronize their execution to ensure mutually exclusive access to shared resources. If a thread performs a lock operation (using qurt_mutex_lock() ) on a mutex that is not being used, the thread gains access to the shared resource that is protected by the mutex, and it continues executing. If a thread performs a lock operation on a mutex that is already being used by another thread, the thread is suspended on the mutex. When the mutex becomes available (because the other thread has unlocked it), the suspended thread is awakened and gains access to the shared resource. Signals Threads use signals to synchronize their execution based on the occurrence of one or more internal events. If a thread is waiting on a signal object for a specified set of signals to be set, and one or more of those signals is set in the signal object, the thread is awakened. The qurt_signal_wait() and qurt_signal_wait_cancellable() functions wait for any or all signals, depending on their wait type arguments. Semaphores Threads use semaphores to synchronize their access to shared resources. When a semaphore is initialized, it is assigned an integer count value. This value indicates the number of threads that can simultaneously access a shared resource through the semaphore. The default value is 1. When a thread performs a Down operation on a semaphore, the result depends on the semaphore count value: If the count value is nonzero, it is decremented. The thread gains access to the shared resource and continues executing. If the count value is zero, it is not decremented, and the thread is suspended on the semaphore. When the count value becomes nonzero (because another thread released the semaphore) it is decremented, and the suspended thread is awakened and gains access to the shared resource. When a thread performs an Up operation on a semaphore, the semaphore count value is incremented. The result depends on the number of threads waiting on the semaphore: If no threads are waiting, the current thread releases access to the shared resource and continues executing. If one or more threads are waiting and the semaphore count value is nonzero, the kernel awakens the highest priority waiting thread and decrements the semaphore count value. If the awakened thread has a higher priority than the current thread, a context switch might occur. Barriers Threads use barriers to synchronize their execution at a specific point in a program. When a barrier is initialized, it is assigned a user-specified integer value. This value indicates the number of threads to synchronize on the barrier. When a thread waits on a barrier, it is suspended on the barrier: If the total number of threads waiting on the barrier is less than the barrier\u2019s assigned value, no other action occurs. If the total number of threads waiting on the barrier equals the barrier\u2019s assigned value, all threads currently waiting on the barrier are awakened, allowing them to execute past the barrier. After a barrier's waiting threads are awakened, it is automatically reset and can be used again in the program without the need for reinitialization. Condition variables Threads use condition variables to synchronize their execution based on the value in a shared data item. Condition variables are useful in cases where a thread would continuously poll a data item until it contained a specific value. Using a condition variable, the thread can efficiently accomplish the same task without the need for polling. Memory management QuRT offers memory mapping and allocation APIs that are exposed indirectly to user PDs through the HAP_mem APIs . Cache management The DSP has a two-level cache memory subsystem. On the cDSP with HVX, L1 is only accessible to the scalar unit, making L2 the second level memory for the scalar unit and the first level memory for the HVX coprocessor. For more details on L1 and L2 caches, see the Memory subsystem documentation . To maintain coherence, cache IO coherency allows the DSP to snoop into the CPU L2 cache. IO coherency is enabled by default for CPU-cached buffers shared with the DSP. However, clients can disable IO coherency for ION buffers by registering buffers as non-coherent using remote_register_buf_attr() with the FASTRPC_ATTR_NON_COHERENT attribute. The remote_register_buf_attr() function is explained in Remote APIs . Another option is to map a buffer as non-coherent upon its allocation by passing the RPCMEM_HEAP_NONCOHERENT flag when calling rpcmem_alloc() . uint32 flags = ION_FLAG_CACHED | RPCMEM_HEAP_NONCOHERENT rpcmem_alloc(heapid, flags, size); The rpcmem_alloc() function is documented as part of the RPCMEM APIs . IO-coherent buffers require no CPU-side cache maintenance, thus facilitating reduced FastRPC overhead with little dependence on the total size of buffers being shared with the DSP. PMU management QuRT offers PMU configuration APIs for counting various hardware events that are useful in profiling. These APIs are exposed indirectly to the user space through the SysMon application. To profile the DSP workload, see the Sysmon_Profiler application .","title":"DSP OS QuRT"},{"location":"software/os/os_support_dsp.html#dsp-os-and-resource-management","text":"","title":"DSP OS and resource management"},{"location":"software/os/os_support_dsp.html#qurt","text":"Following is a brief overview of Qualcomm Hexagon RTOS (QuRT) and some usage examples involving QuRT API calls. For more details on QuRT, see the QuRT user guide . QuRT is a simple real-time operating system (RTOS) that runs on the DSP and supports multithreading, thread communication and synchronization, interrupt handling, and memory management. The DSP can execute a \ufb01xed number of threads simultaneously. These threads are referred as hardware threads. For example, the cDSP of the SM8150 and SM8250 devices includes four hardware threads, while Lahaina's cDSP includes six hardware threads. These hardware threads might need to share resources. For example, the six hardware threads in Lahaina share four HVX contexts, one HMX context, and the memory subsystem. QuRT provides support for sharing these hardware resources to run a large number of software threads with different requirements. Specifically, QuRT supports real-time priority-based preemptive multithreading: Multithreading means that multiple software threads can execute at the same time in a user program. QuRT initially assigns a single thread of execution to a user executable, and the program can then create additional threads. Priority-based means that each thread is assigned a priority level. The priority determines which threads have execution priority. Preemptive means that a thread can be preempted (the processor is taken away)when a higher priority thread is ready to execute. Real-time means the operating system can perform operations within fixed time constraints. QuRT consists of the a kernel and a user library: The kernel provides a minimal set of operating system facilities, including thread creation, scheduling, and blocking. It also performs basic memory management. The QURT user library provides an API to the kernel operations and some additional library functions to aid in programming.","title":"QuRT"},{"location":"software/os/os_support_dsp.html#thread-management","text":"QuRT supports real-time priority-based preemptive multithreading, with no form of time-slicing for same-priority threads. For an example that demonstrates QuRT thread APIs, see multithreading .","title":"Thread management"},{"location":"software/os/os_support_dsp.html#thread-attributes-and-contexts","text":"QURT provides the qurt_thread_create() function to create a thread with specified attributes. Threads have two kinds of attributes: Static attributes, which cannot be changed after a thread is created. Static attributes are set before a thread is created (using the qurt_thread_attr_init() and qurt_thread_attr_set() functions) and when a thread is created (by directly passing the attributes as arguments to qurt_thread_create() ). Dynamic attributes, which can be changed after the thread is created. Dynamic attributes are set after a thread is created using one of the qurt_thread_set_*() functions such as qurt_thread_set_priority() . Thread priority is an example of an attribute that allows both static and dynamic configurations. It is important to set the initial thread priority statically to avoid its being given a very low default priority by QuRT. Because the DSP is a multi-user environment, it can be difficult to know the appropriate thread priority for a new thread being spawned, so the general recommendation is to query the priority of the thread doing the spawning, and either use the same, or slightly above or below, based on the relative priority of the threads under the user's control. A thread can also dynamically change its priority any time. All threads are initialized to the Ready state. During system startup, the scheduler selects the highest-priority threads for execution and changes their thread state to Running. The action of suspending one thread and resuming another is called a context switch . The following operations can cause a context switch: Creating or exiting a thread Changing a thread priority Waiting on or releasing a mutex or semaphore Waiting on or resuming from a signal, barrier, or condition variable Reading or writing from a pipe Servicing an Interrupt","title":"Thread attributes and contexts"},{"location":"software/os/os_support_dsp.html#thread-synchronization-mechanisms","text":"","title":"Thread synchronization mechanisms"},{"location":"software/os/os_support_dsp.html#mutex","text":"Threads use mutexes to synchronize their execution to ensure mutually exclusive access to shared resources. If a thread performs a lock operation (using qurt_mutex_lock() ) on a mutex that is not being used, the thread gains access to the shared resource that is protected by the mutex, and it continues executing. If a thread performs a lock operation on a mutex that is already being used by another thread, the thread is suspended on the mutex. When the mutex becomes available (because the other thread has unlocked it), the suspended thread is awakened and gains access to the shared resource.","title":"Mutex"},{"location":"software/os/os_support_dsp.html#signals","text":"Threads use signals to synchronize their execution based on the occurrence of one or more internal events. If a thread is waiting on a signal object for a specified set of signals to be set, and one or more of those signals is set in the signal object, the thread is awakened. The qurt_signal_wait() and qurt_signal_wait_cancellable() functions wait for any or all signals, depending on their wait type arguments.","title":"Signals"},{"location":"software/os/os_support_dsp.html#semaphores","text":"Threads use semaphores to synchronize their access to shared resources. When a semaphore is initialized, it is assigned an integer count value. This value indicates the number of threads that can simultaneously access a shared resource through the semaphore. The default value is 1. When a thread performs a Down operation on a semaphore, the result depends on the semaphore count value: If the count value is nonzero, it is decremented. The thread gains access to the shared resource and continues executing. If the count value is zero, it is not decremented, and the thread is suspended on the semaphore. When the count value becomes nonzero (because another thread released the semaphore) it is decremented, and the suspended thread is awakened and gains access to the shared resource. When a thread performs an Up operation on a semaphore, the semaphore count value is incremented. The result depends on the number of threads waiting on the semaphore: If no threads are waiting, the current thread releases access to the shared resource and continues executing. If one or more threads are waiting and the semaphore count value is nonzero, the kernel awakens the highest priority waiting thread and decrements the semaphore count value. If the awakened thread has a higher priority than the current thread, a context switch might occur.","title":"Semaphores"},{"location":"software/os/os_support_dsp.html#barriers","text":"Threads use barriers to synchronize their execution at a specific point in a program. When a barrier is initialized, it is assigned a user-specified integer value. This value indicates the number of threads to synchronize on the barrier. When a thread waits on a barrier, it is suspended on the barrier: If the total number of threads waiting on the barrier is less than the barrier\u2019s assigned value, no other action occurs. If the total number of threads waiting on the barrier equals the barrier\u2019s assigned value, all threads currently waiting on the barrier are awakened, allowing them to execute past the barrier. After a barrier's waiting threads are awakened, it is automatically reset and can be used again in the program without the need for reinitialization.","title":"Barriers"},{"location":"software/os/os_support_dsp.html#condition-variables","text":"Threads use condition variables to synchronize their execution based on the value in a shared data item. Condition variables are useful in cases where a thread would continuously poll a data item until it contained a specific value. Using a condition variable, the thread can efficiently accomplish the same task without the need for polling.","title":"Condition variables"},{"location":"software/os/os_support_dsp.html#memory-management","text":"QuRT offers memory mapping and allocation APIs that are exposed indirectly to user PDs through the HAP_mem APIs .","title":"Memory management"},{"location":"software/os/os_support_dsp.html#cache-management","text":"The DSP has a two-level cache memory subsystem. On the cDSP with HVX, L1 is only accessible to the scalar unit, making L2 the second level memory for the scalar unit and the first level memory for the HVX coprocessor. For more details on L1 and L2 caches, see the Memory subsystem documentation . To maintain coherence, cache IO coherency allows the DSP to snoop into the CPU L2 cache. IO coherency is enabled by default for CPU-cached buffers shared with the DSP. However, clients can disable IO coherency for ION buffers by registering buffers as non-coherent using remote_register_buf_attr() with the FASTRPC_ATTR_NON_COHERENT attribute. The remote_register_buf_attr() function is explained in Remote APIs . Another option is to map a buffer as non-coherent upon its allocation by passing the RPCMEM_HEAP_NONCOHERENT flag when calling rpcmem_alloc() . uint32 flags = ION_FLAG_CACHED | RPCMEM_HEAP_NONCOHERENT rpcmem_alloc(heapid, flags, size); The rpcmem_alloc() function is documented as part of the RPCMEM APIs . IO-coherent buffers require no CPU-side cache maintenance, thus facilitating reduced FastRPC overhead with little dependence on the total size of buffers being shared with the DSP.","title":"Cache management"},{"location":"software/os/os_support_dsp.html#pmu-management","text":"QuRT offers PMU configuration APIs for counting various hardware events that are useful in profiling. These APIs are exposed indirectly to the user space through the SysMon application. To profile the DSP workload, see the Sysmon_Profiler application .","title":"PMU management"},{"location":"software/os/snaps.html","text":"Snap support This page documents the process of building and running an application using Snap within the Hexagon SDK. Build binaries Setup the Hexagon SDK environment Download the 64-bit flavor of the Linaro tools and extract them to $HEXAGON_SDK_ROOT/tools/ folder following these instructions Choose an SDK example For example: cd $HEXAGON_SDK_ROOT/examples/calculator Build the application and DSP code using make.d or CMake For example (on a Windows machine): cmake_build.cmd ubuntuARM cmake_build.cmd hexagon DSP_ARCH=v66 Sign the Hexagon shared object For example: python $HEXAGON_SDK_ROOT/utils/script/signer.py -i libcalculator_skel.so Create a Snap package Copy both apps and DSP binaries from ship dir to a host with a Snapcraft environment scp -r username@hostname:source_location username@hostname:dest_location Compile Snap with destructive-mode You may need to set environment variables as follows: HLOS_APP_PREBUILTS_PATH=$(pwd)/UbuntuARM64/ ADSPRPC_TAR_PATH=$(pwd)/../ HEXAGON_DSP_PREBUILTS_PATH=$(pwd)/hexagon/ DSP_LIB_PATH=$(pwd)/qcom-fw From Linux running on AMD/X86, you can cross-compile Snap files for an ARM64 architecture as follows: snapcraft --destructive-mode --target-arch=arm64 --enable-experimental-target-arch From target you can compile Snap files as follows: snapcraft --destructive-mode Once your Snap creation is successful, you will see {your-snap-name-version}-{arch}.snap file being generated For example: qti-calculator_0.1_arm64.snap Install and run Snap Copy the Snap to the target using the scp command, access the target using COM port or ssh command and install the Snap on target scp -r username@hostname:source_location username@hostname:dest_location ssh username@hostname sudo snap install qti-calculator_0.1_arm64.snap --dangerous Connect the plugs sudo snap connect qti-calculator:ion sudo snap connect qti-calculator:adsprpc-smd Check the details about your installed snap using the info command sudo snap info qti-calculator Run your application using the command defined inside the snapcraft.yaml file sudo qti-calculator.testapp","title":"Snap support"},{"location":"software/os/snaps.html#snap-support","text":"This page documents the process of building and running an application using Snap within the Hexagon SDK.","title":"Snap support"},{"location":"software/os/snaps.html#build-binaries","text":"Setup the Hexagon SDK environment Download the 64-bit flavor of the Linaro tools and extract them to $HEXAGON_SDK_ROOT/tools/ folder following these instructions Choose an SDK example For example: cd $HEXAGON_SDK_ROOT/examples/calculator Build the application and DSP code using make.d or CMake For example (on a Windows machine): cmake_build.cmd ubuntuARM cmake_build.cmd hexagon DSP_ARCH=v66 Sign the Hexagon shared object For example: python $HEXAGON_SDK_ROOT/utils/script/signer.py -i libcalculator_skel.so","title":"Build binaries"},{"location":"software/os/snaps.html#create-a-snap-package","text":"Copy both apps and DSP binaries from ship dir to a host with a Snapcraft environment scp -r username@hostname:source_location username@hostname:dest_location Compile Snap with destructive-mode You may need to set environment variables as follows: HLOS_APP_PREBUILTS_PATH=$(pwd)/UbuntuARM64/ ADSPRPC_TAR_PATH=$(pwd)/../ HEXAGON_DSP_PREBUILTS_PATH=$(pwd)/hexagon/ DSP_LIB_PATH=$(pwd)/qcom-fw From Linux running on AMD/X86, you can cross-compile Snap files for an ARM64 architecture as follows: snapcraft --destructive-mode --target-arch=arm64 --enable-experimental-target-arch From target you can compile Snap files as follows: snapcraft --destructive-mode Once your Snap creation is successful, you will see {your-snap-name-version}-{arch}.snap file being generated For example: qti-calculator_0.1_arm64.snap","title":"Create a Snap package"},{"location":"software/os/snaps.html#install-and-run-snap","text":"Copy the Snap to the target using the scp command, access the target using COM port or ssh command and install the Snap on target scp -r username@hostname:source_location username@hostname:dest_location ssh username@hostname sudo snap install qti-calculator_0.1_arm64.snap --dangerous Connect the plugs sudo snap connect qti-calculator:ion sudo snap connect qti-calculator:adsprpc-smd Check the details about your installed snap using the info command sudo snap info qti-calculator Run your application using the command defined inside the snapcraft.yaml file sudo qti-calculator.testapp","title":"Install and run Snap"},{"location":"software/system_libraries/index.html","text":"System libraries This page lists all the system-level APIs provided in the Hexagon base SDK and where you can find more information about them. Usage for all system-level APIs is illustrated in the HAP example except for the ION memory allocator , which is illustrated in the calculator example . Application-side APIs APIs Summary ION memory allocator Allocate ION memory to be shared between CPU and DSP Remote APIs Query and manage DSP capabilities DSP-side APIs The Hexagon SDK includes a number of APIs referred as Hexagon Access Program (HAP) APIs, which provide access to Hexagon capabilities. These APIs in the table below. APIs Summary Performance and power manager Manage performance and power modes of the DSP L2 cache manager Lock & unlock L2 cache VTCM manager Allocate VTCM Compute resource manager Allocate compute resources PMU manager Track hardware events Performance timers Measure processing time Heap and mapping manager Manage the user PD heap memory and control buffer mapping into this memory DSP worker pool General purpose, efficient thread worker pool to assist in multithreading compute workloads. Available under $HEXAGON_SDK_ROOT/libs/worker_pool","title":"System libraries"},{"location":"software/system_libraries/index.html#system-libraries","text":"This page lists all the system-level APIs provided in the Hexagon base SDK and where you can find more information about them. Usage for all system-level APIs is illustrated in the HAP example except for the ION memory allocator , which is illustrated in the calculator example .","title":"System libraries"},{"location":"software/system_libraries/index.html#application-side-apis","text":"APIs Summary ION memory allocator Allocate ION memory to be shared between CPU and DSP Remote APIs Query and manage DSP capabilities","title":"Application-side APIs"},{"location":"software/system_libraries/index.html#dsp-side-apis","text":"The Hexagon SDK includes a number of APIs referred as Hexagon Access Program (HAP) APIs, which provide access to Hexagon capabilities. These APIs in the table below. APIs Summary Performance and power manager Manage performance and power modes of the DSP L2 cache manager Lock & unlock L2 cache VTCM manager Allocate VTCM Compute resource manager Allocate compute resources PMU manager Track hardware events Performance timers Measure processing time Heap and mapping manager Manage the user PD heap memory and control buffer mapping into this memory DSP worker pool General purpose, efficient thread worker pool to assist in multithreading compute workloads. Available under $HEXAGON_SDK_ROOT/libs/worker_pool","title":"DSP-side APIs"},{"location":"software/system_performance/dsp_optimizations.html","text":"DSP optimizations There are some basic concepts to keep in mind when writing efficient assembly code. These concepts are also relevant when writing with intrinsics because you should know approximately how the compiler is able to group and schedule intrinsics in order to achieve maximum performance. This page briefly describes some of these fundamental concepts. TIP: When writing code with intrinsics or assembly instructions, it is best to start by writing functional code using the right instructions, and then only optimizing code further by following the optimization guidelines below. Attempting to do too much at once becomes more time consuming because low-level optimizations typically obscure the clarity of the code. Packing instructions Each hardware thread is capable of executing a packet of up to four instructions in a given thread cycle. Instructions occurring in a packet execute in parallel, making it possible within one packet to consume a register and update its content at the same time. For example, the following packet consumes the contents of R0 that were produced in the packets that preceded it and then updates its content with R4. { R2 = vaddh(R0,R1) R0 = R4 } The exception to this rule is when the .new specifier is used within an instruction. For example, the following packet first computes R2 and then store its content to memory. { R2 = memh(R4+#8) memw(R5) = R2.new } To maximize the number of instructions per packet, it is important to understand how instructions can or cannot be combined in each packet. The restrictions are different for scalar and HVX instructions. Scalar packing rules Rules on how to form packets are explained in the Instruction packets section of the Hexagon V66 Programmer's Reference Manual (80-N2040-42). Let's focus on mastering the most important restriction to packing instructions together: resource constraints. The simplest way to understand the impact of resource constraints on grouping with V66 is to think simply in terms of slots. Because of the resources they use, each type of instruction can only execute on one specific slot, out of four available slots. For example, logical and multiply operations consume either slot 2 or 3, whereas a load instruction consumes either slot 0 or 1. This means that at most two logical operations, or one logical and one multiply operation, or two multiply operations, can execute in a single packet. The description of which type of instruction is acceptable for each slot is provided in Figure 3-1 of the Hexagon V66 Programmer's Reference Manual (80-N2040-42) as well as in the detailed description of each instruction. For convenience, this figure is reproduced below. Instruction slots Other restrictions outlined in the Instruction packets section of the same manual occasionally cause a packet that follows the resource constraints to still generate an error message at compile time. However, these other rules come into play much less frequently, and you can learn about them over time. HVX packing rules Rules on how to form packets in HVX are explained in the VLIW packing rules section of the Hexagon V66 HVX Programmer's Reference Manual (80-N2040-44). HVX instructions share the same slots as V6x, and there are restrictions on which slot each HVX instruction uses. However, unlike with V6x, resources and slots are not correlated one-to-one. When grouping HVX instructions, it is best to focus on understanding which resources each HVX instruction share, and understanding how this impacts the ability to group instructions in a packet; slot restrictions rarely come into consideration when writing HVX code. Resource restrictions are summarized in the Hexagon V66 HVX Programmer's Reference Manual (80-N2040-44) and reproduced in the table below for convenience. The detailed description of each HVX instruction also indicates which HVX resources it consumes. HVX slot/resource/latency summary Be aware that some instructions, such as vrmpy , come in different flavors: some that consume two HVX resources, and some that consume only one resource. For example, halfword multiplies use both multiply resources, which means that no other HVX multiply instruction is present in the same packet. When trying to optimize the inner loop of a function multiply-bound, plan early on how to maximize multiply resource utilization in as many packets as possible. NOTE: Unlike scalar stores, you cannot group two HVX stores in a single packet. Reduce stalls In addition to executing as many instructions as possible in any given packet to maximize parallelization, it is important to be aware of latencies that might cause the processor to stall. This section discusses the most common causes of stalls that deteriorate performance. Instruction latencies Thread vs. processor cycles Hexagon cores dynamically schedule packets from threads into the core pipeline. The number of cycles to execute a packet varies depending on the behavior of other threads. The optimal schedule for a single thread running in isolation can be different than the optimal schedule for a thread running with other threads. For example, when multiple threads execute in parallel, one thread executes every other processor cycle. This means that if the processor is clocked at 800 MHz and four threads execute in parallel, each thread runs effectively at 400 MHz. However, when a thread is idle, another thread might be able to steal some of its cycles, allowing it to run faster than it runs if all hardware threads were busy. In practice, on Hexagon versions up through the SM8250 device, a single-threaded workload might run up to 20% to 30% faster if it is the only running thread, compared to when it is concurrent with other running software threads that are consuming all the hardware threads. The following sections provide general rules on latency scheduling assuming at least two threads are running. These rules provide a simplified programming model that is reasonable to use when writing optimized code. Using this model, we introduce the concept of a packet delay. This delay is the number of packets that are to be scheduled between two dependent packets to prevent the thread from stalling. Scalar latencies Instructions have no packet delays, with the following exceptions: Instructions that are paired with the .new predicate, which allows two sequential instructions to execute within the same packet. Mispredicted jumps, which typically incur around five packet stalls. For more information on speculative branches, see the Compare jumps section in the Hexagon V66 Programmer's Reference Manual (80-N2040-42). NOTE: When possible, try using hardware loops; they do not generate stalls, even when exiting the loop. Long-latency instructions that consume the result of another long-latency instruction. These instructions experience a one-packet delay with the exception of back-to-back multiplies that share the same accumulator and thus do not experience any delay. Long latency instructions are all the load, multiply, and float instructions. For example, the following instruction sequences stall: { R2=mpy(R0.L,R0.L) } // one-cycle stall { R3=mpy(R2.L,R2.L) } ... { R2 = memw(R1) } // one-cycle stall { R3=mpy(R2.L,R2.L) } But this instruction sequence does not stall: { R2=mpy(R0.L,R0.L) } { R2+=mpy(R1.L,R1.L) } // no stall HVX latencies The Instruction latency section of the Hexagon V66 HVX Programmer's Reference Manual (80-N2040-44) discusses latencies. This section discusses the most common HVX arithmetic instruction sequences responsible for stalls. The most common causes of stalls are one-packet delays present when the following instructions consume a result that was produced in the previous packet: Multiplies NOTE: Back-to-back multiplies that only share the same accumulator do not stall. Shift and permute operations The Instruction latency section provides some examples of these rules: { V8 = VADD(V0,V0) } { V0 = VADD(V8,V9) } // NO STALL { V1 = VMPY(V0,R0) } // STALL due to V0 { V2 = VSUB(V2,V1) } // NO STALL on V1 { V5:4 = VUNPACK(V2) } // STALL due to V2 { V2 = VADD(V0,V4) } // NO STALL on V4 Memory latencies Scalar data memory accesses go through a two-level cache hierarchy, while HVX memory accesses only transit through the L2 memory. Cache sizes vary depending on the exact chip variant: L1 cache sizes are 16 to 32 KB L2 cache sizes are 128 to 1024 KB on aDSP variants, and 512 to 2048 KB on cDSP variants To avoid cache misses when writing optimized applications, it is critical to reduce the data memory throughput and maximize data locality. Common data optimization techniques exploiting data locality include the following techniques: Register data reuse The application stores values into registers for later use. For example, applying a filter on multiple lines at the same time allows holding of coefficients in registers, thus reducing the overall data bandwidth. Tiling A tile defines a small region of an image. The application processes an image one tile at a time or a few tiles at a time. This approach might be appropriate when using scalar instructions rather than HVX instructions because it allows preservation of data within L1 and thus maximizes scalar processing throughput. Larger buffers, such as groups of image lines, are typically too large for L1. Using a tiling approach typically comes at the cost of greater programming complexity and more non-linear data addressing. Line processing The application processes an image a few lines at a time. This is the most common approach for HVX implementations as it allows to load entire HVX vectors and leverage the large L2 memory cache size. Another type of cache optimization consists of explicitly managing cache contents by way of prefetching data into cache and, more rarely, invalidating cache line contents. Leave this optimization for the end after you have already ensured a maximum of data locality in your code. The Hexagon V66 Programmer's Reference Manual (80-N2040-42) details the L2 cache prefetching mechanism. For an example that shows how to perform L2 prefetching, see the multithreading project example. NOTE: It is common to optimize an application on a single-thread first and then multi-thread the code later. When using that approach, extrapolating multi-threaded performance from single-thread performance can occasionally be misleading: memory bandwidth might not be a bottleneck when only one thread is running, but it become one of the limiting resources when more threads run in parallel. As a result, it is a good practice to write applications as conservatively as possible with respect to memory bandwidth usage, regardless of the performance of the single-threaded code. Although data memory latencies depend on many parameters and architecture variants, it is useful to know to a first order the cost of memory accesses when planning the optimization of an application. The following numbers are rough estimates on what to expect in making memory accesses: DDR memory access: ~250 ns L2 read latency: 6 thread cycles HVX has a mechanism for pushing HVX instructions into a queue called VFIFO. As long as no mispredicted branches occur, this queue remains full. The L2 reads triggered by VMEM instructions occur enough in advance that the result from a VMEM load is available in the next cycle without stalling. In other words, for HVX, L2 reads have a one-cycle latency, and the following instruction sequence does not stall as long as no mispredicted branch has occurred recently: { V0 = VMEM(R0++) } { V1 = VADD(V0,V1) } // No stall if no recent mispredicted branch Note: The L2 VMEM read latency is higher--around 15 thread cycles--when following an L2 VMEM store to the same location. The reason for this delay is that the store must fully reach L2 before the load starts. For active data that do not fit in the vector register file, consider using VTCM instead of L2 to reduce the store-to-load penalty. Maximum sustainable read-write L2 bandwidth with no bank conflicts: 128 bytes per processor cycle Also, the HVX engine is directly connected to L2 cache, bypassing L1. HVX instructions are pipelined deeply enough to avoid any observed latencies for L2 loads or stores (when the pipeline is full and L2 traffic is not very congested). However, due to the depth of the HVX pipeline, it is expensive to do a transfer from an HVX register to a scalar register, or to perform a scalar memory load from an address following an HVX store to the same cache line. When using HVX in performance-sensitive code, do all loads, stores, and arithmetic via HVX instructions, and use scalar instructions and registers only for addressing, controlling, or processing on a different data set than that being done in HVX. Software pipelining The Hexagon instruction set allows multiple unrelated instructions within one packet. This flexibility provides great opportunities for parallelizing the code, which are best exploited by doing software pipelining. Software pipelining consists of processing a few consecutive instances of a loop in parallel in order to reduce data dependencies and provide more opportunities for operations to be executed in parallel. This approach comes at the expense of having separate code for prologue and epilogue code. For example, a loop does the following: Processes data loads for iteration n+2 Performs some computations for iterations n and n+1 Stores the results of iteration n In this case, the prologue and epilogue code must handle the first and last two- or three-loop iterations separately, and it handles cases where the number of loop iterations is small. The Hexagon instruction set allows for a decrease in the complexity of the prologue and epilogue code by supporting pipelined hardware loops. Pipelined hardware loops set predicate registers after a loop has been iterated a specific number of times, thus allowing some operations (typically stores) to execute only after a few loop iterations. For more information on this approach, see the Pipelined hardware loops section in the Hexagon V66 Programmer's Reference Manual (80-N2040-42). NOTE: The Hexagon compiler automatically conducts software pipelining of appropriate loops. HVX optimizations HVX adds a powerful set of instructions that allow processing of large vectors very efficiently. The Hexagon V66 HVX Programmer's Reference Manual (80-N2040-44) is the authoritative source for HVX instruction syntax and behavior for any given revision. The following sections highlight some these instructions. When to use HVX HVX vectors are 128-byte wide. As a result, HVX lends itself well to sequences of identical operations on contiguous 32-bit, 16-bit, or 8-bit elements. Thus, HVX is ideally suited for some application spaces such as image processing where many operations are to be applied independently to continuous pixels. However, it does not mean that HVX is restricted to only perform operations on contiguous elements in memory, as explained below. Although HVX memory loads and memory stores access contiguous elements in memory, HVX provides several powerful instructions for shuffling and interleaving elements between and within HVX vectors. These instructions allow HVX to efficiently process non-continuous elements that follow some predictable patterns, such as odd and even elements or vertical lines. The next section discusses these instructions in more detail. For portions of code that only operate sequentially, one element at a time, and where no parallelism opportunity is found, using V6x instructions and letting other threads use the HVX resources is often the best approach. HVX byte manipulations Depending on the nature of the algorithm being ported on HVX, it might be necessary to rearrange elements from an HVX vector or pair of HVX vectors in various ways. Several HVX instructions allow you to address this challenge. The following figure describes these instructions and provides a visual summary of the instructions. Summary of the most common HVX element manipulations valign, vlalign, vror These three instructions are straightforward: valign and vlalign create an HVX vector made out of the lowest bytes of one vector and the upper bytes of another vector. vror performs a circular rotation of an HVX vector by an arbitrary number of bytes. vpacke, vpacko, vpack, vunpack, vunpacko, vdeal vpacke and vpacko pack the even or odd 32-bit or 16-bit elements of two HVX vectors into one vector. vpack performs an element size reduction, shrinking the contents of two HVX vectors into one vector after saturation. vunpack and vunpacko are the opposite forms of vpack , respectively unpacking the even and odd 8-bit or 16-bit elements into elements twice as large. vdeal (the flavor that consumes one input register) operates in the same way as vpacke and vpacko combined, but it operates on half the number of elements: it packs the even elements from the input register into the lower half of the output register, and packs the odd elements into the upper half of the register. vshuffe, vshuffo, vshuffoe, vshuff vshuffe and vshuffo are similar to their counterparts, vpacke and vpacko , in that they move the even or odd elements of two HVX into one HVX vector. The difference with their vpack counterparts is that elements from both input HVX vectors are interleaved (the contents of both input register is being shuffled into one register). vshuffoe executes both vshuffe and vshuffo at the same time and generates a register pair where the two registers in the pair are the output from the vshuffo and vshuffe instructions. vshuff (the flavor that consumes one input register) interleaves the elements from the upper and lower parts of a register into another register. The vshuff and vpack variants are helpful in different use cases. For example, if an HVX register contains pairs of (x,y) coordinates, use vpacke and vpacko to separate the x and y elements into different vectors. On the other hand, vshuffo or vshuffe can follow HVX instructions that produce HVX vectors with double precision, and store the results of consecutive operations in the upper and lower registers of a pair. vasr A narrowing shift: it takes two input HVX vectors and returns one output HVX vector. The narrowing shift is applied on each element of the two input HVX registers, and thus it produces output with the same order as a vshuffe or vshuffo instruction. Cross-lane vshuff, vdeal These instructions are very powerful but not trivial to understand. They perform a multi-level transpose operation between groups of registers. The most common configurations used with vshuff and vdeal are for positive and negative powers of 2. vshuff with an element size of Rt = 2 N places the 2 N -byte even elements from both input vectors into the low register of the output pair, and the 2 N -byte odd elements into the high register. This operation is a generalization of vshuffoe to larger element sizes. vshuff with an element size of Rt = -2 N interleaves the 2 N -byte elements from both input vectors into the output register. This operation is a generalization of the non-Cross-lane variant of vshuff to larger element sizes. vdeal with an element size of Rt = 2 N is identical to vshuff . vdeal with an element size of Rt = -2 N packs the 2 N -byte even elements from both input vectors into the low output register pair, and the odd elements into the high pair. For N=0 , this instruction is the same as executing packo and packe at the same time. vdelta, vrdelta vdelta and vrdelta use a network of switchboxes to permute or copy bytes within an HVX vector. Consider using these instructions when you need transforms with some irregular patterns not covered in the operations listed above. vdelta and vrdelta are configured with an HVX vector. The simplest and safest way of determining the configuration values for this register is to use a configuration tool that is provided with the SDK tools under {HEXAGON_SDK_ROOT}/tools/HEXAGON_Tools/<version number>/Examples/libcore/Vdelta_Helper/General_permute_network.html . At the bottom of this html page, you can specify a pattern in which the bytes of the input vector should be reordered. For example, if bytes 1 and 5 are to be dropped from the input vector, specify in the TPERM[N] control box a sequence of bytes that begins with 0, 2, 3, 4, 6, ... . Once the output pattern is fully specified, click Submit for Benes or Submit for Delta to retrieve the configuration, if one exists, and perform the pattern transformation using either a sequence of a vrdelta and vdelta instructions (Benes approach) or one vrdelta instruction. Memory operations Aligned and unaligned HVX memory operations may be performed in C without using intrinsics. Aligned HVX loads and stores Dereferencing an HVX pointer using the HVX_Vector type defined in $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/target/hexagon/include/hexagon_types.h results in aligned HVX VMEM instructions for loads and stores that ignore the lowest bits of the pointer to always align the load or store to the HVX vector size: HVX_Vector* hvx_ptr = (HVX_Vector*)ptr; HVX_Vector hvx_value = *hvx_ptr; Assuming hvx_value is mapped into V0 and ptr into R0, this code is compiled into the following instruction in assembly: V0 = VMEM(R0) // (R0 & 127) bits are ignored Similarly, storing to an HVX pointer results in ignoring the lowest bits to perform a VMEM store. The following code: HVX_Vector* hvx_ptr = (HVX_Vector*)ptr; *hvx_ptr = hvx_value; is compiled into the following single instruction, assuming hvx_value is mapped into V0 and ptr into R0: VMEM(R0) = V0 // (R0 & 127) bits are ignored Unaligned HVX loads and stores To perform unaligned loads and stores, you can define the following macro: #define vmemu(A) *(( HVX_UVector*)(A)) NOTE: Using unaligned HVX load/store operations is inherently less efficient than using aligned load/store operations combined with explicit HVX alignment instructions VALIGN and VLALIGN. With these, you can perform unaligned loads and stores in C the same way that you would in assembly: HVX_Vector value = vmemu(ptr) vmemu(ptr) = new_value results in assembly in an unaligned VMEMU load and an unaligned VMEMU store operations. V0 = VMEMU(R0) // All address bits from R0 are used when performing the unaligned load VMEMU(R0) = V1 // All address bits from R0 are used when performing the unaligned store Array stores of arbitrary sizes Stores of arrays of arbitrary sizes may be accomplished with the following C helper function that leverages HVX bytewise-enabled stores. Note that this approach is only recommended for rare boundary cases, as its performance is significantly worse than regular aligned or unaligned stores. #include \"hexagon_types.h\" #define VLEN 128 // This stores the first n bytes from vector vin to address 'addr'. // n must be in range 1..VLEN, addr may have any alignment. // Implementation does one or two masked stores. static inline void q6op_vstu_variable_ARV( void * addr, int n, HVX_Vector vin) { vin = Q6_V_vlalign_VVR( vin, vin, (size_t)addr); //rotate as needed. unsigned left_off = (size_t)addr & (#VLEN-1); unsigned right_off = left_off + n; HVX_VectorPred qL_not = Q6_Q_vsetq_R( (size_t)addr ); HVX_VectorPred qR = Q6_Q_vsetq2_R( right_off ); if( right_off > 128 ){ Q6_vmaskedstoreq_QAV( qR, (HVX_Vector*)addr + 1, vin); qR = Q6_Q_vcmp_eq_VbVb( vin,vin); // all 1's } qL_not = Q6_Q_or_QQn( qL_not, qR ); Q6_vmaskedstorenq_QAV( qL_not,(HVX_Vector*)addr, vin ); } Accessing scalar contents from an HVX register The recommended way to access an element from an HVX register is to go through memory instead of using vextract. This approach is best accomplished using a union. For example, the following code extracts the first 16-bit element of HVX register hvx_value into the 16-bit variable first_element : union {int16_t array[ELEM_PER_VEC]; HVX_Vector vector; } HVX_and_array; HVX_and_array.vector = hvx_value; // turns into an HVX store to memory int16_t first_element = HVX_and_array.array[0]; // turns into a scalar read from memory Expect a delay of tens of cycles between the HVX write to memory and the scalar read. VTCM/lookup V6x supports scatter/gather operations, allowing you to perform vectorized random-access memory lookups that are not limited to 256 entries as is the case with the vlut instruction. For more details on this instruction, see the Hexagon V66 HVX Programmer's Reference Manual User Guide . NOTE: The scatter/gather operations cause significant traffic in the VTCM subsytem, so they are prone to stalling when memory conflicts occur. For information on how to avoid scatter/gather stalls, see the Avoid scatter/gather stalls section of the Hexagon V66 HVX Programmer's Reference Manual User Guide . Float support Float to qfloat conversions V68 supports HVX operations with float and qfloat data types. Conversion between these types is simply achieved using instructions that consume one type of data and produce another. For example: Conversion from an HVX vector made of IEEE 754 single float elements to an HVX vector made of qfloat elements: HVX_Vector vqf32 = Q6_Vqf32_vadd_VsfVsf(vsf, Q6_V_vzero()) Conversion from an HVX vector made of qfloat elements to an HVX vector made of IEEE 754 single float elements: HVX_Vector vsf = Q6_Vsf_equals_Vqf32(vqf32); The QHL HVX APIs include a number of helper functions to perform conversions between IEEE and Qualcomm float numbers including those shown above. qfloat precision Unlike float operations, qfloat operations do not normalize their ouput vectors. As a result, sequences of qfloat multiplies may lose some accuracy compared to their float equivalent and forcing normalization prior to the multiplies may be needed to achieve greater accuracy. This is for example the case when using the Horner's method for polynomial approximations where a variable goes through a chain of multiplies. The simplest way to normalize a number consists in adding zero to it. For example, 16-bit qfloat normalization is achieved as follows: x = Q6_Vqf16_vadd_Vqf16Vhf(x, Q6_V_vzero()); As a general rule of thumb, for greater accuracy: inputs to qfloat vmpy instructions should be normalized no normalization is needed for the inputs to a qfloat vadd operation","title":"DSP optimizations"},{"location":"software/system_performance/dsp_optimizations.html#dsp-optimizations","text":"There are some basic concepts to keep in mind when writing efficient assembly code. These concepts are also relevant when writing with intrinsics because you should know approximately how the compiler is able to group and schedule intrinsics in order to achieve maximum performance. This page briefly describes some of these fundamental concepts. TIP: When writing code with intrinsics or assembly instructions, it is best to start by writing functional code using the right instructions, and then only optimizing code further by following the optimization guidelines below. Attempting to do too much at once becomes more time consuming because low-level optimizations typically obscure the clarity of the code.","title":"DSP optimizations"},{"location":"software/system_performance/dsp_optimizations.html#packing-instructions","text":"Each hardware thread is capable of executing a packet of up to four instructions in a given thread cycle. Instructions occurring in a packet execute in parallel, making it possible within one packet to consume a register and update its content at the same time. For example, the following packet consumes the contents of R0 that were produced in the packets that preceded it and then updates its content with R4. { R2 = vaddh(R0,R1) R0 = R4 } The exception to this rule is when the .new specifier is used within an instruction. For example, the following packet first computes R2 and then store its content to memory. { R2 = memh(R4+#8) memw(R5) = R2.new } To maximize the number of instructions per packet, it is important to understand how instructions can or cannot be combined in each packet. The restrictions are different for scalar and HVX instructions.","title":"Packing instructions"},{"location":"software/system_performance/dsp_optimizations.html#scalar-packing-rules","text":"Rules on how to form packets are explained in the Instruction packets section of the Hexagon V66 Programmer's Reference Manual (80-N2040-42). Let's focus on mastering the most important restriction to packing instructions together: resource constraints. The simplest way to understand the impact of resource constraints on grouping with V66 is to think simply in terms of slots. Because of the resources they use, each type of instruction can only execute on one specific slot, out of four available slots. For example, logical and multiply operations consume either slot 2 or 3, whereas a load instruction consumes either slot 0 or 1. This means that at most two logical operations, or one logical and one multiply operation, or two multiply operations, can execute in a single packet. The description of which type of instruction is acceptable for each slot is provided in Figure 3-1 of the Hexagon V66 Programmer's Reference Manual (80-N2040-42) as well as in the detailed description of each instruction. For convenience, this figure is reproduced below. Instruction slots Other restrictions outlined in the Instruction packets section of the same manual occasionally cause a packet that follows the resource constraints to still generate an error message at compile time. However, these other rules come into play much less frequently, and you can learn about them over time.","title":"Scalar packing rules"},{"location":"software/system_performance/dsp_optimizations.html#hvx-packing-rules","text":"Rules on how to form packets in HVX are explained in the VLIW packing rules section of the Hexagon V66 HVX Programmer's Reference Manual (80-N2040-44). HVX instructions share the same slots as V6x, and there are restrictions on which slot each HVX instruction uses. However, unlike with V6x, resources and slots are not correlated one-to-one. When grouping HVX instructions, it is best to focus on understanding which resources each HVX instruction share, and understanding how this impacts the ability to group instructions in a packet; slot restrictions rarely come into consideration when writing HVX code. Resource restrictions are summarized in the Hexagon V66 HVX Programmer's Reference Manual (80-N2040-44) and reproduced in the table below for convenience. The detailed description of each HVX instruction also indicates which HVX resources it consumes. HVX slot/resource/latency summary Be aware that some instructions, such as vrmpy , come in different flavors: some that consume two HVX resources, and some that consume only one resource. For example, halfword multiplies use both multiply resources, which means that no other HVX multiply instruction is present in the same packet. When trying to optimize the inner loop of a function multiply-bound, plan early on how to maximize multiply resource utilization in as many packets as possible. NOTE: Unlike scalar stores, you cannot group two HVX stores in a single packet.","title":"HVX packing rules"},{"location":"software/system_performance/dsp_optimizations.html#reduce-stalls","text":"In addition to executing as many instructions as possible in any given packet to maximize parallelization, it is important to be aware of latencies that might cause the processor to stall. This section discusses the most common causes of stalls that deteriorate performance.","title":"Reduce stalls"},{"location":"software/system_performance/dsp_optimizations.html#instruction-latencies","text":"","title":"Instruction latencies"},{"location":"software/system_performance/dsp_optimizations.html#thread-vs-processor-cycles","text":"Hexagon cores dynamically schedule packets from threads into the core pipeline. The number of cycles to execute a packet varies depending on the behavior of other threads. The optimal schedule for a single thread running in isolation can be different than the optimal schedule for a thread running with other threads. For example, when multiple threads execute in parallel, one thread executes every other processor cycle. This means that if the processor is clocked at 800 MHz and four threads execute in parallel, each thread runs effectively at 400 MHz. However, when a thread is idle, another thread might be able to steal some of its cycles, allowing it to run faster than it runs if all hardware threads were busy. In practice, on Hexagon versions up through the SM8250 device, a single-threaded workload might run up to 20% to 30% faster if it is the only running thread, compared to when it is concurrent with other running software threads that are consuming all the hardware threads. The following sections provide general rules on latency scheduling assuming at least two threads are running. These rules provide a simplified programming model that is reasonable to use when writing optimized code. Using this model, we introduce the concept of a packet delay. This delay is the number of packets that are to be scheduled between two dependent packets to prevent the thread from stalling.","title":"Thread vs. processor cycles"},{"location":"software/system_performance/dsp_optimizations.html#scalar-latencies","text":"Instructions have no packet delays, with the following exceptions: Instructions that are paired with the .new predicate, which allows two sequential instructions to execute within the same packet. Mispredicted jumps, which typically incur around five packet stalls. For more information on speculative branches, see the Compare jumps section in the Hexagon V66 Programmer's Reference Manual (80-N2040-42). NOTE: When possible, try using hardware loops; they do not generate stalls, even when exiting the loop. Long-latency instructions that consume the result of another long-latency instruction. These instructions experience a one-packet delay with the exception of back-to-back multiplies that share the same accumulator and thus do not experience any delay. Long latency instructions are all the load, multiply, and float instructions. For example, the following instruction sequences stall: { R2=mpy(R0.L,R0.L) } // one-cycle stall { R3=mpy(R2.L,R2.L) } ... { R2 = memw(R1) } // one-cycle stall { R3=mpy(R2.L,R2.L) } But this instruction sequence does not stall: { R2=mpy(R0.L,R0.L) } { R2+=mpy(R1.L,R1.L) } // no stall","title":"Scalar latencies"},{"location":"software/system_performance/dsp_optimizations.html#hvx-latencies","text":"The Instruction latency section of the Hexagon V66 HVX Programmer's Reference Manual (80-N2040-44) discusses latencies. This section discusses the most common HVX arithmetic instruction sequences responsible for stalls. The most common causes of stalls are one-packet delays present when the following instructions consume a result that was produced in the previous packet: Multiplies NOTE: Back-to-back multiplies that only share the same accumulator do not stall. Shift and permute operations The Instruction latency section provides some examples of these rules: { V8 = VADD(V0,V0) } { V0 = VADD(V8,V9) } // NO STALL { V1 = VMPY(V0,R0) } // STALL due to V0 { V2 = VSUB(V2,V1) } // NO STALL on V1 { V5:4 = VUNPACK(V2) } // STALL due to V2 { V2 = VADD(V0,V4) } // NO STALL on V4","title":"HVX latencies"},{"location":"software/system_performance/dsp_optimizations.html#memory-latencies","text":"Scalar data memory accesses go through a two-level cache hierarchy, while HVX memory accesses only transit through the L2 memory. Cache sizes vary depending on the exact chip variant: L1 cache sizes are 16 to 32 KB L2 cache sizes are 128 to 1024 KB on aDSP variants, and 512 to 2048 KB on cDSP variants To avoid cache misses when writing optimized applications, it is critical to reduce the data memory throughput and maximize data locality. Common data optimization techniques exploiting data locality include the following techniques: Register data reuse The application stores values into registers for later use. For example, applying a filter on multiple lines at the same time allows holding of coefficients in registers, thus reducing the overall data bandwidth. Tiling A tile defines a small region of an image. The application processes an image one tile at a time or a few tiles at a time. This approach might be appropriate when using scalar instructions rather than HVX instructions because it allows preservation of data within L1 and thus maximizes scalar processing throughput. Larger buffers, such as groups of image lines, are typically too large for L1. Using a tiling approach typically comes at the cost of greater programming complexity and more non-linear data addressing. Line processing The application processes an image a few lines at a time. This is the most common approach for HVX implementations as it allows to load entire HVX vectors and leverage the large L2 memory cache size. Another type of cache optimization consists of explicitly managing cache contents by way of prefetching data into cache and, more rarely, invalidating cache line contents. Leave this optimization for the end after you have already ensured a maximum of data locality in your code. The Hexagon V66 Programmer's Reference Manual (80-N2040-42) details the L2 cache prefetching mechanism. For an example that shows how to perform L2 prefetching, see the multithreading project example. NOTE: It is common to optimize an application on a single-thread first and then multi-thread the code later. When using that approach, extrapolating multi-threaded performance from single-thread performance can occasionally be misleading: memory bandwidth might not be a bottleneck when only one thread is running, but it become one of the limiting resources when more threads run in parallel. As a result, it is a good practice to write applications as conservatively as possible with respect to memory bandwidth usage, regardless of the performance of the single-threaded code. Although data memory latencies depend on many parameters and architecture variants, it is useful to know to a first order the cost of memory accesses when planning the optimization of an application. The following numbers are rough estimates on what to expect in making memory accesses: DDR memory access: ~250 ns L2 read latency: 6 thread cycles HVX has a mechanism for pushing HVX instructions into a queue called VFIFO. As long as no mispredicted branches occur, this queue remains full. The L2 reads triggered by VMEM instructions occur enough in advance that the result from a VMEM load is available in the next cycle without stalling. In other words, for HVX, L2 reads have a one-cycle latency, and the following instruction sequence does not stall as long as no mispredicted branch has occurred recently: { V0 = VMEM(R0++) } { V1 = VADD(V0,V1) } // No stall if no recent mispredicted branch Note: The L2 VMEM read latency is higher--around 15 thread cycles--when following an L2 VMEM store to the same location. The reason for this delay is that the store must fully reach L2 before the load starts. For active data that do not fit in the vector register file, consider using VTCM instead of L2 to reduce the store-to-load penalty. Maximum sustainable read-write L2 bandwidth with no bank conflicts: 128 bytes per processor cycle Also, the HVX engine is directly connected to L2 cache, bypassing L1. HVX instructions are pipelined deeply enough to avoid any observed latencies for L2 loads or stores (when the pipeline is full and L2 traffic is not very congested). However, due to the depth of the HVX pipeline, it is expensive to do a transfer from an HVX register to a scalar register, or to perform a scalar memory load from an address following an HVX store to the same cache line. When using HVX in performance-sensitive code, do all loads, stores, and arithmetic via HVX instructions, and use scalar instructions and registers only for addressing, controlling, or processing on a different data set than that being done in HVX.","title":"Memory latencies"},{"location":"software/system_performance/dsp_optimizations.html#software-pipelining","text":"The Hexagon instruction set allows multiple unrelated instructions within one packet. This flexibility provides great opportunities for parallelizing the code, which are best exploited by doing software pipelining. Software pipelining consists of processing a few consecutive instances of a loop in parallel in order to reduce data dependencies and provide more opportunities for operations to be executed in parallel. This approach comes at the expense of having separate code for prologue and epilogue code. For example, a loop does the following: Processes data loads for iteration n+2 Performs some computations for iterations n and n+1 Stores the results of iteration n In this case, the prologue and epilogue code must handle the first and last two- or three-loop iterations separately, and it handles cases where the number of loop iterations is small. The Hexagon instruction set allows for a decrease in the complexity of the prologue and epilogue code by supporting pipelined hardware loops. Pipelined hardware loops set predicate registers after a loop has been iterated a specific number of times, thus allowing some operations (typically stores) to execute only after a few loop iterations. For more information on this approach, see the Pipelined hardware loops section in the Hexagon V66 Programmer's Reference Manual (80-N2040-42). NOTE: The Hexagon compiler automatically conducts software pipelining of appropriate loops.","title":"Software pipelining"},{"location":"software/system_performance/dsp_optimizations.html#hvx-optimizations","text":"HVX adds a powerful set of instructions that allow processing of large vectors very efficiently. The Hexagon V66 HVX Programmer's Reference Manual (80-N2040-44) is the authoritative source for HVX instruction syntax and behavior for any given revision. The following sections highlight some these instructions.","title":"HVX optimizations"},{"location":"software/system_performance/dsp_optimizations.html#when-to-use-hvx","text":"HVX vectors are 128-byte wide. As a result, HVX lends itself well to sequences of identical operations on contiguous 32-bit, 16-bit, or 8-bit elements. Thus, HVX is ideally suited for some application spaces such as image processing where many operations are to be applied independently to continuous pixels. However, it does not mean that HVX is restricted to only perform operations on contiguous elements in memory, as explained below. Although HVX memory loads and memory stores access contiguous elements in memory, HVX provides several powerful instructions for shuffling and interleaving elements between and within HVX vectors. These instructions allow HVX to efficiently process non-continuous elements that follow some predictable patterns, such as odd and even elements or vertical lines. The next section discusses these instructions in more detail. For portions of code that only operate sequentially, one element at a time, and where no parallelism opportunity is found, using V6x instructions and letting other threads use the HVX resources is often the best approach.","title":"When to use HVX"},{"location":"software/system_performance/dsp_optimizations.html#hvx-byte-manipulations","text":"Depending on the nature of the algorithm being ported on HVX, it might be necessary to rearrange elements from an HVX vector or pair of HVX vectors in various ways. Several HVX instructions allow you to address this challenge. The following figure describes these instructions and provides a visual summary of the instructions. Summary of the most common HVX element manipulations valign, vlalign, vror These three instructions are straightforward: valign and vlalign create an HVX vector made out of the lowest bytes of one vector and the upper bytes of another vector. vror performs a circular rotation of an HVX vector by an arbitrary number of bytes. vpacke, vpacko, vpack, vunpack, vunpacko, vdeal vpacke and vpacko pack the even or odd 32-bit or 16-bit elements of two HVX vectors into one vector. vpack performs an element size reduction, shrinking the contents of two HVX vectors into one vector after saturation. vunpack and vunpacko are the opposite forms of vpack , respectively unpacking the even and odd 8-bit or 16-bit elements into elements twice as large. vdeal (the flavor that consumes one input register) operates in the same way as vpacke and vpacko combined, but it operates on half the number of elements: it packs the even elements from the input register into the lower half of the output register, and packs the odd elements into the upper half of the register. vshuffe, vshuffo, vshuffoe, vshuff vshuffe and vshuffo are similar to their counterparts, vpacke and vpacko , in that they move the even or odd elements of two HVX into one HVX vector. The difference with their vpack counterparts is that elements from both input HVX vectors are interleaved (the contents of both input register is being shuffled into one register). vshuffoe executes both vshuffe and vshuffo at the same time and generates a register pair where the two registers in the pair are the output from the vshuffo and vshuffe instructions. vshuff (the flavor that consumes one input register) interleaves the elements from the upper and lower parts of a register into another register. The vshuff and vpack variants are helpful in different use cases. For example, if an HVX register contains pairs of (x,y) coordinates, use vpacke and vpacko to separate the x and y elements into different vectors. On the other hand, vshuffo or vshuffe can follow HVX instructions that produce HVX vectors with double precision, and store the results of consecutive operations in the upper and lower registers of a pair. vasr A narrowing shift: it takes two input HVX vectors and returns one output HVX vector. The narrowing shift is applied on each element of the two input HVX registers, and thus it produces output with the same order as a vshuffe or vshuffo instruction. Cross-lane vshuff, vdeal These instructions are very powerful but not trivial to understand. They perform a multi-level transpose operation between groups of registers. The most common configurations used with vshuff and vdeal are for positive and negative powers of 2. vshuff with an element size of Rt = 2 N places the 2 N -byte even elements from both input vectors into the low register of the output pair, and the 2 N -byte odd elements into the high register. This operation is a generalization of vshuffoe to larger element sizes. vshuff with an element size of Rt = -2 N interleaves the 2 N -byte elements from both input vectors into the output register. This operation is a generalization of the non-Cross-lane variant of vshuff to larger element sizes. vdeal with an element size of Rt = 2 N is identical to vshuff . vdeal with an element size of Rt = -2 N packs the 2 N -byte even elements from both input vectors into the low output register pair, and the odd elements into the high pair. For N=0 , this instruction is the same as executing packo and packe at the same time. vdelta, vrdelta vdelta and vrdelta use a network of switchboxes to permute or copy bytes within an HVX vector. Consider using these instructions when you need transforms with some irregular patterns not covered in the operations listed above. vdelta and vrdelta are configured with an HVX vector. The simplest and safest way of determining the configuration values for this register is to use a configuration tool that is provided with the SDK tools under {HEXAGON_SDK_ROOT}/tools/HEXAGON_Tools/<version number>/Examples/libcore/Vdelta_Helper/General_permute_network.html . At the bottom of this html page, you can specify a pattern in which the bytes of the input vector should be reordered. For example, if bytes 1 and 5 are to be dropped from the input vector, specify in the TPERM[N] control box a sequence of bytes that begins with 0, 2, 3, 4, 6, ... . Once the output pattern is fully specified, click Submit for Benes or Submit for Delta to retrieve the configuration, if one exists, and perform the pattern transformation using either a sequence of a vrdelta and vdelta instructions (Benes approach) or one vrdelta instruction.","title":"HVX byte manipulations"},{"location":"software/system_performance/dsp_optimizations.html#memory-operations","text":"Aligned and unaligned HVX memory operations may be performed in C without using intrinsics.","title":"Memory operations"},{"location":"software/system_performance/dsp_optimizations.html#aligned-hvx-loads-and-stores","text":"Dereferencing an HVX pointer using the HVX_Vector type defined in $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/target/hexagon/include/hexagon_types.h results in aligned HVX VMEM instructions for loads and stores that ignore the lowest bits of the pointer to always align the load or store to the HVX vector size: HVX_Vector* hvx_ptr = (HVX_Vector*)ptr; HVX_Vector hvx_value = *hvx_ptr; Assuming hvx_value is mapped into V0 and ptr into R0, this code is compiled into the following instruction in assembly: V0 = VMEM(R0) // (R0 & 127) bits are ignored Similarly, storing to an HVX pointer results in ignoring the lowest bits to perform a VMEM store. The following code: HVX_Vector* hvx_ptr = (HVX_Vector*)ptr; *hvx_ptr = hvx_value; is compiled into the following single instruction, assuming hvx_value is mapped into V0 and ptr into R0: VMEM(R0) = V0 // (R0 & 127) bits are ignored","title":"Aligned HVX loads and stores"},{"location":"software/system_performance/dsp_optimizations.html#unaligned-hvx-loads-and-stores","text":"To perform unaligned loads and stores, you can define the following macro: #define vmemu(A) *(( HVX_UVector*)(A)) NOTE: Using unaligned HVX load/store operations is inherently less efficient than using aligned load/store operations combined with explicit HVX alignment instructions VALIGN and VLALIGN. With these, you can perform unaligned loads and stores in C the same way that you would in assembly: HVX_Vector value = vmemu(ptr) vmemu(ptr) = new_value results in assembly in an unaligned VMEMU load and an unaligned VMEMU store operations. V0 = VMEMU(R0) // All address bits from R0 are used when performing the unaligned load VMEMU(R0) = V1 // All address bits from R0 are used when performing the unaligned store","title":"Unaligned HVX loads and stores"},{"location":"software/system_performance/dsp_optimizations.html#array-stores-of-arbitrary-sizes","text":"Stores of arrays of arbitrary sizes may be accomplished with the following C helper function that leverages HVX bytewise-enabled stores. Note that this approach is only recommended for rare boundary cases, as its performance is significantly worse than regular aligned or unaligned stores. #include \"hexagon_types.h\" #define VLEN 128 // This stores the first n bytes from vector vin to address 'addr'. // n must be in range 1..VLEN, addr may have any alignment. // Implementation does one or two masked stores. static inline void q6op_vstu_variable_ARV( void * addr, int n, HVX_Vector vin) { vin = Q6_V_vlalign_VVR( vin, vin, (size_t)addr); //rotate as needed. unsigned left_off = (size_t)addr & (#VLEN-1); unsigned right_off = left_off + n; HVX_VectorPred qL_not = Q6_Q_vsetq_R( (size_t)addr ); HVX_VectorPred qR = Q6_Q_vsetq2_R( right_off ); if( right_off > 128 ){ Q6_vmaskedstoreq_QAV( qR, (HVX_Vector*)addr + 1, vin); qR = Q6_Q_vcmp_eq_VbVb( vin,vin); // all 1's } qL_not = Q6_Q_or_QQn( qL_not, qR ); Q6_vmaskedstorenq_QAV( qL_not,(HVX_Vector*)addr, vin ); }","title":"Array stores of arbitrary sizes"},{"location":"software/system_performance/dsp_optimizations.html#accessing-scalar-contents-from-an-hvx-register","text":"The recommended way to access an element from an HVX register is to go through memory instead of using vextract. This approach is best accomplished using a union. For example, the following code extracts the first 16-bit element of HVX register hvx_value into the 16-bit variable first_element : union {int16_t array[ELEM_PER_VEC]; HVX_Vector vector; } HVX_and_array; HVX_and_array.vector = hvx_value; // turns into an HVX store to memory int16_t first_element = HVX_and_array.array[0]; // turns into a scalar read from memory Expect a delay of tens of cycles between the HVX write to memory and the scalar read.","title":"Accessing scalar contents from an HVX register"},{"location":"software/system_performance/dsp_optimizations.html#vtcmlookup","text":"V6x supports scatter/gather operations, allowing you to perform vectorized random-access memory lookups that are not limited to 256 entries as is the case with the vlut instruction. For more details on this instruction, see the Hexagon V66 HVX Programmer's Reference Manual User Guide . NOTE: The scatter/gather operations cause significant traffic in the VTCM subsytem, so they are prone to stalling when memory conflicts occur. For information on how to avoid scatter/gather stalls, see the Avoid scatter/gather stalls section of the Hexagon V66 HVX Programmer's Reference Manual User Guide .","title":"VTCM/lookup"},{"location":"software/system_performance/dsp_optimizations.html#float-support","text":"","title":"Float support"},{"location":"software/system_performance/dsp_optimizations.html#float-to-qfloat-conversions","text":"V68 supports HVX operations with float and qfloat data types. Conversion between these types is simply achieved using instructions that consume one type of data and produce another. For example: Conversion from an HVX vector made of IEEE 754 single float elements to an HVX vector made of qfloat elements: HVX_Vector vqf32 = Q6_Vqf32_vadd_VsfVsf(vsf, Q6_V_vzero()) Conversion from an HVX vector made of qfloat elements to an HVX vector made of IEEE 754 single float elements: HVX_Vector vsf = Q6_Vsf_equals_Vqf32(vqf32); The QHL HVX APIs include a number of helper functions to perform conversions between IEEE and Qualcomm float numbers including those shown above.","title":"Float to qfloat conversions"},{"location":"software/system_performance/dsp_optimizations.html#qfloat-precision","text":"Unlike float operations, qfloat operations do not normalize their ouput vectors. As a result, sequences of qfloat multiplies may lose some accuracy compared to their float equivalent and forcing normalization prior to the multiplies may be needed to achieve greater accuracy. This is for example the case when using the Horner's method for polynomial approximations where a variable goes through a chain of multiplies. The simplest way to normalize a number consists in adding zero to it. For example, 16-bit qfloat normalization is achieved as follows: x = Q6_Vqf16_vadd_Vqf16Vhf(x, Q6_V_vzero()); As a general rule of thumb, for greater accuracy: inputs to qfloat vmpy instructions should be normalized no normalization is needed for the inputs to a qfloat vadd operation","title":"qfloat precision"},{"location":"software/system_performance/resource_management.html","text":"Resource management Software running on the Hexagon DSP can use several different resources: hardware threads for code execution, external memory, caches, HVX, HMX, and VTCM. The DSP supports multiple applications running in parallel, each with multiple threads of execution, and thus the DSP resources must be shared across multiple processes. While many of the resources are managed automatically by the hardware or operating system, other resources must be managed explicitly by the application (notably HMX and VTCM). In both cases, many applications are better off explicitly reserving how much of the resources they need, executing one workload at a time at maximum efficiency when all resources are available, and releasing the resources for other clients to use next. This section discusses how the different resources are shared explicitly or implicitly, the impact on performance, and the use of the Compute Resource Manager to manage resources. For additional information see: Architecture overview for a discussion of Hexagon DSP hardware resources. DSP OS for information on the QuRT OS and OS-level resources such as threads. HAP Compute Resource Manager API for full documentation on the Compute Resource Manager APIs. Feature matrix for an overview of different Hexagon DSP versions in different products and their feature set differences. Compute Resource Manager Applications can use the HAP Compute Resource Manager APIs to reserve resources such as HMX and VTCM, and to serialize access to other resources shared automatically. In summary, most clients should do the following: Allocate and release all resources they need with one resource manager call. Use HAP_compute_res_attr_set_serialize() to serialize their access to DSP resources. Enable higher-priority clients to gain access to the shared resources. This step is needed when processing tasks that require more than a few milliseconds (about 5 ms) to complete. Allowing other higher-priority clients to acquire shared resources may be accomplished in one of two ways: Prior to Lahaina, the client should release and re-acquire resources frequently, every few milliseconds. Starting with Lahaina, the client should implement instead a resource release callback For details on the Compute Resource Manager, see the API documentation . The rest of this section discusses resource management performance implications and application design choices. In most cases, applications should reserve all the resources they require with a single resource manager call for each frame or inference to be processed. This avoids receiving partial allocations and reduces the number of resource manager calls made. However, if a client requires different resources for significantly different time durations, it should make two separate requests instead. For example, a client that requires VTCM through processing an entire 30 ms frame, but will only use HMX for the first few milliseconds of processing, might allocate HMX separately and release it early for other clients to use. However, HMX is unusable without VTCM, so clients that use all VTCM in the system can keep HMX reserved for the duration of their VTCM reservation without additional impact. Even clients that do not use explicitly managed resources such as HMX and VTCM can benefit from using the resource manager to serialize resource access. This avoids multiple applications competing to access processor cycles or memory, which can lead to unnecessary cache thrashing and context switches. In most cases it is more efficient to run each workload independently to completion, using all the DSP resources available, instead of running multiple workloads simultaneously on different hardware threads. Clients can reserve resources with the HAP_compute_res_attr_set_serialize() attribute set to serialize access to DSP resources; only one client with the serialize flag will run at a given time. Starting with Lahaina, the Compute Resource Manager supports release callbacks. Clients register a callback function with HAP_compute_res_attr_set_release_callback() , and the callback will be called when a higher-priority client requires some or all of the resources reserved. The client must finish its resource use within a short time window--for example, around five milliseconds--and release the resources back to the resource manager so the higher-priority client can acquire them. Clients can attempt to re-acquire resources immediately after releasing them; the resource manager will ensure the highest-priority client gets the resources as they become available. On devices earlier than Lahaina, the resource manager does not support release callbacks. On those devices, clients must periodically release their resources to ensure that they do not starve other higher-priority clients. Typically, this is done on a frame or inference boundary; but for long-running operations, clients might need to release and re-acquire resources more frequently. On targets with the resource release callback available through the compute_resource_attr_set_release_callback() API, clients are not required to release resources periodically if they have work items available. Instead, they can continue processing further workloads until they receive a release callback or run out of work. However, it is important to release resources when the client becomes idle to ensure that lower priority clients can make progress. For information on which resource manager features are supported on which chipset version, see the Compute Resource Manager documentation and the Feature matrix . External memory and caches External memory and DSP internal L1/L2 caches are readily accessible by all active threads. For those resources, your only concern is to ensure that each thread uses these resources as efficiently as possible because they are shared. For example, it is typically preferable to rewrite an algorithm to consume less memory even if doing so does not show any improvement when the algorithm executes in a single-threaded environment; reducing memory bandwidth will reduce the pressure on the memory system and potentially show benefit in a multithreaded context. Typically, the DSP L2 cache is fully configured as a cache and is managed automatically by the DSP cache controller. For most applications this is appropriate: you should aim to use the cache efficiently, maintaining good cache locality and using prefetch instructions where possible; but, you should let the system manage the cache automatically. However, the L2 cache also supports line locking, where portions of the L2 cache can be locked to specific sections of memory. L2 line locking is primarily required for two use cases: the camera streamer can use locked L2 as its target, and the UBWCDMA operates between external memory and locked L2 cache lines. Applications that require locked L2 cache can use the CDSP L2 Cache locking manager API to lock and unlock parts of the L2 cache. Only a subset of the L2 cache can be locked (for details, see the API). L2 cache line locking is only available for signed PDs. For discussion on signed vs unsigned PDs, see the system integration document . HVX HVX is shared among software threads without any direct intervention. The DSP has several HVX contexts, which the DSP OS allocates automatically to threads as they execute HVX instructions. The OS also saves and restores HVX registers and state as necessary to switch contexts between threads. Releasing and acquiring an HVX context is costly because HVX registers contain a large amount of data that must be saved and restored. Avoid unnecessary HVX context switches by ensuring that thread priorities are set appropriately, and do not attempt to use more HVX contexts than the system has. Use qurt_hvx_get_units() to query the number of HVX units that are available. HMX HMX is not shared automatically by the OS or hardware. Instead, clients must use the the HAP Compute Resource Manager API to reserve HMX before attempting to use it, and release it promptly after use. For more information on resource management, see Compute Resource Manager above. Only one application can use HMX at a time, making resource management especially important for HMX-based applications. Most developers do not use HMX directly, but instead they access it through libraries such as QNN. In this case, the library takes care of HMX resource management. HMX cannot be used without VTCM, so all clients that require HMX must also allocate at least some VTCM by using the resource manager. VTCM Like HMX, VTCM is not an automatically shared resource. Instead, clients must use the HAP Compute Resource Manager API to reserve allocate VTCM before using it, and release it promptly when not in use. Unlike HMX however, there is a pool of VTCM in the DSP, and multiple clients can allocate subsets of it simultaneously. Managing VTCM allocations across applications is important for the best performance. Many applications perform better when more VTCM is available to them. This can be especially true for machine learning runtime libraries. For those applications, the best VTCM management strategy is to allocate all VTCM available, use it together with other system resources until the workload is complete or a higher priority client requires some of the resources, and release all resources when done. This can lead to best performance for the application, but it blocks other applications from using VTCM in parallel. It forces them to wait for the current VTCM user to complete before they can allocate any resources, which can lead to delays of several milliseconds. On devices with critical use cases that require VTCM such as camera streaming, the system integrator can create dedicated VTCM partitions for dedicated use cases. This will reduce the amount of VTCM available for regular applications, but it will ensure that such critical applications always have VTCM available when required. For a discussion on VTCM partitions, see the System integration page . NOTE: Clients that require HMX must wait for it to become available, even if they have a dedicated VTCM partition available.","title":"Resource management"},{"location":"software/system_performance/resource_management.html#resource-management","text":"Software running on the Hexagon DSP can use several different resources: hardware threads for code execution, external memory, caches, HVX, HMX, and VTCM. The DSP supports multiple applications running in parallel, each with multiple threads of execution, and thus the DSP resources must be shared across multiple processes. While many of the resources are managed automatically by the hardware or operating system, other resources must be managed explicitly by the application (notably HMX and VTCM). In both cases, many applications are better off explicitly reserving how much of the resources they need, executing one workload at a time at maximum efficiency when all resources are available, and releasing the resources for other clients to use next. This section discusses how the different resources are shared explicitly or implicitly, the impact on performance, and the use of the Compute Resource Manager to manage resources. For additional information see: Architecture overview for a discussion of Hexagon DSP hardware resources. DSP OS for information on the QuRT OS and OS-level resources such as threads. HAP Compute Resource Manager API for full documentation on the Compute Resource Manager APIs. Feature matrix for an overview of different Hexagon DSP versions in different products and their feature set differences.","title":"Resource management"},{"location":"software/system_performance/resource_management.html#compute-resource-manager","text":"Applications can use the HAP Compute Resource Manager APIs to reserve resources such as HMX and VTCM, and to serialize access to other resources shared automatically. In summary, most clients should do the following: Allocate and release all resources they need with one resource manager call. Use HAP_compute_res_attr_set_serialize() to serialize their access to DSP resources. Enable higher-priority clients to gain access to the shared resources. This step is needed when processing tasks that require more than a few milliseconds (about 5 ms) to complete. Allowing other higher-priority clients to acquire shared resources may be accomplished in one of two ways: Prior to Lahaina, the client should release and re-acquire resources frequently, every few milliseconds. Starting with Lahaina, the client should implement instead a resource release callback For details on the Compute Resource Manager, see the API documentation . The rest of this section discusses resource management performance implications and application design choices. In most cases, applications should reserve all the resources they require with a single resource manager call for each frame or inference to be processed. This avoids receiving partial allocations and reduces the number of resource manager calls made. However, if a client requires different resources for significantly different time durations, it should make two separate requests instead. For example, a client that requires VTCM through processing an entire 30 ms frame, but will only use HMX for the first few milliseconds of processing, might allocate HMX separately and release it early for other clients to use. However, HMX is unusable without VTCM, so clients that use all VTCM in the system can keep HMX reserved for the duration of their VTCM reservation without additional impact. Even clients that do not use explicitly managed resources such as HMX and VTCM can benefit from using the resource manager to serialize resource access. This avoids multiple applications competing to access processor cycles or memory, which can lead to unnecessary cache thrashing and context switches. In most cases it is more efficient to run each workload independently to completion, using all the DSP resources available, instead of running multiple workloads simultaneously on different hardware threads. Clients can reserve resources with the HAP_compute_res_attr_set_serialize() attribute set to serialize access to DSP resources; only one client with the serialize flag will run at a given time. Starting with Lahaina, the Compute Resource Manager supports release callbacks. Clients register a callback function with HAP_compute_res_attr_set_release_callback() , and the callback will be called when a higher-priority client requires some or all of the resources reserved. The client must finish its resource use within a short time window--for example, around five milliseconds--and release the resources back to the resource manager so the higher-priority client can acquire them. Clients can attempt to re-acquire resources immediately after releasing them; the resource manager will ensure the highest-priority client gets the resources as they become available. On devices earlier than Lahaina, the resource manager does not support release callbacks. On those devices, clients must periodically release their resources to ensure that they do not starve other higher-priority clients. Typically, this is done on a frame or inference boundary; but for long-running operations, clients might need to release and re-acquire resources more frequently. On targets with the resource release callback available through the compute_resource_attr_set_release_callback() API, clients are not required to release resources periodically if they have work items available. Instead, they can continue processing further workloads until they receive a release callback or run out of work. However, it is important to release resources when the client becomes idle to ensure that lower priority clients can make progress. For information on which resource manager features are supported on which chipset version, see the Compute Resource Manager documentation and the Feature matrix .","title":"Compute Resource Manager"},{"location":"software/system_performance/resource_management.html#external-memory-and-caches","text":"External memory and DSP internal L1/L2 caches are readily accessible by all active threads. For those resources, your only concern is to ensure that each thread uses these resources as efficiently as possible because they are shared. For example, it is typically preferable to rewrite an algorithm to consume less memory even if doing so does not show any improvement when the algorithm executes in a single-threaded environment; reducing memory bandwidth will reduce the pressure on the memory system and potentially show benefit in a multithreaded context. Typically, the DSP L2 cache is fully configured as a cache and is managed automatically by the DSP cache controller. For most applications this is appropriate: you should aim to use the cache efficiently, maintaining good cache locality and using prefetch instructions where possible; but, you should let the system manage the cache automatically. However, the L2 cache also supports line locking, where portions of the L2 cache can be locked to specific sections of memory. L2 line locking is primarily required for two use cases: the camera streamer can use locked L2 as its target, and the UBWCDMA operates between external memory and locked L2 cache lines. Applications that require locked L2 cache can use the CDSP L2 Cache locking manager API to lock and unlock parts of the L2 cache. Only a subset of the L2 cache can be locked (for details, see the API). L2 cache line locking is only available for signed PDs. For discussion on signed vs unsigned PDs, see the system integration document .","title":"External memory and caches"},{"location":"software/system_performance/resource_management.html#hvx","text":"HVX is shared among software threads without any direct intervention. The DSP has several HVX contexts, which the DSP OS allocates automatically to threads as they execute HVX instructions. The OS also saves and restores HVX registers and state as necessary to switch contexts between threads. Releasing and acquiring an HVX context is costly because HVX registers contain a large amount of data that must be saved and restored. Avoid unnecessary HVX context switches by ensuring that thread priorities are set appropriately, and do not attempt to use more HVX contexts than the system has. Use qurt_hvx_get_units() to query the number of HVX units that are available.","title":"HVX"},{"location":"software/system_performance/resource_management.html#hmx","text":"HMX is not shared automatically by the OS or hardware. Instead, clients must use the the HAP Compute Resource Manager API to reserve HMX before attempting to use it, and release it promptly after use. For more information on resource management, see Compute Resource Manager above. Only one application can use HMX at a time, making resource management especially important for HMX-based applications. Most developers do not use HMX directly, but instead they access it through libraries such as QNN. In this case, the library takes care of HMX resource management. HMX cannot be used without VTCM, so all clients that require HMX must also allocate at least some VTCM by using the resource manager.","title":"HMX"},{"location":"software/system_performance/resource_management.html#vtcm","text":"Like HMX, VTCM is not an automatically shared resource. Instead, clients must use the HAP Compute Resource Manager API to reserve allocate VTCM before using it, and release it promptly when not in use. Unlike HMX however, there is a pool of VTCM in the DSP, and multiple clients can allocate subsets of it simultaneously. Managing VTCM allocations across applications is important for the best performance. Many applications perform better when more VTCM is available to them. This can be especially true for machine learning runtime libraries. For those applications, the best VTCM management strategy is to allocate all VTCM available, use it together with other system resources until the workload is complete or a higher priority client requires some of the resources, and release all resources when done. This can lead to best performance for the application, but it blocks other applications from using VTCM in parallel. It forces them to wait for the current VTCM user to complete before they can allocate any resources, which can lead to delays of several milliseconds. On devices with critical use cases that require VTCM such as camera streaming, the system integrator can create dedicated VTCM partitions for dedicated use cases. This will reduce the amount of VTCM available for regular applications, but it will ensure that such critical applications always have VTCM available when required. For a discussion on VTCM partitions, see the System integration page . NOTE: Clients that require HMX must wait for it to become available, even if they have a dedicated VTCM partition available.","title":"VTCM"},{"location":"software/system_performance/system_optimizations.html","text":"System optimizations This page discusses OS and interprocessor considerations that impact performance. For more information on how to optimize algorithmic code running on the DSP, see the DSP optimization page instead. Offload tasks onto the DSP Using the DSP offers several benefits: Compared to the CPU, the Hexagon DSP typically consumes much less power and is less susceptible to thermal concerns. In many cases that vectorize well on HVX, the DSP performs the same computations in less time (while at a lower clock) than multiple CPU cores. Moving large blocks of computational software to the DSP keeps the CPU unburdened for other tasks that might work well only on the CPU. Separately, the DSP is best suited for signal processing tasks, and it excels at any type of operation that can be parallelized. Running such tasks on the DSP uses the DSP to the best of its ability and results in significant gains with respect to power consumption. In summary, prioritize moving large signal-processing tasks onto the DSP and let the CPU run the control-oriented code and short individual processing functions. IPC performance considerations Communication between the CPU and DSP is performed through shared memory with interrupts. Offloading tasks from the CPU onto the DSP comes with a communication overhead. Because the CPU and DSP do not share a cache, maintenance operations are required on all buffers transacted between them. These operations can take a minimum of a few hundred microseconds (on DSPs without hardware IO-coherence with the CPU L2 cache, as discussed later in this section). Depending on system clock settings and CPU sleep modes enabled, the overhead for each invocation to the DSP could extend to several milliseconds. Hence, it is preferable to offload large tasks onto the DSP instead of invoking the DSP for small trivial tasks. To understand and improve overall performance, it is important to know what factors contribute to this overhead. FastRPC latency The latency of a FastRPC synchronous call is the amount of time spent from when the CPU thread initiates a call to the DSP until it can resume its operation, less the amount of time spent by the DSP to execute the task itself. Under optimized conditions, the FastRPC round-trip average latency is on the order of 200 to 700 microseconds on the latest targets. It is recommended to measure average FastRPC latency over multiple RPC calls instead of one call for consistent results as it depends on variable latencies like CPU wake up and scheduler delays. Reduce FastRPC overhead We begin with a brief list of recommendations for accomplishing the best FastRPC performance and then detail each of the main contributing factor to FastRPC performance. Recommendations for best FastRPC performance Allocate ION buffers using the RPCMEM APIs Enable FastRPC QoS mode with a low latency tolerance using the remote APIs For best performance, use the PM_QOS mode and a recommended QoS latency of 100 microseconds Use HAP_power APIs to vote for DSP clocks and DCVS according to requirements For best performance, vote for TURBO or TURBO_L1, and a vote for a 40 microsecond DSP sleep latency Make sure you are using an Android kernel build with performance kernel defconfig settings Use early wakeup hints using the HAP_send_early_signal API if possible ION buffers To achieve low FastRPC overhead, it is important to use ION buffers (available on Android targets), which do not require extra copy when shared between the DSP and CPU. Each non-ION buffer passed to the DSP in a FastRPC call is automatically copied by the FastRPC framework, thus resulting in higher FastRPC overhead when large buffers are used. Register the ION buffers with the FastRPC library. Otherwise, the driver treats unregistered ION buffers as non-ION and results in extra copy. The RPCMEM library provides an API for allocating shared buffers and automatically registering buffers with the FastRPC library. For more information, see RPCMEM API . Pre-allocated ION buffers can be directly registered with the FastRPC library using the remote_register_buf() function defined as part of the remote interface. For more information, see remote API . Cache coherence Coherency ensures that all processors see the latest data when accessing shared buffers through their respective caches. The FastRPC driver maintains cache coherency between the CPU and DSP for shared buffers that are accessed in a FastRPC call. Hardware-based IO coherency for the CPU is supported on most of the recent Snapdragon chipsets, which helps to reduce FastRPC latency significantly from several milliseconds to approximately one millisecond. IO coherence, also called one-way coherence, allows DSPs that support it to access the CPU caches on DSP load or store operations to maintain coherence continuously throughout DSP operations on shared buffers. For example, when the DSP populates a shared coherent output buffer, the CPU can read the data immediately without invalidating cache lines. IO coherency is enabled by default for all buffers. On chipsets without IO coherency hardware support, the FastRPC software driver invalidates and flushes CPU cache lines as necessary to ensure coherency and results in higher latency. For details, see the feature matrix . The FastRPC software driver also invalidates and flushes DSP cache lines to ensure DSP cache coherency. Cache maintenance time varies and depends on the DSP clocks and size of total buffers used in an RPC call. The driver cleans all cache lines of a user process (instead of cleaning by buffer addresses and lines) when the total size of shared buffers to be cleaned exceeds 1 MB. CPU wakeup and scheduling delays Idle CPU cores can enter low power sleep modes for saving power after sending a message to the DSP. Some of the power saving sleep modes include shutting down L1/L2 cache and core clocks. CPU wakeup delay can be several hundreds of microseconds. CPU wakeup delay varies and depends on the current system load and sleep mode. After receiving a response from the DSP, the CPU handles the response in the interrupt handler and sets a signal to the actual thread waiting for job completion. The CPU scheduler is invoked to schedule the actual waiting thread for further processing; this can add variable latency to the FastRPC overhead. The Hexagon SDK allows the user to select CPU modes that help manage FastRPC performance in typical conditions by disabling certain sleep states. These modes, referred as PM_QOS and ADAPT_QOS, mitigate the CPU wakeup latency and may be selected using the remote_handle_control() APIs from the remote library . When selecting one of these modes, the user also specifies a wakeup latency, which the driver will try to satisfy by enabling certain features and available techniques available on a given target. Note: There is no guarantee that the driver can meet the requested latency. Another way of reducing the wakeup latency is for the DSP to send anticipatory early completion signals to prompt the CPU to wakeup. This approach is suited for situations when the developer can determine when the DSP is about to complete the task that it was assigned. If the CPU wakes up before the DSP has completed its task, it will pull continuously until completion of the FastRPC call. It is currently supported with the HAP_send_early_signal API, which is replacing the deprecated API fastrpc_send_early_signal , both defined in $HEXAGON_SDK_ROOT/incs/HAP_ps.h . Pre-map buffers to the DSP The FastRPC driver supports transient and persistent mapping of a buffer to the remote DSP. By default, the FastRPC driver maps and unmaps buffers passed as arguments of FastRPC invocation at the beginning and end of that invocation respectively, consuming several uSec in each invocation. On SM8150, for example it was observed that mapping and unmapping a large buffer is taking ~20 us overhead on the average when L2 cache fully evicted by previous RPC call. The FastRPC library supports a flag RPCMEM_TRY_MAP_STATIC for implicitly mapping a buffer to the DSP during allocation. The FastRPC library tries to map buffers allocated with the RPCMEM_TRY_MAP_STATIC flag to the remote process of all current and new FastRPC sessions. In case of failure to map, the FastRPC library ignores the error and continues to open the session without pre-mapping the buffer. In case of success, buffers allocated with this flag will be pre-mapped to reduce the latency of upcoming FastRPC calls. Pre-mapped buffers will be automatically unmapped at either buffer free or session close. Note: RPC memory flag RPCMEM_TRY_MAP_STATIC and buffer attribute FASTRPC_ATTR_TRY_MAP_STATIC are supported from Lahaina and later targets only. Older targets ignore these flags and has no impact on the latency. The FastRPC driver searches each buffer passed to remote call in static mapping list and re-assign the same virtual address on DSP. As a result, pre-mapping many buffers can also add an extra latency for all FastRPC calls. Hence pre-mapping a buffer with RPCMEM_TRY_MAP_STATIC is recommended to use only for large buffers which are used with latency-critical RPC calls after profiling and estimating the actual reduction in FastRPC overhead. Pre-map during memory allocation with RPCMEM_TRY_MAP_STATIC flag using the following approach #include \"rpcmem.h\" src = (uint8_t *)rpcmem_alloc(RPCMEM_HEAP_ID_SYSTEM, RPCMEM_DEFAULT_FLAGS | RPCMEM_TRY_MAP_STATIC, srcSize); //... RPC calls with src buffer as parameter ... rpcmem_free(src); When an ION buffer is allocated without using rpcmem_alloc() function, register the buffer with an attribute FASTRPC_ATTR_TRY_MAP_STATIC for pre-map: #include \"remote.h\" remote_register_buf_attr(buffer, size, fd, FASTRPC_ATTR_TRY_MAP_STATIC); //... RPC calls with src buffer as parameter ... remote_register_buf(buffer, size, -1); // the -1 argument results in unregistering the buffer For more information refer to rpcmem API and remote API . CPU build configuration The CPU software builds on development platforms might have additional logging and debugging code built in, which can significantly impact FastRPC performance. QTI recommends measuring FastRPC overhead with full performance builds. Production devices always use performance builds. Development builds should use performance kernel defconfig settings. DSP sleep latency The DSP can enter low power sleep mode after sending a response to the CPU, and if it becomes idle with no further jobs to process. The time to bring the DSP out of sleep mode after it receives an interrupt from the CPU adds additional latency to FastRPC overhead for processing the newly offloaded job. The impact of sleep latency on FastRPC latency is higher when the jobs are submitted periodically and the idle DSP enters sleep mode between the jobs. If the jobs are submitted back-to-back, the DSP might not go to sleep and latency impact will be minimal. The FastRPC driver votes for a default sleep latency during session open. QTI recommends overwriting the sleep latency setting based on the use case and latency requirement. For more information, see the HAP power API . Speed and power management Controlling the clock speed of the various performance-critical components on the chip allows the application to trade power for speed. The Hexagon SDK provides a set of APIs that allow the DSP to control its own performance and power modes. To learn more about these APIs, see Performance and power manager .","title":"System-level optimizations"},{"location":"software/system_performance/system_optimizations.html#system-optimizations","text":"This page discusses OS and interprocessor considerations that impact performance. For more information on how to optimize algorithmic code running on the DSP, see the DSP optimization page instead.","title":"System optimizations"},{"location":"software/system_performance/system_optimizations.html#offload-tasks-onto-the-dsp","text":"Using the DSP offers several benefits: Compared to the CPU, the Hexagon DSP typically consumes much less power and is less susceptible to thermal concerns. In many cases that vectorize well on HVX, the DSP performs the same computations in less time (while at a lower clock) than multiple CPU cores. Moving large blocks of computational software to the DSP keeps the CPU unburdened for other tasks that might work well only on the CPU. Separately, the DSP is best suited for signal processing tasks, and it excels at any type of operation that can be parallelized. Running such tasks on the DSP uses the DSP to the best of its ability and results in significant gains with respect to power consumption. In summary, prioritize moving large signal-processing tasks onto the DSP and let the CPU run the control-oriented code and short individual processing functions.","title":"Offload tasks onto the DSP"},{"location":"software/system_performance/system_optimizations.html#ipc-performance-considerations","text":"Communication between the CPU and DSP is performed through shared memory with interrupts. Offloading tasks from the CPU onto the DSP comes with a communication overhead. Because the CPU and DSP do not share a cache, maintenance operations are required on all buffers transacted between them. These operations can take a minimum of a few hundred microseconds (on DSPs without hardware IO-coherence with the CPU L2 cache, as discussed later in this section). Depending on system clock settings and CPU sleep modes enabled, the overhead for each invocation to the DSP could extend to several milliseconds. Hence, it is preferable to offload large tasks onto the DSP instead of invoking the DSP for small trivial tasks. To understand and improve overall performance, it is important to know what factors contribute to this overhead.","title":"IPC performance considerations"},{"location":"software/system_performance/system_optimizations.html#fastrpc-latency","text":"The latency of a FastRPC synchronous call is the amount of time spent from when the CPU thread initiates a call to the DSP until it can resume its operation, less the amount of time spent by the DSP to execute the task itself. Under optimized conditions, the FastRPC round-trip average latency is on the order of 200 to 700 microseconds on the latest targets. It is recommended to measure average FastRPC latency over multiple RPC calls instead of one call for consistent results as it depends on variable latencies like CPU wake up and scheduler delays.","title":"FastRPC latency"},{"location":"software/system_performance/system_optimizations.html#reduce-fastrpc-overhead","text":"We begin with a brief list of recommendations for accomplishing the best FastRPC performance and then detail each of the main contributing factor to FastRPC performance.","title":"Reduce FastRPC overhead"},{"location":"software/system_performance/system_optimizations.html#recommendations-for-best-fastrpc-performance","text":"Allocate ION buffers using the RPCMEM APIs Enable FastRPC QoS mode with a low latency tolerance using the remote APIs For best performance, use the PM_QOS mode and a recommended QoS latency of 100 microseconds Use HAP_power APIs to vote for DSP clocks and DCVS according to requirements For best performance, vote for TURBO or TURBO_L1, and a vote for a 40 microsecond DSP sleep latency Make sure you are using an Android kernel build with performance kernel defconfig settings Use early wakeup hints using the HAP_send_early_signal API if possible","title":"Recommendations for best FastRPC performance"},{"location":"software/system_performance/system_optimizations.html#ion-buffers","text":"To achieve low FastRPC overhead, it is important to use ION buffers (available on Android targets), which do not require extra copy when shared between the DSP and CPU. Each non-ION buffer passed to the DSP in a FastRPC call is automatically copied by the FastRPC framework, thus resulting in higher FastRPC overhead when large buffers are used. Register the ION buffers with the FastRPC library. Otherwise, the driver treats unregistered ION buffers as non-ION and results in extra copy. The RPCMEM library provides an API for allocating shared buffers and automatically registering buffers with the FastRPC library. For more information, see RPCMEM API . Pre-allocated ION buffers can be directly registered with the FastRPC library using the remote_register_buf() function defined as part of the remote interface. For more information, see remote API .","title":"ION buffers"},{"location":"software/system_performance/system_optimizations.html#cache-coherence","text":"Coherency ensures that all processors see the latest data when accessing shared buffers through their respective caches. The FastRPC driver maintains cache coherency between the CPU and DSP for shared buffers that are accessed in a FastRPC call. Hardware-based IO coherency for the CPU is supported on most of the recent Snapdragon chipsets, which helps to reduce FastRPC latency significantly from several milliseconds to approximately one millisecond. IO coherence, also called one-way coherence, allows DSPs that support it to access the CPU caches on DSP load or store operations to maintain coherence continuously throughout DSP operations on shared buffers. For example, when the DSP populates a shared coherent output buffer, the CPU can read the data immediately without invalidating cache lines. IO coherency is enabled by default for all buffers. On chipsets without IO coherency hardware support, the FastRPC software driver invalidates and flushes CPU cache lines as necessary to ensure coherency and results in higher latency. For details, see the feature matrix . The FastRPC software driver also invalidates and flushes DSP cache lines to ensure DSP cache coherency. Cache maintenance time varies and depends on the DSP clocks and size of total buffers used in an RPC call. The driver cleans all cache lines of a user process (instead of cleaning by buffer addresses and lines) when the total size of shared buffers to be cleaned exceeds 1 MB.","title":"Cache coherence"},{"location":"software/system_performance/system_optimizations.html#cpu-wakeup-and-scheduling-delays","text":"Idle CPU cores can enter low power sleep modes for saving power after sending a message to the DSP. Some of the power saving sleep modes include shutting down L1/L2 cache and core clocks. CPU wakeup delay can be several hundreds of microseconds. CPU wakeup delay varies and depends on the current system load and sleep mode. After receiving a response from the DSP, the CPU handles the response in the interrupt handler and sets a signal to the actual thread waiting for job completion. The CPU scheduler is invoked to schedule the actual waiting thread for further processing; this can add variable latency to the FastRPC overhead. The Hexagon SDK allows the user to select CPU modes that help manage FastRPC performance in typical conditions by disabling certain sleep states. These modes, referred as PM_QOS and ADAPT_QOS, mitigate the CPU wakeup latency and may be selected using the remote_handle_control() APIs from the remote library . When selecting one of these modes, the user also specifies a wakeup latency, which the driver will try to satisfy by enabling certain features and available techniques available on a given target. Note: There is no guarantee that the driver can meet the requested latency. Another way of reducing the wakeup latency is for the DSP to send anticipatory early completion signals to prompt the CPU to wakeup. This approach is suited for situations when the developer can determine when the DSP is about to complete the task that it was assigned. If the CPU wakes up before the DSP has completed its task, it will pull continuously until completion of the FastRPC call. It is currently supported with the HAP_send_early_signal API, which is replacing the deprecated API fastrpc_send_early_signal , both defined in $HEXAGON_SDK_ROOT/incs/HAP_ps.h .","title":"CPU wakeup and scheduling delays"},{"location":"software/system_performance/system_optimizations.html#pre-map-buffers-to-the-dsp","text":"The FastRPC driver supports transient and persistent mapping of a buffer to the remote DSP. By default, the FastRPC driver maps and unmaps buffers passed as arguments of FastRPC invocation at the beginning and end of that invocation respectively, consuming several uSec in each invocation. On SM8150, for example it was observed that mapping and unmapping a large buffer is taking ~20 us overhead on the average when L2 cache fully evicted by previous RPC call. The FastRPC library supports a flag RPCMEM_TRY_MAP_STATIC for implicitly mapping a buffer to the DSP during allocation. The FastRPC library tries to map buffers allocated with the RPCMEM_TRY_MAP_STATIC flag to the remote process of all current and new FastRPC sessions. In case of failure to map, the FastRPC library ignores the error and continues to open the session without pre-mapping the buffer. In case of success, buffers allocated with this flag will be pre-mapped to reduce the latency of upcoming FastRPC calls. Pre-mapped buffers will be automatically unmapped at either buffer free or session close. Note: RPC memory flag RPCMEM_TRY_MAP_STATIC and buffer attribute FASTRPC_ATTR_TRY_MAP_STATIC are supported from Lahaina and later targets only. Older targets ignore these flags and has no impact on the latency. The FastRPC driver searches each buffer passed to remote call in static mapping list and re-assign the same virtual address on DSP. As a result, pre-mapping many buffers can also add an extra latency for all FastRPC calls. Hence pre-mapping a buffer with RPCMEM_TRY_MAP_STATIC is recommended to use only for large buffers which are used with latency-critical RPC calls after profiling and estimating the actual reduction in FastRPC overhead. Pre-map during memory allocation with RPCMEM_TRY_MAP_STATIC flag using the following approach #include \"rpcmem.h\" src = (uint8_t *)rpcmem_alloc(RPCMEM_HEAP_ID_SYSTEM, RPCMEM_DEFAULT_FLAGS | RPCMEM_TRY_MAP_STATIC, srcSize); //... RPC calls with src buffer as parameter ... rpcmem_free(src); When an ION buffer is allocated without using rpcmem_alloc() function, register the buffer with an attribute FASTRPC_ATTR_TRY_MAP_STATIC for pre-map: #include \"remote.h\" remote_register_buf_attr(buffer, size, fd, FASTRPC_ATTR_TRY_MAP_STATIC); //... RPC calls with src buffer as parameter ... remote_register_buf(buffer, size, -1); // the -1 argument results in unregistering the buffer For more information refer to rpcmem API and remote API .","title":"Pre-map buffers to the DSP"},{"location":"software/system_performance/system_optimizations.html#cpu-build-configuration","text":"The CPU software builds on development platforms might have additional logging and debugging code built in, which can significantly impact FastRPC performance. QTI recommends measuring FastRPC overhead with full performance builds. Production devices always use performance builds. Development builds should use performance kernel defconfig settings.","title":"CPU build configuration"},{"location":"software/system_performance/system_optimizations.html#dsp-sleep-latency","text":"The DSP can enter low power sleep mode after sending a response to the CPU, and if it becomes idle with no further jobs to process. The time to bring the DSP out of sleep mode after it receives an interrupt from the CPU adds additional latency to FastRPC overhead for processing the newly offloaded job. The impact of sleep latency on FastRPC latency is higher when the jobs are submitted periodically and the idle DSP enters sleep mode between the jobs. If the jobs are submitted back-to-back, the DSP might not go to sleep and latency impact will be minimal. The FastRPC driver votes for a default sleep latency during session open. QTI recommends overwriting the sleep latency setting based on the use case and latency requirement. For more information, see the HAP power API .","title":"DSP sleep latency"},{"location":"software/system_performance/system_optimizations.html#speed-and-power-management","text":"Controlling the clock speed of the various performance-critical components on the chip allows the application to trade power for speed. The Hexagon SDK provides a set of APIs that allow the DSP to control its own performance and power modes. To learn more about these APIs, see Performance and power manager .","title":"Speed and power management"},{"location":"tools/build.html","text":"Build resources Introduction This page explains the three make systems supported by this SDK. At least one example using each is included in the $HEXAGON_SDK_ROOT/examples folder. Make system Reference SDK example Description make.d Brew calculator Legacy Hexagon SDK make framework, based on GNU make, with a set of common definitions files and templates for project make files. cmake cmake calculator Support for cross-platform builds GNU make make qhl Legacy Make system make.d The make.d framework is built upon two main files in the SDK: defines.min and rules.min. Each user project contains a file named Makefile that includes these two files and defines the project to be built according to make.d variables and conventions. File Name Description Makefile A user-defined file that includes make.d's defines.min, sets BUILD variables, and lastly includes $(RULES_MIN). defines.min Customized for Hexagon SDK. It sets up all the predefined variables and functions exported by make.d and also includes the all default rule definition. rules.min Customized for Hexagon SDK. It sets up all the rules based on the make.d primitives, tells GNU Make what to execute and how. Once created for a new project, the SDK user will typically not have to modify any of these files. defines.min should be included at the beginning of the makefile and rules.min at the end. Snippet from the Calculator C++ Makefile: ... project definition ... include $(V_TARGET)_deps.min include $(HEXAGON_SDK_ROOT)/build/make.d/$(V_TARGET)_vs.min include $(HEXAGON_SDK_ROOT)/build/defines.min include $(V_TARGET).min include $(RULES_MIN) make.d user-project files A project Makefile typically includes a local project-defined <V_TARGET>.min and <V_TARGET>_deps.min file, which specify the custom build rules for that project: <V_TARGET>.min specifies the sources and dependencies needed to generate the executables or shared objects. <V_TARGET>_deps.min specifies the dependencies of the project. <V_TARGET> , which is derived from the make command , can take one of the following values: hexagon , for building rules applying to the Hexagon DSP android , for building rules applying to the application processor running Android UbuntuARM , for building rules applying to the application processor running Ubuntu Build goals The make command has the following syntax: make <action> [options] Note that action is also referred as build goal in the document. The make.d system defines the following target goals: Action Description hexagon Builds all binaries for Hexagon platform and runs QEXES tests on simulator if the project is updated hexagonsim Builds all binaries for Hexagon platform and runs QEXES tests on simulator forcefully android Builds the project and its dependencies for Android platform with default options ubuntuARM Builds the project and its dependencies for ubuntu platform with default options hexagon_clean Cleans project and its dependencies for hexagon variant. Prebuilt dependencies are not removed. android_clean Cleans project and its dependencies for Android variant. Prebuilt dependencies are not removed. ubuntuARM_clean Cleans project and its dependencies for Ubuntu variant. Prebuilt dependencies are not removed. help Prints brief description on the usage and usage examples Make.d supports a syntax for specifying the build goal where options can be given as separate arguments and options may be omitted to rely on defaults. List of options to build for hexagon and Android/ubuntu are given below: Options for all Android and ubuntuArm actions(android, android_clean, UbuntuARM, UbuntuARM_clean) Options Values Description BUILD ReleaseG*, Release, Debug Builds ReleaseG, Release or Debug executable HLOS_ARCH 32*,64 Builds 32-bit or 64-bit executable tree 0, 1* Builds only the project and not dependencies if tree=0, Build the project and dependencies if tree=1. VERBOSE any value Displays all the outputs from the build process. If VERBOSE is not defined, the build system displays only error messages. BUILD_OUTPUT_DIR any directory name Generates all the project binaries in the specified directory Note the default values for each option are identified with *. Also cleaning the build with tree=1 does not remove prebuilt dependencies. Options for all hexagon actions(hexagon, hexagonsim, hexagon_clean) Options Values Description BUILD ReleaseG*, Release, Debug Builds ReleaseG, Release or Debug executable DSP_ARCH v65*, v66, v68, ... Builds executable for the specified DSP architecture tree 0, 1* Build only the project and not dependencies if tree=0, Build the project and dependencies if tree=1 NO_QURT_INC 0*, 1 Do not include QuRT as a dependency when NO_QURT_INC=1 VERBOSE any value Displays all the outputs from the build process and simulator tests. If VERBOSE is not defined, the build system displays only error messages. BUILD_OUTPUT_DIR any directory name Generates all the project binaries in the specified directory Most of the user applications running on the DSP require QURT OS support. If you want to run your application/example in a standalone build without OS support, you can select NO_QURT_INC=1 Refer to $HEXAGON_SDK_ROOT/examples/calculator for an example that does not need QURT OS support and $HEXAGON_SDK_ROOT/examples/multithreading for an example that needs QURT OS support. Description for BUILD option values Value Description ReleaseG optimized, symbols not removed Release optimized , symbols removed Debug unoptimized, symbols not removed Usage examples Refer to calculator example for more details. Below are some example make.d invocations: make android builds 32-bit Android libraries (and dependencies) for the project defined in the user Makefile, in ReleaseG mode. make hexagon will build the project's Hexagon artifacts, in ReleaseG mode, for architecture version v65, with default toolchain version supplied in the SDK. make hexagon VERBOSE=1 is same as make hexagon , but displays all the outputs from the build process and simulator tests. The make command prints the default options used in building the project and the build output directory as shown below: make hexagonsim ==== Building for ReleaseG variant of hexagon architecture v65 ==== ==== Using Hexagon Tools at <HEXAGON_SDK_ROOT>/tools/HEXAGON_Tools/8.4.12 ==== Build output directory: <HEXAGON_SDK_ROOT>/examples/calculator/hexagon_ReleaseG_toolv84_v65/ship If BUILD_QEXES is defined, as part of build process, the build outputs for hexagon are validated by running them on simulator ensuring a successful build completion. Build targets The <V_TARGET>.min file must specify its build targets. Supported build targets are listed below. Target Description Example BUILD_LIBS Name of static libraries built BUILD_LIBS += libcalculator_skel BUILD_EXES Name of executable targets BUILD_EXES += calculator BUILD_QEXES Name of quick executables. These executables are run automatically on simulator as part of any build command to ensure basic tests are passing BUILD_QEXES += calculator_q QEXE_ARGS List of arguments to the quick executable multithreading_q_QEXE_ARGS += arg1 arg2 QEXE_SIM_OPTIONS List of arguments to the Hexagon simulator when running the quick executable QEXE_SIM_OPTIONS += --dsp_clock 1000 --ahb:lowaddr 0xc0000000 --ahb:highaddr 0xc0ffffff BUILD_DLLS Name of shared or dynamic libraries built BUILD_DLLS += libcalculator PRIMORDIAL_STACK_SIZE User-thread stack size for Qurt-based simulator tests(default is 256 KB) multithreading_q_PRIMORDIAL_STACK_SIZE = 0x80000 BUILD_COPIES Files to be copied to output directories see build copies below Tool variables Some variables control the flags and options to be used with the building tools: Variable Description DEFINES Specifies #define names passed to the compiler. Always append to this variable when adding defines. When appropriate, use the alternate <objectname>_DEFINES to be more specific. Note: -D should not be appended to the variable names. E.g. DEFINES += MYDEFINE1 MYDEFINE2 should be used to define variables MYDEFINE1 and MYDEFINE2. INCDIRS Specifies additional include paths that are passed to the compiler. Always append to this variable when adding include paths. When possible, use the alternate <objectname>_INCDIRS . LIBDIRS Specifies the paths to search for libraries, DLLS, and import libraries. Always append to this variable when adding paths. C_FLAGS or CC_FLAGS Specifies compiler flags applying to all C objects. CPP_FLAGS or CXX_FLAGS Specifies compiler flags applying to all C++ objects. Target-specific variables After defining the build targets, the .min file should define the sources, objects and options needed to generate them. Some of the frequently used options are listed below. For an exhaustive list refer to build_SRCS . Target source Type of Source file definition Example definition SDK .min file example <targetName>_C_SRCS List of .c source files. Do not specify the .c extension. calculator_q_C_SRCS = src/calculator_test_main calculator_c++/hexagon.min <targetName>_CPP_SRCS List of .cpp source files. Do not specify the .c extension. calculator_q_CPP_SRCS = src/calculator_dsp calculator_c++/hexagon.min <targetName>_ASM_SRCS List of ASM source files. Do not specify the .S extension libqprintf_example_skel_ASM_SRCS += asm_src/qprintf_example_asm qprintf_example/hexagon.min <targetName>_LIBS list of libs to link with this target. These libs are dependencies for the application and they are listed in DEPENDENCIES variable in corresponding _deps.min file. calculator_plus_LIBS += rpcmem calculator_c++/UbuntuARM.min <targetName>_DIR Source directory from which a lib needs to be generated RPCMEM_DIR = $(HEXAGON_SDK_ROOT)/ipc/fastrpc/rpcmem calculator_c++/UbuntuARM_deps.min <targetName>_QAICIDLS IDL files to be compiled into C/C++ prior to building the target calculator_q_QAICIDLS = inc/calculator calculator/hexagon.min Dependency specification A project's <V_TARGET>_deps.min file defines its dependencies, which may be picked up from a prebuilt binary or built from source. Here is an example of how dependencies are specified(from $HEXAGON_SDK_ROOT/example/calculator/hexagon_deps.min): DEPENDENCIES += \\ ATOMIC \\ RPCMEM \\ TEST_MAIN \\ TEST_UTIL The Hexagon SDK comes with dependencies in prebuilt binaries. They are located in their respective directories. For an example please see the $HEXAGON_SDK_ROOT/libs/atomic/prebuilt/ directory. Most of these prebuilt libraries contain debug symbols as they are built with \"-g\" compiler flag. When libraries are built with -g, the symbols they contain will be part of the final build output. For most of the dependencies, source code also has been provided in the SDK. For an example please see the $HEXAGON_SDK_ROOT/libs/atomic/ directory. The $HEXAGON_SDK_ROOT/build/default_deps.min file specifies whether a prebuilt library needs to be used or built from sources by default and where to find the source or binary for that library. This file needs to be included in <V_TARGET>_deps.min file. If you want to override the way default_deps.min specifies how dependencies should be accessed, you need to set <LIB_NAME>_[PREBUILT_]DIR in your local <V_TARGET>_deps.min to point to the desired location of the dependency, as shown below: ATOMIC_PREBUILT_DIR = $(HEXAGON_SDK_ROOT)/libs/atomic The line above indicates that the ATOMIC library should be retrieved as a prebuilt binary from the $(HEXAGON_SDK_ROOT)/libs/atomic folder. Alternatively, you may specify the following in your local dependency file: ATOMIC_DIR = $(HEXAGON_SDK_ROOT)/libs/atomic The line above would indicate that the ATOMIC library should be built from source located under $(HEXAGON_SDK_ROOT)/libs/atomic. Note: For each library, you should only set <LIB_NAME>_DIR or <LIB_NAME>_PREBUILT_DIR in the <V_TARGET>_deps.min . See $HEXAGON_SDK_ROOT/build/default_deps.min and $HEXAGON_SDK_ROOT/examples/calculator/hexagon_deps.min for more clarity. Customizing build rules It is sometimes necessary to define custom build rules. For example, when a single file needs a custom compile option or when a tool is needed to generate a source file. These rules can be added to the makefile after the include $(RULES_MIN) line. For example: $(OBJ_DIR)/example.c : example.bin bin2src.exe -s$(subst /,\\\\,$<) -dfs:/example.bin -o$@ Additional Information Build Sources The variables below allow to change definitions for a specific target. Target-specific variables Description <targetName>_C_SRCS Specifies a list of .c source files required to build this target. Do not specify the .c extension. <targetName>.C_SRCS Specifies a list of C sources files required to build this target. You must specify the file's extension. <targetName>_CPP_SRCS Specifies a list of CPP source files required to build this target. Do not specify the .cpp extension. <targetName>.CPP_SRCS Specifies a list of CPP sources files required to build this target. You must specify the file's extension. <targetName>_CXX_SRCS , <targetName>.CXX_SRCS Specifies a list of CPP source files required to build this target. Do not specify the .cxx extension. ` _ASM_SRCS Specifies a list of ASM source files required to build this target. Do not specify the .S extension. <targetName>.ASM_SRCS Specifies a list of ASM sources files required to build this target. You must specify the file's extension. This is useful when you need to distinguish between ASM sources that use the extension .S vs .S <targetName>_LIBS Specifies a list of libraries to link with this target. When a variable _DIR is defined for any of these libraries, the make system will recurse to that directory to update the library dependency. ` _DIR Similar to _LIBS. <targetName>_DLLS Specifies a list of shared objects from which symbols will be imported (similar to IMPLIB). ` _RCS Specifies a list of resource (RES) files. <targetName>_LD_FLAGS Used to specify additional linker flags for a specific shared object or executable target. Linker flags may be compiler specific; this variable may need to be set conditionally. <targetName>_DEFS Specifies DLL-specific DEF files. <targetName>_ZIP_SPEC Specifies the files to include in the ZIP file. Paths should be specified relative to _ZIP_SRCHROOT. Wildcards are supported for directories and files. <targetName>_ZIP_SRCHROOT Specifies the base path for files listed in _ZIP_SPEC. Paths in the ZIP file are relative to this base. <targetName>_QAICIDLS Specifies the IDL files to be compiled into C/C++ prior to building the target. Targets should add the stub or skel sources to their list of C sources. <targetName>_QAICIDLS <targetName>_QAICIDLS=<idl> <targetName>_C_SRCS+=$V/<interface>_stub or <targetName>_QAICIDLS=<idl> <targetName>_C_SRCS+=$V/<interface>_skel : Typically only the stub or the skel is built into a single target. The variables below allow to modify the build options for a specific object. Build variables Description <objectName>_CC_FLAGS , <objectName>_C_FLAGS Used to specify additional compiler flags for a specific C object. Compiler flags may be compiler specific; this variable may need to be set conditionally. <objectName>_CXX_FLAGS , <objectName>_CPP_FLAGS Used to specify additional compiler flags for a specific C++ object. Compiler flags may be compiler specific; this variable may need to be set conditionally. ` _CC Used to change the default tool used to compile this C object. <objectName>_CXX Used to change the default tool used to compile this CPP or CXX object. <objectName>_INCDIRS Specifies additional include paths to use for <objectName> . <objectName>_DEFINES Specifies additional defines that are added when building <objectName> . Note: <objectname>_DEFINES += -DMYDEFINE1 \u2013DMYDEFINE2 should not be used, instead <objectname>_DEFINES += MYDEFINE1 MYDEFINE2 should be used, -D will be picked up by default. <objectName>_UNDEFINES Removes the specified defines when building <objectName> . Exporting build outputs After the project is built, the final built outputs need to be validated on target or/and an application needs to invoke the APIs exported by the project. In order to consolidate these outputs all the required header files, libraries, dlls, executables are typically copied to a directory called <output>/ship , where output is the output directory name. The make.d build system supports a build target called \"BUILD_COPIES\" to achieve this. Here is an example of <target.min> containing BUILD_COPIES definition: BUILD_COPIES = \\ $(DLLS) \\ $(EXES) \\ $V/calculator.h \\ $(LIBS) \\ $(SHIP_DIR)/ ; Which copies all the generated DLLs, executables, calculator.h, libraries to $(SHIP_DIR). The ship directory can be overridden by redefining SHIP_DIR after including defines.min as follows in the makefile,. include $(HEXAGON_SDK_ROOT)/build/defines.min SHIP_DIR=$(V)/ship_new Toolchain Support in Android NDK The Android NDK r19c comes with Clang toolchain support. However, Hexagon SDK build supports GCC toolchain to enable users to compile using other NDK versions. To use the GCC toolchain, set ANDROID_ROOT_DIR to the Android root directory and use V_GCC=1 option in the make command. Note that Android NDK r19c does not support GCC and hence the below command will not work with NDK r19c. make android V_GCC=1 The default Android API level is set to 26 (i.e. Version 8 OREO). The API level can be modified by passing API_LEVEL= <> in the make command. make android API_LEVEL=28 To use an older Android NDK such as r14b, it is recommended to change the Android API level to 24 or below. API level 24 works only with the full Android NDK and not with the minimal Android NDK. make android V_GCC=1 API_LEVEL=24 Users need to make sure the Android NDK platform at $HEXAGON_SDK_ROOT/tools/android-ndk-r19c/platforms folder has a respective API folder present before they set API level. CMake CMake is an open source cross-platform tool designed to build, test and package complex software projects. The software compilation in CMake is controlled using simple, platform- and compiler-independent configuration files. In Hexagon SDK, CMake build system is supported along with make.d-based build system and offers an alternative to make.d to build projects. Using both CMake and make.d to build a single project is also possible with some limitations that are described in the sections below. CMake configuration files The toolchain and configuration files that are needed to use the CMake build system are located in the $HEXAGON_SDK_ROOT/build/cmake directory. The description of each of these files is as follows: cmake_configure.bash : This file contains the environment setup and configuration of the CMake build system based on the options provided by the user. This file also generates the necessary MakeFiles for Hexagon or Android targets. hexagon_fun.cmake : This file contains the support functions that are needed to simplify the compilation of IDL files, running on simulator and invoking make.d build commands from CMake. hexagon_toolchain.cmake : This file defines the cross-compilation tools needed to compile code for Hexagon targets. This contains all the options related to the Hexagon LLVM toolchain. ubuntuARM32_toolchain.cmake : This file defines the cross-compilation tools needed to compile code for UbuntuARM 32-bit targets. This contains all the options related to the UbuntuARM 32-bit toolchain. ubuntuARM64_toolchain.cmake : This file defines the cross-compilation tools needed to compile code for UbuntuARM 64-bit targets. This contains all the options related to the UbuntuARM 64-bit toolchain. custom_toolchain.cmake : This file can be used to define the compilation tools needed to compile code for custom hosts OS different than Android, UbuntuARM, Windows and QNX. The Android toolchain file used to build the application code is taken from the Android NDK installed in the Hexagon SDK, located at: $HEXAGON_SDK_ROOT/tools/android-ndk-r19c/build/cmake/android.toolchain.cmake. Using CMake build system To use the CMake build system to compile code for Hexagon or HLOS targets, the user has to create a CMakeLists.txt file which defines the necessary rules to build an HLOS executable and DSP library. To keep the build commands identical for both make.d and CMake, there is a simple shell script \"cmake_build.bash\" that is added in each example. This shell script also helps to run the example on target device attached to the host. A cmake_build.cmd is also available to provide the same functionality on Windows. The usage of the CMake build systems is as described below: cmake_build.cmd/bash <action> [Options] CMake build system supports the following target options Action Description hexagon Build a dynamic DSP lib .so hexagonsim Build a dynamic DSP lib or executable for hexagon and run on simulator android Build the Android executable ubuntuARM Build the UbuntuARM executable <CUSTOM_NAME> <CUSTOM_NAME> helps to build the custom HLOS executable. <CUSTOM_NAME> can take any name other than hexagon, hexagonsim, android, windows, qnx and ubuntuARM. Custom Toolchain explains how to use the custom toolchain for CMake projects V= <valid make.d build ID> Build as with make.d VERBOSE=1 Displays all the outputs from the build process. If VERBOSE is not defined, the build system displays only error messages. Note: the valid make.d build ID will be of the form: hexagon_<Debug/Release/ReelaseG>_dynamic_toolv84_<v65/v66/v68> or hexagon_<Debug/Release/ReelaseG>_toolv84_<v65/v66/v68> for Hexagon and android_<ReleaseG/Release/Debug>_aarch64 or android_<ReleaseG/Release/Debug> for Android. Hexagon Options Acceptable Options( * denotes defaults ) Description BUILD *ReleaseG, Debug, Release Build Variant DSP_ARCH *v65, v66, v68 Target Variant NO_QURT_INC 0*, 1 Do not include QuRT as a dependency when NO_QURT_INC=1 HLOS Options Acceptable Options( * denotes defaults ) Description BUILD *ReleaseG, Debug, Release Build Variant HLOS_ARCH 32, *64 HLOS architecture variant DOMAIN_FLAG 0, 1, 2, 3* Select the fastrpc domain Usage examples Arguments to cmake_build.<bash/cmd> for some common usage examples are listed in the table below: Arguments to cmake_build.<bash/cmd> Description hexagonsim Builds binary for hexagon target and runs on simulator hexagon Builds _skel.so hexagon_clean Cleans the hexagon build directory and the dependency build directories if built android Builds Android target related files android_clean Cleans the Android build directory and the dependency build directories if built ubuntuARM Builds UbuntuARM 64-bit target related files ubuntuARM_clean HLOS_ARCH=32 Cleans the UbuntuARM 32-bit build directory and the dependency build directories if built V=android_ReleaseG_aarch64 Builds ReleaseG variant of Android for 64-bit arch V=hexagon_Debug_toolsv84_v66 Builds Debug variant of hexagon target V66 using Hexagon tools version 8.4.* android BUILD=ReleaseG HLOS_ARCH=32 Builds ReleaseG variant of Android for 32-bit arch hexagon BUILD=Debug DSP_ARCH=v68 Builds Debug variant of hexagon target V68 to run on top of QURT hexagon BUILD=Debug DSP_ARCH=v68 NO_QURT_INC=1 Builds baremetal Debug variant of hexagon target V68 V=hexagon_Debug_dynamic_toolv84_v66 Builds Debug variant of Hexagon target V66 V=hexagon_ReleaseG_toolv84_v68 Builds ReleaseG variant of Hexagon target V68 Mixing CMake and make.d build systems Mixing of CMake and make.d is needed if a CMake-based project has a dependency on a library built only with make.d. In this scenario, the CMake project should invoke make.d commands to build the required libraries. This approach is illustrated in the benchmark example of the compute add-on. Since all the Hexagon SDK libraries have make.d support, there should never be a case where a make.d project needs to invoke a CMake command. CMake helper functions The table below describes some CMake helper functions that are commonly used in building software: Helper function Usage summary Documentation find_library Find a library in the specified locations find_library add_dependencies Add a dependency between top level targets add_dependencies add_library Add a library target, built from the sources specified add_library target_link_directories Specify the search paths for the linker target_link_directories target_link_libraries Specify the libraries to be used to link a given target target_link_libraries add_custom_command Add a custom build rule to the generated build system add_custom_command ExternalProject_Add Build a target from sources outside of the current CMake project ExternalProject Hexagon Cmake helper functions The table below describes some CMake helper function that are present when you include $(HEXAGON_SDK_ROOT)/build/cmake/hexagon_fun.cmake . Helper function Usage summary Example build_idl(<idlFile> <target>) Set up a custom_target to build <idlFile> using qaic IDL compiler and also add the custom_target created as the dependency of <target> build_idl(inc/calculator calculator) link_options(<target>) Set up the Arch-specific linker flags for the <target> link_options(calculator_device) link_custom_library(<target> <custom_library>) Builds the <custom_library> and links to the target. <custom_library> can take one of (rpcmem, atomic, test_util, rtld, qhl, qhl_hvx) link_custom_library(calculator_device rpcmem) choose_dsprpc(<domain> <target>) Takes <domain> as argument and returns the corresponding remote library name in <target> . <domain> takes value from 0-3 and default is 3. choose_dsprpc(\"3\", calculator) Custom Toolchain custom_toolchain.cmake helps users to run HLOS toolchains not supported in the Hexagon SDK to compile the code. The table below contains target variables needed to be updated in custom_toolchain.cmake . Target variable Description CUSTOM_PREFIX Full custom toolchain prefix CUSTOM_LIB_DIR Path for finding target-specific libs RELEASE_FLAGS Common compiler flags needed in the Release variant DEBUG_FLAGS Common compiler flags needed in the Debug variant CXX_FLAGS Specific compiler flags needed for building CXX-executable C_FLAGS Specific compiler flags needed for building C-executable EXE_LD_FLAGS Linker flags needed to build an executable DLL_LD_FLAGS Linker flags needed to build a shared library To build with a custom toolchain, use any target name other than the pre-existing target names android, ubuntuARM, hexagon, qnx and windows in the cmake_build command. The output folder name wil be <CUSTOM_NAME>_<BUILD_VARIANT> for 32-bit Arch and <CUSTOM_NAME>_<BUILD_VARIANT>_aarch64 for 64-bit arch where <CUSTOM_NAME> is the target name provided. For example, to compile code for Debian HLOS. Update the custom_toolchain.cmake target specific variables mentioned above with Debian configurations. To build an executable run cmake_build.<bash/cmd> debian BUILD=Debug HLOS_ARCH=32'. In the command is debian` and HLOS_ARCH is 32-bit. The binaries built using the Debian toolchain will be available in the debian_Debug folder. Additional Notes In CMakeLists.txt file, the CMAKE_SYSTEM_NAME variable must match the <CUSTOM_NAME> name passed with cmake_build.<bash/cmd> command. This variable can be used to add custom target code in CMakeLists.txt If the user is using both make.d and cmake build systems, clean the output directories before switching between build systems. GNU make It is possible to construct traditional GNU makefiles for Hexagon SDK projects. One example is given at $HEXAGON_SDK_ROOT/examples/qhl. Building Android application Steps to build an Android application are mentioned in C++_APK example Using libraries residing on target Some APIs are implemented directly in the Hexagon DSP image. This is the case for example for memscpy or HAP_power_request . Projects that use these libraries need to specify test_util as a library dependency to the project in order to be able to run the application on the hexagon simulator. Reference documents Reference documents for compiler and linker Documentation on the Hexagon toolchain is provided as part of the Hexagon SDK: * Compiler documentation * Linker section in Hexagon Utilities documentation Interface Description Language (IDL) The IDL used in the SDK describes the interface between the application processor and the Hexagon DSPs communicating using FastRPC .","title":"Building"},{"location":"tools/build.html#build-resources","text":"","title":"Build resources"},{"location":"tools/build.html#introduction","text":"This page explains the three make systems supported by this SDK. At least one example using each is included in the $HEXAGON_SDK_ROOT/examples folder. Make system Reference SDK example Description make.d Brew calculator Legacy Hexagon SDK make framework, based on GNU make, with a set of common definitions files and templates for project make files. cmake cmake calculator Support for cross-platform builds GNU make make qhl Legacy Make system","title":"Introduction"},{"location":"tools/build.html#maked","text":"The make.d framework is built upon two main files in the SDK: defines.min and rules.min. Each user project contains a file named Makefile that includes these two files and defines the project to be built according to make.d variables and conventions. File Name Description Makefile A user-defined file that includes make.d's defines.min, sets BUILD variables, and lastly includes $(RULES_MIN). defines.min Customized for Hexagon SDK. It sets up all the predefined variables and functions exported by make.d and also includes the all default rule definition. rules.min Customized for Hexagon SDK. It sets up all the rules based on the make.d primitives, tells GNU Make what to execute and how. Once created for a new project, the SDK user will typically not have to modify any of these files. defines.min should be included at the beginning of the makefile and rules.min at the end. Snippet from the Calculator C++ Makefile: ... project definition ... include $(V_TARGET)_deps.min include $(HEXAGON_SDK_ROOT)/build/make.d/$(V_TARGET)_vs.min include $(HEXAGON_SDK_ROOT)/build/defines.min include $(V_TARGET).min include $(RULES_MIN)","title":"make.d"},{"location":"tools/build.html#maked-user-project-files","text":"A project Makefile typically includes a local project-defined <V_TARGET>.min and <V_TARGET>_deps.min file, which specify the custom build rules for that project: <V_TARGET>.min specifies the sources and dependencies needed to generate the executables or shared objects. <V_TARGET>_deps.min specifies the dependencies of the project. <V_TARGET> , which is derived from the make command , can take one of the following values: hexagon , for building rules applying to the Hexagon DSP android , for building rules applying to the application processor running Android UbuntuARM , for building rules applying to the application processor running Ubuntu","title":"make.d user-project files"},{"location":"tools/build.html#build-goals","text":"The make command has the following syntax: make <action> [options] Note that action is also referred as build goal in the document. The make.d system defines the following target goals: Action Description hexagon Builds all binaries for Hexagon platform and runs QEXES tests on simulator if the project is updated hexagonsim Builds all binaries for Hexagon platform and runs QEXES tests on simulator forcefully android Builds the project and its dependencies for Android platform with default options ubuntuARM Builds the project and its dependencies for ubuntu platform with default options hexagon_clean Cleans project and its dependencies for hexagon variant. Prebuilt dependencies are not removed. android_clean Cleans project and its dependencies for Android variant. Prebuilt dependencies are not removed. ubuntuARM_clean Cleans project and its dependencies for Ubuntu variant. Prebuilt dependencies are not removed. help Prints brief description on the usage and usage examples Make.d supports a syntax for specifying the build goal where options can be given as separate arguments and options may be omitted to rely on defaults. List of options to build for hexagon and Android/ubuntu are given below: Options for all Android and ubuntuArm actions(android, android_clean, UbuntuARM, UbuntuARM_clean) Options Values Description BUILD ReleaseG*, Release, Debug Builds ReleaseG, Release or Debug executable HLOS_ARCH 32*,64 Builds 32-bit or 64-bit executable tree 0, 1* Builds only the project and not dependencies if tree=0, Build the project and dependencies if tree=1. VERBOSE any value Displays all the outputs from the build process. If VERBOSE is not defined, the build system displays only error messages. BUILD_OUTPUT_DIR any directory name Generates all the project binaries in the specified directory Note the default values for each option are identified with *. Also cleaning the build with tree=1 does not remove prebuilt dependencies. Options for all hexagon actions(hexagon, hexagonsim, hexagon_clean) Options Values Description BUILD ReleaseG*, Release, Debug Builds ReleaseG, Release or Debug executable DSP_ARCH v65*, v66, v68, ... Builds executable for the specified DSP architecture tree 0, 1* Build only the project and not dependencies if tree=0, Build the project and dependencies if tree=1 NO_QURT_INC 0*, 1 Do not include QuRT as a dependency when NO_QURT_INC=1 VERBOSE any value Displays all the outputs from the build process and simulator tests. If VERBOSE is not defined, the build system displays only error messages. BUILD_OUTPUT_DIR any directory name Generates all the project binaries in the specified directory Most of the user applications running on the DSP require QURT OS support. If you want to run your application/example in a standalone build without OS support, you can select NO_QURT_INC=1 Refer to $HEXAGON_SDK_ROOT/examples/calculator for an example that does not need QURT OS support and $HEXAGON_SDK_ROOT/examples/multithreading for an example that needs QURT OS support. Description for BUILD option values Value Description ReleaseG optimized, symbols not removed Release optimized , symbols removed Debug unoptimized, symbols not removed","title":"Build goals"},{"location":"tools/build.html#usage-examples","text":"Refer to calculator example for more details. Below are some example make.d invocations: make android builds 32-bit Android libraries (and dependencies) for the project defined in the user Makefile, in ReleaseG mode. make hexagon will build the project's Hexagon artifacts, in ReleaseG mode, for architecture version v65, with default toolchain version supplied in the SDK. make hexagon VERBOSE=1 is same as make hexagon , but displays all the outputs from the build process and simulator tests. The make command prints the default options used in building the project and the build output directory as shown below: make hexagonsim ==== Building for ReleaseG variant of hexagon architecture v65 ==== ==== Using Hexagon Tools at <HEXAGON_SDK_ROOT>/tools/HEXAGON_Tools/8.4.12 ==== Build output directory: <HEXAGON_SDK_ROOT>/examples/calculator/hexagon_ReleaseG_toolv84_v65/ship If BUILD_QEXES is defined, as part of build process, the build outputs for hexagon are validated by running them on simulator ensuring a successful build completion.","title":"Usage examples"},{"location":"tools/build.html#build-targets","text":"The <V_TARGET>.min file must specify its build targets. Supported build targets are listed below. Target Description Example BUILD_LIBS Name of static libraries built BUILD_LIBS += libcalculator_skel BUILD_EXES Name of executable targets BUILD_EXES += calculator BUILD_QEXES Name of quick executables. These executables are run automatically on simulator as part of any build command to ensure basic tests are passing BUILD_QEXES += calculator_q QEXE_ARGS List of arguments to the quick executable multithreading_q_QEXE_ARGS += arg1 arg2 QEXE_SIM_OPTIONS List of arguments to the Hexagon simulator when running the quick executable QEXE_SIM_OPTIONS += --dsp_clock 1000 --ahb:lowaddr 0xc0000000 --ahb:highaddr 0xc0ffffff BUILD_DLLS Name of shared or dynamic libraries built BUILD_DLLS += libcalculator PRIMORDIAL_STACK_SIZE User-thread stack size for Qurt-based simulator tests(default is 256 KB) multithreading_q_PRIMORDIAL_STACK_SIZE = 0x80000 BUILD_COPIES Files to be copied to output directories see build copies below","title":"Build targets"},{"location":"tools/build.html#tool-variables","text":"Some variables control the flags and options to be used with the building tools: Variable Description DEFINES Specifies #define names passed to the compiler. Always append to this variable when adding defines. When appropriate, use the alternate <objectname>_DEFINES to be more specific. Note: -D should not be appended to the variable names. E.g. DEFINES += MYDEFINE1 MYDEFINE2 should be used to define variables MYDEFINE1 and MYDEFINE2. INCDIRS Specifies additional include paths that are passed to the compiler. Always append to this variable when adding include paths. When possible, use the alternate <objectname>_INCDIRS . LIBDIRS Specifies the paths to search for libraries, DLLS, and import libraries. Always append to this variable when adding paths. C_FLAGS or CC_FLAGS Specifies compiler flags applying to all C objects. CPP_FLAGS or CXX_FLAGS Specifies compiler flags applying to all C++ objects.","title":"Tool variables"},{"location":"tools/build.html#target-specific-variables","text":"After defining the build targets, the .min file should define the sources, objects and options needed to generate them. Some of the frequently used options are listed below. For an exhaustive list refer to build_SRCS . Target source Type of Source file definition Example definition SDK .min file example <targetName>_C_SRCS List of .c source files. Do not specify the .c extension. calculator_q_C_SRCS = src/calculator_test_main calculator_c++/hexagon.min <targetName>_CPP_SRCS List of .cpp source files. Do not specify the .c extension. calculator_q_CPP_SRCS = src/calculator_dsp calculator_c++/hexagon.min <targetName>_ASM_SRCS List of ASM source files. Do not specify the .S extension libqprintf_example_skel_ASM_SRCS += asm_src/qprintf_example_asm qprintf_example/hexagon.min <targetName>_LIBS list of libs to link with this target. These libs are dependencies for the application and they are listed in DEPENDENCIES variable in corresponding _deps.min file. calculator_plus_LIBS += rpcmem calculator_c++/UbuntuARM.min <targetName>_DIR Source directory from which a lib needs to be generated RPCMEM_DIR = $(HEXAGON_SDK_ROOT)/ipc/fastrpc/rpcmem calculator_c++/UbuntuARM_deps.min <targetName>_QAICIDLS IDL files to be compiled into C/C++ prior to building the target calculator_q_QAICIDLS = inc/calculator calculator/hexagon.min","title":"Target-specific variables"},{"location":"tools/build.html#dependency-specification","text":"A project's <V_TARGET>_deps.min file defines its dependencies, which may be picked up from a prebuilt binary or built from source. Here is an example of how dependencies are specified(from $HEXAGON_SDK_ROOT/example/calculator/hexagon_deps.min): DEPENDENCIES += \\ ATOMIC \\ RPCMEM \\ TEST_MAIN \\ TEST_UTIL The Hexagon SDK comes with dependencies in prebuilt binaries. They are located in their respective directories. For an example please see the $HEXAGON_SDK_ROOT/libs/atomic/prebuilt/ directory. Most of these prebuilt libraries contain debug symbols as they are built with \"-g\" compiler flag. When libraries are built with -g, the symbols they contain will be part of the final build output. For most of the dependencies, source code also has been provided in the SDK. For an example please see the $HEXAGON_SDK_ROOT/libs/atomic/ directory. The $HEXAGON_SDK_ROOT/build/default_deps.min file specifies whether a prebuilt library needs to be used or built from sources by default and where to find the source or binary for that library. This file needs to be included in <V_TARGET>_deps.min file. If you want to override the way default_deps.min specifies how dependencies should be accessed, you need to set <LIB_NAME>_[PREBUILT_]DIR in your local <V_TARGET>_deps.min to point to the desired location of the dependency, as shown below: ATOMIC_PREBUILT_DIR = $(HEXAGON_SDK_ROOT)/libs/atomic The line above indicates that the ATOMIC library should be retrieved as a prebuilt binary from the $(HEXAGON_SDK_ROOT)/libs/atomic folder. Alternatively, you may specify the following in your local dependency file: ATOMIC_DIR = $(HEXAGON_SDK_ROOT)/libs/atomic The line above would indicate that the ATOMIC library should be built from source located under $(HEXAGON_SDK_ROOT)/libs/atomic. Note: For each library, you should only set <LIB_NAME>_DIR or <LIB_NAME>_PREBUILT_DIR in the <V_TARGET>_deps.min . See $HEXAGON_SDK_ROOT/build/default_deps.min and $HEXAGON_SDK_ROOT/examples/calculator/hexagon_deps.min for more clarity.","title":"Dependency specification"},{"location":"tools/build.html#customizing-build-rules","text":"It is sometimes necessary to define custom build rules. For example, when a single file needs a custom compile option or when a tool is needed to generate a source file. These rules can be added to the makefile after the include $(RULES_MIN) line. For example: $(OBJ_DIR)/example.c : example.bin bin2src.exe -s$(subst /,\\\\,$<) -dfs:/example.bin -o$@","title":"Customizing build rules"},{"location":"tools/build.html#additional-information","text":"","title":"Additional Information"},{"location":"tools/build.html#build-sources","text":"The variables below allow to change definitions for a specific target. Target-specific variables Description <targetName>_C_SRCS Specifies a list of .c source files required to build this target. Do not specify the .c extension. <targetName>.C_SRCS Specifies a list of C sources files required to build this target. You must specify the file's extension. <targetName>_CPP_SRCS Specifies a list of CPP source files required to build this target. Do not specify the .cpp extension. <targetName>.CPP_SRCS Specifies a list of CPP sources files required to build this target. You must specify the file's extension. <targetName>_CXX_SRCS , <targetName>.CXX_SRCS Specifies a list of CPP source files required to build this target. Do not specify the .cxx extension. ` _ASM_SRCS Specifies a list of ASM source files required to build this target. Do not specify the .S extension. <targetName>.ASM_SRCS Specifies a list of ASM sources files required to build this target. You must specify the file's extension. This is useful when you need to distinguish between ASM sources that use the extension .S vs .S <targetName>_LIBS Specifies a list of libraries to link with this target. When a variable _DIR is defined for any of these libraries, the make system will recurse to that directory to update the library dependency. ` _DIR Similar to _LIBS. <targetName>_DLLS Specifies a list of shared objects from which symbols will be imported (similar to IMPLIB). ` _RCS Specifies a list of resource (RES) files. <targetName>_LD_FLAGS Used to specify additional linker flags for a specific shared object or executable target. Linker flags may be compiler specific; this variable may need to be set conditionally. <targetName>_DEFS Specifies DLL-specific DEF files. <targetName>_ZIP_SPEC Specifies the files to include in the ZIP file. Paths should be specified relative to _ZIP_SRCHROOT. Wildcards are supported for directories and files. <targetName>_ZIP_SRCHROOT Specifies the base path for files listed in _ZIP_SPEC. Paths in the ZIP file are relative to this base. <targetName>_QAICIDLS Specifies the IDL files to be compiled into C/C++ prior to building the target. Targets should add the stub or skel sources to their list of C sources. <targetName>_QAICIDLS <targetName>_QAICIDLS=<idl> <targetName>_C_SRCS+=$V/<interface>_stub or <targetName>_QAICIDLS=<idl> <targetName>_C_SRCS+=$V/<interface>_skel : Typically only the stub or the skel is built into a single target. The variables below allow to modify the build options for a specific object. Build variables Description <objectName>_CC_FLAGS , <objectName>_C_FLAGS Used to specify additional compiler flags for a specific C object. Compiler flags may be compiler specific; this variable may need to be set conditionally. <objectName>_CXX_FLAGS , <objectName>_CPP_FLAGS Used to specify additional compiler flags for a specific C++ object. Compiler flags may be compiler specific; this variable may need to be set conditionally. ` _CC Used to change the default tool used to compile this C object. <objectName>_CXX Used to change the default tool used to compile this CPP or CXX object. <objectName>_INCDIRS Specifies additional include paths to use for <objectName> . <objectName>_DEFINES Specifies additional defines that are added when building <objectName> . Note: <objectname>_DEFINES += -DMYDEFINE1 \u2013DMYDEFINE2 should not be used, instead <objectname>_DEFINES += MYDEFINE1 MYDEFINE2 should be used, -D will be picked up by default. <objectName>_UNDEFINES Removes the specified defines when building <objectName> .","title":"Build Sources"},{"location":"tools/build.html#exporting-build-outputs","text":"After the project is built, the final built outputs need to be validated on target or/and an application needs to invoke the APIs exported by the project. In order to consolidate these outputs all the required header files, libraries, dlls, executables are typically copied to a directory called <output>/ship , where output is the output directory name. The make.d build system supports a build target called \"BUILD_COPIES\" to achieve this. Here is an example of <target.min> containing BUILD_COPIES definition: BUILD_COPIES = \\ $(DLLS) \\ $(EXES) \\ $V/calculator.h \\ $(LIBS) \\ $(SHIP_DIR)/ ; Which copies all the generated DLLs, executables, calculator.h, libraries to $(SHIP_DIR). The ship directory can be overridden by redefining SHIP_DIR after including defines.min as follows in the makefile,. include $(HEXAGON_SDK_ROOT)/build/defines.min SHIP_DIR=$(V)/ship_new","title":"Exporting build outputs"},{"location":"tools/build.html#toolchain-support-in-android-ndk","text":"The Android NDK r19c comes with Clang toolchain support. However, Hexagon SDK build supports GCC toolchain to enable users to compile using other NDK versions. To use the GCC toolchain, set ANDROID_ROOT_DIR to the Android root directory and use V_GCC=1 option in the make command. Note that Android NDK r19c does not support GCC and hence the below command will not work with NDK r19c. make android V_GCC=1 The default Android API level is set to 26 (i.e. Version 8 OREO). The API level can be modified by passing API_LEVEL= <> in the make command. make android API_LEVEL=28 To use an older Android NDK such as r14b, it is recommended to change the Android API level to 24 or below. API level 24 works only with the full Android NDK and not with the minimal Android NDK. make android V_GCC=1 API_LEVEL=24 Users need to make sure the Android NDK platform at $HEXAGON_SDK_ROOT/tools/android-ndk-r19c/platforms folder has a respective API folder present before they set API level.","title":"Toolchain Support in Android NDK"},{"location":"tools/build.html#cmake","text":"CMake is an open source cross-platform tool designed to build, test and package complex software projects. The software compilation in CMake is controlled using simple, platform- and compiler-independent configuration files. In Hexagon SDK, CMake build system is supported along with make.d-based build system and offers an alternative to make.d to build projects. Using both CMake and make.d to build a single project is also possible with some limitations that are described in the sections below.","title":"CMake"},{"location":"tools/build.html#cmake-configuration-files","text":"The toolchain and configuration files that are needed to use the CMake build system are located in the $HEXAGON_SDK_ROOT/build/cmake directory. The description of each of these files is as follows: cmake_configure.bash : This file contains the environment setup and configuration of the CMake build system based on the options provided by the user. This file also generates the necessary MakeFiles for Hexagon or Android targets. hexagon_fun.cmake : This file contains the support functions that are needed to simplify the compilation of IDL files, running on simulator and invoking make.d build commands from CMake. hexagon_toolchain.cmake : This file defines the cross-compilation tools needed to compile code for Hexagon targets. This contains all the options related to the Hexagon LLVM toolchain. ubuntuARM32_toolchain.cmake : This file defines the cross-compilation tools needed to compile code for UbuntuARM 32-bit targets. This contains all the options related to the UbuntuARM 32-bit toolchain. ubuntuARM64_toolchain.cmake : This file defines the cross-compilation tools needed to compile code for UbuntuARM 64-bit targets. This contains all the options related to the UbuntuARM 64-bit toolchain. custom_toolchain.cmake : This file can be used to define the compilation tools needed to compile code for custom hosts OS different than Android, UbuntuARM, Windows and QNX. The Android toolchain file used to build the application code is taken from the Android NDK installed in the Hexagon SDK, located at: $HEXAGON_SDK_ROOT/tools/android-ndk-r19c/build/cmake/android.toolchain.cmake.","title":"CMake configuration files"},{"location":"tools/build.html#using-cmake-build-system","text":"To use the CMake build system to compile code for Hexagon or HLOS targets, the user has to create a CMakeLists.txt file which defines the necessary rules to build an HLOS executable and DSP library. To keep the build commands identical for both make.d and CMake, there is a simple shell script \"cmake_build.bash\" that is added in each example. This shell script also helps to run the example on target device attached to the host. A cmake_build.cmd is also available to provide the same functionality on Windows. The usage of the CMake build systems is as described below: cmake_build.cmd/bash <action> [Options] CMake build system supports the following target options Action Description hexagon Build a dynamic DSP lib .so hexagonsim Build a dynamic DSP lib or executable for hexagon and run on simulator android Build the Android executable ubuntuARM Build the UbuntuARM executable <CUSTOM_NAME> <CUSTOM_NAME> helps to build the custom HLOS executable. <CUSTOM_NAME> can take any name other than hexagon, hexagonsim, android, windows, qnx and ubuntuARM. Custom Toolchain explains how to use the custom toolchain for CMake projects V= <valid make.d build ID> Build as with make.d VERBOSE=1 Displays all the outputs from the build process. If VERBOSE is not defined, the build system displays only error messages. Note: the valid make.d build ID will be of the form: hexagon_<Debug/Release/ReelaseG>_dynamic_toolv84_<v65/v66/v68> or hexagon_<Debug/Release/ReelaseG>_toolv84_<v65/v66/v68> for Hexagon and android_<ReleaseG/Release/Debug>_aarch64 or android_<ReleaseG/Release/Debug> for Android. Hexagon Options Acceptable Options( * denotes defaults ) Description BUILD *ReleaseG, Debug, Release Build Variant DSP_ARCH *v65, v66, v68 Target Variant NO_QURT_INC 0*, 1 Do not include QuRT as a dependency when NO_QURT_INC=1 HLOS Options Acceptable Options( * denotes defaults ) Description BUILD *ReleaseG, Debug, Release Build Variant HLOS_ARCH 32, *64 HLOS architecture variant DOMAIN_FLAG 0, 1, 2, 3* Select the fastrpc domain","title":"Using CMake build system"},{"location":"tools/build.html#usage-examples_1","text":"Arguments to cmake_build.<bash/cmd> for some common usage examples are listed in the table below: Arguments to cmake_build.<bash/cmd> Description hexagonsim Builds binary for hexagon target and runs on simulator hexagon Builds _skel.so hexagon_clean Cleans the hexagon build directory and the dependency build directories if built android Builds Android target related files android_clean Cleans the Android build directory and the dependency build directories if built ubuntuARM Builds UbuntuARM 64-bit target related files ubuntuARM_clean HLOS_ARCH=32 Cleans the UbuntuARM 32-bit build directory and the dependency build directories if built V=android_ReleaseG_aarch64 Builds ReleaseG variant of Android for 64-bit arch V=hexagon_Debug_toolsv84_v66 Builds Debug variant of hexagon target V66 using Hexagon tools version 8.4.* android BUILD=ReleaseG HLOS_ARCH=32 Builds ReleaseG variant of Android for 32-bit arch hexagon BUILD=Debug DSP_ARCH=v68 Builds Debug variant of hexagon target V68 to run on top of QURT hexagon BUILD=Debug DSP_ARCH=v68 NO_QURT_INC=1 Builds baremetal Debug variant of hexagon target V68 V=hexagon_Debug_dynamic_toolv84_v66 Builds Debug variant of Hexagon target V66 V=hexagon_ReleaseG_toolv84_v68 Builds ReleaseG variant of Hexagon target V68","title":"Usage examples"},{"location":"tools/build.html#mixing-cmake-and-maked-build-systems","text":"Mixing of CMake and make.d is needed if a CMake-based project has a dependency on a library built only with make.d. In this scenario, the CMake project should invoke make.d commands to build the required libraries. This approach is illustrated in the benchmark example of the compute add-on. Since all the Hexagon SDK libraries have make.d support, there should never be a case where a make.d project needs to invoke a CMake command.","title":"Mixing CMake and make.d build systems"},{"location":"tools/build.html#cmake-helper-functions","text":"The table below describes some CMake helper functions that are commonly used in building software: Helper function Usage summary Documentation find_library Find a library in the specified locations find_library add_dependencies Add a dependency between top level targets add_dependencies add_library Add a library target, built from the sources specified add_library target_link_directories Specify the search paths for the linker target_link_directories target_link_libraries Specify the libraries to be used to link a given target target_link_libraries add_custom_command Add a custom build rule to the generated build system add_custom_command ExternalProject_Add Build a target from sources outside of the current CMake project ExternalProject","title":"CMake helper functions"},{"location":"tools/build.html#hexagon-cmake-helper-functions","text":"The table below describes some CMake helper function that are present when you include $(HEXAGON_SDK_ROOT)/build/cmake/hexagon_fun.cmake . Helper function Usage summary Example build_idl(<idlFile> <target>) Set up a custom_target to build <idlFile> using qaic IDL compiler and also add the custom_target created as the dependency of <target> build_idl(inc/calculator calculator) link_options(<target>) Set up the Arch-specific linker flags for the <target> link_options(calculator_device) link_custom_library(<target> <custom_library>) Builds the <custom_library> and links to the target. <custom_library> can take one of (rpcmem, atomic, test_util, rtld, qhl, qhl_hvx) link_custom_library(calculator_device rpcmem) choose_dsprpc(<domain> <target>) Takes <domain> as argument and returns the corresponding remote library name in <target> . <domain> takes value from 0-3 and default is 3. choose_dsprpc(\"3\", calculator)","title":"Hexagon Cmake helper functions"},{"location":"tools/build.html#custom-toolchain","text":"custom_toolchain.cmake helps users to run HLOS toolchains not supported in the Hexagon SDK to compile the code. The table below contains target variables needed to be updated in custom_toolchain.cmake . Target variable Description CUSTOM_PREFIX Full custom toolchain prefix CUSTOM_LIB_DIR Path for finding target-specific libs RELEASE_FLAGS Common compiler flags needed in the Release variant DEBUG_FLAGS Common compiler flags needed in the Debug variant CXX_FLAGS Specific compiler flags needed for building CXX-executable C_FLAGS Specific compiler flags needed for building C-executable EXE_LD_FLAGS Linker flags needed to build an executable DLL_LD_FLAGS Linker flags needed to build a shared library To build with a custom toolchain, use any target name other than the pre-existing target names android, ubuntuARM, hexagon, qnx and windows in the cmake_build command. The output folder name wil be <CUSTOM_NAME>_<BUILD_VARIANT> for 32-bit Arch and <CUSTOM_NAME>_<BUILD_VARIANT>_aarch64 for 64-bit arch where <CUSTOM_NAME> is the target name provided. For example, to compile code for Debian HLOS. Update the custom_toolchain.cmake target specific variables mentioned above with Debian configurations. To build an executable run cmake_build.<bash/cmd> debian BUILD=Debug HLOS_ARCH=32'. In the command is debian` and HLOS_ARCH is 32-bit. The binaries built using the Debian toolchain will be available in the debian_Debug folder.","title":"Custom Toolchain"},{"location":"tools/build.html#additional-notes","text":"In CMakeLists.txt file, the CMAKE_SYSTEM_NAME variable must match the <CUSTOM_NAME> name passed with cmake_build.<bash/cmd> command. This variable can be used to add custom target code in CMakeLists.txt If the user is using both make.d and cmake build systems, clean the output directories before switching between build systems.","title":"Additional Notes"},{"location":"tools/build.html#gnu-make","text":"It is possible to construct traditional GNU makefiles for Hexagon SDK projects. One example is given at $HEXAGON_SDK_ROOT/examples/qhl.","title":"GNU make"},{"location":"tools/build.html#building-android-application","text":"Steps to build an Android application are mentioned in C++_APK example","title":"Building Android application"},{"location":"tools/build.html#using-libraries-residing-on-target","text":"Some APIs are implemented directly in the Hexagon DSP image. This is the case for example for memscpy or HAP_power_request . Projects that use these libraries need to specify test_util as a library dependency to the project in order to be able to run the application on the hexagon simulator.","title":"Using libraries residing on target"},{"location":"tools/build.html#reference-documents","text":"","title":"Reference documents"},{"location":"tools/build.html#reference-documents-for-compiler-and-linker","text":"Documentation on the Hexagon toolchain is provided as part of the Hexagon SDK: * Compiler documentation * Linker section in Hexagon Utilities documentation","title":"Reference documents for compiler and linker"},{"location":"tools/build.html#interface-description-language-idl","text":"The IDL used in the SDK describes the interface between the application processor and the Hexagon DSPs communicating using FastRPC .","title":"Interface Description Language (IDL)"},{"location":"tools/clone.html","text":"Cloning Overview The Hexagon SDK includes a script that clones an existing project from the command line and updates the library name accordingly to the new name specified provided by the end user. This tool comes in handy if you want to use an existing project as a starting point and do not want to manually update all the API function names: the script will perform a global search and replace to convert function names and symbols to match the new project name you provide. Usage clone_project.py <path of project to clone> <name of new project> The script will create a new project that resides next to the project that is cloned. For example, python $HEXAGON_SDK_ROOT/utils/scripts/clone_project.py $HEXAGON_SDK_ROOT/examples/calculator my_project will generate a new project $HEXAGON_SDK_ROOT/examples/my_project modeled after the calculator project but producing a library libmy_project_skel.so instead of libcalculator_skel.so . This library will include a my_project_sum function with the same implementation as provided in the calculator_sum function in the original calculator project. Note: Remember to do a clean build when you are working with a cloned directory before recompiling your project.","title":"Project cloning"},{"location":"tools/clone.html#cloning","text":"","title":"Cloning"},{"location":"tools/clone.html#overview","text":"The Hexagon SDK includes a script that clones an existing project from the command line and updates the library name accordingly to the new name specified provided by the end user. This tool comes in handy if you want to use an existing project as a starting point and do not want to manually update all the API function names: the script will perform a global search and replace to convert function names and symbols to match the new project name you provide.","title":"Overview"},{"location":"tools/clone.html#usage","text":"clone_project.py <path of project to clone> <name of new project> The script will create a new project that resides next to the project that is cloned. For example, python $HEXAGON_SDK_ROOT/utils/scripts/clone_project.py $HEXAGON_SDK_ROOT/examples/calculator my_project will generate a new project $HEXAGON_SDK_ROOT/examples/my_project modeled after the calculator project but producing a library libmy_project_skel.so instead of libcalculator_skel.so . This library will include a my_project_sum function with the same implementation as provided in the calculator_sum function in the original calculator project. Note: Remember to do a clean build when you are working with a cloned directory before recompiling your project.","title":"Usage"},{"location":"tools/debug.html","text":"Debugging resources Introduction Why is it not working? All software developers find themselves in this situation at one time or another. And there are many ways of approaching such a situation, with or without the use of tools. Debugging is best when you don't have to do it. New Hexagon SDK users should refer to the FastRPC debugging documentation , which discusses a number of common issues that users run into and that are difficult to debug. When issues occur on the simulator or target, understanding and using proper debugging tools can make the debugging process less painful. The debugging section below provides an introduction of the Hexagon LLDB tool, which can be used to debug your application either on simulator or on target . Debugging tools qprintf The SDK comes with the qprintf library. This library provides different APIs to display scalar and vector registers in various formats from C/C++ or assembly. This library works on both the simulator and the target. For example, after including qprintf_asm.h to your assembly code, insert the following line. qprintf(\"v0 = %d\",v0); This will display the contents of HVX register V0 as a succession of 32-bit integers in the logs that are accessible using mini-dm or logcat : qprintf_example_asm.S[174]: v0 = -1,-1,-1,-1,-1,-1,-1,-1 -1,-1,-1,-1,-1,-1,-1,-1 -1,-1,-1,-1,-1,-1,-1,-1 -1,-1,-1,-1,-1,-1,-1,-1 The library also supports more advanced options for controlling the format of the output, the elements to hide or display, or the number of columns for displaying the elements of a vector register. For example: qprintf(\"Displaying scalar registers in various formats: %u, %x. %23d. %+.6d or %5.2f. Etc.\",r20,r20,r20,r20,r21); The output will be qprintf_example_asm.S[162]: Displaying scalar registers in various formats: 20, 14, 20, +000020, 4.00. Etc. Another example: qprintf(\"Masked vector register contents as 16-bit unsigned integers in rows of 5 elements: %m(5)uu\",v0); The output will be qprintf_example_asm.S[183]: Masked vector register contents as 16-bit unsigned integers in rows of 5 elements: [7e]= 65535,[7c]= 65535,[76]= 65535,[74]= 65535,[6e]= 65535 [6c]= 65535,[66]= 65535,[64]= 65535,[5e]= 65535,[5c]= 65535 [56]= 65535,[54]= 65535,[4e]= 65535,[4c]= 65535,[46]= 65535 [44]= 65535,[3a]= 65535,[38]= 65535,[32]= 65535,[30]= 65535 [2a]= 65535,[28]= 65535,[22]= 65535,[20]= 65535,[1a]= 65535 [18]= 65535,[12]= 65535,[10]= 65535,[0a]= 65535,[08]= 65535 [02]= 65535,[00]= 65535, For more details on this library, see the library package in $HEXAGON_SDK_ROOT/libs/qprintf and its documentation . For examples on how to use the library, see the code example in qprintf_example . Debugging on Simulator It is possible to debug a process on simulator from either the command line or the Eclipse IDE Debug process on Hexagon simulator using command line This chapter describes how to debug the multithreading example on simulator from command line. Please use the following steps. Here we are debugging a v65 architecture based application. Steps to debug v66/v68 application are also same as below, it is as simple as replacing v65 with v66/v68 everywhere it appears in the instructions. Build project cd %HEXAGON_SDK_ROOT% for Windows or cd $HEXAGON_SDK_ROOT for Linux setup_sdk_env.cmd for Windows or setup_sdk_env.source for Linux cd examples\\multithreading make hexagon DSP_ARCH=v65 BUILD=Debug VERBOSE=1 The above make command builds the multithreading example and runs it on simulator using run_main_on_hexagon You can re-run the code on simulator with the following command which is also provided as part of %HEXAGON_SDK_ROOT%/examples/multithreading/hexagon_Debug_toolv84_v65/pmu_stats.txt . %DEFAULT_HEXAGON_TOOLS_ROOT%/Tools/bin/hexagon-sim -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --dsp_clock 1000 --ahb:lowaddr 0xc0000000 --ahb:highaddr 0xc0ffffff --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg %HEXAGON_SDK_ROOT%/rtos/qurt//computev65/sdksim_bin/runelf.pbn -- %HEXAGON_SDK_ROOT%/libs/run_main_on_hexagon/ship/hexagon_toolv84_v65/run_main_on_hexagon_sim -- multithreading_q.so Define LLDB_HEXAGON_BOOTER_PATH Before beginning to debug the multithreading project, the LLDB_HEXAGON_BOOTER_PATH environment variable needs to be defined. Define the environment variable LLDB_HEXAGON_BOOTER_PATH with the path to the booter executable. For Windows: set LLDB_HEXAGON_BOOTER_PATH=%HEXAGON_SDK_ROOT%\\rtos\\qurt\\computev65\\sdksim_bin\\runelf.pbn For Linux: export LLDB_HEXAGON_BOOTER_PATH=$HEXAGON_SDK_ROOT/rtos/qurt/computev65/sdksim_bin/runelf.pbn When LLDB gets down to the simulator launch, it will check for this environment variable. If it exists, it will treat this environment variable as the main target (runelf.pbn) and pick the original target(multithreading_q) as first argument to main target(runelf.pbn). Note. Once LLDB_HEXAGON_BOOTER_PATH is defined, we should not pass runelf.pbn as simulator argument. Build run_main_on_hexagon Build the run_main_on_hexagon library for the Debug build flavor. cd %HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon make hexagon DSP_ARCH=v65 BUILD=Debug The above make command builds the run_main_on_hexagon library. Debug project Please run the following command to launch the debugger on the multithreading example For Windows: %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-lldb.exe %HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\hexagon_Debug_toolv84_v65\\run_main_on_hexagon_sim -o \"image search-paths add . hexagon_Debug_toolv84_v65\" -- -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --dsp_clock 1000 --ahb:lowaddr 0xc0000000 --ahb:highaddr 0xc0ffffff --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg -- -- ./hexagon_Debug_toolv84_v65/multithreading_q.so For Linux: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-lldb $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/hexagon_Debug_toolv84_v65/run_main_on_hexagon_sim -o \"image search-paths add . hexagon_Debug_toolv84_v65\" -- -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --dsp_clock 1000 --ahb:lowaddr 0xc0000000 --ahb:highaddr 0xc0ffffff --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg -- -- ./hexagon_Debug_toolv84_v65/multithreading_q.so You can also get lldb command for your simulator test using launch-lldb.py script This command will result in the following LLDB output: (lldb) target create \"%HEXAGON_SDK_ROOT%\\\\libs\\\\run_main_on_hexagon\\\\hexagon_Debug_toolv84_v65\\\\run_main_on_hexagon_sim\" Current executable set to '%HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\\\hexagon_Debug_toolv84_v65\\run_main_on_hexagon_sim' (hexagon). (lldb) settings set -- target.run-args \"-mv65\" \"--simulated_returnval\" \"--usefs\" \"hexagon_Debug_toolv84_v65\" \"--pmu_statsfile\" \"hexagon_Debug_toolv84_v65/pmu_stats.txt\" \"--dsp_clock\" \"1000\" \"--ahb:lowaddr\" \"0xc0000000\" \"--ahb:highaddr\" \"0xc0ffffff\" \"--cosim_file\" \"hexagon_Debug_toolv84_v65/q6ss.cfg\" \"--l2tcm_base\" \"0xd800\" \"--rtos\" \"hexagon_Debug_toolv84_v65/osam.cfg\" \"--\" \"--\" \"multithreading_q.so\" (lldb) image search-paths add . hexagon_Debug_toolv84_v65 The first line indicates the name of the Hexagon executable, run_main_on_hexagon_sim , while the third line lists the Hexagon simulator options followed, after -- , by the name of the Hexagon library, multithreading_q.so , containing the main() function to execute. The image search-paths add command can be given to the -o flag as an argument to hexagon-lldb and is explained below . The -o flag can be used to execute the specified command image search-paths add after LLDB loads the executable file. Further details can be found in the Hexagon LLDB Debugger User Guide Please refer to the Hexagon Simulator User Guide for understanding the above arguments. Note. A -- after osam.cfg in LLDB launching command is used to separate simulator arguments from program arguments . Once LLDB is launched you can set breakpoints using b . b multithreading_parallel_sum Breakpoint can be set at main also , but it will hit main() of run_main_on_hexagon_sim first and main() of your simulator test next. To avoid this it is recommended to set breakpoint at multithreading_parallel_sum() so that LLDB can directly break in simulator test. Use r to run the program. It should launch the executable and break at multithreading_parallel_sum() of multithreading_q.so, as shown below. (lldb) r Process 1 launched: '%HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\hexagon_Debug_toolv84_v65\\run_main_on_hexagon_sim' (hexagon) 1 location added to breakpoint 1 Process 1 stopped * thread #17, name = 'ribbon', stop reason = breakpoint 1.1 frame #0: 0xd8044914 multithreading_q.so`multithreading_parallel_sum(h=14593280) at multithreading_imp.c:90:5 87 * We initialize all threads with an equal priority value of: QURT_THREAD_ATTR_PRIORITY_DEFAULT/2 (127) 88 */ 89 -> 90 qurt_thread_attr_init(&attr1); 91 qurt_thread_attr_set_name(&attr1, (char *)\"cntr1\"); 92 qurt_thread_attr_set_stack_addr(&attr1, malloc(1024)); 93 qurt_thread_attr_set_stack_size(&attr1, 1024); (lldb) From here on you can step, continue, look at the thread information, register information, print variable values and continue debugging. Some useful LLDB commands command description breakpoint list list of breakpoints registers read list the registers thread list list the threads in the system at that instance of time bt show trace of where you are currently. Prints stack backtrace frame v print values of local variables fr v variable-name print value stored in variable fr v -f x variable-name print variable in hex finish go to the end of the program quit exit LLDB debugger If you are debugging multiple shared objects, then you have to add the path for the other shared objects. Use image search-paths add or target modules search-paths add (image is an alias for target modules) to tell LLDB where to find shared libraries. image search-paths add old_so_path new_so_path Libraries are always loaded on QuRT with \u201c./\u201d as the path, so you want to remap ./ to the path to your library. If your library is in /local/mnt/workspace/, you\u2019d say image search-paths add . /local/mnt/workspace/ Usage of launch-lldb script The launch-lldb.py script automates the process of launching the debugger on an executable that was previously simulated. The script is located under $HEXAGON_SDK_ROOT/utils/debugger/launch-lldb.py . The script parses a simulator command stored in a sim_cmd_line.txt file and launches hexagon-lldb to run the same code within the debugger. python $(HEXAGON_SDK_ROOT)/utils/debugger/launch-lldb.py sim_cmd_line.txt Note. sim_cmd_line.txt is automatically generated when the application is run on the simulator by make.d build system. This file is currently not generated when cmake build system is used. Debug process on Hexagon simulator using Eclipse IDE Please refer to Eclipse IDE and follow the steps to debug on the Hexagon simulator using Eclipse IDE. Debugging QuRT crash on Hexagon Simulator When a process execution fails on the Hexagon simulator, a QuRT exception handling mechanism retrieves the relevant information from the process state and displays it on the console. The QuRT error code returned as part of this exception can be analyzed using the qurt_error_info.py script in $HEXAGON_SDK_ROOT/utils/scripts. For example, in the following error log: !!! Exception occurred QuRT error code 0x2101 Thread ID 0x59 SP 0x23091810 ELR 0x23018d34 BADVA 0x1 The QuRT error code is 0x2101 which can be deciphered as below: python3 qurt_error_info.py 0x2101 Cause type: QURT_EXCEPT_PRECISE ( 0x1 ) Cause details: A precise exception occurred. Cause2 details: Exception type: Precise ( 0x21 ) Exception description: Store to misaligned address. The exception here points to \"Store to misaligned address\", which can be further debugged using the lldb debugger. Refer to Section-19 and Appendix-B of the QuRT document to read more about exception handling and understanding the QuRT error codes. Debugging on target The Debug Agent along with the Remote Debug Driver implement a shared memory based transport mechanism that allows for a LLDB debugger running on a host PC to communicate with a remote stub running on peripheral subsystems such as the aDSP, cDSP etc. This approach provide a reliable, responsive, accurate, and secure debug capability without the use of a hardware debugger The diagram below depicts, end to end, the components involved to support remote debugging: Component Description Debugger Debugger application (LLDB) running on the host PC that communicates with the remote stub Debug Agent Software that runs on the Android platform that provides connectivity from the device to the host PC Remote Debug Driver A character-based driver that the Debug Agent uses to transport the payload received from the host to the debug stub running on the subsystem processor over shared memory and vice versa Shared Memory Shared memory from the SMEM pool that is accessible from the Applications Processor (AP) and the subsystem processors Remote Debug Stub Privileged code that runs in the kernels of the subsystem processors that receives debug commands from the debugger running on the host and acts on these commands. These commands include reading and writing to registers and memory belonging to the subsystem's address space, setting breakpoints, single stepping etc. Overall flow When the Debug Agent application starts up, it opens up a shared memory based transport channel to the DSP that will be debugged (aDSP, cDSP, etc...) The Debug Agent application communicates with the DSP to discover the running processes and exposes a port for each one. LLDB on the host machine connects to the port associated with the DSP process that the user wishes to debug LLDB then communicates via this port to debug the process on the DSP. This includes setting breakpoints, reading registers, querying threads, etc... When the process hits a breakpoint it will be halted and control turned over the LLDB. LLDB then provides the user the ability to single step, continue, etc... Verification of software requirements Overview and Purpose The debugger has a few components that need to be present and working correctly in order for LLDB debugging to work. Since you may not know the details of the device you are working on, the Hexagon SDK provides a debugger verification script that will attempt to verify that all the pieces are present and functioning correctly. The debugger verification script contains several error handlers designed to check for several of the most common issues that will prevent the debug agent from initializing properly. These error handlers work to both mitigate these issues by fixing them (i.e. obtaining the remote_debug_agent from the SDK and pushing it to the device) and/or by providing an informative error message to the user. Where to find the script files: The debugger verification script is accessible from the scripts directory in the Hexagon SDK: %HEXAGON_SDK_ROOT%/utils/debugger for Windows or $HEXAGON_SDK_ROOT\\utils\\debugger for Linux How to run the script: Below is the basic procedure for running the debugger verification script. Connect the target device to the host PC using the specified USB cable Open a CLI shell Type \u201cadb devices\u201d Look to make sure the following message is displayed in the CLI shell: \u201cList of devices attached\u201d \u201cabcdef device\u201d ('adcdef' is replaced with unique device ID) Change directory to the Hexagon_SDK root Run setup_sdk_env.cmd from the Hexagon_SDK root location Look to make sure the following message is displayed in the CLI shell: \u201cSetting up the Hexagon SDK environment locally\u201d \u201cDone\u201d Change directory to %HEXAGON_SDK_ROOT%\\utils\\debugger in Windows or $HEXAGON_SDK_ROOT\\utils\\debugger in Linux. Run \u201cpython verify_debugger.py -ADSP\u201d to verify debugger on aDSP. Run \u201cpython verify_debugger.py -CDSP\u201d to verify debugger on cDSP. Displayed message if the script verifies that end-to-end communication is successful between host PC and target device: *** REMOTE DEBUGGER STATUS: remote software debugger is working properly *** Displayed message if the script is unsuccessful at establishing end-to-end communication between host PC and target device: *** REMOTE DEBUGGER STATUS: remote software debugger is NOT initialized / NOT working properly *** Displayed message if the script encounters an error prior to initializing debug agent: *Error message is specific to particular error. No remote debugger status is displayed.* List of Errors Pre-Verification Errors: These errors occur before the debug agent is initialized. Error: SDK environment has not been set up. Display message: \u201cSDK Environment not set up -> please run setup_sdk_env.cmd from SDK's root directory Cause: the SDK environment must be setup in order to ensure the script will function properly. For example, this sets up the %HEXAGON_SDK_ROOT% variable which is a necessary component in the script's verification capability. Solution: change directory within the CLI shell you are using to the Hexagon_SDK root. Type \u201csetup_sdk_env.cmd\u201d and wait for the return message that says \u201cDone\u201d. Error: rdbg.ko driver file is missing on target device. Display message: \u201cERROR: rdbg.ko driver file does not exist. Expected file location on device: /system/lib/modules/rdbg.ko or /vendor/lib/modules/rdbg.ko\u201d Cause: rdbg.ko driver file is not present on the device. This file is necessary to facilitate communication between the host PC and target device. Solution: Please move to different build that has rdbg.ko\u201c. Error: insmod command failed for rdbg.ko module. Display message: \u201cERROR: rdbg.ko module could not be installed/instantiated properly. Expected file location on device: /system/lib/modules/rdbg.ko or /vendor/lib/modules/rdbg.ko. Cause: rdbg.ko file may be missing, device may not to be rooted (adb root command), rdbg.ko file may have incorrect permissions. Solution: Please ensure that rdbg.ko is present on device at either /system/lib/modules/rdbg.ko or /vendor/lib/modules/rdbg.ko and have proper permissions. If present, please check dmesg logs for the errors. Error: unable to change permissions for remote_debug_agent file on target device. Display message: \u201cERROR: unable to chmod remote_debug_agent\u201d Cause: target device may not be rooted (adb root command), may need to be remounted (adb remount command). Possible improper connection between host PC and target device. Solution: disconnect the target device from the host PC and then reconnect. Type \u201cadb devices\u201d into the CLI shell and ensure host PC is recognizing the device. Ensure you are acting as root (\u201cadb root\u201d) and mounted properly on device (\u201cadb remount\u201d). Verification Errors: These errors occur after the debug agent has been initialized. Error: no XML file generated by debug agent software. Display message: \u201cNo XML file exists on the local host for parsing\u201d Cause: debug monitor was not fully initialized and therefore did not generate an XML file on the host with a list of running processes and their attributes. Solution: check http://localhost:5555/pslist.xml to see if a \u201cpslist.xml\u201d file has been generated. It may be corrupt or inaccessible to the host PC for security/permissions reasons. Most likely cause is that the debug agent was instantiated but did not initialize properly. Ensure Linux Android and aDSP build are compatible and are using an appropriate rdbg.ko file. Error: the debug port value assigned to process x is out of acceptable range. Acceptable range is greater than 1 and less than 99999. Display message: \u201cDebug port value out of acceptable range ( <1 or >99999 )\u201d Cause: debug port was assigned a value that is not acceptable. Value will prevent debug monitor from properly attaching to the running process and will restrict or prevent debug capabilities. Solution: Check debug value to see what value, if any, it was assigned. Use this as context to help determine what value was assigned and what caused this error. Error: the debug port value is not an integer data type. Display message: \u201cDebug port value not an integer data type\u201d Cause: debug port was assigned a value that is not acceptable. Value will prevent debug monitor from properly attaching to the running process and will restrict or prevent debug capabilities. Solution: Check debug value to see what value, if any, it was assigned. Use this as context to help determine what value was assigned and what caused this error. Error: no running processes exist on the device. Display message: \u201cNo running processes exist. If you see DM_FAILED -1 then possible causes are: aDSP build does not contain debug software OR LA and aDSP build are not compatible\u201d Cause: debug agent did not initialize properly. aDSP build does not contain debug software OR LA and aDSP build are not compatible. Solution: reflash Linux Android (LA) and aDSP build and make sure they are compatible. Push rdbg.ko to target device again and make sure it is compatible with LA and aDSP builds. Make sure remote_debug_agent is present on target device. Check permissions for remote_debug_agent (755). Ensure you are acting as root (\u201cadb root\u201d) and mounted properly on device (\u201cadb remount\u201d). Next Steps The debugger verification script will provide as detailed and informative error handling information as possible. Use the output messages provided by the script to help debug and fix any issues that arise when attempting to initialize the debug agent software and verify end-to-end communication between the host and the target device. Continue to do this until the debug agent software initializes properly. You will know the debug agent has initialized properly when the resulting output of the script provides the following display message: *** REMOTE DEBUGGER STATUS: remote software debugger is working properly *** Once functioning correctly proceed to the target debugging instructions. Software requirements hexagon-lldb executable from Hexagon tools. This can be found under %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin Debug agent user mode Android app. It is present on the target at /vendor/bin/ location. If your target did not include the remote debug agent the scripts will push it for you from the SDK ($HEXAGON_SDK_ROOT/tools/debug/remote_debug_agent/android). Linux Android build that has the driver (rdbg.ko). To check whether this are part of the image that has been flashed, connect the device to the host machine using a USB cable and do the following in a windows command prompt: adb root adb shell ls /system/lib/modules if rdbg.ko is not present in /system/lib/modules, please check /vendor/lib/modules ls /vendor/lib/modules rdbg.ko should be present in either /system/lib/modules or /vendor/lib/modules. If rdbg.ko is not present then remote debugging is not supported. Please use a build that has rdbg.ko Known issues The device needs to be rebooted between debug sessions. After completing a debug session LLDB will not be able to re-connect and any attempt to do so will result in unpredictable behavior. If you are seeing a hang on the application while debugging with LLDB, it is possible that FastRPC has killed the process on the DSP as part of the last domain close. To avoid being in this situation, make sure that the CPU does not close the remote handle until you are done debugging. If you try and force exit the application that you are debugging (Ctrl-c), you will likely crash the device or at least see some unpredictable behavior. If you need to start over or run a program that you want to debug, you will have to reboot the device. Stdout is getting buffered with ADB versions 1.0.39 when an application (like multithreading) has started and is waiting for debugger to connect. This has been fixed in 1.0.40 or later ADB releases. With 1.0.39 ADB version, users can try any one of the following work-around. Use setbuf(stdout,NULL) in the main() of application to disable buffering on stdout Run the application from the shell . adb shell . :/ # ./vendor/bin//multithreading QuRT does not support reading HVX registers while debugging on V65 targets. This is only supported from V66 Debugging is not supported on unsigned PD. Debugger is not working properly while debugging an application on ADSP on SM6125, SM6150, SM7150, SM8150 and Rennell targets. Modifying HVX register content from the IDE when debugging on target does not work properly. Debugging procedure There are two ways to do on-target debugging: Command line Eclipse IDE Command Line This section provides steps to use the remote debugger command-line interface. Refer to Hexagon LLDB Debugger User Guide to get an overview of the debugger framework and to ensure the hardware and software requirements are met. Run the Debugger Verification Script to ensure that the device being debugged has all the necessary software on it. Steps for debugging Connect a USB cable from the host machine to the device. Launch LLDB issue the `target_connect` command Follow the prompts The following section goes into much more detail and walks you through debugging the multithreading example Debugging the multithreading example Connect a USB cable from the host machine to the device. In this documentation we debug the multithreading example. The first step is to run the multithreading walkthrough to push its binaries and verify it is working properly Open a new Windows cmd shell and run the multithreading walkthrough script. Please refer to multithreading example to run it on target. Open a second Windows cmd shell on the host machine and start LLDB. Note that it is required to run setup_sdk_env.cmd in the second CLI shell before starting LLDB. cd %HEXAGON_SDK_ROOT% setup_sdk_env.cmd %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-lldb.exe You should see the following: Hexagon utilities (pagetable, tlb, pv) loaded Hexagon SDK device_connect command loaded Issue the device_connect command on the LLDB command line and follow the presented instructions (lldb) device_connect When asked to choose which DSP, select the DSP you are debugging. In this case, the multithreading example runs on the cDSP. If you will be debugging on the cDSP, you will be asked to choose the PD where the example will be running. The example runs on the unsigned PD by default. LLDB will eventually pause and ask you to run your program. LLDB has configured your device so that all new user processes will halt on start. This allows LLDB to connect to the process before calling your user code. It also allows you to configure your debug session before running any of your code (such as setting set breakpoints, etc...). This also means you will have to reboot your device after you are done debugging in order to return the device back to the default (non-halting) configuration. Run the multithreading example on the first cmd shell. The multithreading program should hang indefinitely waiting for the debugger to attach. adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin//multithreading This will run the multithreading example on the unsigned PD by default. To run the example on the signed PD, pass the flag -U 0 After you run your program, press ENTER on the LLDB prompt. LLDB will present you a list of processes you can connect to. There will be a few processes displayed on the command line. Choose the one that matches your program's name. In this case the multithreading process. Your list should looks something like this, you would choose #3 to debug multithreading - Reading list of active user processes ... - Please select the process to debug (if you don't see your process ensure you application is running): 1 _ASID0_ 2 1112_3 3 /frpc/c0468710 multithreading LLDB will then set a breakpoint in the user process's exception handler so that if an exception occurs the debugger will stop in the exception so that you can determine the reason for the exception. Please refer to Hexagon exception handling for more information on how to debug an exception. Next LLDB will ask you for the search-path to your shared object. This step is optional but if you choose to enter nothing then you must set a search-path later using the following LLDB command: (lldb) image search-paths add . local path to your shared objects Take note that only one search-path mapping to . can Exist. If you have multiple search-paths the results are unpredictable. In this example we are running multithreading so enter the multithreading's ship directory. Be sure to replace %HEXAGON_SDK_ROOT% with the actual location of the SDK on your host machine. %HEXAGON_SDK_ROOT%/examples/multithreading/hexagon_Debug_toolv84_v65/ship Take note that if you don't add a search-path or the search-path is incorrect, then LLDB won't be able to halt at any of the breakpoints in your code. It's good practice to check your search-path for accuracy if you are having trouble getting LLDB to hit breakpoints in your user code. LLDB will then connect to your user process on device and halt at the start of the process. This halt location is not in your user code, it is an early halt in the start up code of the process. You should see something like this: Process 8418 stopped * thread #1, stop reason = signal SIGTRAP frame #0: 0xe522817c fastrpc_shell_unsigned_3`qurt_ptrace + 4 fastrpc_shell_unsigned_3`qurt_ptrace: -> 0xe522817c <+4>: { jumpr r31 } fastrpc_shell_unsigned_3`qurt_allsignal_wait: 0xe5228180 <+0>: { r2 = memw_locked(r0) } 0xe5228184 <+4>: { memw_locked(r0,p0) = r1 } 0xe5228188 <+8>: { immext(#4294967232) 0xe522818c <+12>: if (!p0) jump:nt 0xe5228180 0xe5228190 <+16>: r4 = add(r0,#4) 0xe5228194 <+20>: r5 = sub(#-1,r1) } This is a good time to set any user breakpoints you wish to set. For this example, we should set a breakpoint in multithreading_parallel_sum() (lldb) b multithreading_parallel_sum You will see something like this: Breakpoint 2: no locations (pending). WARNING: Unable to resolve breakpoint to any actual locations. This is normal and just means that LLDB has added a future breakpoint. Since the multithreading shared object is not loaded yet LLDB has no idea where in memory to put that breakpoint. Once the multithreading shared object is loaded LLDB will set the breakpoint at the correct location In order to avoid setting breakpoints every time you debug, you can instead put your breakpoints (and search path) in a file and have LLDB load and run these commands. There must be one command per line. An example file: echo commands.txt image search-paths add . C:/Qualcomm/Hexagon_SDK/4.4.0.0/examples/multithreading/hexagon_Debug_toolv84_v65/ship/ b multithreading_parallel_sum Then in LLDB you can load and run these commands by issuing: (lldb) commands source commands.txt Now it is time to let multithreading execute and for LLDB to stop at the breakpoint you set. Issue the continue command (lldb) c You should see something like this: Process 8418 resuming 1 location added to breakpoint 2 Process 8418 stopped * thread #3, stop reason = breakpoint 2.1 frame #0: 0xe42f7fe8 libmultithreading_skel.so`multithreading_parallel_sum [inlined] qurt_thread_attr_set_name(attr=<unavailable>, name=0x00000000) at qurt_thread.h:209:5 206 */ 207 static inline void qurt_thread_attr_set_name (qurt_thread_attr_t *attr, char *name) 208 { - 209 strlcpy (attr->name, name, QURT_THREAD_ATTR_NAME_MAXLEN); 210 attr->name[QURT_THREAD_ATTR_NAME_MAXLEN - 1] = 0; 211 } 212 You can see from the output that the multithreading process has stopped and the reason was a breakpoint. LLDB will show you the line number and source code of the location where the breakpoint stopped. If the breakpoint is not hit, check that you entered a correct search path . At this point you are free to use LLDB to debug the program. For a list of LLDB commands take a look at LLDB commands Eclipse IDE Please refer to Eclipse IDE and follow the steps to debug on target using Eclipse IDE. Debugging PD exceptions Debugging with PD exception logs When a user PD crashes on the DSP, the FastRPC exception handler collects the required information from QuRT and flushes the messages to logcat. This allows the user to understand the reason for the crash. It contains information such as PD name, thread name, and last known PC location along with the library name. It also lists the offset and size of all the dynamically loaded objects for user reference. For information on how to recover from a user PD crash, see the discussion on handling exceptions . The details below are provided in the logcat logs or mini-dm output during a user PD crash: User PD name Thread name Name of the shared object and the symbol offset (derived from PC during crash) Kind of exception and details Last known PC Call trace QuRT Error code Here is an example of a crash report collected from logcat: adsprpc : ADSP: ############################### Process on aDSP CRASHED!!!!!!! ######################################## adsprpc : ADSP: --------------------- Crash Details are furnished below ---------------------------------------------------- adsprpc : ADSP: process \"/frpc/f067e6a0 calculator\" crashed in thread \"/frpc/f067e6a0 \" due to \"TLBMISS RW occurrence\" in ./libcalculator_skel.so adsprpc : ADSP: Crashed Shared Object ./libcalculator_skel.so load address : 0xe648c000 adsprpc : ADSP: fastrpc_shell_0 load address : DE500000 and size : D2208 adsprpc : ADSP: Fault PC : 0xE648C8D0 adsprpc : ADSP: LR : 0xE648C8B4 adsprpc : ADSP: SP : 0xAE0B3DC0 adsprpc : ADSP: Bad va : 0x0 adsprpc : ADSP: FP : 0xAE0B3DE8 adsprpc : ADSP: SSR : 0x21970770 adsprpc : ADSP: Error code : 0x7003 adsprpc : ADSP: Call trace: adsprpc : ADSP: [<e648c8b4>] calculator_sum+0xB4: (./libcalculator_skel.so) adsprpc : ADSP: [<e648c7ac>] calculator_skel_invoke+0x23C: (./libcalculator_skel.so) adsprpc : ADSP: [<e648c5c0>] calculator_skel_invoke+0x50: (./libcalculator_skel.so) adsprpc : ADSP: [<de5721a0>] mod_table_invoke+0x2A4: (fastrpc_shell_0) adsprpc : ADSP: [<de5950b4>] fastrpc_invoke_dispatch+0x14D4: (fastrpc_shell_0) adsprpc : ADSP: [<de56cab4>] adsp_current_process_getASID+0x26C: (fastrpc_shell_0) adsprpc : ADSP: [<de56e36c>] _pl_fastrpc_uprocess+0x730: (fastrpc_shell_0) adsprpc : ADSP: ----------------------------- End of Crash Report ----------------------------------------------------------- It is recommended to use the Debug flavor in order to get the correct details from crash report. To debug the PD exception further, you can use the script debug_exceptions.py located under {HEXAGON_SDK_ROOT}/utils/debugger . Below is the command line usage of this script. Debug PD exceptions by parsing crash signature in log file: Usage: debug_exceptions.py --debug_crash log_file [--lib crashed_library] --debug_crash LOG_FILE path of log file containing the crash signature. --lib CRASHED_LIBRARY path of crashed shared library. Parse QuRT error code returned during PD crash: Usage: debug_exceptions.py --parse_error error_code --parse_error ERROR_CODE QuRT Error code to to parsed. ***Note: *** Error code is displayed as part of the crash signature in Lahaina and onward targets only. Debug crash signature As shown above, use the --debug_crash option to debug a crash signature in a log file. For example, the following command: ./debug_exceptions.py --debug_crash log_file.log will parse log_file.log , identify the crash signature, read the QuRT Error code and display the complete crash signature with the reason of crash. With this command: ./debug_exceptions.py --debug_crash log_file.log --lib libcalculator_skel.so the script will also run hexagon-addrline.exe on the crashed library libcalculator_skel.so to find the line number where the crash occurred in the source file. Parse error code As explained above, use the --parse_error option to get the explanation for an error code. For example, ./debug_exceptions.py --parse_error 0x7003 will explain the meaning of the error code 0x7003 Alternatively, please follow the manual steps below to find out the crash location in the source code from a crash signature. A crash signature gives the Fault PC address and the load address of crashed ELF. Crashed Shared Object ./libcalculator_skel.so load address : 0xE42F0000 Fault PC : 0xE42F08B0 Determine the PC offset by calculating difference between the Fault PC and the load address of your crashed shared object. (0xE42F08B0 - 0xE42F0000) = 0x8B0 Run the hexagon-addr2line using this offset and the crashed shared object to get the line number in the source file where the crashed occured. %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-addr2line.exe -e libcalculator_skel.so 0x8B0 hexagon-addr2line.exe output should look like below %HEXAGON_SDK_ROOT%\\examples\\calculator\\src\\calculator_imp.c:24:4 This means source code located at line number 24 in calculator_imp.c caused this PD crash. Also you can run hexagon-llvm-objdump.exe to find out the disassembly of crashing packet. %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-llvm-objdump.exe --disassemble -source libcalculator_skel.so > disassembled_calculator.txt The command above gives disassembly of all instructions in libcalculator_skel.so with source interleaved. In this example the offset of FaultPC is 0x8B0, you can search for instructions at this offset in hexagon-llvm-objdump.exe output. You should see something like below. *res = *p; 8ac: 62 ff 9e 97 979eff62 { r2 = memw(r30+#-20) } 8b0: 02 c0 82 91 9182c002 { r2 = memw(r2+#0) } 8b4: 00 c0 42 84 8442c000 { r1:0 = sxtw(r2) } 8b8: a2 ff 9e 97 979effa2 { r2 = memw(r30+#-12) } 8bc: 00 c0 c2 a1 a1c2c000 { memd(r2+#0) = r1:0 } The instruction at 0x8B0 is causing PD exception in this example. Debugging with PD dumps A Protection Domain (PD) dump captures the state of a user PD when it encounters an exception. It shows what threads and instructions were executing, what modules were loaded and other useful information for analyzing the exception. Enabling PD dump mode Prior to Lahaina Collection of PD dumps is enabled by default for both signed and unsigned PDs. PD dumps are not enabled on devices with debug policy disabled. Lahaina and beyond Once enabled, PD dumps are always collected for unsigned PDs on all devices. However for Signed PDs, PD dumps are not collected on devices with debug policy disabled. Enabling PD dump on signed or unsigned PDs has to be done at PD creation. There is no option to enable PD dump dynamically. PD dump is enabled by following two steps Run the command below before launching the application adb shell setprop vendor.fastrpc.debug.pddump 1 Follow EITHER ONE (not both) of these steps: Push a debug configuration file with debug properties enabled to a process readable location i.e DSP_LIBRARY_PATH on the target. This debug config file is read only during the start of the process and debug properties are set before a FastRPC session is created on DSP. File extension for debug configuration file is .debugconfig. A sample debug configuration file for calculator application will be calculator.debugconfig and the content of this file will be: pddump = 1 Once the file is created, push this file to DSP_LIBRARY_PATH on target. For e.g. adb push calculator.debugconfig /vendor/lib/rfsa/adsp Use the remote_session_control remote API before a FastRPC session is created: struct remote_rpc_control_pd_dump pddump; pddump.domain = CDSP_DOMAIN_ID; pddump.enable = 1; nErr = remote_session_control(FASTRPC_CONTROL_PD_DUMP, &pddump, sizeof(struct remote_rpc_control_pd_dump)); Note: Unlike the first approach, this alternate approach requires the application to be rebuilt. For enabling PD dumps for signed PD, both steps are required. However, PD dumps for unsigned PD can be enabled with step 2 only, but to enable guest OS info, we need to run step 1 as well. Collecting PD dump Once PD dump mode is enabled, dumps will be automatically collected whenever any exception occurs in any PD. This change won't affect the PDs that existed before the PD dump mode was enabled. Prior to Lahaina PD dumps will be generated at ADSP: /vendor/rfs/msm/adsp/ramdumps CDSP: /vendor/rfs/msm/cdsp/ramdumps SLPI: /vendor/rfs/msm/slpi/ramdumps You can use adb to fetch the PD dump files from the target to your local machine. adb pull /vendor/rfs/msm/adsp/ramdumps/pd_dump_/frpc . Lahaina and beyond PD dumps will be generated at ADSP: /data/vendor/pddump/rfs/adsppd_dump_ CDSP: /data/vendor/pddump/rfs/cdsppd_dump_ SLPI: /data/vendor/pddump/rfs/sdsppd_dump_ You can use adb to fetch the PD dump files from the target to your local machine. adb pull /data/vendor/pddump/rfs/adsppd_dump_/frpc . Analyzing PD dumps Prerequisites Hexagon LLDB tools 8.4.05 or above PD dump file collected following the instructions above FastRPC shell image ( ex. fastrpc_shell_0 for ADSP, fastrpc_shell_3 for CDSP etc.). This can be pulled from the target at location /vendor/dsp/adsp or /vendor/dsp/cdsp. To pull the FastRPC shell for the CDSP, run: adb pull /vendor/dsp/cdsp/fastrpc_shell_3 To pull the FastRPC shell for the ADSP, run: adb pull /vendor/dsp/adsp/fastrpc_shell_0 All the required shared object files. This includes the shared object running on the DSP for that user PD and its dependent shared objects. Example We will take the benchmark example from the Hexagon SDK compute add-on to illustrate how we can analyze the dumps. The examples below are for Windows but the same approach also works with Linux using Linux paths and executables. Setup your SDK environment If you want to collect dumps for an application that doesn't behave as you expect but doesn't crash either, you can induce a crash by writing to the NULL address from the DSP side: volatile int* crash=NULL; *crash = 0xdead; This approach is used in benchmark with the -f crash10 option: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin//benchmark -o /data/local/benchmark.csv -P 6 -L 10 -l 10 -s -f crash10 Go to %HEXAGON_TOOLS_ROOT%\\Tools\\bin directory and make sure you copy into your local directory the elf file, which is generated as part of PD dumps, the FastRPC shell image, and all the required shared objects. Run the command below: hexagon-lldb.exe fastrpc_shell_3 -c \"c04e0d60 benchmark.00.elf\" Here c04e0d60 benchmark.00.elf is the name for PD dump file. c04e0d60 is a unique identifier created for a process on the DSP. All threads spawned from this process have the same identifier. After running the above command, you should see a message similar to this: (lldb) target create \"fastrpc_shell_3\" --core \"c04e0d60 benchmark.00.elf\" Core file 'C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0\\tools\\HEXAGON_Tools\\8.4.04\\Tools\\bin\\c04e0d60 benchmark.00.elf' (hexagon) was loaded. List all the required shared objects to make sure you are not missing any (lldb) image list [ 0] 06B067D7 0xe3bb5074 C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0\\tools\\HEXAGON_Tools\\8.4.04\\Tools\\bin\\fastrpc_shell_3 [ 1] CEB4F524 0xe32e0000 .\\libbenchmark_skel.so [ 2] 95A1580D 0xe32f5000 .\\libworker_pool.so If you are missing a shared objects, then copy that .so file to the current directory, exit the lldb and start again. To see the backtrace of the crashed thread, you can use bt command: (lldb) bt thread #6, name = '/frpc/c051d630 ', stop reason = signal Memory write miss * frame #0: 0xe32e9dec frame #1: 0xe32e37e4 libbenchmark_skel.so`benchmark_skel_handle_invoke [inlined] _skel_method_2(_pfn=<unavailable>, _h=<unavailable>, _sc=<unavailable>, _pra=<unavailable>) at benchmark_skel.c:585:4 frame #2: 0xe32e3674 libbenchmark_skel.so`benchmark_skel_handle_invoke(_h=<unavailable>, _sc=<unavailable>, _pra=<unavailable>) at benchmark_skel.c:1135 frame #3: 0xe32e37e4 libbenchmark_skel.so`benchmark_skel_handle_invoke [inlined] _skel_method_2(_pfn=<unavailable>, _h=<unavailable>, _sc=<unavailable>, _pra=<unavailable>) at benchmark_skel.c:585:4 frame #4: 0xe32e3674 libbenchmark_skel.so`benchmark_skel_handle_invoke(_h=<unavailable>, _sc=<unavailable>, _pra=<unavailable>) at benchmark_skel.c:1135 frame #5: 0xe3c31088 fastrpc_shell_3`mod_table_invoke + 576 frame #6: 0xe3c50c24 fastrpc_shell_3`fastrpc_invoke_dispatch + 336 frame #7: 0xe3c2ad4c fastrpc_shell_3 frame #8: 0xe3c2ce2c fastrpc_shell_3 frame #9: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 This shows that the crash occurred when thread 6 processed frame 9. To see the backtrace for all the threads, you can use (lldb) bt all thread #1, name = 'worker3', stop reason = signal 0 frame #0: 0xe3bdd1dc fastrpc_shell_3`qurt_mutex_lock_i + 72 frame #1: 0xe32f5f98 libworker_pool.so`::worker_pool_init(context=<unavailable>) at worker_pool.cpp:212:20 frame #2: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 thread #2, name = 'worker2', stop reason = signal 0 frame #0: 0xe3bdd1dc fastrpc_shell_3`qurt_mutex_lock_i + 72 frame #1: 0xe32f5f98 libworker_pool.so`::worker_pool_init(context=<unavailable>) at worker_pool.cpp:212:20 frame #2: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 thread #3, name = 'worker1', stop reason = signal 0 frame #0: 0xe3bdd1dc fastrpc_shell_3`qurt_mutex_lock_i + 72 frame #1: 0xe32f5f98 libworker_pool.so`::worker_pool_init(context=<unavailable>) at worker_pool.cpp:212:20 frame #2: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 thread #4, name = 'worker0', stop reason = signal 0 frame #0: 0xe3bdd8a0 fastrpc_shell_3`qurt_signal_wait + 100 frame #1: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 Here are some useful commands: command description thread list list the threads in your program thread select select thread for subsequent commands frame select select stack frame for subsequent commands frame info list information about the currently selected frame in the current thread disassemble --frame disassemble the current function for the current frame frame variable print values of all local variables for the current frame register read --all show all registers in all register sets for the current thread memory read 0xe0415e34 read memory from address 0xe0415e34 image dump sections dump information about all the sections of the main executable and all the loaded shared objects","title":"Debugging"},{"location":"tools/debug.html#debugging-resources","text":"","title":"Debugging resources"},{"location":"tools/debug.html#introduction","text":"","title":"Introduction"},{"location":"tools/debug.html#why-is-it-not-working","text":"All software developers find themselves in this situation at one time or another. And there are many ways of approaching such a situation, with or without the use of tools. Debugging is best when you don't have to do it. New Hexagon SDK users should refer to the FastRPC debugging documentation , which discusses a number of common issues that users run into and that are difficult to debug. When issues occur on the simulator or target, understanding and using proper debugging tools can make the debugging process less painful. The debugging section below provides an introduction of the Hexagon LLDB tool, which can be used to debug your application either on simulator or on target .","title":"Why is it not working?"},{"location":"tools/debug.html#debugging-tools","text":"","title":"Debugging tools"},{"location":"tools/debug.html#qprintf","text":"The SDK comes with the qprintf library. This library provides different APIs to display scalar and vector registers in various formats from C/C++ or assembly. This library works on both the simulator and the target. For example, after including qprintf_asm.h to your assembly code, insert the following line. qprintf(\"v0 = %d\",v0); This will display the contents of HVX register V0 as a succession of 32-bit integers in the logs that are accessible using mini-dm or logcat : qprintf_example_asm.S[174]: v0 = -1,-1,-1,-1,-1,-1,-1,-1 -1,-1,-1,-1,-1,-1,-1,-1 -1,-1,-1,-1,-1,-1,-1,-1 -1,-1,-1,-1,-1,-1,-1,-1 The library also supports more advanced options for controlling the format of the output, the elements to hide or display, or the number of columns for displaying the elements of a vector register. For example: qprintf(\"Displaying scalar registers in various formats: %u, %x. %23d. %+.6d or %5.2f. Etc.\",r20,r20,r20,r20,r21); The output will be qprintf_example_asm.S[162]: Displaying scalar registers in various formats: 20, 14, 20, +000020, 4.00. Etc. Another example: qprintf(\"Masked vector register contents as 16-bit unsigned integers in rows of 5 elements: %m(5)uu\",v0); The output will be qprintf_example_asm.S[183]: Masked vector register contents as 16-bit unsigned integers in rows of 5 elements: [7e]= 65535,[7c]= 65535,[76]= 65535,[74]= 65535,[6e]= 65535 [6c]= 65535,[66]= 65535,[64]= 65535,[5e]= 65535,[5c]= 65535 [56]= 65535,[54]= 65535,[4e]= 65535,[4c]= 65535,[46]= 65535 [44]= 65535,[3a]= 65535,[38]= 65535,[32]= 65535,[30]= 65535 [2a]= 65535,[28]= 65535,[22]= 65535,[20]= 65535,[1a]= 65535 [18]= 65535,[12]= 65535,[10]= 65535,[0a]= 65535,[08]= 65535 [02]= 65535,[00]= 65535, For more details on this library, see the library package in $HEXAGON_SDK_ROOT/libs/qprintf and its documentation . For examples on how to use the library, see the code example in qprintf_example .","title":"qprintf"},{"location":"tools/debug.html#debugging-on-simulator","text":"It is possible to debug a process on simulator from either the command line or the Eclipse IDE","title":"Debugging on Simulator"},{"location":"tools/debug.html#debug-process-on-hexagon-simulator-using-command-line","text":"This chapter describes how to debug the multithreading example on simulator from command line. Please use the following steps. Here we are debugging a v65 architecture based application. Steps to debug v66/v68 application are also same as below, it is as simple as replacing v65 with v66/v68 everywhere it appears in the instructions. Build project cd %HEXAGON_SDK_ROOT% for Windows or cd $HEXAGON_SDK_ROOT for Linux setup_sdk_env.cmd for Windows or setup_sdk_env.source for Linux cd examples\\multithreading make hexagon DSP_ARCH=v65 BUILD=Debug VERBOSE=1 The above make command builds the multithreading example and runs it on simulator using run_main_on_hexagon You can re-run the code on simulator with the following command which is also provided as part of %HEXAGON_SDK_ROOT%/examples/multithreading/hexagon_Debug_toolv84_v65/pmu_stats.txt . %DEFAULT_HEXAGON_TOOLS_ROOT%/Tools/bin/hexagon-sim -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --dsp_clock 1000 --ahb:lowaddr 0xc0000000 --ahb:highaddr 0xc0ffffff --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg %HEXAGON_SDK_ROOT%/rtos/qurt//computev65/sdksim_bin/runelf.pbn -- %HEXAGON_SDK_ROOT%/libs/run_main_on_hexagon/ship/hexagon_toolv84_v65/run_main_on_hexagon_sim -- multithreading_q.so Define LLDB_HEXAGON_BOOTER_PATH Before beginning to debug the multithreading project, the LLDB_HEXAGON_BOOTER_PATH environment variable needs to be defined. Define the environment variable LLDB_HEXAGON_BOOTER_PATH with the path to the booter executable. For Windows: set LLDB_HEXAGON_BOOTER_PATH=%HEXAGON_SDK_ROOT%\\rtos\\qurt\\computev65\\sdksim_bin\\runelf.pbn For Linux: export LLDB_HEXAGON_BOOTER_PATH=$HEXAGON_SDK_ROOT/rtos/qurt/computev65/sdksim_bin/runelf.pbn When LLDB gets down to the simulator launch, it will check for this environment variable. If it exists, it will treat this environment variable as the main target (runelf.pbn) and pick the original target(multithreading_q) as first argument to main target(runelf.pbn). Note. Once LLDB_HEXAGON_BOOTER_PATH is defined, we should not pass runelf.pbn as simulator argument. Build run_main_on_hexagon Build the run_main_on_hexagon library for the Debug build flavor. cd %HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon make hexagon DSP_ARCH=v65 BUILD=Debug The above make command builds the run_main_on_hexagon library. Debug project Please run the following command to launch the debugger on the multithreading example For Windows: %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-lldb.exe %HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\hexagon_Debug_toolv84_v65\\run_main_on_hexagon_sim -o \"image search-paths add . hexagon_Debug_toolv84_v65\" -- -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --dsp_clock 1000 --ahb:lowaddr 0xc0000000 --ahb:highaddr 0xc0ffffff --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg -- -- ./hexagon_Debug_toolv84_v65/multithreading_q.so For Linux: $DEFAULT_HEXAGON_TOOLS_ROOT/Tools/bin/hexagon-lldb $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/hexagon_Debug_toolv84_v65/run_main_on_hexagon_sim -o \"image search-paths add . hexagon_Debug_toolv84_v65\" -- -mv65 --simulated_returnval --usefs hexagon_Debug_toolv84_v65 --pmu_statsfile hexagon_Debug_toolv84_v65/pmu_stats.txt --dsp_clock 1000 --ahb:lowaddr 0xc0000000 --ahb:highaddr 0xc0ffffff --cosim_file hexagon_Debug_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v65/osam.cfg -- -- ./hexagon_Debug_toolv84_v65/multithreading_q.so You can also get lldb command for your simulator test using launch-lldb.py script This command will result in the following LLDB output: (lldb) target create \"%HEXAGON_SDK_ROOT%\\\\libs\\\\run_main_on_hexagon\\\\hexagon_Debug_toolv84_v65\\\\run_main_on_hexagon_sim\" Current executable set to '%HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\\\hexagon_Debug_toolv84_v65\\run_main_on_hexagon_sim' (hexagon). (lldb) settings set -- target.run-args \"-mv65\" \"--simulated_returnval\" \"--usefs\" \"hexagon_Debug_toolv84_v65\" \"--pmu_statsfile\" \"hexagon_Debug_toolv84_v65/pmu_stats.txt\" \"--dsp_clock\" \"1000\" \"--ahb:lowaddr\" \"0xc0000000\" \"--ahb:highaddr\" \"0xc0ffffff\" \"--cosim_file\" \"hexagon_Debug_toolv84_v65/q6ss.cfg\" \"--l2tcm_base\" \"0xd800\" \"--rtos\" \"hexagon_Debug_toolv84_v65/osam.cfg\" \"--\" \"--\" \"multithreading_q.so\" (lldb) image search-paths add . hexagon_Debug_toolv84_v65 The first line indicates the name of the Hexagon executable, run_main_on_hexagon_sim , while the third line lists the Hexagon simulator options followed, after -- , by the name of the Hexagon library, multithreading_q.so , containing the main() function to execute. The image search-paths add command can be given to the -o flag as an argument to hexagon-lldb and is explained below . The -o flag can be used to execute the specified command image search-paths add after LLDB loads the executable file. Further details can be found in the Hexagon LLDB Debugger User Guide Please refer to the Hexagon Simulator User Guide for understanding the above arguments. Note. A -- after osam.cfg in LLDB launching command is used to separate simulator arguments from program arguments . Once LLDB is launched you can set breakpoints using b . b multithreading_parallel_sum Breakpoint can be set at main also , but it will hit main() of run_main_on_hexagon_sim first and main() of your simulator test next. To avoid this it is recommended to set breakpoint at multithreading_parallel_sum() so that LLDB can directly break in simulator test. Use r to run the program. It should launch the executable and break at multithreading_parallel_sum() of multithreading_q.so, as shown below. (lldb) r Process 1 launched: '%HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\hexagon_Debug_toolv84_v65\\run_main_on_hexagon_sim' (hexagon) 1 location added to breakpoint 1 Process 1 stopped * thread #17, name = 'ribbon', stop reason = breakpoint 1.1 frame #0: 0xd8044914 multithreading_q.so`multithreading_parallel_sum(h=14593280) at multithreading_imp.c:90:5 87 * We initialize all threads with an equal priority value of: QURT_THREAD_ATTR_PRIORITY_DEFAULT/2 (127) 88 */ 89 -> 90 qurt_thread_attr_init(&attr1); 91 qurt_thread_attr_set_name(&attr1, (char *)\"cntr1\"); 92 qurt_thread_attr_set_stack_addr(&attr1, malloc(1024)); 93 qurt_thread_attr_set_stack_size(&attr1, 1024); (lldb) From here on you can step, continue, look at the thread information, register information, print variable values and continue debugging.","title":"Debug process on Hexagon simulator using command line"},{"location":"tools/debug.html#some-useful-lldb-commands","text":"command description breakpoint list list of breakpoints registers read list the registers thread list list the threads in the system at that instance of time bt show trace of where you are currently. Prints stack backtrace frame v print values of local variables fr v variable-name print value stored in variable fr v -f x variable-name print variable in hex finish go to the end of the program quit exit LLDB debugger If you are debugging multiple shared objects, then you have to add the path for the other shared objects. Use image search-paths add or target modules search-paths add (image is an alias for target modules) to tell LLDB where to find shared libraries. image search-paths add old_so_path new_so_path Libraries are always loaded on QuRT with \u201c./\u201d as the path, so you want to remap ./ to the path to your library. If your library is in /local/mnt/workspace/, you\u2019d say image search-paths add . /local/mnt/workspace/","title":"Some useful LLDB commands"},{"location":"tools/debug.html#usage-of-launch-lldb-script","text":"The launch-lldb.py script automates the process of launching the debugger on an executable that was previously simulated. The script is located under $HEXAGON_SDK_ROOT/utils/debugger/launch-lldb.py . The script parses a simulator command stored in a sim_cmd_line.txt file and launches hexagon-lldb to run the same code within the debugger. python $(HEXAGON_SDK_ROOT)/utils/debugger/launch-lldb.py sim_cmd_line.txt Note. sim_cmd_line.txt is automatically generated when the application is run on the simulator by make.d build system. This file is currently not generated when cmake build system is used.","title":"Usage of launch-lldb script"},{"location":"tools/debug.html#debug-process-on-hexagon-simulator-using-eclipse-ide","text":"Please refer to Eclipse IDE and follow the steps to debug on the Hexagon simulator using Eclipse IDE.","title":"Debug process on Hexagon simulator using Eclipse IDE"},{"location":"tools/debug.html#debugging-qurt-crash-on-hexagon-simulator","text":"When a process execution fails on the Hexagon simulator, a QuRT exception handling mechanism retrieves the relevant information from the process state and displays it on the console. The QuRT error code returned as part of this exception can be analyzed using the qurt_error_info.py script in $HEXAGON_SDK_ROOT/utils/scripts. For example, in the following error log: !!! Exception occurred QuRT error code 0x2101 Thread ID 0x59 SP 0x23091810 ELR 0x23018d34 BADVA 0x1 The QuRT error code is 0x2101 which can be deciphered as below: python3 qurt_error_info.py 0x2101 Cause type: QURT_EXCEPT_PRECISE ( 0x1 ) Cause details: A precise exception occurred. Cause2 details: Exception type: Precise ( 0x21 ) Exception description: Store to misaligned address. The exception here points to \"Store to misaligned address\", which can be further debugged using the lldb debugger. Refer to Section-19 and Appendix-B of the QuRT document to read more about exception handling and understanding the QuRT error codes.","title":"Debugging QuRT crash on Hexagon Simulator"},{"location":"tools/debug.html#debugging-on-target","text":"The Debug Agent along with the Remote Debug Driver implement a shared memory based transport mechanism that allows for a LLDB debugger running on a host PC to communicate with a remote stub running on peripheral subsystems such as the aDSP, cDSP etc. This approach provide a reliable, responsive, accurate, and secure debug capability without the use of a hardware debugger The diagram below depicts, end to end, the components involved to support remote debugging: Component Description Debugger Debugger application (LLDB) running on the host PC that communicates with the remote stub Debug Agent Software that runs on the Android platform that provides connectivity from the device to the host PC Remote Debug Driver A character-based driver that the Debug Agent uses to transport the payload received from the host to the debug stub running on the subsystem processor over shared memory and vice versa Shared Memory Shared memory from the SMEM pool that is accessible from the Applications Processor (AP) and the subsystem processors Remote Debug Stub Privileged code that runs in the kernels of the subsystem processors that receives debug commands from the debugger running on the host and acts on these commands. These commands include reading and writing to registers and memory belonging to the subsystem's address space, setting breakpoints, single stepping etc.","title":"Debugging on target"},{"location":"tools/debug.html#overall-flow","text":"When the Debug Agent application starts up, it opens up a shared memory based transport channel to the DSP that will be debugged (aDSP, cDSP, etc...) The Debug Agent application communicates with the DSP to discover the running processes and exposes a port for each one. LLDB on the host machine connects to the port associated with the DSP process that the user wishes to debug LLDB then communicates via this port to debug the process on the DSP. This includes setting breakpoints, reading registers, querying threads, etc... When the process hits a breakpoint it will be halted and control turned over the LLDB. LLDB then provides the user the ability to single step, continue, etc...","title":"Overall flow"},{"location":"tools/debug.html#verification-of-software-requirements","text":"","title":"Verification of software requirements"},{"location":"tools/debug.html#overview-and-purpose","text":"The debugger has a few components that need to be present and working correctly in order for LLDB debugging to work. Since you may not know the details of the device you are working on, the Hexagon SDK provides a debugger verification script that will attempt to verify that all the pieces are present and functioning correctly. The debugger verification script contains several error handlers designed to check for several of the most common issues that will prevent the debug agent from initializing properly. These error handlers work to both mitigate these issues by fixing them (i.e. obtaining the remote_debug_agent from the SDK and pushing it to the device) and/or by providing an informative error message to the user. Where to find the script files: The debugger verification script is accessible from the scripts directory in the Hexagon SDK: %HEXAGON_SDK_ROOT%/utils/debugger for Windows or $HEXAGON_SDK_ROOT\\utils\\debugger for Linux How to run the script: Below is the basic procedure for running the debugger verification script. Connect the target device to the host PC using the specified USB cable Open a CLI shell Type \u201cadb devices\u201d Look to make sure the following message is displayed in the CLI shell: \u201cList of devices attached\u201d \u201cabcdef device\u201d ('adcdef' is replaced with unique device ID) Change directory to the Hexagon_SDK root Run setup_sdk_env.cmd from the Hexagon_SDK root location Look to make sure the following message is displayed in the CLI shell: \u201cSetting up the Hexagon SDK environment locally\u201d \u201cDone\u201d Change directory to %HEXAGON_SDK_ROOT%\\utils\\debugger in Windows or $HEXAGON_SDK_ROOT\\utils\\debugger in Linux. Run \u201cpython verify_debugger.py -ADSP\u201d to verify debugger on aDSP. Run \u201cpython verify_debugger.py -CDSP\u201d to verify debugger on cDSP. Displayed message if the script verifies that end-to-end communication is successful between host PC and target device: *** REMOTE DEBUGGER STATUS: remote software debugger is working properly *** Displayed message if the script is unsuccessful at establishing end-to-end communication between host PC and target device: *** REMOTE DEBUGGER STATUS: remote software debugger is NOT initialized / NOT working properly *** Displayed message if the script encounters an error prior to initializing debug agent: *Error message is specific to particular error. No remote debugger status is displayed.*","title":"Overview and Purpose"},{"location":"tools/debug.html#list-of-errors","text":"","title":"List of Errors"},{"location":"tools/debug.html#pre-verification-errors-these-errors-occur-before-the-debug-agent-is-initialized","text":"Error: SDK environment has not been set up. Display message: \u201cSDK Environment not set up -> please run setup_sdk_env.cmd from SDK's root directory Cause: the SDK environment must be setup in order to ensure the script will function properly. For example, this sets up the %HEXAGON_SDK_ROOT% variable which is a necessary component in the script's verification capability. Solution: change directory within the CLI shell you are using to the Hexagon_SDK root. Type \u201csetup_sdk_env.cmd\u201d and wait for the return message that says \u201cDone\u201d. Error: rdbg.ko driver file is missing on target device. Display message: \u201cERROR: rdbg.ko driver file does not exist. Expected file location on device: /system/lib/modules/rdbg.ko or /vendor/lib/modules/rdbg.ko\u201d Cause: rdbg.ko driver file is not present on the device. This file is necessary to facilitate communication between the host PC and target device. Solution: Please move to different build that has rdbg.ko\u201c. Error: insmod command failed for rdbg.ko module. Display message: \u201cERROR: rdbg.ko module could not be installed/instantiated properly. Expected file location on device: /system/lib/modules/rdbg.ko or /vendor/lib/modules/rdbg.ko. Cause: rdbg.ko file may be missing, device may not to be rooted (adb root command), rdbg.ko file may have incorrect permissions. Solution: Please ensure that rdbg.ko is present on device at either /system/lib/modules/rdbg.ko or /vendor/lib/modules/rdbg.ko and have proper permissions. If present, please check dmesg logs for the errors. Error: unable to change permissions for remote_debug_agent file on target device. Display message: \u201cERROR: unable to chmod remote_debug_agent\u201d Cause: target device may not be rooted (adb root command), may need to be remounted (adb remount command). Possible improper connection between host PC and target device. Solution: disconnect the target device from the host PC and then reconnect. Type \u201cadb devices\u201d into the CLI shell and ensure host PC is recognizing the device. Ensure you are acting as root (\u201cadb root\u201d) and mounted properly on device (\u201cadb remount\u201d).","title":"Pre-Verification Errors: These errors occur before the debug agent is initialized."},{"location":"tools/debug.html#verification-errors-these-errors-occur-after-the-debug-agent-has-been-initialized","text":"Error: no XML file generated by debug agent software. Display message: \u201cNo XML file exists on the local host for parsing\u201d Cause: debug monitor was not fully initialized and therefore did not generate an XML file on the host with a list of running processes and their attributes. Solution: check http://localhost:5555/pslist.xml to see if a \u201cpslist.xml\u201d file has been generated. It may be corrupt or inaccessible to the host PC for security/permissions reasons. Most likely cause is that the debug agent was instantiated but did not initialize properly. Ensure Linux Android and aDSP build are compatible and are using an appropriate rdbg.ko file. Error: the debug port value assigned to process x is out of acceptable range. Acceptable range is greater than 1 and less than 99999. Display message: \u201cDebug port value out of acceptable range ( <1 or >99999 )\u201d Cause: debug port was assigned a value that is not acceptable. Value will prevent debug monitor from properly attaching to the running process and will restrict or prevent debug capabilities. Solution: Check debug value to see what value, if any, it was assigned. Use this as context to help determine what value was assigned and what caused this error. Error: the debug port value is not an integer data type. Display message: \u201cDebug port value not an integer data type\u201d Cause: debug port was assigned a value that is not acceptable. Value will prevent debug monitor from properly attaching to the running process and will restrict or prevent debug capabilities. Solution: Check debug value to see what value, if any, it was assigned. Use this as context to help determine what value was assigned and what caused this error. Error: no running processes exist on the device. Display message: \u201cNo running processes exist. If you see DM_FAILED -1 then possible causes are: aDSP build does not contain debug software OR LA and aDSP build are not compatible\u201d Cause: debug agent did not initialize properly. aDSP build does not contain debug software OR LA and aDSP build are not compatible. Solution: reflash Linux Android (LA) and aDSP build and make sure they are compatible. Push rdbg.ko to target device again and make sure it is compatible with LA and aDSP builds. Make sure remote_debug_agent is present on target device. Check permissions for remote_debug_agent (755). Ensure you are acting as root (\u201cadb root\u201d) and mounted properly on device (\u201cadb remount\u201d). Next Steps The debugger verification script will provide as detailed and informative error handling information as possible. Use the output messages provided by the script to help debug and fix any issues that arise when attempting to initialize the debug agent software and verify end-to-end communication between the host and the target device. Continue to do this until the debug agent software initializes properly. You will know the debug agent has initialized properly when the resulting output of the script provides the following display message: *** REMOTE DEBUGGER STATUS: remote software debugger is working properly *** Once functioning correctly proceed to the target debugging instructions.","title":"Verification Errors: These errors occur after the debug agent has been initialized."},{"location":"tools/debug.html#software-requirements","text":"hexagon-lldb executable from Hexagon tools. This can be found under %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin Debug agent user mode Android app. It is present on the target at /vendor/bin/ location. If your target did not include the remote debug agent the scripts will push it for you from the SDK ($HEXAGON_SDK_ROOT/tools/debug/remote_debug_agent/android). Linux Android build that has the driver (rdbg.ko). To check whether this are part of the image that has been flashed, connect the device to the host machine using a USB cable and do the following in a windows command prompt: adb root adb shell ls /system/lib/modules if rdbg.ko is not present in /system/lib/modules, please check /vendor/lib/modules ls /vendor/lib/modules rdbg.ko should be present in either /system/lib/modules or /vendor/lib/modules. If rdbg.ko is not present then remote debugging is not supported. Please use a build that has rdbg.ko","title":"Software requirements"},{"location":"tools/debug.html#known-issues","text":"The device needs to be rebooted between debug sessions. After completing a debug session LLDB will not be able to re-connect and any attempt to do so will result in unpredictable behavior. If you are seeing a hang on the application while debugging with LLDB, it is possible that FastRPC has killed the process on the DSP as part of the last domain close. To avoid being in this situation, make sure that the CPU does not close the remote handle until you are done debugging. If you try and force exit the application that you are debugging (Ctrl-c), you will likely crash the device or at least see some unpredictable behavior. If you need to start over or run a program that you want to debug, you will have to reboot the device. Stdout is getting buffered with ADB versions 1.0.39 when an application (like multithreading) has started and is waiting for debugger to connect. This has been fixed in 1.0.40 or later ADB releases. With 1.0.39 ADB version, users can try any one of the following work-around. Use setbuf(stdout,NULL) in the main() of application to disable buffering on stdout Run the application from the shell . adb shell . :/ # ./vendor/bin//multithreading QuRT does not support reading HVX registers while debugging on V65 targets. This is only supported from V66 Debugging is not supported on unsigned PD. Debugger is not working properly while debugging an application on ADSP on SM6125, SM6150, SM7150, SM8150 and Rennell targets. Modifying HVX register content from the IDE when debugging on target does not work properly.","title":"Known issues"},{"location":"tools/debug.html#debugging-procedure","text":"There are two ways to do on-target debugging: Command line Eclipse IDE","title":"Debugging procedure"},{"location":"tools/debug.html#command-line","text":"This section provides steps to use the remote debugger command-line interface. Refer to Hexagon LLDB Debugger User Guide to get an overview of the debugger framework and to ensure the hardware and software requirements are met. Run the Debugger Verification Script to ensure that the device being debugged has all the necessary software on it.","title":"Command Line"},{"location":"tools/debug.html#steps-for-debugging","text":"Connect a USB cable from the host machine to the device. Launch LLDB issue the `target_connect` command Follow the prompts The following section goes into much more detail and walks you through debugging the multithreading example","title":"Steps for debugging"},{"location":"tools/debug.html#debugging-the-multithreading-example","text":"Connect a USB cable from the host machine to the device. In this documentation we debug the multithreading example. The first step is to run the multithreading walkthrough to push its binaries and verify it is working properly Open a new Windows cmd shell and run the multithreading walkthrough script. Please refer to multithreading example to run it on target. Open a second Windows cmd shell on the host machine and start LLDB. Note that it is required to run setup_sdk_env.cmd in the second CLI shell before starting LLDB. cd %HEXAGON_SDK_ROOT% setup_sdk_env.cmd %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-lldb.exe You should see the following: Hexagon utilities (pagetable, tlb, pv) loaded Hexagon SDK device_connect command loaded Issue the device_connect command on the LLDB command line and follow the presented instructions (lldb) device_connect When asked to choose which DSP, select the DSP you are debugging. In this case, the multithreading example runs on the cDSP. If you will be debugging on the cDSP, you will be asked to choose the PD where the example will be running. The example runs on the unsigned PD by default. LLDB will eventually pause and ask you to run your program. LLDB has configured your device so that all new user processes will halt on start. This allows LLDB to connect to the process before calling your user code. It also allows you to configure your debug session before running any of your code (such as setting set breakpoints, etc...). This also means you will have to reboot your device after you are done debugging in order to return the device back to the default (non-halting) configuration. Run the multithreading example on the first cmd shell. The multithreading program should hang indefinitely waiting for the debugger to attach. adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin//multithreading This will run the multithreading example on the unsigned PD by default. To run the example on the signed PD, pass the flag -U 0 After you run your program, press ENTER on the LLDB prompt. LLDB will present you a list of processes you can connect to. There will be a few processes displayed on the command line. Choose the one that matches your program's name. In this case the multithreading process. Your list should looks something like this, you would choose #3 to debug multithreading - Reading list of active user processes ... - Please select the process to debug (if you don't see your process ensure you application is running): 1 _ASID0_ 2 1112_3 3 /frpc/c0468710 multithreading LLDB will then set a breakpoint in the user process's exception handler so that if an exception occurs the debugger will stop in the exception so that you can determine the reason for the exception. Please refer to Hexagon exception handling for more information on how to debug an exception. Next LLDB will ask you for the search-path to your shared object. This step is optional but if you choose to enter nothing then you must set a search-path later using the following LLDB command: (lldb) image search-paths add . local path to your shared objects Take note that only one search-path mapping to . can Exist. If you have multiple search-paths the results are unpredictable. In this example we are running multithreading so enter the multithreading's ship directory. Be sure to replace %HEXAGON_SDK_ROOT% with the actual location of the SDK on your host machine. %HEXAGON_SDK_ROOT%/examples/multithreading/hexagon_Debug_toolv84_v65/ship Take note that if you don't add a search-path or the search-path is incorrect, then LLDB won't be able to halt at any of the breakpoints in your code. It's good practice to check your search-path for accuracy if you are having trouble getting LLDB to hit breakpoints in your user code. LLDB will then connect to your user process on device and halt at the start of the process. This halt location is not in your user code, it is an early halt in the start up code of the process. You should see something like this: Process 8418 stopped * thread #1, stop reason = signal SIGTRAP frame #0: 0xe522817c fastrpc_shell_unsigned_3`qurt_ptrace + 4 fastrpc_shell_unsigned_3`qurt_ptrace: -> 0xe522817c <+4>: { jumpr r31 } fastrpc_shell_unsigned_3`qurt_allsignal_wait: 0xe5228180 <+0>: { r2 = memw_locked(r0) } 0xe5228184 <+4>: { memw_locked(r0,p0) = r1 } 0xe5228188 <+8>: { immext(#4294967232) 0xe522818c <+12>: if (!p0) jump:nt 0xe5228180 0xe5228190 <+16>: r4 = add(r0,#4) 0xe5228194 <+20>: r5 = sub(#-1,r1) } This is a good time to set any user breakpoints you wish to set. For this example, we should set a breakpoint in multithreading_parallel_sum() (lldb) b multithreading_parallel_sum You will see something like this: Breakpoint 2: no locations (pending). WARNING: Unable to resolve breakpoint to any actual locations. This is normal and just means that LLDB has added a future breakpoint. Since the multithreading shared object is not loaded yet LLDB has no idea where in memory to put that breakpoint. Once the multithreading shared object is loaded LLDB will set the breakpoint at the correct location In order to avoid setting breakpoints every time you debug, you can instead put your breakpoints (and search path) in a file and have LLDB load and run these commands. There must be one command per line. An example file: echo commands.txt image search-paths add . C:/Qualcomm/Hexagon_SDK/4.4.0.0/examples/multithreading/hexagon_Debug_toolv84_v65/ship/ b multithreading_parallel_sum Then in LLDB you can load and run these commands by issuing: (lldb) commands source commands.txt Now it is time to let multithreading execute and for LLDB to stop at the breakpoint you set. Issue the continue command (lldb) c You should see something like this: Process 8418 resuming 1 location added to breakpoint 2 Process 8418 stopped * thread #3, stop reason = breakpoint 2.1 frame #0: 0xe42f7fe8 libmultithreading_skel.so`multithreading_parallel_sum [inlined] qurt_thread_attr_set_name(attr=<unavailable>, name=0x00000000) at qurt_thread.h:209:5 206 */ 207 static inline void qurt_thread_attr_set_name (qurt_thread_attr_t *attr, char *name) 208 { - 209 strlcpy (attr->name, name, QURT_THREAD_ATTR_NAME_MAXLEN); 210 attr->name[QURT_THREAD_ATTR_NAME_MAXLEN - 1] = 0; 211 } 212 You can see from the output that the multithreading process has stopped and the reason was a breakpoint. LLDB will show you the line number and source code of the location where the breakpoint stopped. If the breakpoint is not hit, check that you entered a correct search path . At this point you are free to use LLDB to debug the program. For a list of LLDB commands take a look at LLDB commands","title":"Debugging the multithreading example"},{"location":"tools/debug.html#eclipse-ide","text":"Please refer to Eclipse IDE and follow the steps to debug on target using Eclipse IDE.","title":"Eclipse IDE"},{"location":"tools/debug.html#debugging-pd-exceptions","text":"","title":"Debugging PD exceptions"},{"location":"tools/debug.html#debugging-with-pd-exception-logs","text":"When a user PD crashes on the DSP, the FastRPC exception handler collects the required information from QuRT and flushes the messages to logcat. This allows the user to understand the reason for the crash. It contains information such as PD name, thread name, and last known PC location along with the library name. It also lists the offset and size of all the dynamically loaded objects for user reference. For information on how to recover from a user PD crash, see the discussion on handling exceptions . The details below are provided in the logcat logs or mini-dm output during a user PD crash: User PD name Thread name Name of the shared object and the symbol offset (derived from PC during crash) Kind of exception and details Last known PC Call trace QuRT Error code Here is an example of a crash report collected from logcat: adsprpc : ADSP: ############################### Process on aDSP CRASHED!!!!!!! ######################################## adsprpc : ADSP: --------------------- Crash Details are furnished below ---------------------------------------------------- adsprpc : ADSP: process \"/frpc/f067e6a0 calculator\" crashed in thread \"/frpc/f067e6a0 \" due to \"TLBMISS RW occurrence\" in ./libcalculator_skel.so adsprpc : ADSP: Crashed Shared Object ./libcalculator_skel.so load address : 0xe648c000 adsprpc : ADSP: fastrpc_shell_0 load address : DE500000 and size : D2208 adsprpc : ADSP: Fault PC : 0xE648C8D0 adsprpc : ADSP: LR : 0xE648C8B4 adsprpc : ADSP: SP : 0xAE0B3DC0 adsprpc : ADSP: Bad va : 0x0 adsprpc : ADSP: FP : 0xAE0B3DE8 adsprpc : ADSP: SSR : 0x21970770 adsprpc : ADSP: Error code : 0x7003 adsprpc : ADSP: Call trace: adsprpc : ADSP: [<e648c8b4>] calculator_sum+0xB4: (./libcalculator_skel.so) adsprpc : ADSP: [<e648c7ac>] calculator_skel_invoke+0x23C: (./libcalculator_skel.so) adsprpc : ADSP: [<e648c5c0>] calculator_skel_invoke+0x50: (./libcalculator_skel.so) adsprpc : ADSP: [<de5721a0>] mod_table_invoke+0x2A4: (fastrpc_shell_0) adsprpc : ADSP: [<de5950b4>] fastrpc_invoke_dispatch+0x14D4: (fastrpc_shell_0) adsprpc : ADSP: [<de56cab4>] adsp_current_process_getASID+0x26C: (fastrpc_shell_0) adsprpc : ADSP: [<de56e36c>] _pl_fastrpc_uprocess+0x730: (fastrpc_shell_0) adsprpc : ADSP: ----------------------------- End of Crash Report ----------------------------------------------------------- It is recommended to use the Debug flavor in order to get the correct details from crash report. To debug the PD exception further, you can use the script debug_exceptions.py located under {HEXAGON_SDK_ROOT}/utils/debugger . Below is the command line usage of this script. Debug PD exceptions by parsing crash signature in log file: Usage: debug_exceptions.py --debug_crash log_file [--lib crashed_library] --debug_crash LOG_FILE path of log file containing the crash signature. --lib CRASHED_LIBRARY path of crashed shared library. Parse QuRT error code returned during PD crash: Usage: debug_exceptions.py --parse_error error_code --parse_error ERROR_CODE QuRT Error code to to parsed. ***Note: *** Error code is displayed as part of the crash signature in Lahaina and onward targets only.","title":"Debugging with PD exception logs"},{"location":"tools/debug.html#debug-crash-signature","text":"As shown above, use the --debug_crash option to debug a crash signature in a log file. For example, the following command: ./debug_exceptions.py --debug_crash log_file.log will parse log_file.log , identify the crash signature, read the QuRT Error code and display the complete crash signature with the reason of crash. With this command: ./debug_exceptions.py --debug_crash log_file.log --lib libcalculator_skel.so the script will also run hexagon-addrline.exe on the crashed library libcalculator_skel.so to find the line number where the crash occurred in the source file.","title":"Debug crash signature"},{"location":"tools/debug.html#parse-error-code","text":"As explained above, use the --parse_error option to get the explanation for an error code. For example, ./debug_exceptions.py --parse_error 0x7003 will explain the meaning of the error code 0x7003 Alternatively, please follow the manual steps below to find out the crash location in the source code from a crash signature. A crash signature gives the Fault PC address and the load address of crashed ELF. Crashed Shared Object ./libcalculator_skel.so load address : 0xE42F0000 Fault PC : 0xE42F08B0 Determine the PC offset by calculating difference between the Fault PC and the load address of your crashed shared object. (0xE42F08B0 - 0xE42F0000) = 0x8B0 Run the hexagon-addr2line using this offset and the crashed shared object to get the line number in the source file where the crashed occured. %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-addr2line.exe -e libcalculator_skel.so 0x8B0 hexagon-addr2line.exe output should look like below %HEXAGON_SDK_ROOT%\\examples\\calculator\\src\\calculator_imp.c:24:4 This means source code located at line number 24 in calculator_imp.c caused this PD crash. Also you can run hexagon-llvm-objdump.exe to find out the disassembly of crashing packet. %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-llvm-objdump.exe --disassemble -source libcalculator_skel.so > disassembled_calculator.txt The command above gives disassembly of all instructions in libcalculator_skel.so with source interleaved. In this example the offset of FaultPC is 0x8B0, you can search for instructions at this offset in hexagon-llvm-objdump.exe output. You should see something like below. *res = *p; 8ac: 62 ff 9e 97 979eff62 { r2 = memw(r30+#-20) } 8b0: 02 c0 82 91 9182c002 { r2 = memw(r2+#0) } 8b4: 00 c0 42 84 8442c000 { r1:0 = sxtw(r2) } 8b8: a2 ff 9e 97 979effa2 { r2 = memw(r30+#-12) } 8bc: 00 c0 c2 a1 a1c2c000 { memd(r2+#0) = r1:0 } The instruction at 0x8B0 is causing PD exception in this example.","title":"Parse error code"},{"location":"tools/debug.html#debugging-with-pd-dumps","text":"A Protection Domain (PD) dump captures the state of a user PD when it encounters an exception. It shows what threads and instructions were executing, what modules were loaded and other useful information for analyzing the exception.","title":"Debugging with PD dumps"},{"location":"tools/debug.html#enabling-pd-dump-mode","text":"","title":"Enabling PD dump mode"},{"location":"tools/debug.html#prior-to-lahaina","text":"Collection of PD dumps is enabled by default for both signed and unsigned PDs. PD dumps are not enabled on devices with debug policy disabled.","title":"Prior to Lahaina"},{"location":"tools/debug.html#lahaina-and-beyond","text":"Once enabled, PD dumps are always collected for unsigned PDs on all devices. However for Signed PDs, PD dumps are not collected on devices with debug policy disabled. Enabling PD dump on signed or unsigned PDs has to be done at PD creation. There is no option to enable PD dump dynamically. PD dump is enabled by following two steps Run the command below before launching the application adb shell setprop vendor.fastrpc.debug.pddump 1 Follow EITHER ONE (not both) of these steps: Push a debug configuration file with debug properties enabled to a process readable location i.e DSP_LIBRARY_PATH on the target. This debug config file is read only during the start of the process and debug properties are set before a FastRPC session is created on DSP. File extension for debug configuration file is .debugconfig. A sample debug configuration file for calculator application will be calculator.debugconfig and the content of this file will be: pddump = 1 Once the file is created, push this file to DSP_LIBRARY_PATH on target. For e.g. adb push calculator.debugconfig /vendor/lib/rfsa/adsp Use the remote_session_control remote API before a FastRPC session is created: struct remote_rpc_control_pd_dump pddump; pddump.domain = CDSP_DOMAIN_ID; pddump.enable = 1; nErr = remote_session_control(FASTRPC_CONTROL_PD_DUMP, &pddump, sizeof(struct remote_rpc_control_pd_dump)); Note: Unlike the first approach, this alternate approach requires the application to be rebuilt. For enabling PD dumps for signed PD, both steps are required. However, PD dumps for unsigned PD can be enabled with step 2 only, but to enable guest OS info, we need to run step 1 as well.","title":"Lahaina and beyond"},{"location":"tools/debug.html#collecting-pd-dump","text":"Once PD dump mode is enabled, dumps will be automatically collected whenever any exception occurs in any PD. This change won't affect the PDs that existed before the PD dump mode was enabled.","title":"Collecting PD dump"},{"location":"tools/debug.html#prior-to-lahaina_1","text":"PD dumps will be generated at ADSP: /vendor/rfs/msm/adsp/ramdumps CDSP: /vendor/rfs/msm/cdsp/ramdumps SLPI: /vendor/rfs/msm/slpi/ramdumps You can use adb to fetch the PD dump files from the target to your local machine. adb pull /vendor/rfs/msm/adsp/ramdumps/pd_dump_/frpc .","title":"Prior to Lahaina"},{"location":"tools/debug.html#lahaina-and-beyond_1","text":"PD dumps will be generated at ADSP: /data/vendor/pddump/rfs/adsppd_dump_ CDSP: /data/vendor/pddump/rfs/cdsppd_dump_ SLPI: /data/vendor/pddump/rfs/sdsppd_dump_ You can use adb to fetch the PD dump files from the target to your local machine. adb pull /data/vendor/pddump/rfs/adsppd_dump_/frpc .","title":"Lahaina and beyond"},{"location":"tools/debug.html#analyzing-pd-dumps","text":"","title":"Analyzing PD dumps"},{"location":"tools/debug.html#prerequisites","text":"Hexagon LLDB tools 8.4.05 or above PD dump file collected following the instructions above FastRPC shell image ( ex. fastrpc_shell_0 for ADSP, fastrpc_shell_3 for CDSP etc.). This can be pulled from the target at location /vendor/dsp/adsp or /vendor/dsp/cdsp. To pull the FastRPC shell for the CDSP, run: adb pull /vendor/dsp/cdsp/fastrpc_shell_3 To pull the FastRPC shell for the ADSP, run: adb pull /vendor/dsp/adsp/fastrpc_shell_0 All the required shared object files. This includes the shared object running on the DSP for that user PD and its dependent shared objects.","title":"Prerequisites"},{"location":"tools/debug.html#example","text":"We will take the benchmark example from the Hexagon SDK compute add-on to illustrate how we can analyze the dumps. The examples below are for Windows but the same approach also works with Linux using Linux paths and executables. Setup your SDK environment If you want to collect dumps for an application that doesn't behave as you expect but doesn't crash either, you can induce a crash by writing to the NULL address from the DSP side: volatile int* crash=NULL; *crash = 0xdead; This approach is used in benchmark with the -f crash10 option: adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin//benchmark -o /data/local/benchmark.csv -P 6 -L 10 -l 10 -s -f crash10 Go to %HEXAGON_TOOLS_ROOT%\\Tools\\bin directory and make sure you copy into your local directory the elf file, which is generated as part of PD dumps, the FastRPC shell image, and all the required shared objects. Run the command below: hexagon-lldb.exe fastrpc_shell_3 -c \"c04e0d60 benchmark.00.elf\" Here c04e0d60 benchmark.00.elf is the name for PD dump file. c04e0d60 is a unique identifier created for a process on the DSP. All threads spawned from this process have the same identifier. After running the above command, you should see a message similar to this: (lldb) target create \"fastrpc_shell_3\" --core \"c04e0d60 benchmark.00.elf\" Core file 'C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0\\tools\\HEXAGON_Tools\\8.4.04\\Tools\\bin\\c04e0d60 benchmark.00.elf' (hexagon) was loaded. List all the required shared objects to make sure you are not missing any (lldb) image list [ 0] 06B067D7 0xe3bb5074 C:\\Qualcomm\\Hexagon_SDK\\4.4.0.0\\tools\\HEXAGON_Tools\\8.4.04\\Tools\\bin\\fastrpc_shell_3 [ 1] CEB4F524 0xe32e0000 .\\libbenchmark_skel.so [ 2] 95A1580D 0xe32f5000 .\\libworker_pool.so If you are missing a shared objects, then copy that .so file to the current directory, exit the lldb and start again. To see the backtrace of the crashed thread, you can use bt command: (lldb) bt thread #6, name = '/frpc/c051d630 ', stop reason = signal Memory write miss * frame #0: 0xe32e9dec frame #1: 0xe32e37e4 libbenchmark_skel.so`benchmark_skel_handle_invoke [inlined] _skel_method_2(_pfn=<unavailable>, _h=<unavailable>, _sc=<unavailable>, _pra=<unavailable>) at benchmark_skel.c:585:4 frame #2: 0xe32e3674 libbenchmark_skel.so`benchmark_skel_handle_invoke(_h=<unavailable>, _sc=<unavailable>, _pra=<unavailable>) at benchmark_skel.c:1135 frame #3: 0xe32e37e4 libbenchmark_skel.so`benchmark_skel_handle_invoke [inlined] _skel_method_2(_pfn=<unavailable>, _h=<unavailable>, _sc=<unavailable>, _pra=<unavailable>) at benchmark_skel.c:585:4 frame #4: 0xe32e3674 libbenchmark_skel.so`benchmark_skel_handle_invoke(_h=<unavailable>, _sc=<unavailable>, _pra=<unavailable>) at benchmark_skel.c:1135 frame #5: 0xe3c31088 fastrpc_shell_3`mod_table_invoke + 576 frame #6: 0xe3c50c24 fastrpc_shell_3`fastrpc_invoke_dispatch + 336 frame #7: 0xe3c2ad4c fastrpc_shell_3 frame #8: 0xe3c2ce2c fastrpc_shell_3 frame #9: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 This shows that the crash occurred when thread 6 processed frame 9. To see the backtrace for all the threads, you can use (lldb) bt all thread #1, name = 'worker3', stop reason = signal 0 frame #0: 0xe3bdd1dc fastrpc_shell_3`qurt_mutex_lock_i + 72 frame #1: 0xe32f5f98 libworker_pool.so`::worker_pool_init(context=<unavailable>) at worker_pool.cpp:212:20 frame #2: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 thread #2, name = 'worker2', stop reason = signal 0 frame #0: 0xe3bdd1dc fastrpc_shell_3`qurt_mutex_lock_i + 72 frame #1: 0xe32f5f98 libworker_pool.so`::worker_pool_init(context=<unavailable>) at worker_pool.cpp:212:20 frame #2: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 thread #3, name = 'worker1', stop reason = signal 0 frame #0: 0xe3bdd1dc fastrpc_shell_3`qurt_mutex_lock_i + 72 frame #1: 0xe32f5f98 libworker_pool.so`::worker_pool_init(context=<unavailable>) at worker_pool.cpp:212:20 frame #2: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 thread #4, name = 'worker0', stop reason = signal 0 frame #0: 0xe3bdd8a0 fastrpc_shell_3`qurt_signal_wait + 100 frame #1: 0xe3bde200 fastrpc_shell_3`qurt_trampoline + 44 Here are some useful commands: command description thread list list the threads in your program thread select select thread for subsequent commands frame select select stack frame for subsequent commands frame info list information about the currently selected frame in the current thread disassemble --frame disassemble the current function for the current frame frame variable print values of all local variables for the current frame register read --all show all registers in all register sets for the current thread memory read 0xe0415e34 read memory from address 0xe0415e34 image dump sections dump information about all the sections of the main executable and all the loaded shared objects","title":"Example"},{"location":"tools/ide.html","text":"The Hexagon IDE The Hexagon SDK includes a complete set of software development tools. The tools can be used in either of the following environments: The command-line interface (CLI) of the host development system The Hexagon integrated development environment (IDE) This document explains how to use the tools in the Hexagon IDE. The Hexagon IDE is built on top of Eclipse. For an overview of Eclipse concepts, please follow the Eclipse CDT Documentation . Note that all instructions are given assuming you are running Eclipse under Windows but Linux paths should be used when using Eclipse under Linux NOTE: The Hexagon IDE is not installed by default with the Hexagon SDK. To install the Hexagon IDE, either select the Eclipse checkbox when installing the Hexagon SDK, or install it separately following these instructions . Set up the Hexagon IDE Start the IDE To start the IDE from a CLI shell, run the %HEXAGON_SDK_ROOT%\\launch_hexagon_ide.cmd for Windows installation and $HEXAGON_SDK_ROOT/launch_hexagon_ide.sh for Linux installation. The first time you start the IDE a dialog box appears, asking where to create your project workspace. The workspace is where the source code (and related files and settings) for all your projects will be stored. Specify the location where the workspace should be created. Click OK to create the workspace at the specified location. NOTE: Be sure to store your workspace outside the Hexagon SDK install directory \u2013 this will make life easier when you need to upgrade to a newer Hexagon IDE version. NOTE: Workspace names cannot contain spaces. If they do, the Hexagon IDE will have problems building and executing projects. After you launch the IDE, a welcome screen appears. This screen provides a brief description of the IDE, along with quick links to the help systems for the Hexagon IDE, Hexagon SDK, Eclipse CDT, and Eclipse IDE. NOTE: The Hexagon IDE and Hexagon SDK help files are obtained from the Hexagon SDK. Close the welcome screen by clicking X that appears next to the Welcome tab in the upper left-hand corner of the window. After you close the welcome screen, the main IDE window appears. NOTE: The main Hexagon IDE window defaults to the Hexagon perspective (as indicated in the upper-right corner or the window). Set IDE preferences The Hexagon IDE has several data settings which control how it works. These settings are called preferences, and they must be properly set before you can begin using the IDE. NOTE: If the Hexagon IDE was launched from the scripts mentioned in Start the IDE section, the preferences are set automatically, so you can skip this section. To access the IDE preferences, choose Preferences > Hexagon from the IDE\u2019s Window menu. The following dialog box appears: In the Hexagon SDK path field, specify the pathname of the root directory of the Hexagon SDK. In the Hexagon processor version drop-down list, choose the Hexagon processor version that you are developing software for. The default value is V65. The Hexagon tools path field is preset to the pathname of the Hexagon development tools (as defined in the PATH environment variable). Click Apply to save the changes, and then click OK to close the dialog box. Develop your first project Create a new project To create a new project, start the Hexagon IDE as mentioned above section. The main IDE window should appear as shown below: Choose New > Hexagon Project from the File menu: The following dialog box appears: In the Project name field, enter factorial as the name of the new project. NOTE: Project names cannot contain spaces \u2013 if they do, the Hexagon IDE will have problems building and executing the project. Project location defaults to the workspace. Tools location defaults to the value defined in the system environment variable, PATH. You can change either of these fields, or use the checkbox to restore the original default. In the Hexagon SDK location field, specify the SDK location. (This field is auto-filled if the IDE was launched from the scripts mentioned in Start the IDE section.) In the Project type drop-down list, choose Executable (.exe) . In the Architecture drop-down list, choose the required architecture. Click Finish . The main IDE window now displays a new project named factorial : Creating a new project results in generating a Hello World example with a hello.c file. New project By default, new Hexagon Projects can be built with CMake . To build the project, right-click the project in the Project Explorer view and choose Build Project . This will trigger a CMake build, which can be tracked in the console view: Project customization Once you have confirmed that the project is successfully built, you can customize it. For example, let's add another source file factorial.cpp to the project with the factorial program code. To do this, right-click the project in Project Explorer and choose New > Source File . A dialog box appears, prompting you to create a new source file. Enter the name of the source file ( factorial.cpp ) and select the file template ( Default C++ source template ). Click on Finish button to create the new source file. Next, copy the following code (or any other factorial implementation you want to use) to the source file, factorial.cpp . This code computes the factorial of a number passed to the program as a command-line argument. #include <stdio.h> #include <stdlib.h> int fact (int n) { if (n<=1) return 1; else return n*fact(n-1); } int main(int argc, char *argv[]) { int num,factorial; if (argc==2) { num=atoi(argv[1]); printf(\"User input is : %d\",num); } else { printf(\"Invalid number of arguments\"); return -1; } if(num<0) { printf(\"Invalid input - Must be positive\"); return -1; } factorial=fact(num); printf(\"\\nFactorial of %d is %d\",num,factorial); return 0; } Build modified project All source files need to be added manually to the CMakeList.txt file. In the present example, update this file by adding the line below: set(prj_sources factorial.cpp) The default CMake settings not only build the code but also run the the project on the Hexagon Simulator. Runtime arguments are specified as follows: Note: Use the line below in CMakeList.txt before runHexagonSim set(HEXAGON_EXEC_CMD_OPTIONS ${HEXAGON_EXEC_CMD_OPTIONS} 5) This will result in invoking the Hexagon simulator with the factorial binary file and passing the argument '5' To build the project, right-click the project in Project Explorer and choose Build Project . The status of the build can be viewed in the Console tab at the bottom of the main IDE window. The factorial example above builds a standalone executable that does not need QuRT APIs. Note: New projects in the Hexagon IDE are QuRT-disabled by default, which means that the QuRT libraries are not linked to the project. Follow the steps below for QuRT enablement: Right-click the project in Project Explorer and choose Properties Under C/C++ Build navigate to Cmake4Eclipse , select Symbols tab Edit QURT_OS flag and make value as 1 Press Apply to save the changes. Now User can continue building the QuRT enabled project. Run on simulator using the GUI To run the project, right-click the project in Project Explorer and select Run As > Hexagon C/C++ Application . Running the program produces the following output in the console. Note the message Invalid number of arguments , which appears in the console output \u2013 the factorial program expects its input value to be specified as a command-line argument, and generates this error message because no argument was specified. To fix this problem, right-click the project in Project Explorer and choose Run As > Run Configuration . This command displays the Run Configurations dialog box, which enables you to configure the Application, simulator, program arguments, and runtime environment. The dialog box display tabs for configuring the simulator, program arguments, and runtime environment. (Note that the left-hand pane in the dialog box includes a newly-created runtime configuration named factorial , which appears under the item, Hexagon C/C++ Application .) To specify the program argument for the factorial program, click the Arguments tab in the dialog box. The factorial program expects one user argument (namely, the number whose factorial will be computed). Enter the value 5 as a program argument. To execute the factorial program with the specified argument, click Apply and then Run at the bottom of the dialog box. The dialog box closes and the following output (which includes the factorial output) is displayed in the console of the main IDE window. Note: In some instances, running project do not recognize the binaries built which are located inside the project and throws below error To fix this problem, right-click the project in Project Explorer and choose Run As > Run Configuration . This command displays the Run Configurations dialog box below Right click Hexagon C/C++ Application and create New Configuration below Under C/C++ Application field browse to locate the project binaries in file system. Now press Apply and Run. Debug on simulator using the GUI To debug the executable generated by the project, right-click the project in Project Explorer and choose Debug As > Hexagon C/C++ Application . A prompt will appear asking you to Select Preferred Launcher (This dialog won't appear if Run was performed previously) Click Use configuration specific settings and then select Standard Create Debug Process Launcher if the project tool chain is GNU. If project tool chain is LLVM select, Standard Create LLDB Debug Process Launcher . A prompt appears asking you to confirm switching to the IDE debug perspective. Click Yes to switch to the debug perspective. The debug perspective enables you to perform debugging operations such as stepping, disassembly, setting breakpoints, viewing/modifying variables, and viewing registers. For example, clicking on the Registers tab of the debug perspective displays the contents of the Hexagon processor registers. If any debug-related information is not displayed, you can display it by choosing Show View from the Window menu. NOTE: Each Hexagon processor thread has its own set of resources (registers, memory, etc.). If a thread is selected in the Debug tab window, the corresponding resources are displayed. To configure the debug environment, right-click the project in Project Explorer and choose Debug As > Debug configuration . This command displays the Debug Configurations dialog box. Using the tabs in this dialog box you can configure the debugger, program arguments, and runtime environment. Note: In some instances, debugging project do not recognize the binaries built which are located inside the project and throws below error To fix this problem, right-click the project in Project Explorer and choose Debug As > Debug Configuration . This command displays the Debug Configurations dialog box below Right click Hexagon C/C++ Application and create New Configuration below Under C/C++ Application field browse to locate the project binaries in file system. Now press Apply and Debug. Import project with existing code The previous section explained how to create a project from scratch and then run and debug it on the simulator. In this section, we will see how to import an existing project with source code, build, run and debug on simulator and target. We will import the multithreading example found in Hexagon SDK at location %HEXAGON_SDK_ROOT%\\examples\\multithreading . Import Project in Hexagon IDE has two supported project types. Makefile project CMake project Import project as Make File Project Import the multithreading example by right-clicking in Project Explorer and selecting Import -> Hexagon C/C++ -> Import Hexagon Project > Next . In the next dialogue box, set the following project properties: Project type: \u2013 Makefile Project Project Name: multithreading Project Location: %HEXAGON_SDK_ROOT%\\examples\\multithreading Click Finish to close the window. This should result in showing the \"multithreading\" project in the workspace. Build a Makefile project Right-click the \"multithreading\" project and select Properties > C/C++ Build . On the Builder Settings tab, set the build type to External Builder set the build command: Build command: make remove the selection from checkbox Generate Makefiles automatically NOTE: You can set the build flavor to Debug , Release or ReleaseG . While debugging, Debug build flavors are recommended when possible over ReleaseG , to avoid possible confusions caused by the compiler optimizer. In the Behavior tab, set the flags: Build: BUILD=Debug DSP_ARCH=v65 hexagon Clean: BUILD=Debug DSP_ARCH=v65 hexagon_clean Click Apply and Close . To build the project, right-click the multithreading project and select Build Project . Import project as CMake Project Import the \"multithreading\" example by right-clicking in Project Explorer and selecting Import -> Hexagon C/C++ -> Import Hexagon Project > Next . In the next dialogue box, set the following project properties: Project type: \u2013 CMake Defaults Project Name: multithreading Project Location: %HEXAGON_SDK_ROOT%\\examples\\multithreading Click Finish to close the window. The Hexagon IDE will then apply the Build configuration settings to the project. The import behavior depends on whether a CMakeLists.txt is present in the project being imported. The IDE first looks for CMakeLists.txt file presence and performs suitable below operations. If the IDE finds CMakeLists.txt file, it Imports the project as it is and sets additional CMake build properties as below. Note: You can set the build flavor to Debug , Release or ReleaseG following below steps: Select Manage Configurations... Choose desired build flavor row and press Set Active button. You can always can update the CMake build properties above from Project->Properties-> Hexagon page as below: Press Apply to save these settings. The project is then ready for build. If the IDE does not find a CMakeLists.txt file, it will look for a hexagon.min file instead. If the IDE finds a hexagon.min file, it opens a prompt asking confirmation from the user to convert the project to a CMake project. OK: the IDE generates MakeD to CMake build template file and sets additional CMake build properties. Resulting project is ready for Build. Cancel: the IDE exits Import project operation. If the IDE does not find a hexagon.min file, it generates a Hexagon CMakeLists.txt standard file. The project is then ready to be built. Build a CMake Project Once a project has been imported , the user can build the project by right-clicking and choosing Build Project . The user can check the build status in the console view as shown Run and debug on simulator This section explains how to run and debug the code on the simulator. Run The following steps show how to run the project on the Hexagon simulator. Right-click the multithreading project and select Run As > Run Configurations . In the Run Configurations dialog box, select Hexagon C/C++ Application and under the Main tab, set the simulator target: C/C++ Application: <HEXAGON_SDK_ROOT>\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn On the Simulator tab, set the simulator arguments: CPU Architecture: v66 Miscellaneous Flags: --simulated_returnval --usefs hexagon_Debug_toolv84_v66 --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v66\\osam.cfg On the Arguments tab, set the program arguments: <HEXAGON_SDK_ROOT>\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim -- hexagon_Debug_toolv84_v66\\multithreading_q.so To execute the program, click Run . Debug The following steps show how to debug the project Define LLDB_HEXAGON_BOOTER_PATH : The LLDB_HEXAGON_BOOTER_PATH environment variable needs to be defined before the start of debugging on the simulator. Right-click the project and select Properties > C/C++ Build > Environment > Add . In the dialog-box that opens, set the environment variable: Name : LLDB_HEXAGON_BOOTER_PATH Value : <HEXAGON_SDK_ROOT>\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn Click Apply and Close to finish setting the LLDB_HEXAGON_BOOTER_PATH environment variable. Configure the debug environment by right-clicking on project and selecting Debug As > Debug Configurations . On the Main tab, set C/C++ Application field as <HEXAGON_SDK_ROOT>\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim If an error message Multiple launchers available - Select one appears, click Select one . Select the Use configuration specific settings box and choose Standard Create LLDB Debug Process Launcher . Under the Arguments tab, set the program arguments: Program arguments: --hexagon_Debug_toolv84_v66\\\\multithreading_q.so By default, breakpoints are set at the entry point of all functions defined in the IDL files listed in the *_QAICIDLS variable present in the hexagon.min file of the current project. If run_main_on_hexagon is used to execute the DSP code without defining any IDL file, then one breakpoint is set to the main() function for simulator debugging. If you want to override this default behaviour and set a specific breakpoint to one function then follow the steps below. To set up a breakpoint, click the Debugger tab and set Stop on startup at to multithreading_parallel_sum . On the Debugger tab click Shared libraries to select the folders which contain the shared libraries. NOTE: If the Shared libraries path is not added or is incorrect, the debugger will not be able to halt at any of the breakpoints in your code. This sets the breakpoint at the entry point of the function multithreading_parallel_sum . Click Apply and then Debug to start debugging the program on simulator. The Hexagon IDE will then prompt to switch to Debug perspective . Select Switch and the executable is launched and the breakpoint is hit at the multithreading_parallel_sum function. The debug perspective enables you to perform debugging operations such as step, disassembly, set breakpoints, view/modify variables, and view registers. For example, clicking on the Registers tab of the debug perspective displays the contents of the Hexagon processor registers. If any debug-related information is not displayed, you can display it by choosing Show View from the Window menu. Run and debug on target Before you start debugging on the target using the IDE, please go through the debug page for supported targets and software requirements for debugging. This section presents an example which shows how to debug a library project that is called by an application project: The shared library implements a compute engine which runs on the Hexagon processor. The application is an Android NDK multithreading application which offloads specific computations onto the compute engine. The application communicates with the compute engine using FastRPC . The compute engine will be debugged from the IDE while running on the target platform. Build for target This section assumes that the project was imported as a Makefile project. To build the project, right-click on the project name, select Properties and then C/C++ Build . Under Builder settings , enter a make command such as as make BUILD=Debug DSP_ARCH=v66 . It is recommended to start by debugging a shared object built with the Debug flavor. Debugging code built with the ReleaseG flavor is supported but more challenging due to the compiler optimizations making the code harder to follow. Debugging code built with the Release flavor is not supported. Now click the Behavior tab and set Build as hexagon and clean as hexagon_clean . Click Apply and close to apply settings and close window. Build the project by right-clicking on the project and selecting Build Project . Push required binaries to target To build the android binaries, run following command. Note that this command will build 64 bit version of the android side binaries. For 32 bit version, please refer to the building reference instructions . make android VERBOSE=1 Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push the HLOS side multithreading test executable and supporting multithreading stub library to the device adb shell mkdir -p /vendor/bin/ adb push android_ReleaseG_aarch64/multithreading /vendor/bin/ adb shell chmod 777 /vendor/bin/multithreading adb push android_ReleaseG_aarch64/ship/libmultithreading.so /vendor/lib64/ Push the Hexagon Shared Object to the device's file system adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_Debug_toolv84_v66/ship/libmultithreading_skel.so /vendor/lib/rfsa/dsp/sdk Generate a device-specific test signature based on the device's serial number. Follow the steps listed in the walkthrough section of the signing documentation. NOTE: This step only needs to be done once as the same test signature will enable loading any module. Attach to user PD process Set breakpoint in the shared object The imported Hexagon library project must be configured so it automatically attaches to the target process when you first start debugging it. To do this, you need to set breakpoints in the required files in the shared library project where you want the debug session to break. As with simulator debugging, breakpoints are set by default at the entry point of all functions defined in the IDL files listed in the *_QAICIDLS variable present in the hexagon.min file of the current project. If run_main_on_hexagon is used to execute the DSP code without defining any IDL file, no breakpoint will be set. To set a breakpoint at particular function double-click on Run > Debug Configurations C/C++ > Hexagon C/C++ Attach to Remote Application go to Debugger tab and set the function name in the textbox associated with Stop on startup at label. Click on Main Tab to disable auto build as shown in below. Click Apply and then Close . Set up debug configuration Right-click the project in Project Explorer and choose Debug As > Hexagon Attach to Application . The following dialog box appears: In this example we are going to use the aDSP Subsystem for debugging. Note that the procedure is the same for the cDSP. Shared DSP objects cannot be debugged standalone. The debugger needs the base image that initiates calls to the shared object. For the aDSP, fastrpc_shell_0 is the base image and is specified in the Executable field. Users have to pull it from device using the following command. adb pull /vendor/dsp/adsp/fastrpc_shell_0 To debug shared object on cDSP unsigned PD, you need to specify fastrpc_shell_unsigned_3 and for signed PD, you need to specify fastrpc_shell_3 . These files are present in /vendor/dsp/cdsp/ . In the Shared libraries search path field, specify the directory of shared library to be debugged during the debug session. Use File System or Workspace to search for shared libraries. In the Target drop-down list, choose the ID of the target device to debug on. After you choose the target in the Target field, default subsystem will be selected in the Subsystem field and FastRPC processes will be listed in the Process name field. If there are no fastrpc processes running on your target,you will see the Process name field empty. Select the subsystem you want to debug for in the Subsystem field, wait for some time to take it effect. Once your device is online, you will see selected subsystem in the Subsystem field and if there are any fastrpc processes running, they will be listed in the Process name field, otherwise you will find it empty. NOTE: If no target ID is displayed except the simulator, this indicates the target device is not connected properly \u2013 try disconnecting the device and then reconnecting it again. If you get the error as shown below. Then make sure your device does not get disconnected(especially after reboot) when processes are being fetched. The field \u201cProcess name\u201d needs to be set to the application that you will be running on the target. The application will show up in this drop-down list only if it is running on the device. Before you proceed and hit the \u201cDebug\u201d button, make sure the next two steps are completed. Run the Android application Before you can proceed with debugging the Hexagon library project, you will first need to run the multithreading application so it can make a call to the Hexagon library project. Run multithreading from command-line as follows adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin//multithreading \u200b Now multithreading will be waiting for the debugger to connect. Select the user PD process Click Refresh icon and the multithreading process will be shown in the process drop-down list now and select multithreading process. Click Debug to start the shared library debug session. This starts the Hexagon library project, and causes it to automatically connect to the target and then prompt you to switch to the Debug perspective. Debug Your breakpoint will be hit now and you should see the following view. At this point you can debug your code by stepping into or stepping over. When the debug session is completed, the multithreading output can be viewed in the console. Page table viewer When launched through the SDK, the Eclipse IDE includes an additional view: RTOS page table viewer. To open the RTOS page table viewer, choose Show View > Other > Hexagon > Page Table from the Window menu. It shows page table mappings made during a debug session.","title":"Using the IDE"},{"location":"tools/ide.html#the-hexagon-ide","text":"The Hexagon SDK includes a complete set of software development tools. The tools can be used in either of the following environments: The command-line interface (CLI) of the host development system The Hexagon integrated development environment (IDE) This document explains how to use the tools in the Hexagon IDE. The Hexagon IDE is built on top of Eclipse. For an overview of Eclipse concepts, please follow the Eclipse CDT Documentation . Note that all instructions are given assuming you are running Eclipse under Windows but Linux paths should be used when using Eclipse under Linux NOTE: The Hexagon IDE is not installed by default with the Hexagon SDK. To install the Hexagon IDE, either select the Eclipse checkbox when installing the Hexagon SDK, or install it separately following these instructions .","title":"The Hexagon IDE"},{"location":"tools/ide.html#set-up-the-hexagon-ide","text":"","title":"Set up the Hexagon IDE"},{"location":"tools/ide.html#start-the-ide","text":"To start the IDE from a CLI shell, run the %HEXAGON_SDK_ROOT%\\launch_hexagon_ide.cmd for Windows installation and $HEXAGON_SDK_ROOT/launch_hexagon_ide.sh for Linux installation. The first time you start the IDE a dialog box appears, asking where to create your project workspace. The workspace is where the source code (and related files and settings) for all your projects will be stored. Specify the location where the workspace should be created. Click OK to create the workspace at the specified location. NOTE: Be sure to store your workspace outside the Hexagon SDK install directory \u2013 this will make life easier when you need to upgrade to a newer Hexagon IDE version. NOTE: Workspace names cannot contain spaces. If they do, the Hexagon IDE will have problems building and executing projects. After you launch the IDE, a welcome screen appears. This screen provides a brief description of the IDE, along with quick links to the help systems for the Hexagon IDE, Hexagon SDK, Eclipse CDT, and Eclipse IDE. NOTE: The Hexagon IDE and Hexagon SDK help files are obtained from the Hexagon SDK. Close the welcome screen by clicking X that appears next to the Welcome tab in the upper left-hand corner of the window. After you close the welcome screen, the main IDE window appears. NOTE: The main Hexagon IDE window defaults to the Hexagon perspective (as indicated in the upper-right corner or the window).","title":"Start the IDE"},{"location":"tools/ide.html#set-ide-preferences","text":"The Hexagon IDE has several data settings which control how it works. These settings are called preferences, and they must be properly set before you can begin using the IDE. NOTE: If the Hexagon IDE was launched from the scripts mentioned in Start the IDE section, the preferences are set automatically, so you can skip this section. To access the IDE preferences, choose Preferences > Hexagon from the IDE\u2019s Window menu. The following dialog box appears: In the Hexagon SDK path field, specify the pathname of the root directory of the Hexagon SDK. In the Hexagon processor version drop-down list, choose the Hexagon processor version that you are developing software for. The default value is V65. The Hexagon tools path field is preset to the pathname of the Hexagon development tools (as defined in the PATH environment variable). Click Apply to save the changes, and then click OK to close the dialog box.","title":"Set IDE preferences"},{"location":"tools/ide.html#develop-your-first-project","text":"","title":"Develop your first project"},{"location":"tools/ide.html#create-a-new-project","text":"To create a new project, start the Hexagon IDE as mentioned above section. The main IDE window should appear as shown below: Choose New > Hexagon Project from the File menu: The following dialog box appears: In the Project name field, enter factorial as the name of the new project. NOTE: Project names cannot contain spaces \u2013 if they do, the Hexagon IDE will have problems building and executing the project. Project location defaults to the workspace. Tools location defaults to the value defined in the system environment variable, PATH. You can change either of these fields, or use the checkbox to restore the original default. In the Hexagon SDK location field, specify the SDK location. (This field is auto-filled if the IDE was launched from the scripts mentioned in Start the IDE section.) In the Project type drop-down list, choose Executable (.exe) . In the Architecture drop-down list, choose the required architecture. Click Finish . The main IDE window now displays a new project named factorial : Creating a new project results in generating a Hello World example with a hello.c file.","title":"Create a new project"},{"location":"tools/ide.html#new-project","text":"By default, new Hexagon Projects can be built with CMake . To build the project, right-click the project in the Project Explorer view and choose Build Project . This will trigger a CMake build, which can be tracked in the console view:","title":"New project"},{"location":"tools/ide.html#project-customization","text":"Once you have confirmed that the project is successfully built, you can customize it. For example, let's add another source file factorial.cpp to the project with the factorial program code. To do this, right-click the project in Project Explorer and choose New > Source File . A dialog box appears, prompting you to create a new source file. Enter the name of the source file ( factorial.cpp ) and select the file template ( Default C++ source template ). Click on Finish button to create the new source file. Next, copy the following code (or any other factorial implementation you want to use) to the source file, factorial.cpp . This code computes the factorial of a number passed to the program as a command-line argument. #include <stdio.h> #include <stdlib.h> int fact (int n) { if (n<=1) return 1; else return n*fact(n-1); } int main(int argc, char *argv[]) { int num,factorial; if (argc==2) { num=atoi(argv[1]); printf(\"User input is : %d\",num); } else { printf(\"Invalid number of arguments\"); return -1; } if(num<0) { printf(\"Invalid input - Must be positive\"); return -1; } factorial=fact(num); printf(\"\\nFactorial of %d is %d\",num,factorial); return 0; }","title":"Project customization"},{"location":"tools/ide.html#build-modified-project","text":"All source files need to be added manually to the CMakeList.txt file. In the present example, update this file by adding the line below: set(prj_sources factorial.cpp) The default CMake settings not only build the code but also run the the project on the Hexagon Simulator. Runtime arguments are specified as follows: Note: Use the line below in CMakeList.txt before runHexagonSim set(HEXAGON_EXEC_CMD_OPTIONS ${HEXAGON_EXEC_CMD_OPTIONS} 5) This will result in invoking the Hexagon simulator with the factorial binary file and passing the argument '5' To build the project, right-click the project in Project Explorer and choose Build Project . The status of the build can be viewed in the Console tab at the bottom of the main IDE window. The factorial example above builds a standalone executable that does not need QuRT APIs. Note: New projects in the Hexagon IDE are QuRT-disabled by default, which means that the QuRT libraries are not linked to the project. Follow the steps below for QuRT enablement: Right-click the project in Project Explorer and choose Properties Under C/C++ Build navigate to Cmake4Eclipse , select Symbols tab Edit QURT_OS flag and make value as 1 Press Apply to save the changes. Now User can continue building the QuRT enabled project.","title":"Build modified project"},{"location":"tools/ide.html#run-on-simulator-using-the-gui","text":"To run the project, right-click the project in Project Explorer and select Run As > Hexagon C/C++ Application . Running the program produces the following output in the console. Note the message Invalid number of arguments , which appears in the console output \u2013 the factorial program expects its input value to be specified as a command-line argument, and generates this error message because no argument was specified. To fix this problem, right-click the project in Project Explorer and choose Run As > Run Configuration . This command displays the Run Configurations dialog box, which enables you to configure the Application, simulator, program arguments, and runtime environment. The dialog box display tabs for configuring the simulator, program arguments, and runtime environment. (Note that the left-hand pane in the dialog box includes a newly-created runtime configuration named factorial , which appears under the item, Hexagon C/C++ Application .) To specify the program argument for the factorial program, click the Arguments tab in the dialog box. The factorial program expects one user argument (namely, the number whose factorial will be computed). Enter the value 5 as a program argument. To execute the factorial program with the specified argument, click Apply and then Run at the bottom of the dialog box. The dialog box closes and the following output (which includes the factorial output) is displayed in the console of the main IDE window. Note: In some instances, running project do not recognize the binaries built which are located inside the project and throws below error To fix this problem, right-click the project in Project Explorer and choose Run As > Run Configuration . This command displays the Run Configurations dialog box below Right click Hexagon C/C++ Application and create New Configuration below Under C/C++ Application field browse to locate the project binaries in file system. Now press Apply and Run.","title":"Run on simulator using the GUI"},{"location":"tools/ide.html#debug-on-simulator-using-the-gui","text":"To debug the executable generated by the project, right-click the project in Project Explorer and choose Debug As > Hexagon C/C++ Application . A prompt will appear asking you to Select Preferred Launcher (This dialog won't appear if Run was performed previously) Click Use configuration specific settings and then select Standard Create Debug Process Launcher if the project tool chain is GNU. If project tool chain is LLVM select, Standard Create LLDB Debug Process Launcher . A prompt appears asking you to confirm switching to the IDE debug perspective. Click Yes to switch to the debug perspective. The debug perspective enables you to perform debugging operations such as stepping, disassembly, setting breakpoints, viewing/modifying variables, and viewing registers. For example, clicking on the Registers tab of the debug perspective displays the contents of the Hexagon processor registers. If any debug-related information is not displayed, you can display it by choosing Show View from the Window menu. NOTE: Each Hexagon processor thread has its own set of resources (registers, memory, etc.). If a thread is selected in the Debug tab window, the corresponding resources are displayed. To configure the debug environment, right-click the project in Project Explorer and choose Debug As > Debug configuration . This command displays the Debug Configurations dialog box. Using the tabs in this dialog box you can configure the debugger, program arguments, and runtime environment. Note: In some instances, debugging project do not recognize the binaries built which are located inside the project and throws below error To fix this problem, right-click the project in Project Explorer and choose Debug As > Debug Configuration . This command displays the Debug Configurations dialog box below Right click Hexagon C/C++ Application and create New Configuration below Under C/C++ Application field browse to locate the project binaries in file system. Now press Apply and Debug.","title":"Debug on simulator using the GUI"},{"location":"tools/ide.html#import-project-with-existing-code","text":"The previous section explained how to create a project from scratch and then run and debug it on the simulator. In this section, we will see how to import an existing project with source code, build, run and debug on simulator and target. We will import the multithreading example found in Hexagon SDK at location %HEXAGON_SDK_ROOT%\\examples\\multithreading . Import Project in Hexagon IDE has two supported project types. Makefile project CMake project","title":"Import project with existing code"},{"location":"tools/ide.html#import-project-as-make-file-project","text":"Import the multithreading example by right-clicking in Project Explorer and selecting Import -> Hexagon C/C++ -> Import Hexagon Project > Next . In the next dialogue box, set the following project properties: Project type: \u2013 Makefile Project Project Name: multithreading Project Location: %HEXAGON_SDK_ROOT%\\examples\\multithreading Click Finish to close the window. This should result in showing the \"multithreading\" project in the workspace.","title":"Import project as Make File Project"},{"location":"tools/ide.html#build-a-makefile-project","text":"Right-click the \"multithreading\" project and select Properties > C/C++ Build . On the Builder Settings tab, set the build type to External Builder set the build command: Build command: make remove the selection from checkbox Generate Makefiles automatically NOTE: You can set the build flavor to Debug , Release or ReleaseG . While debugging, Debug build flavors are recommended when possible over ReleaseG , to avoid possible confusions caused by the compiler optimizer. In the Behavior tab, set the flags: Build: BUILD=Debug DSP_ARCH=v65 hexagon Clean: BUILD=Debug DSP_ARCH=v65 hexagon_clean Click Apply and Close . To build the project, right-click the multithreading project and select Build Project .","title":"Build a Makefile project"},{"location":"tools/ide.html#import-project-as-cmake-project","text":"Import the \"multithreading\" example by right-clicking in Project Explorer and selecting Import -> Hexagon C/C++ -> Import Hexagon Project > Next . In the next dialogue box, set the following project properties: Project type: \u2013 CMake Defaults Project Name: multithreading Project Location: %HEXAGON_SDK_ROOT%\\examples\\multithreading Click Finish to close the window. The Hexagon IDE will then apply the Build configuration settings to the project. The import behavior depends on whether a CMakeLists.txt is present in the project being imported. The IDE first looks for CMakeLists.txt file presence and performs suitable below operations. If the IDE finds CMakeLists.txt file, it Imports the project as it is and sets additional CMake build properties as below. Note: You can set the build flavor to Debug , Release or ReleaseG following below steps: Select Manage Configurations... Choose desired build flavor row and press Set Active button. You can always can update the CMake build properties above from Project->Properties-> Hexagon page as below: Press Apply to save these settings. The project is then ready for build. If the IDE does not find a CMakeLists.txt file, it will look for a hexagon.min file instead. If the IDE finds a hexagon.min file, it opens a prompt asking confirmation from the user to convert the project to a CMake project. OK: the IDE generates MakeD to CMake build template file and sets additional CMake build properties. Resulting project is ready for Build. Cancel: the IDE exits Import project operation. If the IDE does not find a hexagon.min file, it generates a Hexagon CMakeLists.txt standard file. The project is then ready to be built.","title":"Import project as CMake Project"},{"location":"tools/ide.html#build-a-cmake-project","text":"Once a project has been imported , the user can build the project by right-clicking and choosing Build Project . The user can check the build status in the console view as shown","title":"Build a CMake Project"},{"location":"tools/ide.html#run-and-debug-on-simulator","text":"This section explains how to run and debug the code on the simulator.","title":"Run and debug on simulator"},{"location":"tools/ide.html#run","text":"The following steps show how to run the project on the Hexagon simulator. Right-click the multithreading project and select Run As > Run Configurations . In the Run Configurations dialog box, select Hexagon C/C++ Application and under the Main tab, set the simulator target: C/C++ Application: <HEXAGON_SDK_ROOT>\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn On the Simulator tab, set the simulator arguments: CPU Architecture: v66 Miscellaneous Flags: --simulated_returnval --usefs hexagon_Debug_toolv84_v66 --l2tcm_base 0xd800 --rtos hexagon_Debug_toolv84_v66\\osam.cfg On the Arguments tab, set the program arguments: <HEXAGON_SDK_ROOT>\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim -- hexagon_Debug_toolv84_v66\\multithreading_q.so To execute the program, click Run .","title":"Run"},{"location":"tools/ide.html#debug","text":"The following steps show how to debug the project Define LLDB_HEXAGON_BOOTER_PATH : The LLDB_HEXAGON_BOOTER_PATH environment variable needs to be defined before the start of debugging on the simulator. Right-click the project and select Properties > C/C++ Build > Environment > Add . In the dialog-box that opens, set the environment variable: Name : LLDB_HEXAGON_BOOTER_PATH Value : <HEXAGON_SDK_ROOT>\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn Click Apply and Close to finish setting the LLDB_HEXAGON_BOOTER_PATH environment variable. Configure the debug environment by right-clicking on project and selecting Debug As > Debug Configurations . On the Main tab, set C/C++ Application field as <HEXAGON_SDK_ROOT>\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim If an error message Multiple launchers available - Select one appears, click Select one . Select the Use configuration specific settings box and choose Standard Create LLDB Debug Process Launcher . Under the Arguments tab, set the program arguments: Program arguments: --hexagon_Debug_toolv84_v66\\\\multithreading_q.so By default, breakpoints are set at the entry point of all functions defined in the IDL files listed in the *_QAICIDLS variable present in the hexagon.min file of the current project. If run_main_on_hexagon is used to execute the DSP code without defining any IDL file, then one breakpoint is set to the main() function for simulator debugging. If you want to override this default behaviour and set a specific breakpoint to one function then follow the steps below. To set up a breakpoint, click the Debugger tab and set Stop on startup at to multithreading_parallel_sum . On the Debugger tab click Shared libraries to select the folders which contain the shared libraries. NOTE: If the Shared libraries path is not added or is incorrect, the debugger will not be able to halt at any of the breakpoints in your code. This sets the breakpoint at the entry point of the function multithreading_parallel_sum . Click Apply and then Debug to start debugging the program on simulator. The Hexagon IDE will then prompt to switch to Debug perspective . Select Switch and the executable is launched and the breakpoint is hit at the multithreading_parallel_sum function. The debug perspective enables you to perform debugging operations such as step, disassembly, set breakpoints, view/modify variables, and view registers. For example, clicking on the Registers tab of the debug perspective displays the contents of the Hexagon processor registers. If any debug-related information is not displayed, you can display it by choosing Show View from the Window menu.","title":"Debug"},{"location":"tools/ide.html#run-and-debug-on-target","text":"Before you start debugging on the target using the IDE, please go through the debug page for supported targets and software requirements for debugging. This section presents an example which shows how to debug a library project that is called by an application project: The shared library implements a compute engine which runs on the Hexagon processor. The application is an Android NDK multithreading application which offloads specific computations onto the compute engine. The application communicates with the compute engine using FastRPC . The compute engine will be debugged from the IDE while running on the target platform.","title":"Run and debug on target"},{"location":"tools/ide.html#build-for-target","text":"This section assumes that the project was imported as a Makefile project. To build the project, right-click on the project name, select Properties and then C/C++ Build . Under Builder settings , enter a make command such as as make BUILD=Debug DSP_ARCH=v66 . It is recommended to start by debugging a shared object built with the Debug flavor. Debugging code built with the ReleaseG flavor is supported but more challenging due to the compiler optimizations making the code harder to follow. Debugging code built with the Release flavor is not supported. Now click the Behavior tab and set Build as hexagon and clean as hexagon_clean . Click Apply and close to apply settings and close window. Build the project by right-clicking on the project and selecting Build Project .","title":"Build for target"},{"location":"tools/ide.html#push-required-binaries-to-target","text":"To build the android binaries, run following command. Note that this command will build 64 bit version of the android side binaries. For 32 bit version, please refer to the building reference instructions . make android VERBOSE=1 Use ADB as root and remount system read/write adb root adb wait-for-device adb remount Push the HLOS side multithreading test executable and supporting multithreading stub library to the device adb shell mkdir -p /vendor/bin/ adb push android_ReleaseG_aarch64/multithreading /vendor/bin/ adb shell chmod 777 /vendor/bin/multithreading adb push android_ReleaseG_aarch64/ship/libmultithreading.so /vendor/lib64/ Push the Hexagon Shared Object to the device's file system adb shell mkdir -p /vendor/lib/rfsa/dsp/sdk adb push hexagon_Debug_toolv84_v66/ship/libmultithreading_skel.so /vendor/lib/rfsa/dsp/sdk Generate a device-specific test signature based on the device's serial number. Follow the steps listed in the walkthrough section of the signing documentation. NOTE: This step only needs to be done once as the same test signature will enable loading any module.","title":"Push required binaries to target"},{"location":"tools/ide.html#attach-to-user-pd-process","text":"Set breakpoint in the shared object The imported Hexagon library project must be configured so it automatically attaches to the target process when you first start debugging it. To do this, you need to set breakpoints in the required files in the shared library project where you want the debug session to break. As with simulator debugging, breakpoints are set by default at the entry point of all functions defined in the IDL files listed in the *_QAICIDLS variable present in the hexagon.min file of the current project. If run_main_on_hexagon is used to execute the DSP code without defining any IDL file, no breakpoint will be set. To set a breakpoint at particular function double-click on Run > Debug Configurations C/C++ > Hexagon C/C++ Attach to Remote Application go to Debugger tab and set the function name in the textbox associated with Stop on startup at label. Click on Main Tab to disable auto build as shown in below. Click Apply and then Close .","title":"Attach to user PD process"},{"location":"tools/ide.html#set-up-debug-configuration","text":"Right-click the project in Project Explorer and choose Debug As > Hexagon Attach to Application . The following dialog box appears: In this example we are going to use the aDSP Subsystem for debugging. Note that the procedure is the same for the cDSP. Shared DSP objects cannot be debugged standalone. The debugger needs the base image that initiates calls to the shared object. For the aDSP, fastrpc_shell_0 is the base image and is specified in the Executable field. Users have to pull it from device using the following command. adb pull /vendor/dsp/adsp/fastrpc_shell_0 To debug shared object on cDSP unsigned PD, you need to specify fastrpc_shell_unsigned_3 and for signed PD, you need to specify fastrpc_shell_3 . These files are present in /vendor/dsp/cdsp/ . In the Shared libraries search path field, specify the directory of shared library to be debugged during the debug session. Use File System or Workspace to search for shared libraries. In the Target drop-down list, choose the ID of the target device to debug on. After you choose the target in the Target field, default subsystem will be selected in the Subsystem field and FastRPC processes will be listed in the Process name field. If there are no fastrpc processes running on your target,you will see the Process name field empty. Select the subsystem you want to debug for in the Subsystem field, wait for some time to take it effect. Once your device is online, you will see selected subsystem in the Subsystem field and if there are any fastrpc processes running, they will be listed in the Process name field, otherwise you will find it empty. NOTE: If no target ID is displayed except the simulator, this indicates the target device is not connected properly \u2013 try disconnecting the device and then reconnecting it again. If you get the error as shown below. Then make sure your device does not get disconnected(especially after reboot) when processes are being fetched. The field \u201cProcess name\u201d needs to be set to the application that you will be running on the target. The application will show up in this drop-down list only if it is running on the device. Before you proceed and hit the \u201cDebug\u201d button, make sure the next two steps are completed. Run the Android application Before you can proceed with debugging the Hexagon library project, you will first need to run the multithreading application so it can make a call to the Hexagon library project. Run multithreading from command-line as follows adb wait-for-device shell export LD_LIBRARY_PATH=/vendor/lib64/:$LD_LIBRARY_PATH ADSP_LIBRARY_PATH=\"/vendor/lib/rfsa/dsp/sdk\\;/vendor/lib/rfsa/dsp/testsig;\" /vendor/bin//multithreading \u200b Now multithreading will be waiting for the debugger to connect. Select the user PD process Click Refresh icon and the multithreading process will be shown in the process drop-down list now and select multithreading process. Click Debug to start the shared library debug session. This starts the Hexagon library project, and causes it to automatically connect to the target and then prompt you to switch to the Debug perspective.","title":"Set up debug configuration"},{"location":"tools/ide.html#debug_1","text":"Your breakpoint will be hit now and you should see the following view. At this point you can debug your code by stepping into or stepping over. When the debug session is completed, the multithreading output can be viewed in the console.","title":"Debug"},{"location":"tools/ide.html#page-table-viewer","text":"When launched through the SDK, the Eclipse IDE includes an additional view: RTOS page table viewer. To open the RTOS page table viewer, choose Show View > Other > Hexagon > Page Table from the Window menu. It shows page table mappings made during a debug session.","title":"Page table viewer"},{"location":"tools/messaging.html","text":"Messaging resources Overview This page discusses the tools available for logging debug messages from the DSP. The DSP can generate messages using the Hexagon SDK's FARF API or through regular IO functions like printf . These messages are sent to a diagnostic (or DIAG) framework present on the DSP, from which they can be collected. They can also be collected via USB with a tool called mini-dm running on the host computer. If the USB port is not connected to collect these messages, the DIAG framework has very low run-time overhead. The DIAG framework also includes a circular buffer, which allows the most recent messages to be read from crash dumps during debugging. Optionally, DSP messages can be routed to the CPU in parallel with the DIAG framework, thus allowing logcat to collect DSP messages from the CPU. Routing DSP messages to the CPU has a small but measurable impact on power and thus is not enabled by default. Depending on the types of FARF messages being used, the user can enable or disable messages at different priority levels, at compile time or run time. Generating messages Static FARF FARF API The FARF API is defined in ${HEXAGON_SDK_ROOT}/inc/HAP_farf.h. It allows the caller to send diagnostic messages: FARF(level, msg, ...) The FARF level allows the user to selectively enable or disable certain types of messages according to their priority level. The following levels are supported and listed in increasing priority: LOW MEDIUM HIGH ERROR FATAL ALWAYS Compile-time enablement of static FARF FARF messages whose compilation is controlled via conditional compilation using the FARF levels are referred as static FARF messages as opposed to run-time FARF , which are always compiled in but activated at runtime, as explained further below. A FARF level must be set to 1 for FARF macros to be compiled in. For example: #define FARF_LOW 1 #include <HAP_farf.h> FARF(LOW, \"something happened: %s\", (const char*)string); will result in enabling LOW FARF messages and thus compiling the line above. Executing this line on the cDSP will result in sending a diagnostic message. The code snipped above generates the following output: QDSP6/Low [ <source file name>.c 147] 00dd:0c: CDSP: something happened: Example String In the output above, the portion after CDSP: is the actual string logged by the user. The portion before CDSP: i.e. 00dd:0c: represents 'ThreadId:ProcessID' on the DSP. Note that FARF_LOW needs to be defined before HAP_farf.h is included. The reason for this approach is that including HAP_farf.h will result in setting default values for each FARF level not already defined. Setting a FARF level after including the header file will result in a warning or error (depending on the compilation flags) for redefining a macro value without undefining it first. If FARF_LOW is set to 0, as it is by default, the above FARF command will not be compiled in. When building the Debug variant or with any build defining _DEBUG, the following FARF levels are enabled: HIGH ERROR FATAL ALWAYS When building a Release variant, the FARF HIGH level is disabled by default: only ERROR , FATAL , and ALWAYS are enabled by default. Custom levels The FARF mechanism also allows users to define their own custom levels. For example: #include <HAP_farf.h> #define FARF_MYTRACE 1 #define FARF_MYTRACE_LEVEL HAP_LEVEL_LOW FARF(MYTRACE, \"custom trace in file %s on line %d\", __FILE__, __LINE__); will send this message whenever the FARF LOW level is enabled. Config file enablement of static FARF Static FARF messages that are enabled on the DSP can be sent to logcat by following the steps below: Create a <rpc_hlos_process_name>.farf configuration file An empty file is sufficient to send to logcat any FARF message that was compiled following the approach described above . Push this file on device on the path specified by [A]DSP_LIBRARY_PATH For example, if your CPU executable name is calculator , you need to create a calculator.farf file in the path specified by DSP_LIBRARY_PATH . Concretely, assuming DSP_LIBRARY_PATH includes /vendor/lib/rfsa/dsp/sdk/ , a file enabling FARF messages at all levels to be routed to logcat can be created with just one command line on your connected host: adb shell \"touch /vendor/lib/rfsa/dsp/sdk/calculator.farf\" printf printf is supported in its standard prototype. printf messages sent to stdout are converted by the Hexagon run-time into static FARF messages of HIGH priority. printf messages sent to stderr are converted to static FARF messages of ERROR priority. Run-time FARF In addition to static FARF messages that are enabled or disabled at compile time, the SDK supports run-time FARF messages, which are always compiled in, but not sent to the DIAG framework unless enabled at run-time. Run-time FARF messages that are not enabled add negligible run-time overhead. Run-time FARF messages are used in a similar way to static FARF messages, but with \"RUNTIME_\" prefixed to the message level in the FARF invocation: #include <HAP_farf.h> FARF(RUNTIME_LOW,\"Run-time Low FARF message\"); FARF(RUNTIME_MEDIUM,\"Run-time Medium FARF message\"); FARF(RUNTIME_HIGH,\"Run-time High FARF message\"); FARF(RUNTIME_ERROR,\"Run-time Error FARF message\"); FARF(RUNTIME_FATAL,\"Run-time Fatal FARF message\"); Just as with static FARF, the level allows to specify which FARF messages to enable or not. The user can set run-time FARF levels in one of two ways: Adding a config file to the HLOS file system in the [A]DSP_LIBRARY_PATH Calling an API from the HLOS application code Config file enablement of run-time FARF The process for enabling run-time FARF messages via config files is similar to the process for routing static messages to logcat using config files but the contents of the config <rpc_hlos_process_name>.farf file is also relevant as is explained further below. The presence of the file will result, for the levels enabled, in: enabling run-time FARF messages routing run-time (as well as static) FARF messages to logcat (in addition to sending them to the DIAG framework) Run-time FARF messages can be sent to logcat by following the steps below: Create a <rpc_hlos_process_name>.farf configuration file Edit this file with a mask value to enable specific run-time FARF levels Push this file on device on the path specified by [A]DSP_LIBRARY_PATH For example, if your CPU executable name is calculator , you need to create a calculator.farf file in the path specified by DSP_LIBRARY_PATH . Concretely, assuming DSP_LIBRARY_PATH includes /vendor/lib/rfsa/dsp/sdk/ , a file enabling run-time FARF messages at all levels to be routed to logcat can be created with just one command line on your connected host: adb shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/calculator.farf\" The content of the config file is formatted as follows: <hex_mask> [comma-separated filenames (optional)] The mask has one bit for each level: 0x01 - LOW 0x02 - MEDIUM 0x04 - HIGH 0x08 - ERROR 0x10 - FATAL For example To route all levels of run-time messages to logcat (in addition to the DIAG framework) for all files in the process, use: 0x1f To route all levels of run-time messages but LOW to logcat for the file test.c only, use: 0x1e test.c To route all levels of run-time messages to logcat for the files test.c and foo.c , use: 0x1f test.c,foo.c Programmatic enablement of run-time FARF You can also enable run-time FARF programmatically with the following API available from HAP_farf.h : int HAP_setFARFRuntimeLoggingParams(unsigned int mask, const char* files[], short numberOfFiles); This approach follows the same format as that used in <rpc_hlos_process_name>.farf config files: a mask must be provided to enable or disable specific FARF levels a list of file names may be provided to only enable FARF messages for specific files to enable FARF for all files, use a NULL pointer for the file array and 0 for the number of files being specified Unlike the enablement of run-time messages via config file, this approach enables messages to be routed to the DIAG framework but not to logcat. To see an illustration of this approach, please refer to the HAP example . Capturing messages mini-dm mini-dm is a tool that queries and displays diagnostic messages generated by the Hexagon DSP. The tool is located under tools/debug/mini-dm . Connecting mini-dm on Linux The SDK includes versions of mini-dm for Ubuntu versions 16 and 18. If you only have one connected device, simply running mini-dm will connect the tool to your device automatically: $ mini-dm Running mini-dm version: 3.3 Completed processing command line --- Connecting to the only usbport connected mini-dm is waiting for a DMSS connection... DMSS is connected. Running mini-dm... ------------Mini-dm is ready to log------------- If you have multiple connected devices, you need to indicate which device to connect mini-dm to, by using the --usbport option followed by the relevant port number. In order to figure out which port to use, simply run adb devices -l : $ adb devices -l List of devices attached 5449b015 device usb:1-10 product:msmnile model:msmnile_for_arm64 device:msmnile transport_id:11 f09566c8 device usb:2-7 product:kona model:Kona_for_arm64 device:kona transport_id:12 In the example above, port 1-10 should be used to connect to device 5449b015 (an SM8150), and port 2-7 should be used to connect to device f09566c8 (Kona). To connect to the Kona device for example, simply run: $ mini-dm --usbport 2-7 Running mini-dm version: 3.3 Completed processing command line --- Connecting to usbport 2-7 mini-dm is waiting for a DMSS connection... DMSS is connected. Running mini-dm... ------------Mini-dm is ready to log------------- Note: The usbport will be expressed differently if the device is connected to Linux via one or more USB hubs. A . in a port number represents a USB hub. For example, if the device is connected via two USB hubs, its USB port number be reported as follows: device usb:a-b.c.d product:<device> This representation is interpreted as follows: * First USB hub is connected to port `b` of USB bus `a` * Second USB hub is connected to port `c` of first USB hub * Device is connected to port `d` of the second USB hub In this case, mini-dm will expect the port a-b.c.d to be passed as an argument to connect to this device. For example, if adb devices -l prints the port number in the format device usb:2-10.1 , run mini-dm as follows: $ mini-dm --usbport 2-10.1 Connecting mini-dm on Windows mini-dm on Windows differs from Linux in that it cannot detect automatically the port to connect to and that adb doesn't return the port numbers of the connected devices either. Instead, the SDK includes the %HEXAGON_SDK_ROOT%\\utils\\scripts\\com_finder.py script that prints a listing of all the active USB ports to which a Qualcomm device is connected: usage: com_finder.py [-h] [-v] [-i] optional arguments: -h, --help show this help message and exit -v prints verbose information -i enable debugging info For example: > scripts\\com_finder.py Qualcomm HS-USB Com ports found: COM15 You would then connect mini-dm to the device using the following command: > mini-dm --usbport 15 Logging options Many mini-dm command line options are specific to the sensor subsystem, and should not be used by the general SDK user. Following are the options useful to the general SDK user. --format Control the output format of diagnostic messages. Default: \"[%05d{SS}/%02d{MASK}] %02d{M}:%02d{S}.%03d{MS} %s{STR} %04d{L} %s{F}\" --show_color_schemes Show the available color schemes for diagnostic messages. Default: false Note that colors are only supported on Windows. --color_scheme <color_scheme_number> Set color scheme for diag message output. Default: 0 (no color) Color schemes 1-5 highlight FARF messages in different colors depending on their levels. Note that colors are only supported on Windows. To enable filtering of logs, mini-dm supports the following options which can be used along with other logging options. The search pattern may include one or more strings separated by the & or | . The & operator defines a pattern where all strings must be present and the | operator a pattern where only one of the strings must be present. No filter is applied by default. --filter-in \"pattern\" Only keep log messages matching the pattern. --filter-out \"pattern\" Discard log messages matching the pattern. For example: To use filter-in and filter-out options, use: > mini-dm --usbport 7 --filter-in \"cdsp\" --filter-out \"power\" This will display logs containing keyword cdsp and not keyword power . To use filter-in option with & , use: > mini-dm --usbport 7 --filter-in \"cdsp & power\" This will display only the logs containing both keywords cdsp and power . The following mini-dm options can be used to enable/disable logging from the sensor subsystem. --enable_sensor_ssids <true/false> Enable logging from sensor subsystems. If true, logging from extended SNS subsystems (FRAMEWORK, PLATFORM, SENSOR INT and SENSOR EXT) are enabled, along with the default subsystems, QDSP6 and SNS. --subsystem_masks: <mask_value1,mask_value2,...> Control the logging mask for the subsystems enabled with --enable_sensor_ssids . This options accepts 6 comma-separated log mask values for the 6 subsystems respectively. --default_ss_mask: <mask_value> Configure the default log mask for all subsystems using this option. Default value of default log mask for all subsystems is 0x1F , which enables all log levels. This option can also be used to configure the log mask for the default subsystems. For example: To configure default log mask for default subsystems, use: > mini-dm --usbport 7 --subsystem_masks 0x7,0x7 This will allow messages from the default subsystems, only with log levels LOW, MEDIUM and HIGH. To enable sensor subsystems and configure default log masks for all, use: > mini-dm --usbport 7 --enable_sensor_ssids --default_ss_mask 0x7 This will allow messages from all sensor and default subsystems, only with log levels LOW, MEDIUM and HIGH. To enable sensor subsystems and configure different log masks for different subsystems, use: > mini-dm --usbport 7 --enable_sensor_ssids --subsystem_masks 0x0,0x1,0x2,0x4,0x8,0x10,0x1F This will allow the following messages from the different subsystems. QDSP6 - No messages SNS - LOW SNS FRAMEWORK - MEDIUM SNS PLATFORM - HIGH SNS SENSOR INT - ERROR SNS SENSOR EXT - FATAL logcat Once messages are enabled to be routed to logcat , the FARF and printf messages are visible with logcat. The logcat filtering option -s adsprpc allows to display only those messages generated using FARF or printf from any DSP (ADSP/CDSP/SLPI/MDSP) since all these messages are tagged with adsprpc : adb logcat -s adsprpc A sample output should look like this: 05-25 19:25:57.275 4442 4443 W adsprpc : calculator_imp.c:17:0x40a6:=============== DSP: sum result 499500 =============== Note: The FastRPC logging framework may not flush the last few messages during a crash or exit when the application opens a remote session without domains. To avoid this, follow the approach used by all Hexagon SDK examples and use the multi-domain feature when initiating FastRPC calls.","title":"Message logging"},{"location":"tools/messaging.html#messaging-resources","text":"","title":"Messaging resources"},{"location":"tools/messaging.html#overview","text":"This page discusses the tools available for logging debug messages from the DSP. The DSP can generate messages using the Hexagon SDK's FARF API or through regular IO functions like printf . These messages are sent to a diagnostic (or DIAG) framework present on the DSP, from which they can be collected. They can also be collected via USB with a tool called mini-dm running on the host computer. If the USB port is not connected to collect these messages, the DIAG framework has very low run-time overhead. The DIAG framework also includes a circular buffer, which allows the most recent messages to be read from crash dumps during debugging. Optionally, DSP messages can be routed to the CPU in parallel with the DIAG framework, thus allowing logcat to collect DSP messages from the CPU. Routing DSP messages to the CPU has a small but measurable impact on power and thus is not enabled by default. Depending on the types of FARF messages being used, the user can enable or disable messages at different priority levels, at compile time or run time.","title":"Overview"},{"location":"tools/messaging.html#generating-messages","text":"","title":"Generating messages"},{"location":"tools/messaging.html#static-farf","text":"","title":"Static FARF"},{"location":"tools/messaging.html#farf-api","text":"The FARF API is defined in ${HEXAGON_SDK_ROOT}/inc/HAP_farf.h. It allows the caller to send diagnostic messages: FARF(level, msg, ...) The FARF level allows the user to selectively enable or disable certain types of messages according to their priority level. The following levels are supported and listed in increasing priority: LOW MEDIUM HIGH ERROR FATAL ALWAYS","title":"FARF API"},{"location":"tools/messaging.html#compile-time-enablement-of-static-farf","text":"FARF messages whose compilation is controlled via conditional compilation using the FARF levels are referred as static FARF messages as opposed to run-time FARF , which are always compiled in but activated at runtime, as explained further below. A FARF level must be set to 1 for FARF macros to be compiled in. For example: #define FARF_LOW 1 #include <HAP_farf.h> FARF(LOW, \"something happened: %s\", (const char*)string); will result in enabling LOW FARF messages and thus compiling the line above. Executing this line on the cDSP will result in sending a diagnostic message. The code snipped above generates the following output: QDSP6/Low [ <source file name>.c 147] 00dd:0c: CDSP: something happened: Example String In the output above, the portion after CDSP: is the actual string logged by the user. The portion before CDSP: i.e. 00dd:0c: represents 'ThreadId:ProcessID' on the DSP. Note that FARF_LOW needs to be defined before HAP_farf.h is included. The reason for this approach is that including HAP_farf.h will result in setting default values for each FARF level not already defined. Setting a FARF level after including the header file will result in a warning or error (depending on the compilation flags) for redefining a macro value without undefining it first. If FARF_LOW is set to 0, as it is by default, the above FARF command will not be compiled in. When building the Debug variant or with any build defining _DEBUG, the following FARF levels are enabled: HIGH ERROR FATAL ALWAYS When building a Release variant, the FARF HIGH level is disabled by default: only ERROR , FATAL , and ALWAYS are enabled by default.","title":"Compile-time enablement of static FARF"},{"location":"tools/messaging.html#custom-levels","text":"The FARF mechanism also allows users to define their own custom levels. For example: #include <HAP_farf.h> #define FARF_MYTRACE 1 #define FARF_MYTRACE_LEVEL HAP_LEVEL_LOW FARF(MYTRACE, \"custom trace in file %s on line %d\", __FILE__, __LINE__); will send this message whenever the FARF LOW level is enabled.","title":"Custom levels"},{"location":"tools/messaging.html#config-file-enablement-of-static-farf","text":"Static FARF messages that are enabled on the DSP can be sent to logcat by following the steps below: Create a <rpc_hlos_process_name>.farf configuration file An empty file is sufficient to send to logcat any FARF message that was compiled following the approach described above . Push this file on device on the path specified by [A]DSP_LIBRARY_PATH For example, if your CPU executable name is calculator , you need to create a calculator.farf file in the path specified by DSP_LIBRARY_PATH . Concretely, assuming DSP_LIBRARY_PATH includes /vendor/lib/rfsa/dsp/sdk/ , a file enabling FARF messages at all levels to be routed to logcat can be created with just one command line on your connected host: adb shell \"touch /vendor/lib/rfsa/dsp/sdk/calculator.farf\"","title":"Config file enablement of static FARF"},{"location":"tools/messaging.html#printf","text":"printf is supported in its standard prototype. printf messages sent to stdout are converted by the Hexagon run-time into static FARF messages of HIGH priority. printf messages sent to stderr are converted to static FARF messages of ERROR priority.","title":"printf"},{"location":"tools/messaging.html#run-time-farf","text":"In addition to static FARF messages that are enabled or disabled at compile time, the SDK supports run-time FARF messages, which are always compiled in, but not sent to the DIAG framework unless enabled at run-time. Run-time FARF messages that are not enabled add negligible run-time overhead. Run-time FARF messages are used in a similar way to static FARF messages, but with \"RUNTIME_\" prefixed to the message level in the FARF invocation: #include <HAP_farf.h> FARF(RUNTIME_LOW,\"Run-time Low FARF message\"); FARF(RUNTIME_MEDIUM,\"Run-time Medium FARF message\"); FARF(RUNTIME_HIGH,\"Run-time High FARF message\"); FARF(RUNTIME_ERROR,\"Run-time Error FARF message\"); FARF(RUNTIME_FATAL,\"Run-time Fatal FARF message\"); Just as with static FARF, the level allows to specify which FARF messages to enable or not. The user can set run-time FARF levels in one of two ways: Adding a config file to the HLOS file system in the [A]DSP_LIBRARY_PATH Calling an API from the HLOS application code","title":"Run-time FARF"},{"location":"tools/messaging.html#config-file-enablement-of-run-time-farf","text":"The process for enabling run-time FARF messages via config files is similar to the process for routing static messages to logcat using config files but the contents of the config <rpc_hlos_process_name>.farf file is also relevant as is explained further below. The presence of the file will result, for the levels enabled, in: enabling run-time FARF messages routing run-time (as well as static) FARF messages to logcat (in addition to sending them to the DIAG framework) Run-time FARF messages can be sent to logcat by following the steps below: Create a <rpc_hlos_process_name>.farf configuration file Edit this file with a mask value to enable specific run-time FARF levels Push this file on device on the path specified by [A]DSP_LIBRARY_PATH For example, if your CPU executable name is calculator , you need to create a calculator.farf file in the path specified by DSP_LIBRARY_PATH . Concretely, assuming DSP_LIBRARY_PATH includes /vendor/lib/rfsa/dsp/sdk/ , a file enabling run-time FARF messages at all levels to be routed to logcat can be created with just one command line on your connected host: adb shell \"echo 0x1f > /vendor/lib/rfsa/dsp/sdk/calculator.farf\" The content of the config file is formatted as follows: <hex_mask> [comma-separated filenames (optional)] The mask has one bit for each level: 0x01 - LOW 0x02 - MEDIUM 0x04 - HIGH 0x08 - ERROR 0x10 - FATAL For example To route all levels of run-time messages to logcat (in addition to the DIAG framework) for all files in the process, use: 0x1f To route all levels of run-time messages but LOW to logcat for the file test.c only, use: 0x1e test.c To route all levels of run-time messages to logcat for the files test.c and foo.c , use: 0x1f test.c,foo.c","title":"Config file enablement of run-time FARF"},{"location":"tools/messaging.html#programmatic-enablement-of-run-time-farf","text":"You can also enable run-time FARF programmatically with the following API available from HAP_farf.h : int HAP_setFARFRuntimeLoggingParams(unsigned int mask, const char* files[], short numberOfFiles); This approach follows the same format as that used in <rpc_hlos_process_name>.farf config files: a mask must be provided to enable or disable specific FARF levels a list of file names may be provided to only enable FARF messages for specific files to enable FARF for all files, use a NULL pointer for the file array and 0 for the number of files being specified Unlike the enablement of run-time messages via config file, this approach enables messages to be routed to the DIAG framework but not to logcat. To see an illustration of this approach, please refer to the HAP example .","title":"Programmatic enablement of run-time FARF"},{"location":"tools/messaging.html#capturing-messages","text":"","title":"Capturing messages"},{"location":"tools/messaging.html#mini-dm","text":"mini-dm is a tool that queries and displays diagnostic messages generated by the Hexagon DSP. The tool is located under tools/debug/mini-dm .","title":"mini-dm"},{"location":"tools/messaging.html#connecting-mini-dm-on-linux","text":"The SDK includes versions of mini-dm for Ubuntu versions 16 and 18. If you only have one connected device, simply running mini-dm will connect the tool to your device automatically: $ mini-dm Running mini-dm version: 3.3 Completed processing command line --- Connecting to the only usbport connected mini-dm is waiting for a DMSS connection... DMSS is connected. Running mini-dm... ------------Mini-dm is ready to log------------- If you have multiple connected devices, you need to indicate which device to connect mini-dm to, by using the --usbport option followed by the relevant port number. In order to figure out which port to use, simply run adb devices -l : $ adb devices -l List of devices attached 5449b015 device usb:1-10 product:msmnile model:msmnile_for_arm64 device:msmnile transport_id:11 f09566c8 device usb:2-7 product:kona model:Kona_for_arm64 device:kona transport_id:12 In the example above, port 1-10 should be used to connect to device 5449b015 (an SM8150), and port 2-7 should be used to connect to device f09566c8 (Kona). To connect to the Kona device for example, simply run: $ mini-dm --usbport 2-7 Running mini-dm version: 3.3 Completed processing command line --- Connecting to usbport 2-7 mini-dm is waiting for a DMSS connection... DMSS is connected. Running mini-dm... ------------Mini-dm is ready to log------------- Note: The usbport will be expressed differently if the device is connected to Linux via one or more USB hubs. A . in a port number represents a USB hub. For example, if the device is connected via two USB hubs, its USB port number be reported as follows: device usb:a-b.c.d product:<device> This representation is interpreted as follows: * First USB hub is connected to port `b` of USB bus `a` * Second USB hub is connected to port `c` of first USB hub * Device is connected to port `d` of the second USB hub In this case, mini-dm will expect the port a-b.c.d to be passed as an argument to connect to this device. For example, if adb devices -l prints the port number in the format device usb:2-10.1 , run mini-dm as follows: $ mini-dm --usbport 2-10.1","title":"Connecting mini-dm on Linux"},{"location":"tools/messaging.html#connecting-mini-dm-on-windows","text":"mini-dm on Windows differs from Linux in that it cannot detect automatically the port to connect to and that adb doesn't return the port numbers of the connected devices either. Instead, the SDK includes the %HEXAGON_SDK_ROOT%\\utils\\scripts\\com_finder.py script that prints a listing of all the active USB ports to which a Qualcomm device is connected: usage: com_finder.py [-h] [-v] [-i] optional arguments: -h, --help show this help message and exit -v prints verbose information -i enable debugging info For example: > scripts\\com_finder.py Qualcomm HS-USB Com ports found: COM15 You would then connect mini-dm to the device using the following command: > mini-dm --usbport 15","title":"Connecting mini-dm on Windows"},{"location":"tools/messaging.html#logging-options","text":"Many mini-dm command line options are specific to the sensor subsystem, and should not be used by the general SDK user. Following are the options useful to the general SDK user. --format Control the output format of diagnostic messages. Default: \"[%05d{SS}/%02d{MASK}] %02d{M}:%02d{S}.%03d{MS} %s{STR} %04d{L} %s{F}\" --show_color_schemes Show the available color schemes for diagnostic messages. Default: false Note that colors are only supported on Windows. --color_scheme <color_scheme_number> Set color scheme for diag message output. Default: 0 (no color) Color schemes 1-5 highlight FARF messages in different colors depending on their levels. Note that colors are only supported on Windows. To enable filtering of logs, mini-dm supports the following options which can be used along with other logging options. The search pattern may include one or more strings separated by the & or | . The & operator defines a pattern where all strings must be present and the | operator a pattern where only one of the strings must be present. No filter is applied by default. --filter-in \"pattern\" Only keep log messages matching the pattern. --filter-out \"pattern\" Discard log messages matching the pattern. For example: To use filter-in and filter-out options, use: > mini-dm --usbport 7 --filter-in \"cdsp\" --filter-out \"power\" This will display logs containing keyword cdsp and not keyword power . To use filter-in option with & , use: > mini-dm --usbport 7 --filter-in \"cdsp & power\" This will display only the logs containing both keywords cdsp and power . The following mini-dm options can be used to enable/disable logging from the sensor subsystem. --enable_sensor_ssids <true/false> Enable logging from sensor subsystems. If true, logging from extended SNS subsystems (FRAMEWORK, PLATFORM, SENSOR INT and SENSOR EXT) are enabled, along with the default subsystems, QDSP6 and SNS. --subsystem_masks: <mask_value1,mask_value2,...> Control the logging mask for the subsystems enabled with --enable_sensor_ssids . This options accepts 6 comma-separated log mask values for the 6 subsystems respectively. --default_ss_mask: <mask_value> Configure the default log mask for all subsystems using this option. Default value of default log mask for all subsystems is 0x1F , which enables all log levels. This option can also be used to configure the log mask for the default subsystems. For example: To configure default log mask for default subsystems, use: > mini-dm --usbport 7 --subsystem_masks 0x7,0x7 This will allow messages from the default subsystems, only with log levels LOW, MEDIUM and HIGH. To enable sensor subsystems and configure default log masks for all, use: > mini-dm --usbport 7 --enable_sensor_ssids --default_ss_mask 0x7 This will allow messages from all sensor and default subsystems, only with log levels LOW, MEDIUM and HIGH. To enable sensor subsystems and configure different log masks for different subsystems, use: > mini-dm --usbport 7 --enable_sensor_ssids --subsystem_masks 0x0,0x1,0x2,0x4,0x8,0x10,0x1F This will allow the following messages from the different subsystems. QDSP6 - No messages SNS - LOW SNS FRAMEWORK - MEDIUM SNS PLATFORM - HIGH SNS SENSOR INT - ERROR SNS SENSOR EXT - FATAL","title":"Logging options"},{"location":"tools/messaging.html#logcat","text":"Once messages are enabled to be routed to logcat , the FARF and printf messages are visible with logcat. The logcat filtering option -s adsprpc allows to display only those messages generated using FARF or printf from any DSP (ADSP/CDSP/SLPI/MDSP) since all these messages are tagged with adsprpc : adb logcat -s adsprpc A sample output should look like this: 05-25 19:25:57.275 4442 4443 W adsprpc : calculator_imp.c:17:0x40a6:=============== DSP: sum result 499500 =============== Note: The FastRPC logging framework may not flush the last few messages during a crash or exit when the application opens a remote session without domains. To avoid this, follow the approach used by all Hexagon SDK examples and use the multi-domain feature when initiating FastRPC calls.","title":"logcat"},{"location":"tools/profile.html","text":"Profiling resources The Hexagon SDK provides a number of profiling tools serving different purposes. The table below summarizes these options and their availability on the Hexagon simulator, target devices, or both. The most common profiling tools and techniques are illustrated in the profiling example. Tool Description Simulator Device HAP_perf timer APIs APIs to access elapsed cycles or time Yes Yes Hexagon simulator and profiler Generate instruction and data cache analysis as well as function-level profiling reports Yes No Hexagon Trace Analyzer A trace analyzer utility that generates various analysis reports for cDSP Yes Yes sysMon A set of monitoring tools that collect and display high-level DSP profiling information No Yes Timers The timer APIs may be used to time a specific section of code by accessing the elapsed time or number of processor cycles. Measuring time The most accurate way of measuring time consists of accessing the Hexagon UTIMERHI:UTIMERLO registers. This pair of 32-bit register is a direct measure of the elapsed time reported in ticks. One tick is 1/19.2 MHz seconds. These registers can be accessed from C with the HAP_perf_get_qtimer_count API by including HAP_perf.h . Alternatively, you can invoke HAP_perf_get_time_us accessible from C by including HAP_perf.h . This API returns directly the elapsed time in microseconds by deriving the time from the number of ticks and performing a division, which consumes some extra cycles. Note: Measuring time is only meaningful when running on device. Measuring processor cycles For measuring elapsed processor cycles, access the UPCYCLEHI:UPCYCLELO registers using the HAP_perf_get_pcycles API available from C by including HAP_perf.h . Simulator profiling The simulator generates profiling data that help assess where the bottlenecks are in an application. This information is useful to identify which functions will lead the most saving if properly optimized. When profiling an application on the simulator, ensure that the simulator runs in its timing mode. That mode is enabled by using the --timing option. This option results in the simulator counting cycles accurately at the cost of increasing simulation time. The --statsfile and --pmu_statsfile options direct the simulator to gather statistics on instruction and cache utilization. For more information about these options, see the Simulator statistics files and PMU statistics files sections in the Hexagon Simulator User Guide (80-N2040-17). In addition, the simulator may also be used to understand how cycles are spent in specific code regions. In order to do so, you must first run the simulator with the option --packet_analyze . When run in timing mode and with this option, the simulator generates a packet analysis file, which can be postprocessed by the Hexagon profiler to generate a user-friendly HTML file displaying the following information: Total number of cycles executed Total number of stall cycles Highest cycle or stall counts (by function or instruction packet) Commit and stall statistics (by function or instruction packet) PMU event counts (by event type or instruction packet) Annotated disassembly of instruction packets Assembly instruction counts For more information about the Hexagon profiler, see the Hexagon Profiler User Guide (80 N2040-10) . Load address of dynamic libraries The profiler takes a comma-separated list of ELF files, but for shared objects you need to provide the run-time load address of these objects using the [:reloc] option: hexagon-profiler --help --elf=<file>[:reloc][,<file>[:reloc],...] Input one or more Hexagon elf/obj/lib files for disassembly, with optional relocation offsets to match their memory locations when the Hexagon code was run by hexagon-sim The relocation address is displayed by the SDK loader when executing the code on the simulator: %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-sim -mv66g_1024 --packet_analyze calculator.json --simulated_returnval --usefs hexagon_ReleaseG_toolv84_v66 --pmu_statsfile hexagon_ReleaseG_toolv84_v66/pmu_stats.txt --cosim_file hexagon_ReleaseG_toolv84_v66/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_ReleaseG_toolv84_v66\\osam.cfg %HEXAGON_SDK_ROOT%\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn -- %HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim -- calculator_q.so ... try ./calculator_q.so: HIGH:0x5A:81:search.c fs_region_create request with addr=0, size=1000 fs_region_create map region with vaddr=d8041000, paddr=5009d000, size=1000, perm=b, region_handle=1e03fc90 read headers 0x0 -> d8041000 (0x1000 B): HIGH:0x5A:539:map_object.c _rtld_map_object_ex: sigverify skipped for ./calculator_q.so, no function specified!: HIGH:0x5A:598:map_object.c fs_region_create request with addr=0, size=9000 fs_region_create map region with vaddr=d81f7000, paddr=500f7000, size=9000, perm=b, region_handle=1e03fcf0 mapped [d81f7000 - d8200000] (36864 Bytes): HIGH:0x5A:729:map_object.c This indicates that the load address of the calculator_q.so is 0xd81f7000 , which can then be passed to the hexagon-profiler as follows: %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-profiler --packet_analyze --json=calculator.json --elf=%HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim,hexagon_ReleaseG_toolv84_v66\\ship\\calculator_q.so:0xd81f7000 -o calculator.html Hexagon Trace Analyzer The Hexagon Trace Analyzer, or HexTA, is a software trace analysis tool. It processes Hexagon ETM (Embedded Trace Macrocell) traces generated by the software running on the cDSP and derives the flow of each thread of the processor. It is a valuable tool for giving insights into code execution, and allows in-depth analysis and optimization. It can process traces generated on target or by the simulator. Hexagon Trace Analyzer requires ETM traces to be collected from the cDSP. These traces are then parsed for the binaries that are loaded and post-processed to present data in a meaningful manner. The outputs of Hexagon Trace Analyzer include various .csv files that give per function, per instruction and per section statistics. It also generates flame graphs , which provide a graphical view of the execution tree. Hexagon Trace Analyzer can be found in the Hexagon SDK at the following location $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer . Prerequisites Following are the prerequisites for using Hexagon Trace Analyzer: Hexagon Trace Analyzer executable: <HEXAGON_SDK_ROOT>/tools/debug/hexagon-trace-analyzer/hexagon-trace-analyzer Linux Python 2.7 installed with required python packages listed under $HEXAGON_SDK_ROOT/utils/scripts/python_requirements.txt sysMonApp This application is used to collect ETM traces and retrieve Hexagon shared object load addresses. ADB drivers Two visualizing tools, Flame Graph and Catapult , are needed to display the output from the Hexagon Trace Analyzer. These tools should be installed in the hexagon-trace-analyzer directory as follows (using Linux instructions): cd ${HEXAGON_SDK_ROOT}/tools/debug/hexagon-trace-analyzer wget https://github.com/brendangregg/FlameGraph/archive/master.zip && unzip master.zip && rm master.zip && mv FlameGraph-master FlameGraph wget https://github.com/catapult-project/catapult/archive/master.zip && unzip master.zip -x *.json \"*tracing/test_data/*\" && rm master.zip && mv catapult-master catapult Usage Please refer to the profiling example for an example on how to collect a trace on simulator or device and process it with the Hexagon Trace Analyzer tool. The example also describes the various files generated by the tool. sysMon Qualcomm provides a set of tools for monitoring high-level statistics for an application running on target. These tools are: The sysMonApp , which is an Android executable that provides user functionalities such as profiling DSP workload, getting or setting various core and bus clocks, or collecting various software thread information. The sysMon DSP profiler , which is an Android UI application using the sysMonApp profiler service to profile DSP workloads. This application is useful to monitor load distribution and bus activity over time. The sysMon Parser , which is an executable used to postprocess profiling data collected by sysMonApp or the sysMon DSP profiler. The sysMon marker API , which enables the collection of profiling data for a specific code region.","title":"Profiling"},{"location":"tools/profile.html#profiling-resources","text":"The Hexagon SDK provides a number of profiling tools serving different purposes. The table below summarizes these options and their availability on the Hexagon simulator, target devices, or both. The most common profiling tools and techniques are illustrated in the profiling example. Tool Description Simulator Device HAP_perf timer APIs APIs to access elapsed cycles or time Yes Yes Hexagon simulator and profiler Generate instruction and data cache analysis as well as function-level profiling reports Yes No Hexagon Trace Analyzer A trace analyzer utility that generates various analysis reports for cDSP Yes Yes sysMon A set of monitoring tools that collect and display high-level DSP profiling information No Yes","title":"Profiling resources"},{"location":"tools/profile.html#timers","text":"The timer APIs may be used to time a specific section of code by accessing the elapsed time or number of processor cycles.","title":"Timers"},{"location":"tools/profile.html#measuring-time","text":"The most accurate way of measuring time consists of accessing the Hexagon UTIMERHI:UTIMERLO registers. This pair of 32-bit register is a direct measure of the elapsed time reported in ticks. One tick is 1/19.2 MHz seconds. These registers can be accessed from C with the HAP_perf_get_qtimer_count API by including HAP_perf.h . Alternatively, you can invoke HAP_perf_get_time_us accessible from C by including HAP_perf.h . This API returns directly the elapsed time in microseconds by deriving the time from the number of ticks and performing a division, which consumes some extra cycles. Note: Measuring time is only meaningful when running on device.","title":"Measuring time"},{"location":"tools/profile.html#measuring-processor-cycles","text":"For measuring elapsed processor cycles, access the UPCYCLEHI:UPCYCLELO registers using the HAP_perf_get_pcycles API available from C by including HAP_perf.h .","title":"Measuring processor cycles"},{"location":"tools/profile.html#simulator-profiling","text":"The simulator generates profiling data that help assess where the bottlenecks are in an application. This information is useful to identify which functions will lead the most saving if properly optimized. When profiling an application on the simulator, ensure that the simulator runs in its timing mode. That mode is enabled by using the --timing option. This option results in the simulator counting cycles accurately at the cost of increasing simulation time. The --statsfile and --pmu_statsfile options direct the simulator to gather statistics on instruction and cache utilization. For more information about these options, see the Simulator statistics files and PMU statistics files sections in the Hexagon Simulator User Guide (80-N2040-17). In addition, the simulator may also be used to understand how cycles are spent in specific code regions. In order to do so, you must first run the simulator with the option --packet_analyze . When run in timing mode and with this option, the simulator generates a packet analysis file, which can be postprocessed by the Hexagon profiler to generate a user-friendly HTML file displaying the following information: Total number of cycles executed Total number of stall cycles Highest cycle or stall counts (by function or instruction packet) Commit and stall statistics (by function or instruction packet) PMU event counts (by event type or instruction packet) Annotated disassembly of instruction packets Assembly instruction counts For more information about the Hexagon profiler, see the Hexagon Profiler User Guide (80 N2040-10) .","title":"Simulator profiling"},{"location":"tools/profile.html#load-address-of-dynamic-libraries","text":"The profiler takes a comma-separated list of ELF files, but for shared objects you need to provide the run-time load address of these objects using the [:reloc] option: hexagon-profiler --help --elf=<file>[:reloc][,<file>[:reloc],...] Input one or more Hexagon elf/obj/lib files for disassembly, with optional relocation offsets to match their memory locations when the Hexagon code was run by hexagon-sim The relocation address is displayed by the SDK loader when executing the code on the simulator: %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-sim -mv66g_1024 --packet_analyze calculator.json --simulated_returnval --usefs hexagon_ReleaseG_toolv84_v66 --pmu_statsfile hexagon_ReleaseG_toolv84_v66/pmu_stats.txt --cosim_file hexagon_ReleaseG_toolv84_v66/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_ReleaseG_toolv84_v66\\osam.cfg %HEXAGON_SDK_ROOT%\\rtos\\qurt\\computev66\\sdksim_bin\\runelf.pbn -- %HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim -- calculator_q.so ... try ./calculator_q.so: HIGH:0x5A:81:search.c fs_region_create request with addr=0, size=1000 fs_region_create map region with vaddr=d8041000, paddr=5009d000, size=1000, perm=b, region_handle=1e03fc90 read headers 0x0 -> d8041000 (0x1000 B): HIGH:0x5A:539:map_object.c _rtld_map_object_ex: sigverify skipped for ./calculator_q.so, no function specified!: HIGH:0x5A:598:map_object.c fs_region_create request with addr=0, size=9000 fs_region_create map region with vaddr=d81f7000, paddr=500f7000, size=9000, perm=b, region_handle=1e03fcf0 mapped [d81f7000 - d8200000] (36864 Bytes): HIGH:0x5A:729:map_object.c This indicates that the load address of the calculator_q.so is 0xd81f7000 , which can then be passed to the hexagon-profiler as follows: %DEFAULT_HEXAGON_TOOLS_ROOT%\\Tools\\bin\\hexagon-profiler --packet_analyze --json=calculator.json --elf=%HEXAGON_SDK_ROOT%\\libs\\run_main_on_hexagon\\ship\\hexagon_toolv84_v66\\run_main_on_hexagon_sim,hexagon_ReleaseG_toolv84_v66\\ship\\calculator_q.so:0xd81f7000 -o calculator.html","title":"Load address of dynamic libraries"},{"location":"tools/profile.html#hexagon-trace-analyzer","text":"The Hexagon Trace Analyzer, or HexTA, is a software trace analysis tool. It processes Hexagon ETM (Embedded Trace Macrocell) traces generated by the software running on the cDSP and derives the flow of each thread of the processor. It is a valuable tool for giving insights into code execution, and allows in-depth analysis and optimization. It can process traces generated on target or by the simulator. Hexagon Trace Analyzer requires ETM traces to be collected from the cDSP. These traces are then parsed for the binaries that are loaded and post-processed to present data in a meaningful manner. The outputs of Hexagon Trace Analyzer include various .csv files that give per function, per instruction and per section statistics. It also generates flame graphs , which provide a graphical view of the execution tree. Hexagon Trace Analyzer can be found in the Hexagon SDK at the following location $HEXAGON_SDK_ROOT/tools/debug/hexagon-trace-analyzer .","title":"Hexagon Trace Analyzer"},{"location":"tools/profile.html#prerequisites","text":"Following are the prerequisites for using Hexagon Trace Analyzer: Hexagon Trace Analyzer executable: <HEXAGON_SDK_ROOT>/tools/debug/hexagon-trace-analyzer/hexagon-trace-analyzer Linux Python 2.7 installed with required python packages listed under $HEXAGON_SDK_ROOT/utils/scripts/python_requirements.txt sysMonApp This application is used to collect ETM traces and retrieve Hexagon shared object load addresses. ADB drivers Two visualizing tools, Flame Graph and Catapult , are needed to display the output from the Hexagon Trace Analyzer. These tools should be installed in the hexagon-trace-analyzer directory as follows (using Linux instructions): cd ${HEXAGON_SDK_ROOT}/tools/debug/hexagon-trace-analyzer wget https://github.com/brendangregg/FlameGraph/archive/master.zip && unzip master.zip && rm master.zip && mv FlameGraph-master FlameGraph wget https://github.com/catapult-project/catapult/archive/master.zip && unzip master.zip -x *.json \"*tracing/test_data/*\" && rm master.zip && mv catapult-master catapult","title":"Prerequisites"},{"location":"tools/profile.html#usage","text":"Please refer to the profiling example for an example on how to collect a trace on simulator or device and process it with the Hexagon Trace Analyzer tool. The example also describes the various files generated by the tool.","title":"Usage"},{"location":"tools/profile.html#sysmon","text":"Qualcomm provides a set of tools for monitoring high-level statistics for an application running on target. These tools are: The sysMonApp , which is an Android executable that provides user functionalities such as profiling DSP workload, getting or setting various core and bus clocks, or collecting various software thread information. The sysMon DSP profiler , which is an Android UI application using the sysMonApp profiler service to profile DSP workloads. This application is useful to monitor load distribution and bus activity over time. The sysMon Parser , which is an executable used to postprocess profiling data collected by sysMonApp or the sysMon DSP profiler. The sysMon marker API , which enables the collection of profiling data for a specific code region.","title":"sysMon"},{"location":"tools/qhcg.html","text":"Qualcomm HVX Code Generator (QHCG) Qualcomm HVX code generator (QHCG) is a tool generating optimized HVX code for polynomial approximations of arbitrary functions over a fixed number of intervals. Overview QHCG is available under $HEXAGON_SDK_ROOT/tools/qhcg . QHCG is a tool that generates automatically an optimized HVX implementation of any arbitrary function. The QHCG user specifies an arithmetic function to be approximated, the range over which to approximate it, and the desired data type to use to represent the input and output data. The desired accuracy or polynomial order may also be provided and will be the same for all 16 evenly-spaced intervals on which the function is approximated. The default desired accuracy of the reference polynomial approximation, when not specified by the user, is 1 LSB for fixed-point implementations and 2 ULP for floating-point implementations. QHCG finds the reference polynomial approximation of the lowest order that satisfies the desired accuracy. This polynomial approximation is then automatically implemented as optimized HVX code. QHCG also generates reports containing accuracy test results of HVX code, in which the output from the reference implementation in numpy is used as reference. Limitations The QHCG approximation tool doesn't always manage to produce an approximation that is accurate enough. Some functions lend themselves better to polynomial approximations than others. In particular, function with high derivative values may not be approximated as accurately as other functions. For example, since the derivative of sqrt(x) is infinite for x=0, requesting a polynomial approximation of the function starting at the origin will result in a polynomial approximation with poor accuracy in the first segment. The tool guarantees that the reference polynomial approximation meets the desired accuracy. However, the conversion of that polynomial into an HVX implementation degrades the accuracy further. It may therefore be necessary to experiment with the polynomial_order option to see directly the impact of a higher or lower order on the HVX implementation accuracy. In some cases, the approximation may generate an incorrect value at one of the interval boundaries. Please review carefully the output pdf report to make sure the approximation is correct on the entire range of interest. If facing this bug, simply specify a slightly larger input interval than desired. Setup Prerequisites Python 3 and pip3 are required to use QHCG. The minimal supported version of Python is 3.4. Python 3.7 is recommended. Linux setup The setup script automates the steps of setting up a virtual environment venv3 in which all python dependencies needed to run the QHCG tool will be installed. To run the script, simply run: cd setup source setup.sh During the QHCG setup, the Python module for creating a virtual environment should be automatically installed. If this step fails during the setup, try to install the virtual environment venv module manually: With Python 3.4, install venv module with: sudo apt install python3.4-venv With Python version 3.5 or later, install venv module with: sudo apt install python3-venv With Python 3.4, the Tkinter Python module is required and is expected to be installed automatically during QHCG setup. If this step fails during the setup, install it manually: sudo apt install python3-tk Windows setup The setup script automates the steps of setting up a virtual environment venv in which all python dependencies needed to run the QHCG tool will be installed. To run the script, simply run: cd setup setup.cmd If Python 3 is installed through the Windows Store, no additional setup is required. If Python 3 is installed manually, do the following before running the QHCG setup or using QHCG: In the Python 3 installation directory, copy python.exe to python3.exe Make sure this directory is in the PATH and that no Python 2 directory is in the PATH Running the tool Usage If the user wants to generate a report that measures the accuracy of the generated HVX implementation, he or she should first run the setup_sdk_env script . The user can start QHCG by using run_qhcg.cmd on Windows and run_qhcg.sh on Linux. Both scripts have the same arguments. Some of the arguments are required, while other are optional. Required arguments: --input_range <input_range_start> <input_range_end> Float input range on which the function needs to be approximated. <input_range_start> Start value of function input range <input_range_end> End value of function input range --function <input_function> \"<input_function>\" String description of the function to approximate. --input_type <in_type> HVX input vector type to use to generate an approximation. <in_type> Supported values: int16, int32, float16, float32 The HVX output vector will be of the same type as HVX input vector. Float HVX implementations will only run on Lahaina. Optional arguments: -h, --help Show help message and exit --tolerance <desired_accuracy> Desired accuracy in LSB for fixed-point approximations and ULP for the floating-point python polynomial approximation. <desired_accuracy> Float value describing the desired accuracy. --toolchain <hexagon_toolchain_path> Set custom toolchain root path for Hexagon Tools. By default, QHCG will search for the Hexagon tools included in the SDK once the setup_sdk_env script has been run. If no toolchain is found, an HVX implementation will still be generated but without an accuracy report. <hexagon_toolchain_path> Hexagon tools path --output_path <output_path> Set custom output directory path. By default, `output` is used. Each new execution overwrites any existing contents. <output_path> Output folder. Both relative and absolute paths are supported. --func_name <func_name> Set custom function name. By default, `qhcg_approximation` will be used. <func_name> Desired custom function name. --optimize_by <mode> Supported values: `accuracy` or `speed`. Default value is `accuracy`. The `speed` option allows to produce a faster HVX implementation but with some degradation on the accuracy. This option is currently supported for input types int16 and float16. For int32 and float32 input types, the speed option is ignored and defaults to accuracy. --polynomial_order <order> Positive integer representing the order of the polynomial approximation to use. By default, QHCG searches for the lowest polynomial order that meets the accuracy requirements of the reference floating-point python polynomial approximation. --exec_bench Run the HVX approximation on the Hexagon simulator to estimate the cycle count performance element of the generated HVX approximation. This option increases the processing time. --bundle Execute QHCG in bundle mode. The destination folder will not be deleted initially and new HVX implementations will be added to the same destination folder. --summary Generate a shorter accuracy report that does not generate an error graph for each approximation interval. Example The following command will generate an HVX implementation comprised of 16-bit input and output fixed-point elements. This implementation will approximate the inverse function over the range [-2;-0.5] within 1 LSB. ./run_qhcg.sh --input_range -2 -0.5 --function \"1/x\" --input_type int16 Note: If this is the first time QHCG is run, the setup script introduced in the previous section will be invoked automatically. Tool output The structure of the output directory that QHCG generates is as follows. Note: The example is given here for a 16-bit fixed-point implementation. Names will vary slightly for other requested data types. . \u251c\u2500\u2500 qhcg_approximation_coefficients.txt \u251c\u2500\u2500 HVX_code \u2502 \u251c\u2500\u2500 build \u2502 \u2502 \u251c\u2500\u2500 bench_qhcg_approximation.elf \u2502 \u2502 \u251c\u2500\u2500 bench_qhcg_approximation.o \u2502 \u2502 \u251c\u2500\u2500 qhcg_approximation.o \u2502 \u2502 \u251c\u2500\u2500 test_qhcg_approximation.elf \u2502 \u2502 \u2514\u2500\u2500 test_qhcg_approximation.o \u2502 \u251c\u2500\u2500 inc \u2502 \u2502 \u2514\u2500\u2500 qhcg_approximation.h \u2502 \u251c\u2500\u2500 Makefile \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u251c\u2500\u2500 disassembly \u2502 \u2502 \u2502 \u2514\u2500\u2500 qhcg_approximation.S.dump \u2502 \u2502 \u2514\u2500\u2500 c \u2502 \u2502 \u2514\u2500\u2500 qhcg_approximation.c \u2502 \u2514\u2500\u2500 test \u2502 \u251c\u2500\u2500 bench_qhcg_approximation.c \u2502 \u251c\u2500\u2500 test_data \u2502 \u2502 \u251c\u2500\u2500 inputs_16f.txt \u2502 \u2502 \u251c\u2500\u2500 outputs_16f_hvx.txt \u2502 \u2502 \u2514\u2500\u2500 outputs_16f_np.txt \u2502 \u2514\u2500\u2500 test_qhcg_approximation.c \u251c\u2500\u2500 qhcg_args.tapproximation_args.txt \u251c\u2500\u2500 qhcg_consolapproximation_console.log \u251c\u2500\u2500 README \u251c\u2500\u2500 report \u2502 \u251c\u2500\u2500 qhcg_approximation_acc_report.txt \u2502 \u251c\u2500\u2500 qhcg_approximation_detailed_report.pdf \u2502 \u2514\u2500\u2500 qhcg_approximation_summary_report.pdf \u2514\u2500\u2500 versions.log The HVX_code directory contains source C code with intrinsics, header files, and disassembly of the generated HVX code as well as accuracy and performance tests. The report directory contains results of accuracy and performance tests. Performance tests indicating how many cycles it takes to process one element on average are only generated when the exec_bench option is used. The report also provides various graphical reports, including: a graphical representation of the reference function and its approximations on the requested interval a graphical comparison between the reference function and the generated Python polynomial approximation in IEEE floating-point a graphical comparison between the reference function and the generated HVX implementation (with 16-bit fixed-point data in the present example) Note: As shown in the graph above, the error on the HVX implementation is either 6.1 10^-5 or very small. This means that the error generated by the polynomial approximation is much smaller than the error generated with the HVX implementation, which works on fixed-point data in this example. If the error of the HVX implementation is smaller than required by the end user, it is then worth reducing the polynomial order of the approximation to find an implementation for which the error caused by the floating-point polynomial approximation is in the same range as the error caused by the data conversion performed in the HVX implementation. (If instead the error of the HVX implementation is larger than required by the end user, the user will need to use a different data type, such as int32 or qf32.) For example, reducing to a polynomial order of degree 2 using the option --polynomial_order 2 , we increase the maximum error of the polynomial approximation in floating-point format to 3 10^-4: In that case, the additional error introduced by manipulating fixed-point data in HVX has a only a small impact on the error caused by the polynomial approximation: max error of 3.6 10^-4 instead of 3 10^-4: The HVX output directory also contains the polynomial coefficients, the QHCG arguments that were used, and the console log generated when running the tool. The console log output provides various information including: the accuracy achieved by the polynomial approximation in floating-point using python Python polynomial approximation (NumPy polynomials package): Accuracy: Max error [float]: 2.6212205517239795e-08 RMS error [float]: 4.358666622225132e-09 Mean error [float]: 1.4734534072136096e-09 the selected polynomial order, Chosen polynomial order: 5 the accuracy of the HVX polynomial approximation implementation: HVX polynomial approximation: Accuracy: Max error [float]: 6.103515625e-05 RMS error [float]: 3.0517498897803145e-05 Mean error [float]: 3.0448039372762043e-05 Max error [ULP]: 0.125 RMS error [ULP]: 0.05625569552185018 Mean error [ULP]: 0.05219268798828125 A detailed explanation of each of the generated file can be found in the README file generated by QHCG in the output directory.","title":"QHCG"},{"location":"tools/qhcg.html#qualcomm-hvx-code-generator-qhcg","text":"Qualcomm HVX code generator (QHCG) is a tool generating optimized HVX code for polynomial approximations of arbitrary functions over a fixed number of intervals.","title":"Qualcomm HVX Code Generator (QHCG)"},{"location":"tools/qhcg.html#overview","text":"QHCG is available under $HEXAGON_SDK_ROOT/tools/qhcg . QHCG is a tool that generates automatically an optimized HVX implementation of any arbitrary function. The QHCG user specifies an arithmetic function to be approximated, the range over which to approximate it, and the desired data type to use to represent the input and output data. The desired accuracy or polynomial order may also be provided and will be the same for all 16 evenly-spaced intervals on which the function is approximated. The default desired accuracy of the reference polynomial approximation, when not specified by the user, is 1 LSB for fixed-point implementations and 2 ULP for floating-point implementations. QHCG finds the reference polynomial approximation of the lowest order that satisfies the desired accuracy. This polynomial approximation is then automatically implemented as optimized HVX code. QHCG also generates reports containing accuracy test results of HVX code, in which the output from the reference implementation in numpy is used as reference.","title":"Overview"},{"location":"tools/qhcg.html#limitations","text":"The QHCG approximation tool doesn't always manage to produce an approximation that is accurate enough. Some functions lend themselves better to polynomial approximations than others. In particular, function with high derivative values may not be approximated as accurately as other functions. For example, since the derivative of sqrt(x) is infinite for x=0, requesting a polynomial approximation of the function starting at the origin will result in a polynomial approximation with poor accuracy in the first segment. The tool guarantees that the reference polynomial approximation meets the desired accuracy. However, the conversion of that polynomial into an HVX implementation degrades the accuracy further. It may therefore be necessary to experiment with the polynomial_order option to see directly the impact of a higher or lower order on the HVX implementation accuracy. In some cases, the approximation may generate an incorrect value at one of the interval boundaries. Please review carefully the output pdf report to make sure the approximation is correct on the entire range of interest. If facing this bug, simply specify a slightly larger input interval than desired.","title":"Limitations"},{"location":"tools/qhcg.html#setup","text":"","title":"Setup"},{"location":"tools/qhcg.html#prerequisites","text":"Python 3 and pip3 are required to use QHCG. The minimal supported version of Python is 3.4. Python 3.7 is recommended.","title":"Prerequisites"},{"location":"tools/qhcg.html#linux-setup","text":"The setup script automates the steps of setting up a virtual environment venv3 in which all python dependencies needed to run the QHCG tool will be installed. To run the script, simply run: cd setup source setup.sh During the QHCG setup, the Python module for creating a virtual environment should be automatically installed. If this step fails during the setup, try to install the virtual environment venv module manually: With Python 3.4, install venv module with: sudo apt install python3.4-venv With Python version 3.5 or later, install venv module with: sudo apt install python3-venv With Python 3.4, the Tkinter Python module is required and is expected to be installed automatically during QHCG setup. If this step fails during the setup, install it manually: sudo apt install python3-tk","title":"Linux setup"},{"location":"tools/qhcg.html#windows-setup","text":"The setup script automates the steps of setting up a virtual environment venv in which all python dependencies needed to run the QHCG tool will be installed. To run the script, simply run: cd setup setup.cmd If Python 3 is installed through the Windows Store, no additional setup is required. If Python 3 is installed manually, do the following before running the QHCG setup or using QHCG: In the Python 3 installation directory, copy python.exe to python3.exe Make sure this directory is in the PATH and that no Python 2 directory is in the PATH","title":"Windows setup"},{"location":"tools/qhcg.html#running-the-tool","text":"","title":"Running the tool"},{"location":"tools/qhcg.html#usage","text":"If the user wants to generate a report that measures the accuracy of the generated HVX implementation, he or she should first run the setup_sdk_env script . The user can start QHCG by using run_qhcg.cmd on Windows and run_qhcg.sh on Linux. Both scripts have the same arguments. Some of the arguments are required, while other are optional. Required arguments: --input_range <input_range_start> <input_range_end> Float input range on which the function needs to be approximated. <input_range_start> Start value of function input range <input_range_end> End value of function input range --function <input_function> \"<input_function>\" String description of the function to approximate. --input_type <in_type> HVX input vector type to use to generate an approximation. <in_type> Supported values: int16, int32, float16, float32 The HVX output vector will be of the same type as HVX input vector. Float HVX implementations will only run on Lahaina. Optional arguments: -h, --help Show help message and exit --tolerance <desired_accuracy> Desired accuracy in LSB for fixed-point approximations and ULP for the floating-point python polynomial approximation. <desired_accuracy> Float value describing the desired accuracy. --toolchain <hexagon_toolchain_path> Set custom toolchain root path for Hexagon Tools. By default, QHCG will search for the Hexagon tools included in the SDK once the setup_sdk_env script has been run. If no toolchain is found, an HVX implementation will still be generated but without an accuracy report. <hexagon_toolchain_path> Hexagon tools path --output_path <output_path> Set custom output directory path. By default, `output` is used. Each new execution overwrites any existing contents. <output_path> Output folder. Both relative and absolute paths are supported. --func_name <func_name> Set custom function name. By default, `qhcg_approximation` will be used. <func_name> Desired custom function name. --optimize_by <mode> Supported values: `accuracy` or `speed`. Default value is `accuracy`. The `speed` option allows to produce a faster HVX implementation but with some degradation on the accuracy. This option is currently supported for input types int16 and float16. For int32 and float32 input types, the speed option is ignored and defaults to accuracy. --polynomial_order <order> Positive integer representing the order of the polynomial approximation to use. By default, QHCG searches for the lowest polynomial order that meets the accuracy requirements of the reference floating-point python polynomial approximation. --exec_bench Run the HVX approximation on the Hexagon simulator to estimate the cycle count performance element of the generated HVX approximation. This option increases the processing time. --bundle Execute QHCG in bundle mode. The destination folder will not be deleted initially and new HVX implementations will be added to the same destination folder. --summary Generate a shorter accuracy report that does not generate an error graph for each approximation interval.","title":"Usage"},{"location":"tools/qhcg.html#example","text":"The following command will generate an HVX implementation comprised of 16-bit input and output fixed-point elements. This implementation will approximate the inverse function over the range [-2;-0.5] within 1 LSB. ./run_qhcg.sh --input_range -2 -0.5 --function \"1/x\" --input_type int16 Note: If this is the first time QHCG is run, the setup script introduced in the previous section will be invoked automatically.","title":"Example"},{"location":"tools/qhcg.html#tool-output","text":"The structure of the output directory that QHCG generates is as follows. Note: The example is given here for a 16-bit fixed-point implementation. Names will vary slightly for other requested data types. . \u251c\u2500\u2500 qhcg_approximation_coefficients.txt \u251c\u2500\u2500 HVX_code \u2502 \u251c\u2500\u2500 build \u2502 \u2502 \u251c\u2500\u2500 bench_qhcg_approximation.elf \u2502 \u2502 \u251c\u2500\u2500 bench_qhcg_approximation.o \u2502 \u2502 \u251c\u2500\u2500 qhcg_approximation.o \u2502 \u2502 \u251c\u2500\u2500 test_qhcg_approximation.elf \u2502 \u2502 \u2514\u2500\u2500 test_qhcg_approximation.o \u2502 \u251c\u2500\u2500 inc \u2502 \u2502 \u2514\u2500\u2500 qhcg_approximation.h \u2502 \u251c\u2500\u2500 Makefile \u2502 \u251c\u2500\u2500 src \u2502 \u2502 \u251c\u2500\u2500 disassembly \u2502 \u2502 \u2502 \u2514\u2500\u2500 qhcg_approximation.S.dump \u2502 \u2502 \u2514\u2500\u2500 c \u2502 \u2502 \u2514\u2500\u2500 qhcg_approximation.c \u2502 \u2514\u2500\u2500 test \u2502 \u251c\u2500\u2500 bench_qhcg_approximation.c \u2502 \u251c\u2500\u2500 test_data \u2502 \u2502 \u251c\u2500\u2500 inputs_16f.txt \u2502 \u2502 \u251c\u2500\u2500 outputs_16f_hvx.txt \u2502 \u2502 \u2514\u2500\u2500 outputs_16f_np.txt \u2502 \u2514\u2500\u2500 test_qhcg_approximation.c \u251c\u2500\u2500 qhcg_args.tapproximation_args.txt \u251c\u2500\u2500 qhcg_consolapproximation_console.log \u251c\u2500\u2500 README \u251c\u2500\u2500 report \u2502 \u251c\u2500\u2500 qhcg_approximation_acc_report.txt \u2502 \u251c\u2500\u2500 qhcg_approximation_detailed_report.pdf \u2502 \u2514\u2500\u2500 qhcg_approximation_summary_report.pdf \u2514\u2500\u2500 versions.log The HVX_code directory contains source C code with intrinsics, header files, and disassembly of the generated HVX code as well as accuracy and performance tests. The report directory contains results of accuracy and performance tests. Performance tests indicating how many cycles it takes to process one element on average are only generated when the exec_bench option is used. The report also provides various graphical reports, including: a graphical representation of the reference function and its approximations on the requested interval a graphical comparison between the reference function and the generated Python polynomial approximation in IEEE floating-point a graphical comparison between the reference function and the generated HVX implementation (with 16-bit fixed-point data in the present example) Note: As shown in the graph above, the error on the HVX implementation is either 6.1 10^-5 or very small. This means that the error generated by the polynomial approximation is much smaller than the error generated with the HVX implementation, which works on fixed-point data in this example. If the error of the HVX implementation is smaller than required by the end user, it is then worth reducing the polynomial order of the approximation to find an implementation for which the error caused by the floating-point polynomial approximation is in the same range as the error caused by the data conversion performed in the HVX implementation. (If instead the error of the HVX implementation is larger than required by the end user, the user will need to use a different data type, such as int32 or qf32.) For example, reducing to a polynomial order of degree 2 using the option --polynomial_order 2 , we increase the maximum error of the polynomial approximation in floating-point format to 3 10^-4: In that case, the additional error introduced by manipulating fixed-point data in HVX has a only a small impact on the error caused by the polynomial approximation: max error of 3.6 10^-4 instead of 3 10^-4: The HVX output directory also contains the polynomial coefficients, the QHCG arguments that were used, and the console log generated when running the tool. The console log output provides various information including: the accuracy achieved by the polynomial approximation in floating-point using python Python polynomial approximation (NumPy polynomials package): Accuracy: Max error [float]: 2.6212205517239795e-08 RMS error [float]: 4.358666622225132e-09 Mean error [float]: 1.4734534072136096e-09 the selected polynomial order, Chosen polynomial order: 5 the accuracy of the HVX polynomial approximation implementation: HVX polynomial approximation: Accuracy: Max error [float]: 6.103515625e-05 RMS error [float]: 3.0517498897803145e-05 Mean error [float]: 3.0448039372762043e-05 Max error [ULP]: 0.125 RMS error [ULP]: 0.05625569552185018 Mean error [ULP]: 0.05219268798828125 A detailed explanation of each of the generated file can be found in the README file generated by QHCG in the output directory.","title":"Tool output"},{"location":"tools/run.html","text":"Run applications on simulator and target This page describes how you can run your application on the Hexagon simulator and on target by using the command line interface (CLI) and Hexagon integrated development environment (IDE). This page also describes the [run_main_on_hexagon](#run_main_on_hexagon) utility to run dynamic tests on the Hexagon simulator as well as offload a simple computational algorithm to the DSP. Run applications on simulator Framework for unit testing The Hexagon SDK provides a framework that enables building dynamic tests and standalone tests and executing them on the Hexagon simulator as part of the build process. hexagon-lldb also allows to debug the unit test on the simulator. Each of the provided Hexagon SDK example contains its own unit test. A unit test is specified as follows in hexagon.min : BUILD_QEXES += mylib_test # Specify the name of the library (mylib_test.so) or executable (mylib_test) unit test mylib_test_C_SRCS += src_app/mylib_test # List source file(s) to build mylib_test.so. Sources must include a main(), which is the test entry point mylib_test_DLLS+= rpcmem # Specify test dependencies In this example: As explained in the description of the build targets , the use of QEXES instead of EXES indicates that the test binary should be run automatically on the Hexagon simulator after being built The unit test is named mylib_test or mylib_test.so depending on whether we are building a dynamic test or standalone test The test source file only includes one file, src_app/mylib_test.c The test links the rpcmem support library and the test module QuRT-based dynamic tests By default, all unit tests defined using the unit testing framework described above are built and run as dynamic tests, which rely on a utility called [run_main_on_hexagon](#run_main_on_hexagon) to be executed. This approach is the recommended approach to run simulator tests as it provides a simulation environment that closely matches the target environment in which the dynamic library is used. Simulator tests relying on run_main_on_hexagon are referred as dynamic tests because they involve shared objects instead of executables. For example, the calculator C++ example generates a dynamic test calculator_q.so , which is to be passed as an argument to run_main_on_hexagon for execution on the Hexagon simulator hexagon-sim . run_main_on_hexagon calls dlopen() of the simulator test, obtains the address of main() using dlsym() on a shared object handle, and calls the main() function defined in the compiled shared library with argc and argv if provided. All QuRT and test libraries are built into run_main_on_hexagon and QuRT header files are included by default by the build system for simulator tests using run_main_on_hexagon . This means that you should not link rtld, test_util and atomic libraries to your dynamic tests. Standalone tests Standalone tests are simulator test executables that do not use QuRT. These tests are executed directly by the Hexagon simulator and do not run on target. Simulator tests with a QuRT dependency should be using run_main_on_hexagon . The calculator example is an example that generates a standalone test calculator_q . This binary is executed by being passed to the Hexagon simulator . A standalone test is defined the same way as a dynamic test with the exception that hexagon.min must set the NO_QURT_INC variable to 1 : This results in skipping the inclusion of QuRT libraries and headers when building the test: NO_QURT_INC = 1 # Do not include QuRT dependencies and generate a standalone test instead of a dynamic test Since standalone tests do not run with run_main_on_hexagon , any test library dependency such as rtld , test_util or atomic needs to be listed in <unit_test_name>_DLLS . For example, to modify the calculator example use a dynamic test instead of a standalone test, simply remove NO_QURT_INC = 1 line in hexagon.min and remove rtld test_util atomic from the list of dependencies specified in calculator_q_LIBS . Note: It is not possible to test a dynamic library with a standalone test as a dynamic library can not be linked to a static standalone executable. Note: You can handle C/C++ library dependencies in your standalone code as follows: #include <dlfcn.h> ... DL_vtbl vtbl; char *builtin[] = { (char*)\"libc.so\", (char*)\"libgcc.so\"}; // Note: you may specify additional dependencies such as (char*)\"libgcc.so\" if need be memset(&vtbl, 0, sizeof(DL_vtbl)); vtbl.size = sizeof(DL_vtbl); vtbl.msg = HAP_debug_v2; (void)dlinitex(2, builtin, &vtbl); The code sample above tells the loader that libc and libgcc.so symbols are built-in and therefore, that the shared objects libc and libgcc.so should not be loaded even if specified as dependent libraries in the Makefile . This approach is not needed for dynamic tests and for libraries running on target as run_main_on_hexagon and fastrpc_shell , the FastRPC user PD ELF on target, execute similar instructions. Test library support The Hexagon SDK provides unit test support libraries. For detailed information see the header and source files contained in the $HEXAGON_SDK_ROOT/utils directory. test_util This library provides support for the unit test environment. It includes simulator versions of features otherwise present on the target DSP image, such as memory allocation, debug message support, VTCM manager, L2 cache locking manager, and performance measurement. Test execution The build framework builds and executes unit tests automatically. In the case of standalone tests , the unit test is built into a Hexagon simulator executable, which is then run by the Hexagon simulator. In the case of a dynamic tests , the simulator executes run_main_on_hexagon , which calls the main() defined in the dynamic library to execute the unit test. In the event of a test failure, the build will fail and display the output of the failing test. To enable more detailed information for the make process, including the output of a successful test run, set VERBOSE=1 on the make command. For example: make hexagon DSP_ARCH=v65 BUILD=Debug VERBOSE=1 NOTE: To learn how to run on simulator using the Hexagon IDE, see IDE . Run applications on target Connect to the device To install USB drivers on Windows/Linux and establish a connection between the device and host PC, see USB drivers setup and ADB driver setup . Compile the binaries Before you push the required executables and shared objects to the device, make sure you have compiled the HLOS and DSP binaries. For instructions on compiling, see build . File locations of binaries Once the binaries are compiled, push them to the appropriate location on the device. On Linux Android (LA), the recommended location on target for the application executable is /vendor/bin , and /vendor/lib64 for their libraries. On Linux Environment (LE), the recommended location for the application executable is /usr/bin , and /usr/lib64 for their libraries. For recommended locations on where to push DSP binaries, see the discussion on the remote file systen . Run the examples To run an example on target, see the calculator example. To run an example on target using the IDE, see the IDE section . run_main_on_hexagon run_main_on_hexagon is a utility to run dynamic tests on the simulator as well as offload an algorithm compiled as a shared object to the DSP. On both the Hexagon simulator and on the device, run_main_on_hexagon calls dlopen() on the dynamic shared object, obtains the address of main() using dlsym() on the shared object handle, and calls the main() function defined in the compiled shared object with argc and argv if provided. The main() is called in a new thread spawned with a stack whose size may be configured with the optional argument stack_size . run_main_on_hexagon allows to invoke the main() of a DSP shared library without the need for providing an IDL file and the source for a CPU executable. This approach is illustrated in the QHL example and the QHL HVX example . As explained above , run_main_on_hexagon is also used for dynamic simulator tests where it provides a simulation environment that closely matches the target environment in which the dynamic library will be used. This is achieved with the help of QuRT-provided root program called runelf.pbn . runelf.pbn is a boot loader that loads the dynamic executable on top of hexagon-sim . It has a main thread that loads the user program and an exception thread that waits to process any exceptions that are then routed back to root. Exceptions in the user program are displayed in the console and can be debugged further using this approach . Using run_main_on_hexagon also allows to abstract the linking of the simulator test with QuRT libraries and other common libraries. NOTE: * For more details on how run_main_on_hexagon is implemented, see the $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/src directory. On-target usage Usage: adb shell /vendor/bin/run_main_on_hexagon <domain> <path> [stack_size=<size>] [args] OPTIONS: domain: Specify the DSP domain on which to offload the program, expressed as numeric domain id supported domains: 0 (ADSP), 1 (MDSP), 2 (SDSP), 3 (CDSP) path: File path of the shared object that includes the definition of `main()` stack_size= : Optional argument to configure the stack size of new `ribbon` thread that runs main() default stack size is 256kb and stack size less than 256kb is not supported args: Optional string arguments to pass to main() Note: The stack size of the thread where the main() is called is configurable to allow users to offload complex computations to the DSP conveniently. Here is a command-line example invoking run_main_on_hexagon on target. adb shell /vendor/bin/run_main_on_hexagon 0 test_main.so stack_size=0x400000 1 foo2 2 bar With this command line, run_main_on_hexagon calls main() defined in test_main.so in a thread of stack size 0x400000 with arguments 1 foo2 2 bar , on domain 0, i.e. on the ADSP . Simulator usage run_main_on_hexagon can also be invoked from the Hexagon simulator. Here is a command-line example that calls the main function in test_main.so on the hexagon simulator (assuming the desired Hexagon architecture version is v65): hexagon-sim -mv65 --simulated_returnval --usefs hexagon_ReleaseG_toolv84_v65 --pmu_statsfile hexagon_ReleaseG_toolv84_v65/pmu_stats.txt --cosim_file hexagon_ReleaseG_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_ReleaseG_toolv84_v65/osam.cfg $HEXAGON_SDK_ROOT/rtos/qurt/computev65/sdksim_bin/runelf.pbn -- hexagon_ReleaseG_toolv84_v65/run_main_on_hexagon_sim stack_size=0x400000 -- test_main.so 1 foo 2 bar With this command line, run_main_on_hexagon_sim calls main() defined in test_main.so in a thread of stack size 0x400000 with arguments 1 foo2 2 bar on the simulator. Note: In simulator tests, the argument stack_size is to be passed before -- . Any argument passed after -- is considered as an argument to the main implemented in the shared object.","title":"Running"},{"location":"tools/run.html#run-applications-on-simulator-and-target","text":"This page describes how you can run your application on the Hexagon simulator and on target by using the command line interface (CLI) and Hexagon integrated development environment (IDE). This page also describes the [run_main_on_hexagon](#run_main_on_hexagon) utility to run dynamic tests on the Hexagon simulator as well as offload a simple computational algorithm to the DSP.","title":"Run applications on simulator and target"},{"location":"tools/run.html#run-applications-on-simulator","text":"","title":"Run applications on simulator"},{"location":"tools/run.html#framework-for-unit-testing","text":"The Hexagon SDK provides a framework that enables building dynamic tests and standalone tests and executing them on the Hexagon simulator as part of the build process. hexagon-lldb also allows to debug the unit test on the simulator. Each of the provided Hexagon SDK example contains its own unit test. A unit test is specified as follows in hexagon.min : BUILD_QEXES += mylib_test # Specify the name of the library (mylib_test.so) or executable (mylib_test) unit test mylib_test_C_SRCS += src_app/mylib_test # List source file(s) to build mylib_test.so. Sources must include a main(), which is the test entry point mylib_test_DLLS+= rpcmem # Specify test dependencies In this example: As explained in the description of the build targets , the use of QEXES instead of EXES indicates that the test binary should be run automatically on the Hexagon simulator after being built The unit test is named mylib_test or mylib_test.so depending on whether we are building a dynamic test or standalone test The test source file only includes one file, src_app/mylib_test.c The test links the rpcmem support library and the test module","title":"Framework for unit testing"},{"location":"tools/run.html#qurt-based-dynamic-tests","text":"By default, all unit tests defined using the unit testing framework described above are built and run as dynamic tests, which rely on a utility called [run_main_on_hexagon](#run_main_on_hexagon) to be executed. This approach is the recommended approach to run simulator tests as it provides a simulation environment that closely matches the target environment in which the dynamic library is used. Simulator tests relying on run_main_on_hexagon are referred as dynamic tests because they involve shared objects instead of executables. For example, the calculator C++ example generates a dynamic test calculator_q.so , which is to be passed as an argument to run_main_on_hexagon for execution on the Hexagon simulator hexagon-sim . run_main_on_hexagon calls dlopen() of the simulator test, obtains the address of main() using dlsym() on a shared object handle, and calls the main() function defined in the compiled shared library with argc and argv if provided. All QuRT and test libraries are built into run_main_on_hexagon and QuRT header files are included by default by the build system for simulator tests using run_main_on_hexagon . This means that you should not link rtld, test_util and atomic libraries to your dynamic tests.","title":"QuRT-based dynamic tests"},{"location":"tools/run.html#standalone-tests","text":"Standalone tests are simulator test executables that do not use QuRT. These tests are executed directly by the Hexagon simulator and do not run on target. Simulator tests with a QuRT dependency should be using run_main_on_hexagon . The calculator example is an example that generates a standalone test calculator_q . This binary is executed by being passed to the Hexagon simulator . A standalone test is defined the same way as a dynamic test with the exception that hexagon.min must set the NO_QURT_INC variable to 1 : This results in skipping the inclusion of QuRT libraries and headers when building the test: NO_QURT_INC = 1 # Do not include QuRT dependencies and generate a standalone test instead of a dynamic test Since standalone tests do not run with run_main_on_hexagon , any test library dependency such as rtld , test_util or atomic needs to be listed in <unit_test_name>_DLLS . For example, to modify the calculator example use a dynamic test instead of a standalone test, simply remove NO_QURT_INC = 1 line in hexagon.min and remove rtld test_util atomic from the list of dependencies specified in calculator_q_LIBS . Note: It is not possible to test a dynamic library with a standalone test as a dynamic library can not be linked to a static standalone executable. Note: You can handle C/C++ library dependencies in your standalone code as follows: #include <dlfcn.h> ... DL_vtbl vtbl; char *builtin[] = { (char*)\"libc.so\", (char*)\"libgcc.so\"}; // Note: you may specify additional dependencies such as (char*)\"libgcc.so\" if need be memset(&vtbl, 0, sizeof(DL_vtbl)); vtbl.size = sizeof(DL_vtbl); vtbl.msg = HAP_debug_v2; (void)dlinitex(2, builtin, &vtbl); The code sample above tells the loader that libc and libgcc.so symbols are built-in and therefore, that the shared objects libc and libgcc.so should not be loaded even if specified as dependent libraries in the Makefile . This approach is not needed for dynamic tests and for libraries running on target as run_main_on_hexagon and fastrpc_shell , the FastRPC user PD ELF on target, execute similar instructions.","title":"Standalone tests"},{"location":"tools/run.html#test-library-support","text":"The Hexagon SDK provides unit test support libraries. For detailed information see the header and source files contained in the $HEXAGON_SDK_ROOT/utils directory.","title":"Test library support"},{"location":"tools/run.html#test_util","text":"This library provides support for the unit test environment. It includes simulator versions of features otherwise present on the target DSP image, such as memory allocation, debug message support, VTCM manager, L2 cache locking manager, and performance measurement.","title":"test_util"},{"location":"tools/run.html#test-execution","text":"The build framework builds and executes unit tests automatically. In the case of standalone tests , the unit test is built into a Hexagon simulator executable, which is then run by the Hexagon simulator. In the case of a dynamic tests , the simulator executes run_main_on_hexagon , which calls the main() defined in the dynamic library to execute the unit test. In the event of a test failure, the build will fail and display the output of the failing test. To enable more detailed information for the make process, including the output of a successful test run, set VERBOSE=1 on the make command. For example: make hexagon DSP_ARCH=v65 BUILD=Debug VERBOSE=1 NOTE: To learn how to run on simulator using the Hexagon IDE, see IDE .","title":"Test execution"},{"location":"tools/run.html#run-applications-on-target","text":"","title":"Run applications on target"},{"location":"tools/run.html#connect-to-the-device","text":"To install USB drivers on Windows/Linux and establish a connection between the device and host PC, see USB drivers setup and ADB driver setup .","title":"Connect to the device"},{"location":"tools/run.html#compile-the-binaries","text":"Before you push the required executables and shared objects to the device, make sure you have compiled the HLOS and DSP binaries. For instructions on compiling, see build .","title":"Compile the binaries"},{"location":"tools/run.html#file-locations-of-binaries","text":"Once the binaries are compiled, push them to the appropriate location on the device. On Linux Android (LA), the recommended location on target for the application executable is /vendor/bin , and /vendor/lib64 for their libraries. On Linux Environment (LE), the recommended location for the application executable is /usr/bin , and /usr/lib64 for their libraries. For recommended locations on where to push DSP binaries, see the discussion on the remote file systen .","title":"File locations of binaries"},{"location":"tools/run.html#run-the-examples","text":"To run an example on target, see the calculator example. To run an example on target using the IDE, see the IDE section .","title":"Run the examples"},{"location":"tools/run.html#run_main_on_hexagon","text":"run_main_on_hexagon is a utility to run dynamic tests on the simulator as well as offload an algorithm compiled as a shared object to the DSP. On both the Hexagon simulator and on the device, run_main_on_hexagon calls dlopen() on the dynamic shared object, obtains the address of main() using dlsym() on the shared object handle, and calls the main() function defined in the compiled shared object with argc and argv if provided. The main() is called in a new thread spawned with a stack whose size may be configured with the optional argument stack_size . run_main_on_hexagon allows to invoke the main() of a DSP shared library without the need for providing an IDL file and the source for a CPU executable. This approach is illustrated in the QHL example and the QHL HVX example . As explained above , run_main_on_hexagon is also used for dynamic simulator tests where it provides a simulation environment that closely matches the target environment in which the dynamic library will be used. This is achieved with the help of QuRT-provided root program called runelf.pbn . runelf.pbn is a boot loader that loads the dynamic executable on top of hexagon-sim . It has a main thread that loads the user program and an exception thread that waits to process any exceptions that are then routed back to root. Exceptions in the user program are displayed in the console and can be debugged further using this approach . Using run_main_on_hexagon also allows to abstract the linking of the simulator test with QuRT libraries and other common libraries. NOTE: * For more details on how run_main_on_hexagon is implemented, see the $HEXAGON_SDK_ROOT/libs/run_main_on_hexagon/src directory.","title":"run_main_on_hexagon"},{"location":"tools/run.html#on-target-usage","text":"Usage: adb shell /vendor/bin/run_main_on_hexagon <domain> <path> [stack_size=<size>] [args] OPTIONS: domain: Specify the DSP domain on which to offload the program, expressed as numeric domain id supported domains: 0 (ADSP), 1 (MDSP), 2 (SDSP), 3 (CDSP) path: File path of the shared object that includes the definition of `main()` stack_size= : Optional argument to configure the stack size of new `ribbon` thread that runs main() default stack size is 256kb and stack size less than 256kb is not supported args: Optional string arguments to pass to main() Note: The stack size of the thread where the main() is called is configurable to allow users to offload complex computations to the DSP conveniently. Here is a command-line example invoking run_main_on_hexagon on target. adb shell /vendor/bin/run_main_on_hexagon 0 test_main.so stack_size=0x400000 1 foo2 2 bar With this command line, run_main_on_hexagon calls main() defined in test_main.so in a thread of stack size 0x400000 with arguments 1 foo2 2 bar , on domain 0, i.e. on the ADSP .","title":"On-target usage"},{"location":"tools/run.html#simulator-usage","text":"run_main_on_hexagon can also be invoked from the Hexagon simulator. Here is a command-line example that calls the main function in test_main.so on the hexagon simulator (assuming the desired Hexagon architecture version is v65): hexagon-sim -mv65 --simulated_returnval --usefs hexagon_ReleaseG_toolv84_v65 --pmu_statsfile hexagon_ReleaseG_toolv84_v65/pmu_stats.txt --cosim_file hexagon_ReleaseG_toolv84_v65/q6ss.cfg --l2tcm_base 0xd800 --rtos hexagon_ReleaseG_toolv84_v65/osam.cfg $HEXAGON_SDK_ROOT/rtos/qurt/computev65/sdksim_bin/runelf.pbn -- hexagon_ReleaseG_toolv84_v65/run_main_on_hexagon_sim stack_size=0x400000 -- test_main.so 1 foo 2 bar With this command line, run_main_on_hexagon_sim calls main() defined in test_main.so in a thread of stack size 0x400000 with arguments 1 foo2 2 bar on the simulator. Note: In simulator tests, the argument stack_size is to be passed before -- . Any argument passed after -- is considered as an argument to the main implemented in the shared object.","title":"Simulator usage"},{"location":"tools/setup.html","text":"Setup SDK environment Before using the SDK tools from the command line, run the SDK setup script, which points environment variables to the tools and utilities required by the Hexagon SDK. If some of the tools are missing or the environment is already setup, the script will generate some warnings. NOTE: The system path and tool changes are made locally only for the current shell or terminal. They do not have any effect on the global variables. The instructions for running the setup script are different for Windows and Linux. Windows To set up the SDK environment in Windows, run the following command from the Hexagon SDK root directory: setup_sdk_env.cmd Linux To set up SDK environment in Linux, you must first to switch to a bash shell. To switch from any unknown shell to a bash shell in Linux, enter bash in the terminal. This step is required because the setup script works in the bash environment. Once in a bash shell, run the setup script: source setup_sdk_env.source Or . setup_sdk_env.source Hexagon tools By default, the Hexagon SDK uses the Hexagon tools installed with the SDK to compile and run the code on the simulator. You can override this default behavior by setting the HEXAGON_TOOLS_ROOT environment variable. Note: Please note that in Windows, the PATH environment variable should NOT contain paths to the Hexagon Tools as this may cause the simulator to crash. Android NDK The setup_sdk_env script of the Hexagon SDK sets the Android Tools location to the default directory where they get installed as part of Hexagon SDK installation. However, this location can be changed by setting the ANDROID_ROOT_DIR environment variable. Device setup verification If the correct drivers and the Android Debug Bridge (ADB) are installed, access the device by entering the following command: adb devices This command lists all devices accessible via ADB. A typical response looks like this: List of devices attached 68070104 device To obtain additional details for each connected device, append the -l option to the the command. If ADB reports one device as being connected, check whether the SDK is installed correctly with the required dependencies by running an SDK example such as the calculator . If your setup is not working properly, the following sections will help you install any missing components. USB drivers Install USB drivers for Windows Install USB drivers by running Setup.exe from %HEXAGON_SDK_ROOT%\\tools\\debug\\usb . After successful installation of USB drivers, run the com_finder.py script to get the port number to which the device is attached. The script returns a message like the following example: Qualcomm HS-USB Com ports found: COM6 If the following error message is returned instead: No comm port found for Qualcomm HS-USB Device Uninstall and reinstall the drivers. If the error persists, see the support instructions . Install USB drivers for Linux Qualcomm does not provide USB drivers to work with devices on Linux. Use the default Linux USB drivers on Ubuntu to connect the devices. After installing ADB , adb logcat may not be able to connect to your device directly: $ adb logcat - waiting for device - adb devices may also indicate that you do not have permission to access your device: $ adb devices List of devices attached ???? no permissions (user is not in the plugdev group); These issues can be resolved with the instructions below: Use the lsusb command to confirm that the device is actually connected: $ lsusb Bus 001 Device 006: ID 05c6:90db Qualcomm, Inc. Ensure that udev rules are set correctly for access to the USB device. You can add udev rules to have the permissions of the device changed. Create or append to /etc/udev/rules.d/99-android.rules : SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"05c6\", ATTRS{idProduct}==\"90db\", MODE=\"0666\", OWNER=\"root\", GROUP=\"plugdev\", SYMLINK+=\"android%n\" For further information on udev refer to the man page . ADB and fastboot ADB and fastboot are required to work with Android devices. The following pages from the Android developer website describe the usage of ADB and how to configure Android devices. ADB: Android Debug Bridge Using hardware devices To install ADB and fastboot on Linux, run the following commands: wget https://dl.google.com/android/repository/platform-tools-latest-linux.zip unzip \\platform-tools-latest-linux.zip sudo cp platform-tools/adb /usr/bin/adb sudo cp platform-tools/fastboot /usr/bin/fastboot If you are still unable to see your device using adb devices, run the following commands: adb kill-server sudo adb start-server SDK dependencies The SDK installer should install all its dependencies. If the installer does not complete successfully or if errors are present when running the setup script, you might need to manually install some of these dependencies. Available Dependencies Component Description Key Windows Linux CMake CMake is a cross-platform free and open-source software tool for managing the build process of software using a compiler-independent method. cmake Yes Yes Devcon DevCon (Devcon.exe), the Device Console, is a command-line tool that displays detailed information about devices on computers running Windows. You can use DevCon to enable, disable, install, configure, and remove devices. devcon Yes No Eclipse Eclipse is an Integrated Development Environment (IDE) with an extensive suite of plugins that allow users to customize its functionality. eclipse Yes Yes Full Android NDK The Android NDK is a toolset that lets you implement parts of your app in native code, using languages such as C and C++. fullndk Yes Yes Gnu on Windows Gnu On Windows (GoW) is the lightweight alternative to Cygwin. gow Yes No Libusb Libusb is a C library that provides generic access to USB devices. It is intended to be used by developers to facilitate the production of applications that communicate with USB hardware. libusb No Yes Minimal Android NDK The minimal NDK is a lightweight version of the Android NDK that excludes unneeded APIs and toolchains. minimalndk Yes Yes Python Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. python Yes No Installing a Missing Dependency A tool for installing missing dependencies has been provided at $SDK_ROOT/utils/scripts/depDl.jar Usage: Windows: %HEXAGON_SDK_ROOT%\\tools\\hexagon_ide\\jre\\bin\\java.exe -jar %HEXAGON_SDK_ROOT%\\utils\\scripts\\depDl.jar -c <key> -t %HEXAGON_SDK_ROOT% Linux: $HEXAGON_SDK_ROOT/tools/hexagon_ide/jre/bin/java -jar $HEXAGON_SDK_ROOT/utils/scripts/depDl.jar -c <key> -t $HEXAGON_SDK_ROOT Defining a Source to Install From If you wish to download these dependencies from a custom location you may define the locations in a config file. The config file is a JSON formatted file named \"hexSDKCFG.json\" This file must be placed in %TEMP% for Windows or /tmp for Linux. The file is able to download from local file systems, network locations, or the internet. An example format is provided below. Windows { \"DependencyPaths\" : { \"7zexe\" : \"C:\\\\7-Zip\\\\7z.exe\", \"minimalandroidndk\" : \"D:\\\\sdk_deps\\\\minimal_android-ndk-r19c-windows-x86_64.zip\", \"fullandroidndk\" : \"D:\\\\sdk_deps\\\\android-ndk-r19c-windows-x86_64.zip\", \"eclipse\" : \"D:\\\\sdk_deps\\\\eclipse_64.zip\", \"gnuwin\" : \"D:\\\\sdk_deps\\\\gow-0.8.0.zip\", \"devcon\" : \"D:\\\\sdk_deps\\\\devcon.exe\", \"python\" : \"D:\\\\sdk_deps\\\\python.2.7.2.msi\", \"cmake\" : \"D:\\\\sdk_deps\\\\cmake-3.17.0-win64-x64.zip\" } } Linux { \"DependencyPaths\" : { \"minimalandroidndk\" : \"/local/sdk/deps/minimal_android-ndk-r19c-linux-x86_64.tar.gz\", \"fullandroidndk\" : \"/local/sdk/deps/android-ndk-r19c-linux-x86_64.tar.gz\", \"eclipse\" : \"/local/sdk/deps/eclipse-cpp-photon-R-linux-gtk-x86_64.tar.gz\", \"libusb\" : \"/local/sdk/deps/libusb-1.0.so\", \"cmake\" : \"\" } } Available Online Sources for Dependencies If you would like to download the dependencies yourself they are available at the locations below. Windows CMake : https://www.intrinsyc.com/hexagonsdk/windows/cmake-3.17.0-win64-x64.zip Devcon : https://www.intrinsyc.com/hexagonsdk/windows/devcon.exe Eclipse : https://www.intrinsyc.com/hexagonsdk/windows/eclipse-cpp-photon-R-win32-x86_64.zip Full Android NDK : https://www.intrinsyc.com/hexagonsdk/windows/android-ndk-r19c-windows-x86_64.zip GoW : https://www.intrinsyc.com/hexagonsdk/windows/gow-0.8.0.zip Minimal Android NDK : https://www.intrinsyc.com/hexagonsdk/windows/minimal_android-ndk-r19c-windows-x86_64.zip Python : https://www.intrinsyc.com/hexagonsdk/windows/python-2.7.2.msi 7Zip : https://www.intrinsyc.com/hexagonsdk/windows/7z1900.msi Linux CMake : https://www.intrinsyc.com/hexagonsdk/linux/cmake-3.17.0-Linux-x86_64.tar.gz Eclipse : https://www.intrinsyc.com/hexagonsdk/linux/eclipse-cpp-photon-R-linux-gtk-x86_64.tar.gz Full Android NDK : https://www.intrinsyc.com/hexagonsdk/linux/android-ndk-r19c-linux-x86_64.zip Libusb : https://www.intrinsyc.com/hexagonsdk/linux/libusb-1.0.so Minimal Android NDK : https://www.intrinsyc.com/hexagonsdk/linux/minimal_android-ndk-r19c-linux-x86_64.zip","title":"Setup"},{"location":"tools/setup.html#setup","text":"","title":"Setup"},{"location":"tools/setup.html#sdk-environment","text":"Before using the SDK tools from the command line, run the SDK setup script, which points environment variables to the tools and utilities required by the Hexagon SDK. If some of the tools are missing or the environment is already setup, the script will generate some warnings. NOTE: The system path and tool changes are made locally only for the current shell or terminal. They do not have any effect on the global variables. The instructions for running the setup script are different for Windows and Linux.","title":"SDK environment"},{"location":"tools/setup.html#windows","text":"To set up the SDK environment in Windows, run the following command from the Hexagon SDK root directory: setup_sdk_env.cmd","title":"Windows"},{"location":"tools/setup.html#linux","text":"To set up SDK environment in Linux, you must first to switch to a bash shell. To switch from any unknown shell to a bash shell in Linux, enter bash in the terminal. This step is required because the setup script works in the bash environment. Once in a bash shell, run the setup script: source setup_sdk_env.source Or . setup_sdk_env.source","title":"Linux"},{"location":"tools/setup.html#hexagon-tools","text":"By default, the Hexagon SDK uses the Hexagon tools installed with the SDK to compile and run the code on the simulator. You can override this default behavior by setting the HEXAGON_TOOLS_ROOT environment variable. Note: Please note that in Windows, the PATH environment variable should NOT contain paths to the Hexagon Tools as this may cause the simulator to crash.","title":"Hexagon tools"},{"location":"tools/setup.html#android-ndk","text":"The setup_sdk_env script of the Hexagon SDK sets the Android Tools location to the default directory where they get installed as part of Hexagon SDK installation. However, this location can be changed by setting the ANDROID_ROOT_DIR environment variable.","title":"Android NDK"},{"location":"tools/setup.html#device-setup-verification","text":"If the correct drivers and the Android Debug Bridge (ADB) are installed, access the device by entering the following command: adb devices This command lists all devices accessible via ADB. A typical response looks like this: List of devices attached 68070104 device To obtain additional details for each connected device, append the -l option to the the command. If ADB reports one device as being connected, check whether the SDK is installed correctly with the required dependencies by running an SDK example such as the calculator . If your setup is not working properly, the following sections will help you install any missing components.","title":"Device setup verification"},{"location":"tools/setup.html#usb-drivers","text":"","title":"USB drivers"},{"location":"tools/setup.html#install-usb-drivers-for-windows","text":"Install USB drivers by running Setup.exe from %HEXAGON_SDK_ROOT%\\tools\\debug\\usb . After successful installation of USB drivers, run the com_finder.py script to get the port number to which the device is attached. The script returns a message like the following example: Qualcomm HS-USB Com ports found: COM6 If the following error message is returned instead: No comm port found for Qualcomm HS-USB Device Uninstall and reinstall the drivers. If the error persists, see the support instructions .","title":"Install USB drivers for Windows"},{"location":"tools/setup.html#install-usb-drivers-for-linux","text":"Qualcomm does not provide USB drivers to work with devices on Linux. Use the default Linux USB drivers on Ubuntu to connect the devices. After installing ADB , adb logcat may not be able to connect to your device directly: $ adb logcat - waiting for device - adb devices may also indicate that you do not have permission to access your device: $ adb devices List of devices attached ???? no permissions (user is not in the plugdev group); These issues can be resolved with the instructions below: Use the lsusb command to confirm that the device is actually connected: $ lsusb Bus 001 Device 006: ID 05c6:90db Qualcomm, Inc. Ensure that udev rules are set correctly for access to the USB device. You can add udev rules to have the permissions of the device changed. Create or append to /etc/udev/rules.d/99-android.rules : SUBSYSTEMS==\"usb\", ATTRS{idVendor}==\"05c6\", ATTRS{idProduct}==\"90db\", MODE=\"0666\", OWNER=\"root\", GROUP=\"plugdev\", SYMLINK+=\"android%n\" For further information on udev refer to the man page .","title":"Install USB drivers for Linux"},{"location":"tools/setup.html#adb-and-fastboot","text":"ADB and fastboot are required to work with Android devices. The following pages from the Android developer website describe the usage of ADB and how to configure Android devices. ADB: Android Debug Bridge Using hardware devices To install ADB and fastboot on Linux, run the following commands: wget https://dl.google.com/android/repository/platform-tools-latest-linux.zip unzip \\platform-tools-latest-linux.zip sudo cp platform-tools/adb /usr/bin/adb sudo cp platform-tools/fastboot /usr/bin/fastboot If you are still unable to see your device using adb devices, run the following commands: adb kill-server sudo adb start-server","title":"ADB and fastboot"},{"location":"tools/setup.html#sdk-dependencies","text":"The SDK installer should install all its dependencies. If the installer does not complete successfully or if errors are present when running the setup script, you might need to manually install some of these dependencies.","title":"SDK dependencies"},{"location":"tools/setup.html#available-dependencies","text":"Component Description Key Windows Linux CMake CMake is a cross-platform free and open-source software tool for managing the build process of software using a compiler-independent method. cmake Yes Yes Devcon DevCon (Devcon.exe), the Device Console, is a command-line tool that displays detailed information about devices on computers running Windows. You can use DevCon to enable, disable, install, configure, and remove devices. devcon Yes No Eclipse Eclipse is an Integrated Development Environment (IDE) with an extensive suite of plugins that allow users to customize its functionality. eclipse Yes Yes Full Android NDK The Android NDK is a toolset that lets you implement parts of your app in native code, using languages such as C and C++. fullndk Yes Yes Gnu on Windows Gnu On Windows (GoW) is the lightweight alternative to Cygwin. gow Yes No Libusb Libusb is a C library that provides generic access to USB devices. It is intended to be used by developers to facilitate the production of applications that communicate with USB hardware. libusb No Yes Minimal Android NDK The minimal NDK is a lightweight version of the Android NDK that excludes unneeded APIs and toolchains. minimalndk Yes Yes Python Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. python Yes No","title":"Available Dependencies"},{"location":"tools/setup.html#installing-a-missing-dependency","text":"A tool for installing missing dependencies has been provided at $SDK_ROOT/utils/scripts/depDl.jar Usage: Windows: %HEXAGON_SDK_ROOT%\\tools\\hexagon_ide\\jre\\bin\\java.exe -jar %HEXAGON_SDK_ROOT%\\utils\\scripts\\depDl.jar -c <key> -t %HEXAGON_SDK_ROOT% Linux: $HEXAGON_SDK_ROOT/tools/hexagon_ide/jre/bin/java -jar $HEXAGON_SDK_ROOT/utils/scripts/depDl.jar -c <key> -t $HEXAGON_SDK_ROOT","title":"Installing a Missing Dependency"},{"location":"tools/setup.html#defining-a-source-to-install-from","text":"If you wish to download these dependencies from a custom location you may define the locations in a config file. The config file is a JSON formatted file named \"hexSDKCFG.json\" This file must be placed in %TEMP% for Windows or /tmp for Linux. The file is able to download from local file systems, network locations, or the internet. An example format is provided below.","title":"Defining a Source to Install From"},{"location":"tools/setup.html#windows_1","text":"{ \"DependencyPaths\" : { \"7zexe\" : \"C:\\\\7-Zip\\\\7z.exe\", \"minimalandroidndk\" : \"D:\\\\sdk_deps\\\\minimal_android-ndk-r19c-windows-x86_64.zip\", \"fullandroidndk\" : \"D:\\\\sdk_deps\\\\android-ndk-r19c-windows-x86_64.zip\", \"eclipse\" : \"D:\\\\sdk_deps\\\\eclipse_64.zip\", \"gnuwin\" : \"D:\\\\sdk_deps\\\\gow-0.8.0.zip\", \"devcon\" : \"D:\\\\sdk_deps\\\\devcon.exe\", \"python\" : \"D:\\\\sdk_deps\\\\python.2.7.2.msi\", \"cmake\" : \"D:\\\\sdk_deps\\\\cmake-3.17.0-win64-x64.zip\" } }","title":"Windows"},{"location":"tools/setup.html#linux_1","text":"{ \"DependencyPaths\" : { \"minimalandroidndk\" : \"/local/sdk/deps/minimal_android-ndk-r19c-linux-x86_64.tar.gz\", \"fullandroidndk\" : \"/local/sdk/deps/android-ndk-r19c-linux-x86_64.tar.gz\", \"eclipse\" : \"/local/sdk/deps/eclipse-cpp-photon-R-linux-gtk-x86_64.tar.gz\", \"libusb\" : \"/local/sdk/deps/libusb-1.0.so\", \"cmake\" : \"\" } }","title":"Linux"},{"location":"tools/setup.html#available-online-sources-for-dependencies","text":"If you would like to download the dependencies yourself they are available at the locations below.","title":"Available Online Sources for Dependencies"},{"location":"tools/setup.html#windows_2","text":"CMake : https://www.intrinsyc.com/hexagonsdk/windows/cmake-3.17.0-win64-x64.zip Devcon : https://www.intrinsyc.com/hexagonsdk/windows/devcon.exe Eclipse : https://www.intrinsyc.com/hexagonsdk/windows/eclipse-cpp-photon-R-win32-x86_64.zip Full Android NDK : https://www.intrinsyc.com/hexagonsdk/windows/android-ndk-r19c-windows-x86_64.zip GoW : https://www.intrinsyc.com/hexagonsdk/windows/gow-0.8.0.zip Minimal Android NDK : https://www.intrinsyc.com/hexagonsdk/windows/minimal_android-ndk-r19c-windows-x86_64.zip Python : https://www.intrinsyc.com/hexagonsdk/windows/python-2.7.2.msi 7Zip : https://www.intrinsyc.com/hexagonsdk/windows/7z1900.msi","title":"Windows"},{"location":"tools/setup.html#linux_2","text":"CMake : https://www.intrinsyc.com/hexagonsdk/linux/cmake-3.17.0-Linux-x86_64.tar.gz Eclipse : https://www.intrinsyc.com/hexagonsdk/linux/eclipse-cpp-photon-R-linux-gtk-x86_64.tar.gz Full Android NDK : https://www.intrinsyc.com/hexagonsdk/linux/android-ndk-r19c-linux-x86_64.zip Libusb : https://www.intrinsyc.com/hexagonsdk/linux/libusb-1.0.so Minimal Android NDK : https://www.intrinsyc.com/hexagonsdk/linux/minimal_android-ndk-r19c-linux-x86_64.zip","title":"Linux"},{"location":"tools/sign.html","text":"Signing process All Qualcomm devices enforce an authentication mechanism on DSP shared libraries. To enable a DSP library to run on test device, such as an MTP or QRD provided by Qualcomm Inc., take one of the following actions: Use a signature-free dynamic module offload Sign a device to enable all DSP libraries to run without requiring to be signed Sign an individual DSP library This page provides detailed explanations on the last two approaches for which a digital signature is used to enable one or more libraries to run with full privilege on a test device. This page also explains the various concepts that are relevant to authorizing DSP libraries to run on the device. Signing concepts The signing process touches on several technical concepts described in this section. Test device A test device or debug device, (Mobile Test Platform) MTP or (Qualcomm Reference Design) QRD, is a device on which the debug fuse is present. This fuse is not present on production devices. You can prototype and test your software on debug devices such as DragonBoards. However, the signing process described on this page does not allow DSP libraries to authenticate successfully on production devices such as OEM phones and tablets. To understand which options are available when working with production devices, see the discussion on system integration . Digital signing Signing an object consists of adding a cryptographic signature to it. This signature is later verified by the loader when it loads the modules dynamically. With test devices, signing an object is accomplished using the signer.py script provided in the Hexagon SDK. Digital signing is required for modules that require resources or privileges unavailable in the unsigned protection domain (unsigned PD) , or on DSPs where unsigned PDs are not supported. Unsigned PD An unsigned PD is a limited-rights process in which signature-free dynamic shared libraries can run without requiring a digital signature. It is also called a signature-free protection domain (signature-free PD). The signature-free dynamic module offload is enabled on the cDSP only, not on other DSPs. For more details about unsigned PDs, see the system integration page . Test signature A test signature (TestSig) is generated based on the serial number of the device. It is a specific signed DSP shared library, which enables DSP shared libraries to run with full privileges on that test device. A device with such a test signature is referred as a signed device. The TestSig shared object is named testsig-<serial-number>.so , where the serial number is specific to the device on which the test signature will be installed. Device serial number The device serial number uniquely identifies a device. It is required to generate a test signature . Use signer.py The signer.py script, located under {HEXAGON_SDK_ROOT}/utils/scripts , can automate any of the steps required to sign an entire device or an individual DSP library. Command line usage Before using the signer script, set up your SDK environment . Usage: ./signer.py <ACTION> [-T <target>] [-s <serial_num] [-d <domain>] [-LE] [-dest <hlos_destination>] [-t <serial_number>] [-i <input_file>] [-o <output_dir>] [-l] [-64] ACTION: sign, getserial OPTIONS: -T <target>: specify target name -s <serial_num>: specify the ADB serial number for the device -d <domain>: specify the domain (ADSP/CDSP) on which the getserial application will run to retrieve the serial number -LE: specify this option for Ubuntu targets. Default assumes an Android target. -dest <destination>: specify the directory (/usr or /data) where to push TestSig. Only for LE. -t <serial_number>: specify the serial number for which to generate a signature. This option allows to generate a signature for a device that is not connected or to bypass the retrieval of the device serial number. -i <in_file>: specify a shared object to be signed -o <out_dir>: specify an output directory in which to generate a signed object. Default is `.`. -l or --local: store the TestSig locally and do not push to target -64: use the 64-bit getserial binary to retrieve the serial number. Only for LE. Sign a device ./signer.py sign Retrieve the device serial number automatically, use it to generate a test signature, and push the test signature on the device. ./signer.py sign -t 0x12345 Same as above, but use the provided serial number to generate a test signature. ./signer.py sign -t 0x12345 -o out_dir Use the provided serial number to generate a test signature and store it locally in the out_dir folder. This approach can be useful in creating a test signature for a device that is not connected. NOTE: The option -T can be used to identify and sign the required device if multiple devices are connected simultaneously. However, if multiple devices of the same type are connected, for example if there are two SM8250 devices connected simultaneously, then use -s option to differentiate between the devices based on the ADB serial numbers. The ADB serial number of a device is a string created by ADB to uniquely identify the device by its port number. adb devices -l e76a39c6 device product:kona model:Kona_for_arm64 device:kona f68b78d7 device product:kona model:Kona_for_arm64 device:kona ./signer.py sign -s e76a39c6 Retrieve the serial number of the device with ADB serial number e76a39c6 , use it to generate a test signature, and push the test signature on the same device. Sign a DSP library ./signer.py sign -i myLib.so -o out_dir Add a digital signature to a shared DSP library ( myLib.so ), and place the signed library in the out_dir folder. Retrieve a serial number ./signer.py getserial Retrieve and display the device serial number. Additional notes The signer.py script automates all the signing steps. You might need additional information to execute some of the steps manually. Serial number An alternative to using signer.py to retrieve the serial number is to run the following ADB command, which returns the serial number in int format for most devices: adb shell cat /sys/devices/soc0/serial_number Install a TestSig Android Push the TestSig to one of the default search paths to make it accessible to all the applications. Alternatively, the TestSig can be pushed anywhere under /vendor/lib/rfsa/dsp/ directory and the directory to which the TestSig is pushed can be specified in [A]DSP_LIBRARY_PATH to make it accessible to limited applications. The walkthrough examples follow the convention of pushing the TestSig under a commonly accessible directory: adb root adb remount adb push testsig-0x12345.so /vendor/lib/rfsa/adsp You only need to execute these steps once until you reflash the Linux image on your device. Windows Phone Use TShell to push Testsig onto the device: putd testsig-0x12345.so C:\\Windows\\system32\\drivers\\Driverdata\\qualcomm\\fastrpc","title":"Device signing"},{"location":"tools/sign.html#signing-process","text":"All Qualcomm devices enforce an authentication mechanism on DSP shared libraries. To enable a DSP library to run on test device, such as an MTP or QRD provided by Qualcomm Inc., take one of the following actions: Use a signature-free dynamic module offload Sign a device to enable all DSP libraries to run without requiring to be signed Sign an individual DSP library This page provides detailed explanations on the last two approaches for which a digital signature is used to enable one or more libraries to run with full privilege on a test device. This page also explains the various concepts that are relevant to authorizing DSP libraries to run on the device.","title":"Signing process"},{"location":"tools/sign.html#signing-concepts","text":"The signing process touches on several technical concepts described in this section.","title":"Signing concepts"},{"location":"tools/sign.html#test-device","text":"A test device or debug device, (Mobile Test Platform) MTP or (Qualcomm Reference Design) QRD, is a device on which the debug fuse is present. This fuse is not present on production devices. You can prototype and test your software on debug devices such as DragonBoards. However, the signing process described on this page does not allow DSP libraries to authenticate successfully on production devices such as OEM phones and tablets. To understand which options are available when working with production devices, see the discussion on system integration .","title":"Test device"},{"location":"tools/sign.html#digital-signing","text":"Signing an object consists of adding a cryptographic signature to it. This signature is later verified by the loader when it loads the modules dynamically. With test devices, signing an object is accomplished using the signer.py script provided in the Hexagon SDK. Digital signing is required for modules that require resources or privileges unavailable in the unsigned protection domain (unsigned PD) , or on DSPs where unsigned PDs are not supported.","title":"Digital signing"},{"location":"tools/sign.html#unsigned-pd","text":"An unsigned PD is a limited-rights process in which signature-free dynamic shared libraries can run without requiring a digital signature. It is also called a signature-free protection domain (signature-free PD). The signature-free dynamic module offload is enabled on the cDSP only, not on other DSPs. For more details about unsigned PDs, see the system integration page .","title":"Unsigned PD"},{"location":"tools/sign.html#test-signature","text":"A test signature (TestSig) is generated based on the serial number of the device. It is a specific signed DSP shared library, which enables DSP shared libraries to run with full privileges on that test device. A device with such a test signature is referred as a signed device. The TestSig shared object is named testsig-<serial-number>.so , where the serial number is specific to the device on which the test signature will be installed.","title":"Test signature"},{"location":"tools/sign.html#device-serial-number","text":"The device serial number uniquely identifies a device. It is required to generate a test signature .","title":"Device serial number"},{"location":"tools/sign.html#use-signerpy","text":"The signer.py script, located under {HEXAGON_SDK_ROOT}/utils/scripts , can automate any of the steps required to sign an entire device or an individual DSP library.","title":"Use signer.py"},{"location":"tools/sign.html#command-line-usage","text":"Before using the signer script, set up your SDK environment . Usage: ./signer.py <ACTION> [-T <target>] [-s <serial_num] [-d <domain>] [-LE] [-dest <hlos_destination>] [-t <serial_number>] [-i <input_file>] [-o <output_dir>] [-l] [-64] ACTION: sign, getserial OPTIONS: -T <target>: specify target name -s <serial_num>: specify the ADB serial number for the device -d <domain>: specify the domain (ADSP/CDSP) on which the getserial application will run to retrieve the serial number -LE: specify this option for Ubuntu targets. Default assumes an Android target. -dest <destination>: specify the directory (/usr or /data) where to push TestSig. Only for LE. -t <serial_number>: specify the serial number for which to generate a signature. This option allows to generate a signature for a device that is not connected or to bypass the retrieval of the device serial number. -i <in_file>: specify a shared object to be signed -o <out_dir>: specify an output directory in which to generate a signed object. Default is `.`. -l or --local: store the TestSig locally and do not push to target -64: use the 64-bit getserial binary to retrieve the serial number. Only for LE.","title":"Command line usage"},{"location":"tools/sign.html#sign-a-device","text":"./signer.py sign Retrieve the device serial number automatically, use it to generate a test signature, and push the test signature on the device. ./signer.py sign -t 0x12345 Same as above, but use the provided serial number to generate a test signature. ./signer.py sign -t 0x12345 -o out_dir Use the provided serial number to generate a test signature and store it locally in the out_dir folder. This approach can be useful in creating a test signature for a device that is not connected. NOTE: The option -T can be used to identify and sign the required device if multiple devices are connected simultaneously. However, if multiple devices of the same type are connected, for example if there are two SM8250 devices connected simultaneously, then use -s option to differentiate between the devices based on the ADB serial numbers. The ADB serial number of a device is a string created by ADB to uniquely identify the device by its port number. adb devices -l e76a39c6 device product:kona model:Kona_for_arm64 device:kona f68b78d7 device product:kona model:Kona_for_arm64 device:kona ./signer.py sign -s e76a39c6 Retrieve the serial number of the device with ADB serial number e76a39c6 , use it to generate a test signature, and push the test signature on the same device.","title":"Sign a device"},{"location":"tools/sign.html#sign-a-dsp-library","text":"./signer.py sign -i myLib.so -o out_dir Add a digital signature to a shared DSP library ( myLib.so ), and place the signed library in the out_dir folder.","title":"Sign a DSP library"},{"location":"tools/sign.html#retrieve-a-serial-number","text":"./signer.py getserial Retrieve and display the device serial number.","title":"Retrieve a serial number"},{"location":"tools/sign.html#additional-notes","text":"The signer.py script automates all the signing steps. You might need additional information to execute some of the steps manually.","title":"Additional notes"},{"location":"tools/sign.html#serial-number","text":"An alternative to using signer.py to retrieve the serial number is to run the following ADB command, which returns the serial number in int format for most devices: adb shell cat /sys/devices/soc0/serial_number","title":"Serial number"},{"location":"tools/sign.html#install-a-testsig","text":"","title":"Install a TestSig"},{"location":"tools/sign.html#android","text":"Push the TestSig to one of the default search paths to make it accessible to all the applications. Alternatively, the TestSig can be pushed anywhere under /vendor/lib/rfsa/dsp/ directory and the directory to which the TestSig is pushed can be specified in [A]DSP_LIBRARY_PATH to make it accessible to limited applications. The walkthrough examples follow the convention of pushing the TestSig under a commonly accessible directory: adb root adb remount adb push testsig-0x12345.so /vendor/lib/rfsa/adsp You only need to execute these steps once until you reflash the Linux image on your device.","title":"Android"},{"location":"tools/sign.html#windows-phone","text":"Use TShell to push Testsig onto the device: putd testsig-0x12345.so C:\\Windows\\system32\\drivers\\Driverdata\\qualcomm\\fastrpc","title":"Windows Phone"},{"location":"tools/sysmon_app.html","text":"sysMonApp sysMonApp is an Android application that configures and displays from multiple performance-related services on the cDSP, aDSP, and sDSP. These services are explained in the following sections. Service Description Profiler Profile any DSP to gather clock information, resource usage, processor and thread load distribution, bus bandwidth metrics, and so on. Getstate Get the current clock voting information and heap statistics of each static Protection Domain. DCVS Enable or disable the Dynamic Clock Voltage Scaling (DCVS) feature executing on each DSP. Clocks Set the DSP core clock and bus clocks. Thread info For each active software thread, display their priority, declared stack size, and maximum stack used. TLP Thread-level profiling (TLP) that monitors software thread activity. ETM trace Enable the ETM to trace the instruction and data memory and provide information about dynamic modules loaded on the cDSP. HTA profiler Profile the HTA to gather clock information, DDR bandwidth, and various HTA activity statistics. getPowerStats This service reports time spent in power collapse, low power island (LPI) and each core clock level of selected Q6 subsystem since device boot up or last reset. pinfo Display the statistics (e.g. thread priority, stack size) of FastRPC-spawned user processes. sysMonApp supports all targets covered by the Hexagon SDK with the exception of Lahaina. For more details, see the feature matrix . Setup The Hexagon SDK includes the following LA and LE variants of sysMonApp. These variants are present at the indicated locations: LA 32-bit version: ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonApp LA 64-bit verion: ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonApp_64Bit LE 32-bit version: ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonAppLE LE 64-bit verion: ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonAppLE_64Bit Push the version for your device to a location of your choice, and then change the permissions to make the file an executable. For example, on rooted Android devices, we recommend you push your executable into a specified folder as part of the data partition: adb push ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonApp /data/local/tmp/ adb shell chmod 777 /data/local/tmp/sysMonApp Execute sysMonApp from the ADB shell by entering the following command: adb shell /data/local/tmp/sysMonApp <service> <arguments related to the service> When sysMonApp is executed without a service name, the help page is displayed: adb shell /data/local/tmp/sysMonApp -------------------- sysMonApp user guide: -------------------- Supported features : 1. profiler: Collect performance statistics of Q6 Available Options for adsp/sdsp/cdsp : (If no argument then uses default): --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP --debugLevel (default:1 - default mode) Input 0 for user mode 0 - User mode logging with 4 PMU events per set 1 - Default mode logging 2 - User mode logging with 8 PMU events per set --profileFastrpcTimeline Profile the time taken by FastRPC communication 0 - Disable 1 - Enable --noMeasured Controls capturing measured clocks periodically (default = 0) 0 - Report measured bus clock frequencies periodically 1 - Disable reporting measured bus clock frequencies Following options are valid only for 'Default mode' logging --samplingPeriod sampling period in ms(default:0 ms - no sampling period override - to be decided by sysMon --duration profiling duration in seconds(default:10s) --dcvsOption 1 - Keep DCVS enabled (if not disabled externally by other clients) for this profiling 0 - disable DCVS for this profiling. Use this option if you want to fix a particular sampling rate. (Default: 1 - Do not disable DCVS) Following options are valid only for 'user mode' logging --samplingPeriod sampling period in ms(default:1 ms) --dcvsOption 1 - Keep DCVS enabled (if not disabled externally by other clients) for this profiling 0 - disable DCVS for this profiling. Use this option if you want to fix a particular sampling rate. (Default: 0 - Disables DCVS) For user mode logging, if you want to override default PMU events used by sysmon app, create /data/pmu_events.txt file with comma separated PMU events in HEX format Example: ./sysMonApp profiler --samplingPeriod 100 Example: ./sysMonApp profiler --q6 adsp --samplingPeriod 100 Example: ./sysMonApp profiler --q6 cdsp --samplingPeriod 100 Available Options for HTA : (If no argument then uses default): --q6 (default selected processor:CDSP): HTA - Selected processor HTA --samplingPeriod sampling period in ms(default:1 ms --cdsp (default:0 - Profiles HTA alone 0 - Profiles HTA alone 1 - Profiles both HTA and CDSP Example: ./sysMonApp profiler --q6 HTA Example: ./sysMonApp profiler --q6 HTA --cdsp 1 2. tlp: Collect thread level performance statistics of Q6 Available Options : (If no argument then uses default): --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP --samplingPeriod sampling period in ms(default:50 ms) --duration profiling duration in seconds(default:10s) --tName Thread names to profile separated with comma(default: Profile all active threads) --profile Runs Sysmon profile in parallel with TLP(default:0) 0 - Sysmon Profile is not executed 1 - Sysmon Profile is executed Example: ./sysMonApp TLP Example: ./sysMonApp TLP --samplingPeriod 50 Example: ./sysMonApp TLP --samplingPeriod 10 --profile 1 Example: ./sysMonApp TLP --samplingPeriod 5 --profile 1 --tName AfeDy4000,MXAR 3. dcvs: Enable or disable DCVS ./sysMonApp dcvs <enable/disable> ./sysMonApp dcvs --q6 sdsp <enable/disable> ./sysMonApp dcvs --q6 adsp <enable/disable> --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP enable: Will enable SysMon DCVS in ADSP if previously disabled disable: Will disable SysMon DCVS in ADSP 4. getstate: Get present clock, heap state Available Options for getstate: (If no argument then uses default parameters): --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP --getvotes (default selected value:0): 1 - List the DCVS_V2/V3 votes done via HAP_power_set() 0 - DCVS_V2/V3 votes are not queried Example : ./sysMonApp getstate --q6 sdsp Example : ./sysMonApp getstate --getvotes 1 --q6 adsp Example : ./sysMonApp getstate 5. clocks: For changing Q6 related clocks ./sysMonApp clocks <set/remove/limit> <options> Available Options: --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP Available Options (for set option): --coreClock <in MHz>(default:0) --busClock <in MHz>(default:0) --hcpBusClock <in MHz>(default:0) --dmaBusClock <in MHz>(default:0) --sleepLatency <in micro-seconds>(default:0) An explicitly vote of 0 removes any previous vote for that option Leave the defaults for the clocks you don't want to change A remove has to be called to revert all the settings once done limit - Limits only the coreClock, all the other clients voting for core clock will be overwritten Example: ./sysMonApp clocks set --coreClock 200 --busClock 100 Example: ./sysMonApp clocks remove Example: ./sysMonApp clocks limit --coreClock 400 6. ./sysMonApp tinfo :Print the all SW threads info at that instance Available Options : (If no argument then uses default): --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP --getstack Provides the declared and max used Stack Size root - Provides Stack info of rootPD audio - Provides Stack info of audioPD sensors - Provides Stack info of sensorsPD user - Provides Stack info of userPD all - Provides Stack info of allPDs If none of the above match provides the defaults Example: ./sysMonApp tinfo Example: ./sysMonApp tinfo --getstack root Example: ./sysMonApp TLP --getstack all 7. ./sysMonApp benchmark :Run benchmark application with desired functions 8. etmTrace: Etm Trace Enablement || DLL Loaded Output ./sysMonApp etmTrace --command (default selected etm): etm - Enable ETM tracing dll - Output DLL --etmType (default selected pc_mem): pc - PC Only Collection mem - Memory Only Collection pc_mem - PC + Memory Collection cc_pc - Cycle Coarse PC Only Collection cc_mem - Cycle Coarse Memory Only Collection cc_pc_mem - Cycle Coarse PC + Memory Collection ca_pc - Cycle Accurate PC Only Collection ca_mem - Cycle Accurate Memory Only Collection ca_pc_mem - Cycle Accurate PC + Memory Collection --d (default 0): 1 - Debug prints enabled 0 - Debug prints disabled 9. DSP Power Statistics getPowerStats: Report time spent in power collapse, low power island (LPI) and each core clock level of selected Q6 subsystem since device boot up or last reset. Available options:(If no argument then uses default) --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP --clear 1 - Clear the stats collected so far and start fresh logging (Default: Statistics are accumulated and not cleared) Example : ./sysMonApp getPowerStats --q6 cdsp Example : ./sysMonApp getPowerStats --clear 1 --q6 adsp 10. pinfo: Displays the statistics related to FastRPC-spawned user process Example : ./sysMonApp pinfo --q6 cdsp Profiler service Use the sysMonApp profiler option to profile services running on one of the Hexagon DSPs. Gather information such as: The clocks voted for Resource usage Load distribution across available hardware threads Load on the processor Bus bandwidth metrics And other profiling metrics The metrics are useful in measuring performance, debugging performance related issues, and identifying possible optimizations. To see the profiler service help page: adb shell /data/local/tmp/sysMonApp profiler -help Usage sysMonApp profiler [options] Options Expected values Default value Description --samplingPeriod Integer >=0 0 if debugLevel==1 1 otherwise Sampling period (in milliseconds) at which the eight PMU events are collected. When samplingPeriod is set to 0, the sampling interval is chipset and subsystem dependent but usually on the order of 1 through 50 ms. --debugLevel 0/1/2 Chipset dependent Select a sampling mode. 0 - User mode with four customized PMU counters. 1 - Default mode. 2 - User mode with eight customized PMU counters. See below for more details. --dcvsOption 0/1 0 if debugLevel==1 1 otherwise Disable (0) or enable (1) DCVS for the profiling duration. --profileFastrpcTimeline 0/1 0 Disable (0) or enable (1) the option of collecting the time spent by the FastRPC interface. --noMeasured 0/1 0 Enable (0) or disable (1) reporting measured bus clock frequencies. --stidArray Comma-separated Software Thread ID (STID) values disabled User-provided list (comma separated) of STID values. For more details, see below . --q6 adsp, cdsp, sdsp adsp Select the Hexagon DSP to profile. Sampling modes Three sampling modes are supported: Default mode, User mode with four customized PMU counters, and User mode with eight customized PMU counters. In Default mode, a fixed set of eight PMU events (MPPS, AXI read and write bandwidth, HVX MPPS, and so on) are monitored. (For more details, see the sysMon parser metric description .) The eight PMU events chosen in this mode are specific to the chipset and subsystem (aDSP/cDSP/sDSP). You cannot override these events. DCVS is enabled by default in this mode. In User mode, you can configure which PMU events to assign to four (or eight) of the PMU counters. Further, you can also use the --stidArray option to configure the counters to only increment when the configured events occur on specified software threads. Select any number of PMU events to count in a /data/pmu_events.txt file. sysMonApp rotates through the requested events, four (or eight) at a time, to generate a sampling of all the requested events. If the --stidArray option is used to confine the counting to only specified software threads, sysMonApp further rotates through the specified software threads for each set of four (or eight) PMU events assigned to the PMU counters at a given time. For example, in User mode 0 (four user-defined PMUs), assume an STID array specifies two threads and 10 user-defined events: The sysMon profiler service collects a set of samples for the first four user-defined PMUs for the first STID, and then for the second STID. It then moves on to collect samples for the next four user-defined PMU events for the first STID, and then the second STID. Next, it collects samples for both the last two and again the first two user-defined PMU events, for the first STID, and then the second STID, and so on. Throughout the entire time, the profiler also collects samples for four predefined PMU events. These events are not filtered by STID: they are collected for all software threads with an STID equal to 0. PMU selection As explained earlier, STIDs allow you to filter some PMU events captured by the profiler service. To filter PMU counters by software thread, follow these steps: Programmatically assign an STID to each thread that is to be filtered by the profiler. Use the qurt_thread_attr_enable_stid API at the time of thread creation. The second parameter of this API is an 8-bit unsigned number that has the following meaning: 0: No STID is assigned. No PMU-based filtering is applied to this thread. 1: QuRT assigns an STID that is not already in use. 2 through 255: This value is used as the STID value. QuRT does not check whether that STID is already in use, and thus multiple threads might share the same STID. The following code snippet shows how to use qurt_thread_attr_enable_stid : // Standard way of setting some thread attributes qurt_thread_attr_t attr; qurt_thread_attr_init (&attr); qurt_thread_attr_set_name(&attr, p_ThreadName); qurt_thread_attr_set_stack_addr (&attr, p_StackBase); qurt_thread_attr_set_stack_size (&attr, stackSize); /* The following line enables STID allocation to the software thread being created by * requesting QuRT to assign an available STID to the thread during qurt_thread_create. */ qurt_thread_attr_enable_stid(&attr, 1); result = qurt_thread_create(&tid, &attr, (void*)entry, NULL); Use the tinfo service while the application is running to determine or confirm which STIDs are assigned to the threads on which STID filtering is required. Run the sysMonApp profiler service with the --stidArray option that lists the STIDs. For example, to start the profiler service on the cDSP, enter the following command: adb shell /data/local/tmp/sysMonApp profiler --debugLevel 0 --stidArray 1,2,3,4 --q6 cdsp This command starts the profiler in User mode with four customized PMU events ( debugLevel 0 ). The --q6 option overrides the default, which is \\adsp`. As explained previously , four user-configured PMU events are selected per sample in User mode, while the other four are the defaults and remain constant. STID filtering only applies to the four user-configured PMU events, and the defaults are not filtered (STID mask is 0) and thus collected continuously. In a profiling sample captured over a provided sampling period, only one STID value at a time is applied as a filter to the four user-configured PMU counters. In this example, a sequence of 50 PMU events for threads with STID values of 1, 2, 3, and 4 are collected one at a time. NOTES: By default, STID-based filtering is disabled. You can enable it by using the --stidArray option in conjunction with --debugLevel to select one of the two user modes. Not all PMU events are maskable under STID. An STID cannot be assigned to a thread that has already been created. Due to time division multiplexing (iterating over provided STIDs over profiling samples), selecting multiple STIDs as filters might not provide a complete picture for a given STID (missing time slots where other STID is configured). In cases where a continuous (time domain) filtering is required per STID, pass only one STID to sysMonApp. The --stidArray list has a limit of eight STIDs. Usage examples Run the sysMonApp profiler service in User mode. adb shell /data/local/tmp/sysMonApp profiler --debugLevel 0 Starting Profiler with parameters: Q6 Processor: adsp Sampling Interval in ms : 1 Total samples :0 samplesInSet: 50 Default Mode : 0 dcvs enable : 0 no. of stids: 0 Domain Configured ADSP Q6 architecture detected as v66... Opening /data/pmu_events.txt file /data/pmu_events.txt not found, going ahead with default events, no. of events = 84 opening outputfile @/sdcard/sysmon.bin Enabling DSP SysMon using FastRPC Allocating output buffer >> Starting thread to Query DSP SysMon for samples >> Waiting for a keyboard iHTAt... Run the sysMonApp profiler service in Default mode. adb shell /data/local/tmp/sysMonApp profiler --debugLevel 1 Starting Profiler with parameters: Q6 Processor: adsp Sampling Interval in ms : 0 Total samples :0 samplesInSet: 50 Default Mode : 1 dcvs enable : 1 no. of stids: 0 Domain Configured ADSP Q6 architecture detected as v66... opening outputfile @/sdcard/sysmon.bin Enabling DSP SysMon using FastRPC Allocating output buffer >> Starting thread to Query DSP SysMon for samples >> Waiting for a keyboard iHTAt... Run the sysMonApp profiler and capture samples at sampling intervals of 10 ms on the cDSP in User mode, and capture the FastRPC timeline packets. adb shell /data/local/tmp/sysMonApp profiler --debugLevel 0 --samplingPeriod 10 --q6 cdsp --profileFastrpcTimeline 1 Starting Profiler with parameters: Q6 Processor: cdsp Sampling Interval in ms : 10 Total samples :0 samplesInSet: 50 Default Mode : 0 dcvs enable : 0 no. of stids: 0 Domain Configured Compute DSP Running FastRPC Timeline Profiling in parallel... Q6 architecture detected as v66... Opening /data/pmu_events.txt file /data/pmu_events.txt not found, going ahead with default events, no. of events = 84 opening outputfile @/sdcard/sysmon_cdsp.bin Enabling DSP SysMon using FastRPC Allocating output buffer >> Starting thread to Query DSP SysMon for samples >> Profiling FastRPC Timelines in parallel >> Waiting for a keyboard iHTAt... Data collection The sysMonApp profiler stores raw profiling data on the device in either the /sdcard or /data folders: For the aDSP, the file name is sysmon.bin . For the cDSP, the file name is sysmon_cdsp.bin . For the sDSP, the file name is sysmon_sdsp.bin . The sysMonApp profiler also prints the output file path with the appropriate file name in the standard output. When you finish profiling, pull the file from the device and postprocess it using the sysmon parser on a host machine. To pull the profiler output file from device using ADB, enter the following command: adb pull /sdcard/sysmon<DSP>.bin <destination directory> Where <DSP> takes the value _cdsp or _sdsp to designate the cDSP or sDSP, respectively. For the aDSP, do not use <DSP> . Data postprocessing To postprocess the profiling data, see the instructions on how to use the sysmon parser . Getstate service Use the getstate service to determine the current clock voting information and heap statistics of each static Protection Domain. Usage sysMonApp getstate [--getVotes <0/1>] [--q6 <dsp>] Where the --q6 option allows you to specify the DSP to query: adsp , cdsp , or sdsp . --getVotes 1 can be used to display DCVS_V2/V3 votes done via HAP_power_set() call. Example The following command queries the clock and heap statistics of the ADSP: adb shell /data/local/tmp/sysMonApp getstate Domain Configured ADSP DSP Core clock :576.00MHz SNOC Vote:1.26MHz MEMNOC Vote:0.00MHz GuestOS : Total Heap:1792.00KB Available Heap:514.49KB Max.Free Bin:433.02KB Audio PD : Total Heap:9216.00KB Available Heap:8170.21KB Max.Free Bin:8059.44KB Measured SNOC (/clk/snoc) :200.00MHz Measured BIMC (/clk/bimc) :681.66MHz Measured CPU L3 clock :1612.80MHz The following command queries the clock and heap statistics of the CDSP: adb shell /data/local/tmp/sysMonApp getstate --q6 cdsp Domain Configured Compute DSP DSP Core clock :384.00MHz SNOC Vote:0.00MHz MEMNOC Vote:0.62MHz GuestOS : Total Heap:2560.00KB Available Heap:1622.56KB Max.Free Bin:1575.28KB Measured SNOC (/clk/snoc) :200.00MHz Measured BIMC (/clk/bimc) :681.66MHz Measured CPU L3 clock :1612.80MHz getPowerStats service Reports time spent in power collapse, low power island (LPI) and each core clock level of selected Q6 subsystem since device boot up or last reset. Usage sysMonApp getPowerStats [--clear 1] [--q6 <dsp>] Where the --q6 option allows you to specify the DSP to query: adsp , cdsp , or sdsp . --clear 1 can be provided to clear the stats collected so far and start fresh logging. Example The following command queries the power statistics of the ADSP: adb shell /data/local/tmp/sysMonApp getPowerStats Domain Configured ADSP Clock freq.(MHz) Active time(seconds) 307.20 7.28 576.00 0.01 614.40 9.60 768.00 0.78 940.80 1.93 960.00 0.10 1171.20 0.05 1324.80 0.00 1401.60 0.03 Power collapse time(seconds): 17181.12 Low Power Island time(seconds): 38.59 Current core clock(MHz): 307.20 Total time(seconds): 17239.49 The following command queries the power statistics of the CDSP: adb shell /data/local/tmp/sysMonApp getPowerStats --q6 cdsp Domain Configured Compute DSP Clock freq.(MHz) Active time(seconds) 364.80 0.10 556.80 0.00 768.00 0.08 960.00 0.00 1171.20 0.18 1324.80 0.00 1382.40 0.00 Power collapse time(seconds): 17276.43 Low Power Island time(seconds): 0.00 Current core clock(MHz): 364.80 Total time(seconds): 17276.79 The following command queries the power statistics of the SDSP: adb shell /data/local/tmp/sysMonApp getPowerStats --q6 sdsp Domain Configured Sensors DSP Clock freq.(MHz) Active time(seconds) 423.00 0.23 557.00 0.00 672.00 0.22 845.00 1.18 960.00 0.00 1075.00 0.01 Power collapse time(seconds): 13988.50 Low Power Island time(seconds): 39.77 Current core clock(MHz): 423.00 Total time(seconds): 14029.91 pinfo service This service displays FastRPC-spawned user process statistics. Usage sysMonApp pinfo [--q6 <dsp>] Where the --q6 option allows you to specify the DSP to query: adsp , cdsp , or sdsp . Example The following command displays the FastRPC-spawned user process statistics of the CDSP: adb shell /data/local/tmp/sysMonApp pinfo --q6 cdsp Domain Configured Compute DSP +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Info of UserPD : 1 NAME is : /frpc/f05b8b10 sysMonApp ASID is : 772 PIDA is : 6493 PD State is : InitDone PD Type is : SignedDynamic User heap Used Bytes in KB : 53.58 RPC Total Memory in KB : 512.00 RPC Memory Used in KB : 35.28 UserPD Threads info: DSP FastRPC Threads: T1 : Name : /frpc/f05b8b10 tidQ : 5494 tidA : 6493 Thread state is : Running Priority : 192 Allocated Stack : 16384 T2 : Name : /frpc/f05b8b10 tidQ : 7553 tidA : 6495 Thread state is : Running Priority : 192 Allocated Stack : 16384 T3 : Name : tidQ : 6534 Thread state is : Running DSP Non-FastRPC Threads: T4 : Thread Name : user_reaper Priority : 64 Allocated Stack : 4096 T5 : Thread Name : gc_thread Priority : 192 Allocated Stack : 4096 T6 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 T7 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 T8 : Thread Name : exception_handl Priority : 192 Allocated Stack : 4096 T9 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 T10 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 T11 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ********************************************************************************** Total memory borrowed from HLOS by all PDs in KB : 512.00 Number of times memory borrowed from HLOS : 1 Total Threads created on CDSP by all PDs: 11 ********************************************************************************** DCVS service This service is used to enable or disable the Dynamic Clock Voltage Scaling (DCVS) feature executing on each DSP. Usage sysMonApp dcvs <enable/disable> [--q6 <dsp>] Where: enable or disable enables or disables DCVS on the selected DSP until the target is rebooted or this setting is modified again. --q6 option allows you to specify the DSP to query: adsp , cdsp , or sdsp . Example To enable DCVS on the aDSP: adb shell /data/local/tmp/sysMonApp dcvs enable Domain Configured ADSP Successfully enabled adsp DCVS Clocks service Use clocks service to set or reset the DSP core clock and bus clocks of the specified DSP. Usage Three clocks service actions are available: set remove limit Clocks set Use the clocks set service to set the minimum clock frequency for a given subsystem or vote for a sleep latency: sysMonApp clocks set [options] Options Values Description --coreClock MHz Sets the minimum clock speed at which the DSP should run. --busClock MHz Sets the minimum AXI (DSP <-> AXI) bus frequency when the DSP is active. --hcpBusClock MHz Sets the minimum HCP (HCP <-> DDR) bus frequency (cDSP only). --dmaBusClock MHz Sets the minimum DMA (DMA <-> DDR) bus frequency (cDSP only). --sleepLatency uSec Specified DSP sleep latency vote. Used by the sleep driver to choose one of the appropriate low power modes that satisfy the latency requirement. --q6 adsp, cdsp, sdsp Execute the service for the selected DSP. NOTE: A vote of 0 resets the settings for that option. Clocks limit Use the clocks limit service to put an upper limit (maximum) on the selected DSP core clock: sysMonApp clocks limit --coreClock <frequency_MHz> [--q6 <dsp>] The DSP core clock is capped at the nearest available clock frequency even when the applications request an available higher frequency. Given that each target has a finite set of available clock rates, you can select exactly one of these clock rates by setting the minimum (with the clocks set service) and maximum (with the clocks limit service) rates. If a selected range does not include a valid clock rate, the system will be forced to pick a clock rate outside the requested range. Clocks remove Use the clock remove service to reset all clock settings to their default values: sysMonApp clocks remove [--q6 <dsp>] Examples NOTE: All sysMonApp invocations are followed by calling the getstate service to show the new DSP clock settings. For the sake of brevity, these calls are not reproduced in the following command output examples. To set the CDSP core clock speed to 400 MHz and the DMA clock speed to 100 MHz: adb shell /data/local/tmp/sysMonApp clocks set clocks set --coreClock 400 --dmaBusClock 100 --q6 cdsp Domain Configured Compute DSP Calling CDSP set clocks function with following parameters: Core clock : 400 MHz DMA AXI clock : 100 MHz Successfully set the required clock configurations, Call the remove API once done... Example: sysMonApp clocks remove To reinitialize the clock speeds to their default values: adb shell /data/local/tmp/sysMonApp clocks remove --q6 cdsp Domain Configured Compute DSP Resetting clock votes and limits... Successful in resetting the votes and limits... To disable the power collapse of the aDSP, use a low sleep latency: adb shell /data/local/tmp/sysMonApp clocks set --sleepLatency 10 Domain Configured ADSP Calling adsp set clocks function with following parameters: Sleep latency vote : 10usec tinfo service For each active software thread, use the tinfo service to display the thread's priority, declared stack size, and maximum stack used. Usage sysMonApp tinfo [--getstack <PD>] [--q6 <dsp>] Where <PD> specifies the protection domain (PD) on which to request information. The values are: audio sensors user all (to display information for all PDs) Examples To see the software thread information for the audio PD on the aDSP: adb shell /data/local/tmp/sysMonApp tinfo --getstack audio Domain Configured ADSP ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ThreadName ThreadID ThreadPrio PID STID DeclaredStackSize(Bytes) MaxUsedStackSize(Bytes) ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ audio_process_r 243 32 1 0 4096 484 user_reaper 240 32 1 0 4096 248 rcinit 239 112 1 0 6144 492 rcinit_worker 238 94 1 0 6144 1528 UIST 237 60 1 0 1952 0 TMR_CLNT_1 236 19 1 0 4096 276 DIAG_LSM 235 237 1 0 4096 476 /frpc/AudioPD 234 192 1 0 4096 844 dog_vir_task1 233 93 1 0 4096 476 dog_hb 232 121 1 0 4096 412 NPA_ASYNC_EVENT 231 48 1 0 8192 136 NPA_ASYNC_REQUE 230 29 1 0 8192 136 GPIOINT_SRV 229 4 1 0 4096 136 DALTF_TH_0 228 252 1 0 8192 48 DALTF_TH_1 227 253 1 0 8192 48 DALTF_TH_2 226 254 1 0 8192 48 DALTF_TH_3 225 255 1 0 8192 48 IPCRTR_RDR 224 204 1 0 4096 640 QMI_PING_SVC 223 10 1 0 2560 540 gen_cb_ctxt 222 205 1 0 2048 204 sr_notif_worker 221 93 1 0 4096 960 sr_notif_signal 220 93 1 0 2048 80 UTMR_CLNT_1 218 18 1 0 4096 168 217 94 1 0 4096 248 qdssc_svc_task 216 10 1 0 4096 48 smlworker 215 152 1 0 4096 840 i2c_qdi_cb 214 14 1 0 4096 344 APR_QDI_USR 213 50 1 0 8064 608 AMDB0 212 69 1 0 8192 728 AMDB1 211 69 1 0 8192 548 AMDB2 210 69 1 0 8192 648 AMDB3 209 69 1 0 8192 728 AVT 208 1 1 0 4096 104 hw_af_ist 207 2 1 0 1024 80 VTM 206 11 1 0 2560 184 VDS1 205 12 1 0 2560 208 VMX1 204 32 1 0 4096 296 VMX2 203 32 1 0 4096 296 VPM 202 52 1 0 12288 336 VSM 201 35 1 0 16384 368 AfeS 200 34 1 0 5120 1520 MXAT 199 38 1 0 8192 336 MXAR 198 38 1 0 8192 820 dma_typ_dflt_is 197 2 1 0 1024 80 dma_typ_hdmi_is 196 2 1 0 1024 80 AfeDataMgr 195 31 1 0 4096 192 CodecIntHldr 194 37 1 0 4096 860 RXSR 193 38 1 0 4096 312 TXSR 192 38 1 0 4096 312 ADM 191 37 1 0 12544 812 ASM 190 37 1 0 16640 736 VfrD 189 10 1 0 4096 480 LSM 188 71 1 0 2048 336 USM 187 69 1 0 4096 384 AfeANC 186 90 1 0 5120 320 ACS 184 37 1 0 16384 792 MVM 183 50 1 0 4096 712 CVD_CAL_LOGG 182 109 1 0 2048 364 SlimBusQmiSvc 181 39 1 0 4096 1068 usb 180 32 1 0 2040 420 irq11 177 4 1 0 2048 344 SlimBusMsg 176 207 1 0 4088 624 SlimBusMaster 175 207 1 0 4088 616 irq86 173 4 1 0 2048 96 SlimBusMsg 172 207 1 0 4088 584 SlimBusMaster 171 207 1 0 4088 544 irq87 4266 4 1 0 2048 336 err_ex_pd_1 167 1 1 0 4096 248 mem_gc_thread 166 192 1 0 4096 56 /frpc/audiopd 165 192 1 0 16384 1172 /frpc/audiopd 164 192 1 0 16384 988 irq12 94382 4 1 0 2048 336 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ To see the software thread information for the user PD on the cDSP: adb shell /data/local/tmp/sysMonApp tinfo --getstack user --q6 cdsp Domain Configured Compute DSP ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ThreadName ThreadID ThreadPrio PID STID DeclaredStackSize(Bytes) MaxUsedStackSize(Bytes) ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ /frpc/c04d3240 41193 32 12 0 65536 940 user_reaper 41190 32 12 0 4096 248 mem_gc_thread 61666 192 12 0 4096 64 HAP_par_thread_ 61662 192 12 0 8192 540 exception_handl 61669 192 12 0 4096 248 /frpc/c04d3240 41187 192 12 0 16384 1196 /frpc/c04d3240 61663 192 12 0 4096 3136 HAP_par_thread_ 61674 192 12 0 8192 2220 HAP_par_thread_ 209124 192 12 0 8192 1008 HAP_par_thread_ 41191 192 12 0 8192 752 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ NOTE: The PID value allows you to distinguish between the different user PDs that are running. Thread level profile (TLP) service The TLP service provides profiling information for specified software threads. The profiling information includes cycles spent and packets executed per software thread. The service also provides a --profile option to run the sysmon profiler service in parallel to collect all DSP profiling data along with the software thread-specific information. Usage sysMonApp tlp [options] Options Values Description --samplingPeriod Integer>=1. Default=50 Sampling period in milliseconds for collecting profile statistics for a given software thread --profile 0 (default)/1 Execute the sysmon profiler in parallel (1). Execute the TLP service only (0). --tname Thread names (case sensitive) to profile separated by , . Default = Profile all active threads. --q6 adsp, cdsp, sdsp Execute the service for the selected DSP. NOTE: Thread names can be accessed by using the tinfo service . To end the profiling service, press Enter . Then enter the following command to retrieve the output binary file created in /sdcard (or /data ): adb pull /sdcard/sysmontlp_<x>dsp.bin Where takes the value a , c , or s to designate the aDSP, cDSP, or sDSP. If the sysmon profiler was executed in parallel using the --profile 1 option, follow the data collection instructions above to retrieve the generated output. Postprocessing Refer to the sysMon parser documentation for postprocessing TLP output binary files. sysmon parser can parse the output from the TLP service with or without the sysmon profiler output. ETM trace service Use the ETM service to trace the instruction and data memory on the cDSP only. The service also provides information about the dynamic modules that are loaded on the cDSP. Usage Two ETM trace services commands are available: dll etm DLL command The DLL command provides information about all the dynamic modules loaded on the cDSP. It also provides the information required to run the Hexagon Trace Analyzer tool . Usage: sysMonApp etmTrace --command dll For each loaded DLL, the following information is provided: ELF_NAME: Name of the dynamic module that was loaded. LOAD_ADDRESS: Virtual address where the dynamic module was loaded. LOAD_TIMESTAMP: Timestamp when the dynamic module was loaded. UNLOAD_TIMESTAMP: Timestamp when the dynamic module was unloaded. NOTE: This command can take around 30 seconds to complete. Following is an example of output data showing when the FastRPC shell was loaded and unloaded. data.ELF_NAME = fastrpc_shell_3 data.LOAD_ADDRESS = 0xe0d00000 data.ELF_IDENTIFIER = 0x00000000 data.LOAD_TIMESTAMP = 0x14b46e27fb1a data.UNLOAD_TIMESTAMP = 0x14b46e3abe76 NOTE: A load address of 0x0 means the library was loaded statically (it is part of the DSP image). ETM command The ETM command traces instruction and data memory on the cDSP. For information on how to retrieve and parse the generated trace file, see the documentation on the Hexagon Trace Analyzer . There are three supported trace modes: Trace mode Description default ETM does not send any cycle information. Keeps the amount of data sent by ETM to a minimum. This mode is helpful in analyzing the flow trace but does not help for any performance analysis. cycle-accurate ETM sends cycle information for each packet. This results in emitting data at a higher rate per instruction and can occasionally lead to data loss due to buffer overlow. This mode is required to postprocess the trace with the Hexagon Trace Analyzer . cycle-coarse ETM still tracks cycles but the tracking is not done per instruction, and thus it results in a lower data rate per instruction. It might be a good compromise for some performance analysis where the cycle-accurate mode overflows. Usage sysMonApp etmTrace --command etm [--etmType <etm_type>] Where the following values are supported for : etm_type Description pc_mem Trace instruction and data memory. (Default.) pc Trace instruction memory. mem Trace data memory. cc_pc Trace instruction memory with cycle-coarse mode. cc_mem Trace data memory with cycle-coarse mode. cc_pc_mem Trace instruction and data memory with cycle-coarse mode. ca_pc Trace instruction memory with cycle-accurate mode. ca_mem Trace data memory with cycle-accurate mode. ca_pc_mem Trace instruction and data memory with cycle-accurate mode. HTA profiler service Use the HTA profiler service to profile services running on the HTA to gather the following information: Clocks voted for DDR read and write bandwidths HTA activity statistics like HTA active time Measured and maximum inferences per second Number of layers Bus bandwidth metrics These profiling metrics are useful in measuring performance, debugging performance-related issues, and identifying possible optimizations. Usage adb shell /data/local/tmp/sysMonApp profiler --q6 HTA [--cdsp 1] Where the --cdsp 1 option allows the cDSP to be profiled in parallel with the HTA. Data collection The HTA profiler service stores raw profiling data in either the sysmon_HTA.bin or sysmon_cdsp_HTA.bin file, depending whether the cDSP was profiled in parallel with the HTA or not. These files are stored in the /sdcard/ or /data/ folders. The sysMonApp profiler also prints the output file path and file name in the standard output. When you are finished with profiling, pull the file from the device and postprocess it using the sysmon parser on a host machine. To pull the profiler output file using ADB: adb pull /sdcard/sysmon[_cdsp]_HTA.bin <destination directory> Data postprocessing To postprocess the HTA profiling data, see the instructions on how to use the sysmon parser .","title":"sysMonApp"},{"location":"tools/sysmon_app.html#sysmonapp","text":"sysMonApp is an Android application that configures and displays from multiple performance-related services on the cDSP, aDSP, and sDSP. These services are explained in the following sections. Service Description Profiler Profile any DSP to gather clock information, resource usage, processor and thread load distribution, bus bandwidth metrics, and so on. Getstate Get the current clock voting information and heap statistics of each static Protection Domain. DCVS Enable or disable the Dynamic Clock Voltage Scaling (DCVS) feature executing on each DSP. Clocks Set the DSP core clock and bus clocks. Thread info For each active software thread, display their priority, declared stack size, and maximum stack used. TLP Thread-level profiling (TLP) that monitors software thread activity. ETM trace Enable the ETM to trace the instruction and data memory and provide information about dynamic modules loaded on the cDSP. HTA profiler Profile the HTA to gather clock information, DDR bandwidth, and various HTA activity statistics. getPowerStats This service reports time spent in power collapse, low power island (LPI) and each core clock level of selected Q6 subsystem since device boot up or last reset. pinfo Display the statistics (e.g. thread priority, stack size) of FastRPC-spawned user processes. sysMonApp supports all targets covered by the Hexagon SDK with the exception of Lahaina. For more details, see the feature matrix .","title":"sysMonApp"},{"location":"tools/sysmon_app.html#setup","text":"The Hexagon SDK includes the following LA and LE variants of sysMonApp. These variants are present at the indicated locations: LA 32-bit version: ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonApp LA 64-bit verion: ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonApp_64Bit LE 32-bit version: ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonAppLE LE 64-bit verion: ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonAppLE_64Bit Push the version for your device to a location of your choice, and then change the permissions to make the file an executable. For example, on rooted Android devices, we recommend you push your executable into a specified folder as part of the data partition: adb push ${HEXAGON_SDK_ROOT}/tools/utils/sysmon/sysMonApp /data/local/tmp/ adb shell chmod 777 /data/local/tmp/sysMonApp Execute sysMonApp from the ADB shell by entering the following command: adb shell /data/local/tmp/sysMonApp <service> <arguments related to the service> When sysMonApp is executed without a service name, the help page is displayed: adb shell /data/local/tmp/sysMonApp -------------------- sysMonApp user guide: -------------------- Supported features : 1. profiler: Collect performance statistics of Q6 Available Options for adsp/sdsp/cdsp : (If no argument then uses default): --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP --debugLevel (default:1 - default mode) Input 0 for user mode 0 - User mode logging with 4 PMU events per set 1 - Default mode logging 2 - User mode logging with 8 PMU events per set --profileFastrpcTimeline Profile the time taken by FastRPC communication 0 - Disable 1 - Enable --noMeasured Controls capturing measured clocks periodically (default = 0) 0 - Report measured bus clock frequencies periodically 1 - Disable reporting measured bus clock frequencies Following options are valid only for 'Default mode' logging --samplingPeriod sampling period in ms(default:0 ms - no sampling period override - to be decided by sysMon --duration profiling duration in seconds(default:10s) --dcvsOption 1 - Keep DCVS enabled (if not disabled externally by other clients) for this profiling 0 - disable DCVS for this profiling. Use this option if you want to fix a particular sampling rate. (Default: 1 - Do not disable DCVS) Following options are valid only for 'user mode' logging --samplingPeriod sampling period in ms(default:1 ms) --dcvsOption 1 - Keep DCVS enabled (if not disabled externally by other clients) for this profiling 0 - disable DCVS for this profiling. Use this option if you want to fix a particular sampling rate. (Default: 0 - Disables DCVS) For user mode logging, if you want to override default PMU events used by sysmon app, create /data/pmu_events.txt file with comma separated PMU events in HEX format Example: ./sysMonApp profiler --samplingPeriod 100 Example: ./sysMonApp profiler --q6 adsp --samplingPeriod 100 Example: ./sysMonApp profiler --q6 cdsp --samplingPeriod 100 Available Options for HTA : (If no argument then uses default): --q6 (default selected processor:CDSP): HTA - Selected processor HTA --samplingPeriod sampling period in ms(default:1 ms --cdsp (default:0 - Profiles HTA alone 0 - Profiles HTA alone 1 - Profiles both HTA and CDSP Example: ./sysMonApp profiler --q6 HTA Example: ./sysMonApp profiler --q6 HTA --cdsp 1 2. tlp: Collect thread level performance statistics of Q6 Available Options : (If no argument then uses default): --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP --samplingPeriod sampling period in ms(default:50 ms) --duration profiling duration in seconds(default:10s) --tName Thread names to profile separated with comma(default: Profile all active threads) --profile Runs Sysmon profile in parallel with TLP(default:0) 0 - Sysmon Profile is not executed 1 - Sysmon Profile is executed Example: ./sysMonApp TLP Example: ./sysMonApp TLP --samplingPeriod 50 Example: ./sysMonApp TLP --samplingPeriod 10 --profile 1 Example: ./sysMonApp TLP --samplingPeriod 5 --profile 1 --tName AfeDy4000,MXAR 3. dcvs: Enable or disable DCVS ./sysMonApp dcvs <enable/disable> ./sysMonApp dcvs --q6 sdsp <enable/disable> ./sysMonApp dcvs --q6 adsp <enable/disable> --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP enable: Will enable SysMon DCVS in ADSP if previously disabled disable: Will disable SysMon DCVS in ADSP 4. getstate: Get present clock, heap state Available Options for getstate: (If no argument then uses default parameters): --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP --getvotes (default selected value:0): 1 - List the DCVS_V2/V3 votes done via HAP_power_set() 0 - DCVS_V2/V3 votes are not queried Example : ./sysMonApp getstate --q6 sdsp Example : ./sysMonApp getstate --getvotes 1 --q6 adsp Example : ./sysMonApp getstate 5. clocks: For changing Q6 related clocks ./sysMonApp clocks <set/remove/limit> <options> Available Options: --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP Available Options (for set option): --coreClock <in MHz>(default:0) --busClock <in MHz>(default:0) --hcpBusClock <in MHz>(default:0) --dmaBusClock <in MHz>(default:0) --sleepLatency <in micro-seconds>(default:0) An explicitly vote of 0 removes any previous vote for that option Leave the defaults for the clocks you don't want to change A remove has to be called to revert all the settings once done limit - Limits only the coreClock, all the other clients voting for core clock will be overwritten Example: ./sysMonApp clocks set --coreClock 200 --busClock 100 Example: ./sysMonApp clocks remove Example: ./sysMonApp clocks limit --coreClock 400 6. ./sysMonApp tinfo :Print the all SW threads info at that instance Available Options : (If no argument then uses default): --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP --getstack Provides the declared and max used Stack Size root - Provides Stack info of rootPD audio - Provides Stack info of audioPD sensors - Provides Stack info of sensorsPD user - Provides Stack info of userPD all - Provides Stack info of allPDs If none of the above match provides the defaults Example: ./sysMonApp tinfo Example: ./sysMonApp tinfo --getstack root Example: ./sysMonApp TLP --getstack all 7. ./sysMonApp benchmark :Run benchmark application with desired functions 8. etmTrace: Etm Trace Enablement || DLL Loaded Output ./sysMonApp etmTrace --command (default selected etm): etm - Enable ETM tracing dll - Output DLL --etmType (default selected pc_mem): pc - PC Only Collection mem - Memory Only Collection pc_mem - PC + Memory Collection cc_pc - Cycle Coarse PC Only Collection cc_mem - Cycle Coarse Memory Only Collection cc_pc_mem - Cycle Coarse PC + Memory Collection ca_pc - Cycle Accurate PC Only Collection ca_mem - Cycle Accurate Memory Only Collection ca_pc_mem - Cycle Accurate PC + Memory Collection --d (default 0): 1 - Debug prints enabled 0 - Debug prints disabled 9. DSP Power Statistics getPowerStats: Report time spent in power collapse, low power island (LPI) and each core clock level of selected Q6 subsystem since device boot up or last reset. Available options:(If no argument then uses default) --q6 (default selected processor:ADSP): adsp - Selected processor ADSP sdsp - Selected processor Sensors DSP cdsp - Selected processor CDSP --clear 1 - Clear the stats collected so far and start fresh logging (Default: Statistics are accumulated and not cleared) Example : ./sysMonApp getPowerStats --q6 cdsp Example : ./sysMonApp getPowerStats --clear 1 --q6 adsp 10. pinfo: Displays the statistics related to FastRPC-spawned user process Example : ./sysMonApp pinfo --q6 cdsp","title":"Setup"},{"location":"tools/sysmon_app.html#profiler-service","text":"Use the sysMonApp profiler option to profile services running on one of the Hexagon DSPs. Gather information such as: The clocks voted for Resource usage Load distribution across available hardware threads Load on the processor Bus bandwidth metrics And other profiling metrics The metrics are useful in measuring performance, debugging performance related issues, and identifying possible optimizations. To see the profiler service help page: adb shell /data/local/tmp/sysMonApp profiler -help","title":"Profiler service"},{"location":"tools/sysmon_app.html#usage","text":"sysMonApp profiler [options] Options Expected values Default value Description --samplingPeriod Integer >=0 0 if debugLevel==1 1 otherwise Sampling period (in milliseconds) at which the eight PMU events are collected. When samplingPeriod is set to 0, the sampling interval is chipset and subsystem dependent but usually on the order of 1 through 50 ms. --debugLevel 0/1/2 Chipset dependent Select a sampling mode. 0 - User mode with four customized PMU counters. 1 - Default mode. 2 - User mode with eight customized PMU counters. See below for more details. --dcvsOption 0/1 0 if debugLevel==1 1 otherwise Disable (0) or enable (1) DCVS for the profiling duration. --profileFastrpcTimeline 0/1 0 Disable (0) or enable (1) the option of collecting the time spent by the FastRPC interface. --noMeasured 0/1 0 Enable (0) or disable (1) reporting measured bus clock frequencies. --stidArray Comma-separated Software Thread ID (STID) values disabled User-provided list (comma separated) of STID values. For more details, see below . --q6 adsp, cdsp, sdsp adsp Select the Hexagon DSP to profile.","title":"Usage"},{"location":"tools/sysmon_app.html#sampling-modes","text":"Three sampling modes are supported: Default mode, User mode with four customized PMU counters, and User mode with eight customized PMU counters. In Default mode, a fixed set of eight PMU events (MPPS, AXI read and write bandwidth, HVX MPPS, and so on) are monitored. (For more details, see the sysMon parser metric description .) The eight PMU events chosen in this mode are specific to the chipset and subsystem (aDSP/cDSP/sDSP). You cannot override these events. DCVS is enabled by default in this mode. In User mode, you can configure which PMU events to assign to four (or eight) of the PMU counters. Further, you can also use the --stidArray option to configure the counters to only increment when the configured events occur on specified software threads. Select any number of PMU events to count in a /data/pmu_events.txt file. sysMonApp rotates through the requested events, four (or eight) at a time, to generate a sampling of all the requested events. If the --stidArray option is used to confine the counting to only specified software threads, sysMonApp further rotates through the specified software threads for each set of four (or eight) PMU events assigned to the PMU counters at a given time. For example, in User mode 0 (four user-defined PMUs), assume an STID array specifies two threads and 10 user-defined events: The sysMon profiler service collects a set of samples for the first four user-defined PMUs for the first STID, and then for the second STID. It then moves on to collect samples for the next four user-defined PMU events for the first STID, and then the second STID. Next, it collects samples for both the last two and again the first two user-defined PMU events, for the first STID, and then the second STID, and so on. Throughout the entire time, the profiler also collects samples for four predefined PMU events. These events are not filtered by STID: they are collected for all software threads with an STID equal to 0.","title":"Sampling modes"},{"location":"tools/sysmon_app.html#pmu-selection","text":"As explained earlier, STIDs allow you to filter some PMU events captured by the profiler service. To filter PMU counters by software thread, follow these steps: Programmatically assign an STID to each thread that is to be filtered by the profiler. Use the qurt_thread_attr_enable_stid API at the time of thread creation. The second parameter of this API is an 8-bit unsigned number that has the following meaning: 0: No STID is assigned. No PMU-based filtering is applied to this thread. 1: QuRT assigns an STID that is not already in use. 2 through 255: This value is used as the STID value. QuRT does not check whether that STID is already in use, and thus multiple threads might share the same STID. The following code snippet shows how to use qurt_thread_attr_enable_stid : // Standard way of setting some thread attributes qurt_thread_attr_t attr; qurt_thread_attr_init (&attr); qurt_thread_attr_set_name(&attr, p_ThreadName); qurt_thread_attr_set_stack_addr (&attr, p_StackBase); qurt_thread_attr_set_stack_size (&attr, stackSize); /* The following line enables STID allocation to the software thread being created by * requesting QuRT to assign an available STID to the thread during qurt_thread_create. */ qurt_thread_attr_enable_stid(&attr, 1); result = qurt_thread_create(&tid, &attr, (void*)entry, NULL); Use the tinfo service while the application is running to determine or confirm which STIDs are assigned to the threads on which STID filtering is required. Run the sysMonApp profiler service with the --stidArray option that lists the STIDs. For example, to start the profiler service on the cDSP, enter the following command: adb shell /data/local/tmp/sysMonApp profiler --debugLevel 0 --stidArray 1,2,3,4 --q6 cdsp This command starts the profiler in User mode with four customized PMU events ( debugLevel 0 ). The --q6 option overrides the default, which is \\adsp`. As explained previously , four user-configured PMU events are selected per sample in User mode, while the other four are the defaults and remain constant. STID filtering only applies to the four user-configured PMU events, and the defaults are not filtered (STID mask is 0) and thus collected continuously. In a profiling sample captured over a provided sampling period, only one STID value at a time is applied as a filter to the four user-configured PMU counters. In this example, a sequence of 50 PMU events for threads with STID values of 1, 2, 3, and 4 are collected one at a time. NOTES: By default, STID-based filtering is disabled. You can enable it by using the --stidArray option in conjunction with --debugLevel to select one of the two user modes. Not all PMU events are maskable under STID. An STID cannot be assigned to a thread that has already been created. Due to time division multiplexing (iterating over provided STIDs over profiling samples), selecting multiple STIDs as filters might not provide a complete picture for a given STID (missing time slots where other STID is configured). In cases where a continuous (time domain) filtering is required per STID, pass only one STID to sysMonApp. The --stidArray list has a limit of eight STIDs.","title":"PMU selection"},{"location":"tools/sysmon_app.html#usage-examples","text":"Run the sysMonApp profiler service in User mode. adb shell /data/local/tmp/sysMonApp profiler --debugLevel 0 Starting Profiler with parameters: Q6 Processor: adsp Sampling Interval in ms : 1 Total samples :0 samplesInSet: 50 Default Mode : 0 dcvs enable : 0 no. of stids: 0 Domain Configured ADSP Q6 architecture detected as v66... Opening /data/pmu_events.txt file /data/pmu_events.txt not found, going ahead with default events, no. of events = 84 opening outputfile @/sdcard/sysmon.bin Enabling DSP SysMon using FastRPC Allocating output buffer >> Starting thread to Query DSP SysMon for samples >> Waiting for a keyboard iHTAt... Run the sysMonApp profiler service in Default mode. adb shell /data/local/tmp/sysMonApp profiler --debugLevel 1 Starting Profiler with parameters: Q6 Processor: adsp Sampling Interval in ms : 0 Total samples :0 samplesInSet: 50 Default Mode : 1 dcvs enable : 1 no. of stids: 0 Domain Configured ADSP Q6 architecture detected as v66... opening outputfile @/sdcard/sysmon.bin Enabling DSP SysMon using FastRPC Allocating output buffer >> Starting thread to Query DSP SysMon for samples >> Waiting for a keyboard iHTAt... Run the sysMonApp profiler and capture samples at sampling intervals of 10 ms on the cDSP in User mode, and capture the FastRPC timeline packets. adb shell /data/local/tmp/sysMonApp profiler --debugLevel 0 --samplingPeriod 10 --q6 cdsp --profileFastrpcTimeline 1 Starting Profiler with parameters: Q6 Processor: cdsp Sampling Interval in ms : 10 Total samples :0 samplesInSet: 50 Default Mode : 0 dcvs enable : 0 no. of stids: 0 Domain Configured Compute DSP Running FastRPC Timeline Profiling in parallel... Q6 architecture detected as v66... Opening /data/pmu_events.txt file /data/pmu_events.txt not found, going ahead with default events, no. of events = 84 opening outputfile @/sdcard/sysmon_cdsp.bin Enabling DSP SysMon using FastRPC Allocating output buffer >> Starting thread to Query DSP SysMon for samples >> Profiling FastRPC Timelines in parallel >> Waiting for a keyboard iHTAt...","title":"Usage examples"},{"location":"tools/sysmon_app.html#data-collection","text":"The sysMonApp profiler stores raw profiling data on the device in either the /sdcard or /data folders: For the aDSP, the file name is sysmon.bin . For the cDSP, the file name is sysmon_cdsp.bin . For the sDSP, the file name is sysmon_sdsp.bin . The sysMonApp profiler also prints the output file path with the appropriate file name in the standard output. When you finish profiling, pull the file from the device and postprocess it using the sysmon parser on a host machine. To pull the profiler output file from device using ADB, enter the following command: adb pull /sdcard/sysmon<DSP>.bin <destination directory> Where <DSP> takes the value _cdsp or _sdsp to designate the cDSP or sDSP, respectively. For the aDSP, do not use <DSP> .","title":"Data collection"},{"location":"tools/sysmon_app.html#data-postprocessing","text":"To postprocess the profiling data, see the instructions on how to use the sysmon parser .","title":"Data postprocessing"},{"location":"tools/sysmon_app.html#getstate-service","text":"Use the getstate service to determine the current clock voting information and heap statistics of each static Protection Domain.","title":"Getstate service"},{"location":"tools/sysmon_app.html#usage_1","text":"sysMonApp getstate [--getVotes <0/1>] [--q6 <dsp>] Where the --q6 option allows you to specify the DSP to query: adsp , cdsp , or sdsp . --getVotes 1 can be used to display DCVS_V2/V3 votes done via HAP_power_set() call.","title":"Usage"},{"location":"tools/sysmon_app.html#example","text":"The following command queries the clock and heap statistics of the ADSP: adb shell /data/local/tmp/sysMonApp getstate Domain Configured ADSP DSP Core clock :576.00MHz SNOC Vote:1.26MHz MEMNOC Vote:0.00MHz GuestOS : Total Heap:1792.00KB Available Heap:514.49KB Max.Free Bin:433.02KB Audio PD : Total Heap:9216.00KB Available Heap:8170.21KB Max.Free Bin:8059.44KB Measured SNOC (/clk/snoc) :200.00MHz Measured BIMC (/clk/bimc) :681.66MHz Measured CPU L3 clock :1612.80MHz The following command queries the clock and heap statistics of the CDSP: adb shell /data/local/tmp/sysMonApp getstate --q6 cdsp Domain Configured Compute DSP DSP Core clock :384.00MHz SNOC Vote:0.00MHz MEMNOC Vote:0.62MHz GuestOS : Total Heap:2560.00KB Available Heap:1622.56KB Max.Free Bin:1575.28KB Measured SNOC (/clk/snoc) :200.00MHz Measured BIMC (/clk/bimc) :681.66MHz Measured CPU L3 clock :1612.80MHz","title":"Example"},{"location":"tools/sysmon_app.html#getpowerstats-service","text":"Reports time spent in power collapse, low power island (LPI) and each core clock level of selected Q6 subsystem since device boot up or last reset.","title":"getPowerStats service"},{"location":"tools/sysmon_app.html#usage_2","text":"sysMonApp getPowerStats [--clear 1] [--q6 <dsp>] Where the --q6 option allows you to specify the DSP to query: adsp , cdsp , or sdsp . --clear 1 can be provided to clear the stats collected so far and start fresh logging.","title":"Usage"},{"location":"tools/sysmon_app.html#example_1","text":"The following command queries the power statistics of the ADSP: adb shell /data/local/tmp/sysMonApp getPowerStats Domain Configured ADSP Clock freq.(MHz) Active time(seconds) 307.20 7.28 576.00 0.01 614.40 9.60 768.00 0.78 940.80 1.93 960.00 0.10 1171.20 0.05 1324.80 0.00 1401.60 0.03 Power collapse time(seconds): 17181.12 Low Power Island time(seconds): 38.59 Current core clock(MHz): 307.20 Total time(seconds): 17239.49 The following command queries the power statistics of the CDSP: adb shell /data/local/tmp/sysMonApp getPowerStats --q6 cdsp Domain Configured Compute DSP Clock freq.(MHz) Active time(seconds) 364.80 0.10 556.80 0.00 768.00 0.08 960.00 0.00 1171.20 0.18 1324.80 0.00 1382.40 0.00 Power collapse time(seconds): 17276.43 Low Power Island time(seconds): 0.00 Current core clock(MHz): 364.80 Total time(seconds): 17276.79 The following command queries the power statistics of the SDSP: adb shell /data/local/tmp/sysMonApp getPowerStats --q6 sdsp Domain Configured Sensors DSP Clock freq.(MHz) Active time(seconds) 423.00 0.23 557.00 0.00 672.00 0.22 845.00 1.18 960.00 0.00 1075.00 0.01 Power collapse time(seconds): 13988.50 Low Power Island time(seconds): 39.77 Current core clock(MHz): 423.00 Total time(seconds): 14029.91","title":"Example"},{"location":"tools/sysmon_app.html#pinfo-service","text":"This service displays FastRPC-spawned user process statistics.","title":"pinfo service"},{"location":"tools/sysmon_app.html#usage_3","text":"sysMonApp pinfo [--q6 <dsp>] Where the --q6 option allows you to specify the DSP to query: adsp , cdsp , or sdsp .","title":"Usage"},{"location":"tools/sysmon_app.html#example_2","text":"The following command displays the FastRPC-spawned user process statistics of the CDSP: adb shell /data/local/tmp/sysMonApp pinfo --q6 cdsp Domain Configured Compute DSP +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Info of UserPD : 1 NAME is : /frpc/f05b8b10 sysMonApp ASID is : 772 PIDA is : 6493 PD State is : InitDone PD Type is : SignedDynamic User heap Used Bytes in KB : 53.58 RPC Total Memory in KB : 512.00 RPC Memory Used in KB : 35.28 UserPD Threads info: DSP FastRPC Threads: T1 : Name : /frpc/f05b8b10 tidQ : 5494 tidA : 6493 Thread state is : Running Priority : 192 Allocated Stack : 16384 T2 : Name : /frpc/f05b8b10 tidQ : 7553 tidA : 6495 Thread state is : Running Priority : 192 Allocated Stack : 16384 T3 : Name : tidQ : 6534 Thread state is : Running DSP Non-FastRPC Threads: T4 : Thread Name : user_reaper Priority : 64 Allocated Stack : 4096 T5 : Thread Name : gc_thread Priority : 192 Allocated Stack : 4096 T6 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 T7 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 T8 : Thread Name : exception_handl Priority : 192 Allocated Stack : 4096 T9 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 T10 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 T11 : Thread Name : HAP_par_thread_ Priority : 192 Allocated Stack : 4096 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ********************************************************************************** Total memory borrowed from HLOS by all PDs in KB : 512.00 Number of times memory borrowed from HLOS : 1 Total Threads created on CDSP by all PDs: 11 **********************************************************************************","title":"Example"},{"location":"tools/sysmon_app.html#dcvs-service","text":"This service is used to enable or disable the Dynamic Clock Voltage Scaling (DCVS) feature executing on each DSP.","title":"DCVS service"},{"location":"tools/sysmon_app.html#usage_4","text":"sysMonApp dcvs <enable/disable> [--q6 <dsp>] Where: enable or disable enables or disables DCVS on the selected DSP until the target is rebooted or this setting is modified again. --q6 option allows you to specify the DSP to query: adsp , cdsp , or sdsp .","title":"Usage"},{"location":"tools/sysmon_app.html#example_3","text":"To enable DCVS on the aDSP: adb shell /data/local/tmp/sysMonApp dcvs enable Domain Configured ADSP Successfully enabled adsp DCVS","title":"Example"},{"location":"tools/sysmon_app.html#clocks-service","text":"Use clocks service to set or reset the DSP core clock and bus clocks of the specified DSP.","title":"Clocks service"},{"location":"tools/sysmon_app.html#usage_5","text":"Three clocks service actions are available: set remove limit","title":"Usage"},{"location":"tools/sysmon_app.html#clocks-set","text":"Use the clocks set service to set the minimum clock frequency for a given subsystem or vote for a sleep latency: sysMonApp clocks set [options] Options Values Description --coreClock MHz Sets the minimum clock speed at which the DSP should run. --busClock MHz Sets the minimum AXI (DSP <-> AXI) bus frequency when the DSP is active. --hcpBusClock MHz Sets the minimum HCP (HCP <-> DDR) bus frequency (cDSP only). --dmaBusClock MHz Sets the minimum DMA (DMA <-> DDR) bus frequency (cDSP only). --sleepLatency uSec Specified DSP sleep latency vote. Used by the sleep driver to choose one of the appropriate low power modes that satisfy the latency requirement. --q6 adsp, cdsp, sdsp Execute the service for the selected DSP. NOTE: A vote of 0 resets the settings for that option.","title":"Clocks set"},{"location":"tools/sysmon_app.html#clocks-limit","text":"Use the clocks limit service to put an upper limit (maximum) on the selected DSP core clock: sysMonApp clocks limit --coreClock <frequency_MHz> [--q6 <dsp>] The DSP core clock is capped at the nearest available clock frequency even when the applications request an available higher frequency. Given that each target has a finite set of available clock rates, you can select exactly one of these clock rates by setting the minimum (with the clocks set service) and maximum (with the clocks limit service) rates. If a selected range does not include a valid clock rate, the system will be forced to pick a clock rate outside the requested range.","title":"Clocks limit"},{"location":"tools/sysmon_app.html#clocks-remove","text":"Use the clock remove service to reset all clock settings to their default values: sysMonApp clocks remove [--q6 <dsp>]","title":"Clocks remove"},{"location":"tools/sysmon_app.html#examples","text":"NOTE: All sysMonApp invocations are followed by calling the getstate service to show the new DSP clock settings. For the sake of brevity, these calls are not reproduced in the following command output examples. To set the CDSP core clock speed to 400 MHz and the DMA clock speed to 100 MHz: adb shell /data/local/tmp/sysMonApp clocks set clocks set --coreClock 400 --dmaBusClock 100 --q6 cdsp Domain Configured Compute DSP Calling CDSP set clocks function with following parameters: Core clock : 400 MHz DMA AXI clock : 100 MHz Successfully set the required clock configurations, Call the remove API once done... Example: sysMonApp clocks remove To reinitialize the clock speeds to their default values: adb shell /data/local/tmp/sysMonApp clocks remove --q6 cdsp Domain Configured Compute DSP Resetting clock votes and limits... Successful in resetting the votes and limits... To disable the power collapse of the aDSP, use a low sleep latency: adb shell /data/local/tmp/sysMonApp clocks set --sleepLatency 10 Domain Configured ADSP Calling adsp set clocks function with following parameters: Sleep latency vote : 10usec","title":"Examples"},{"location":"tools/sysmon_app.html#tinfo-service","text":"For each active software thread, use the tinfo service to display the thread's priority, declared stack size, and maximum stack used.","title":"tinfo service"},{"location":"tools/sysmon_app.html#usage_6","text":"sysMonApp tinfo [--getstack <PD>] [--q6 <dsp>] Where <PD> specifies the protection domain (PD) on which to request information. The values are: audio sensors user all (to display information for all PDs)","title":"Usage"},{"location":"tools/sysmon_app.html#examples_1","text":"To see the software thread information for the audio PD on the aDSP: adb shell /data/local/tmp/sysMonApp tinfo --getstack audio Domain Configured ADSP ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ThreadName ThreadID ThreadPrio PID STID DeclaredStackSize(Bytes) MaxUsedStackSize(Bytes) ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ audio_process_r 243 32 1 0 4096 484 user_reaper 240 32 1 0 4096 248 rcinit 239 112 1 0 6144 492 rcinit_worker 238 94 1 0 6144 1528 UIST 237 60 1 0 1952 0 TMR_CLNT_1 236 19 1 0 4096 276 DIAG_LSM 235 237 1 0 4096 476 /frpc/AudioPD 234 192 1 0 4096 844 dog_vir_task1 233 93 1 0 4096 476 dog_hb 232 121 1 0 4096 412 NPA_ASYNC_EVENT 231 48 1 0 8192 136 NPA_ASYNC_REQUE 230 29 1 0 8192 136 GPIOINT_SRV 229 4 1 0 4096 136 DALTF_TH_0 228 252 1 0 8192 48 DALTF_TH_1 227 253 1 0 8192 48 DALTF_TH_2 226 254 1 0 8192 48 DALTF_TH_3 225 255 1 0 8192 48 IPCRTR_RDR 224 204 1 0 4096 640 QMI_PING_SVC 223 10 1 0 2560 540 gen_cb_ctxt 222 205 1 0 2048 204 sr_notif_worker 221 93 1 0 4096 960 sr_notif_signal 220 93 1 0 2048 80 UTMR_CLNT_1 218 18 1 0 4096 168 217 94 1 0 4096 248 qdssc_svc_task 216 10 1 0 4096 48 smlworker 215 152 1 0 4096 840 i2c_qdi_cb 214 14 1 0 4096 344 APR_QDI_USR 213 50 1 0 8064 608 AMDB0 212 69 1 0 8192 728 AMDB1 211 69 1 0 8192 548 AMDB2 210 69 1 0 8192 648 AMDB3 209 69 1 0 8192 728 AVT 208 1 1 0 4096 104 hw_af_ist 207 2 1 0 1024 80 VTM 206 11 1 0 2560 184 VDS1 205 12 1 0 2560 208 VMX1 204 32 1 0 4096 296 VMX2 203 32 1 0 4096 296 VPM 202 52 1 0 12288 336 VSM 201 35 1 0 16384 368 AfeS 200 34 1 0 5120 1520 MXAT 199 38 1 0 8192 336 MXAR 198 38 1 0 8192 820 dma_typ_dflt_is 197 2 1 0 1024 80 dma_typ_hdmi_is 196 2 1 0 1024 80 AfeDataMgr 195 31 1 0 4096 192 CodecIntHldr 194 37 1 0 4096 860 RXSR 193 38 1 0 4096 312 TXSR 192 38 1 0 4096 312 ADM 191 37 1 0 12544 812 ASM 190 37 1 0 16640 736 VfrD 189 10 1 0 4096 480 LSM 188 71 1 0 2048 336 USM 187 69 1 0 4096 384 AfeANC 186 90 1 0 5120 320 ACS 184 37 1 0 16384 792 MVM 183 50 1 0 4096 712 CVD_CAL_LOGG 182 109 1 0 2048 364 SlimBusQmiSvc 181 39 1 0 4096 1068 usb 180 32 1 0 2040 420 irq11 177 4 1 0 2048 344 SlimBusMsg 176 207 1 0 4088 624 SlimBusMaster 175 207 1 0 4088 616 irq86 173 4 1 0 2048 96 SlimBusMsg 172 207 1 0 4088 584 SlimBusMaster 171 207 1 0 4088 544 irq87 4266 4 1 0 2048 336 err_ex_pd_1 167 1 1 0 4096 248 mem_gc_thread 166 192 1 0 4096 56 /frpc/audiopd 165 192 1 0 16384 1172 /frpc/audiopd 164 192 1 0 16384 988 irq12 94382 4 1 0 2048 336 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ To see the software thread information for the user PD on the cDSP: adb shell /data/local/tmp/sysMonApp tinfo --getstack user --q6 cdsp Domain Configured Compute DSP ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ThreadName ThreadID ThreadPrio PID STID DeclaredStackSize(Bytes) MaxUsedStackSize(Bytes) ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ /frpc/c04d3240 41193 32 12 0 65536 940 user_reaper 41190 32 12 0 4096 248 mem_gc_thread 61666 192 12 0 4096 64 HAP_par_thread_ 61662 192 12 0 8192 540 exception_handl 61669 192 12 0 4096 248 /frpc/c04d3240 41187 192 12 0 16384 1196 /frpc/c04d3240 61663 192 12 0 4096 3136 HAP_par_thread_ 61674 192 12 0 8192 2220 HAP_par_thread_ 209124 192 12 0 8192 1008 HAP_par_thread_ 41191 192 12 0 8192 752 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ NOTE: The PID value allows you to distinguish between the different user PDs that are running.","title":"Examples"},{"location":"tools/sysmon_app.html#thread-level-profile-tlp-service","text":"The TLP service provides profiling information for specified software threads. The profiling information includes cycles spent and packets executed per software thread. The service also provides a --profile option to run the sysmon profiler service in parallel to collect all DSP profiling data along with the software thread-specific information.","title":"Thread level profile (TLP) service"},{"location":"tools/sysmon_app.html#usage_7","text":"sysMonApp tlp [options] Options Values Description --samplingPeriod Integer>=1. Default=50 Sampling period in milliseconds for collecting profile statistics for a given software thread --profile 0 (default)/1 Execute the sysmon profiler in parallel (1). Execute the TLP service only (0). --tname Thread names (case sensitive) to profile separated by , . Default = Profile all active threads. --q6 adsp, cdsp, sdsp Execute the service for the selected DSP. NOTE: Thread names can be accessed by using the tinfo service . To end the profiling service, press Enter . Then enter the following command to retrieve the output binary file created in /sdcard (or /data ): adb pull /sdcard/sysmontlp_<x>dsp.bin Where takes the value a , c , or s to designate the aDSP, cDSP, or sDSP. If the sysmon profiler was executed in parallel using the --profile 1 option, follow the data collection instructions above to retrieve the generated output.","title":"Usage"},{"location":"tools/sysmon_app.html#postprocessing","text":"Refer to the sysMon parser documentation for postprocessing TLP output binary files. sysmon parser can parse the output from the TLP service with or without the sysmon profiler output.","title":"Postprocessing"},{"location":"tools/sysmon_app.html#etm-trace-service","text":"Use the ETM service to trace the instruction and data memory on the cDSP only. The service also provides information about the dynamic modules that are loaded on the cDSP.","title":"ETM trace service"},{"location":"tools/sysmon_app.html#usage_8","text":"Two ETM trace services commands are available: dll etm","title":"Usage"},{"location":"tools/sysmon_app.html#dll-command","text":"The DLL command provides information about all the dynamic modules loaded on the cDSP. It also provides the information required to run the Hexagon Trace Analyzer tool . Usage: sysMonApp etmTrace --command dll For each loaded DLL, the following information is provided: ELF_NAME: Name of the dynamic module that was loaded. LOAD_ADDRESS: Virtual address where the dynamic module was loaded. LOAD_TIMESTAMP: Timestamp when the dynamic module was loaded. UNLOAD_TIMESTAMP: Timestamp when the dynamic module was unloaded. NOTE: This command can take around 30 seconds to complete. Following is an example of output data showing when the FastRPC shell was loaded and unloaded. data.ELF_NAME = fastrpc_shell_3 data.LOAD_ADDRESS = 0xe0d00000 data.ELF_IDENTIFIER = 0x00000000 data.LOAD_TIMESTAMP = 0x14b46e27fb1a data.UNLOAD_TIMESTAMP = 0x14b46e3abe76 NOTE: A load address of 0x0 means the library was loaded statically (it is part of the DSP image).","title":"DLL command"},{"location":"tools/sysmon_app.html#etm-command","text":"The ETM command traces instruction and data memory on the cDSP. For information on how to retrieve and parse the generated trace file, see the documentation on the Hexagon Trace Analyzer . There are three supported trace modes: Trace mode Description default ETM does not send any cycle information. Keeps the amount of data sent by ETM to a minimum. This mode is helpful in analyzing the flow trace but does not help for any performance analysis. cycle-accurate ETM sends cycle information for each packet. This results in emitting data at a higher rate per instruction and can occasionally lead to data loss due to buffer overlow. This mode is required to postprocess the trace with the Hexagon Trace Analyzer . cycle-coarse ETM still tracks cycles but the tracking is not done per instruction, and thus it results in a lower data rate per instruction. It might be a good compromise for some performance analysis where the cycle-accurate mode overflows.","title":"ETM command"},{"location":"tools/sysmon_app.html#usage_9","text":"sysMonApp etmTrace --command etm [--etmType <etm_type>] Where the following values are supported for : etm_type Description pc_mem Trace instruction and data memory. (Default.) pc Trace instruction memory. mem Trace data memory. cc_pc Trace instruction memory with cycle-coarse mode. cc_mem Trace data memory with cycle-coarse mode. cc_pc_mem Trace instruction and data memory with cycle-coarse mode. ca_pc Trace instruction memory with cycle-accurate mode. ca_mem Trace data memory with cycle-accurate mode. ca_pc_mem Trace instruction and data memory with cycle-accurate mode.","title":"Usage"},{"location":"tools/sysmon_app.html#hta-profiler-service","text":"Use the HTA profiler service to profile services running on the HTA to gather the following information: Clocks voted for DDR read and write bandwidths HTA activity statistics like HTA active time Measured and maximum inferences per second Number of layers Bus bandwidth metrics These profiling metrics are useful in measuring performance, debugging performance-related issues, and identifying possible optimizations.","title":"HTA profiler service"},{"location":"tools/sysmon_app.html#usage_10","text":"adb shell /data/local/tmp/sysMonApp profiler --q6 HTA [--cdsp 1] Where the --cdsp 1 option allows the cDSP to be profiled in parallel with the HTA.","title":"Usage"},{"location":"tools/sysmon_app.html#data-collection_1","text":"The HTA profiler service stores raw profiling data in either the sysmon_HTA.bin or sysmon_cdsp_HTA.bin file, depending whether the cDSP was profiled in parallel with the HTA or not. These files are stored in the /sdcard/ or /data/ folders. The sysMonApp profiler also prints the output file path and file name in the standard output. When you are finished with profiling, pull the file from the device and postprocess it using the sysmon parser on a host machine. To pull the profiler output file using ADB: adb pull /sdcard/sysmon[_cdsp]_HTA.bin <destination directory>","title":"Data collection"},{"location":"tools/sysmon_app.html#data-postprocessing_1","text":"To postprocess the HTA profiling data, see the instructions on how to use the sysmon parser .","title":"Data postprocessing"},{"location":"tools/sysmon_parser.html","text":"sysMon Parser The sysMon Parser consumes profiling data generated by the sysMonApp profiler service or sysMon DSP profiler to generate profiling reports. The sysMon Parser executable located in the following folders: Windows version: $HEXAGON_SDK_ROOT/tools/utils/sysmon/parser_win_v2/HTML_Parser Linux version: $HEXAGON_SDK_ROOT/tools/utils/sysmon/parser_linux_v2/HTML_Parser The parser generates an HTML report along with some CSV files. Usage HTML sysMon Parser usage: sysmon_parser <input_file> [--tlp <input_tlp_file>] [--outdir <output_path>] [--summary] Argument Description input_file Input binary generated by the sysMonApp profile service or the sysMon profiler . --tlp <input_tlp_file> Input TLP binary generated by the sysMonApp TLP service . --outdir <output_path> Folder in which to generate the profiling reports. Default is current directory. --summary Generate only a sysMon summary report HTML file and skip the generation of all CSV files. Non-HTML sysMon Parser usage: SysmonParser <input_file> <output_path> <mode_type> Argument Description input_file Input binary generated by the sysMonApp profile service or the sysMon profiler . output_path Folder in which to generate the profiling reports. mode_type Identify which mode was used during the collection of the profiling data. Valid values are default and user . Files generated The sysMon Parser generates the following files from the given input_file : File Description sysmon_report.html Detailed summary report with plots, pie charts, and an HVX analysis (as applicable). post_processed_metrics.csv Includes all metrics computed for all the samples read from the input sysMon raw binary file. This file is useful for plotting metrics against time, and correlating these metrics with other logs. raw_pmu.csv Includes all collected PMU raw values for all samples in the BIN file. This file is useful for computing any metrics relevant to the end user. pmuStats.csv Includes all PMU raw values accumulated for the entire duration of the profiling. (Because not all PMU data are sampled in any given time, missing data is interpolated from the last known values.) Following files are generated when STID/MarkerId enabled: File Description SysMonMarkerID_STID.html Detailed summary report for STID filtered profiling (if enabled) and Markers (if enabled) data. STID_<STID_value>_Metrics.csv Detailed postprocessed metrics of the software thread bearing <STID_value> as its STID. MarkerID_<Marker_value>_PP_Metrics.csv Detailed postprocessed metrics of the marked region bearing <Marker_value> as it's marker identifier. Following files are generated when TLP binary is provided using --tlp option: File Description SysMontlp_report.html Detailed summary report with thread level profiling summary and plots. SysMontlp_rawdata.csv Detailed postprocessed metrics for all the samples read from the input TLP data. SysMontlp_summary_report.csv Summary containing averages per thread. Here are some examples of sysmon_report.html summary report: Here is a sample from a post_processed_metrics.csv file: Sample Index Sampling period(ms) SysClock Time(ms) TimeStamp Effec Q6 Freq(MHz) QDSP6 clk(MHz) Bus Clk vote(MHz) Measured MEMNOC(MHz) MPPS pCPP AXI_RD_BW_128B AXI_WR_BW_128B IU stalls 1 1 16:9:30:657 13.12 1171.2 806 1555.21 2.96 4.43 0.8 0.48 2.1 1 13.8 16:9:30:671 13.12 1171.2 806 1555.21 2.96 4.43 0.8 0.48 2.1 Starting Lahaina, profiling data can capture samples from aDSP/sDSP while in Island mode as well. The parser generates following additional files for samples collected in Island mode: File Description post_processed_metrics_island.csv Includes all Island mode metrics computed from raw samples read from the input sysMon raw binary file. raw_pmu_island.csv Includes raw PMU values corresponding to Island mode operation Here is an example of sysmon_report.html with Island report button under 'Detailed report' column to view a summary of Island mode execution. STID and Markers data Here are some examples of SysMonMarkerID_STID.html report generated with STID filtering/Markers enabled: Here is an example of MarkerID_5_PP_Metrics.csv containing postprocessed metrics associated with the marker ID as 5 (<Marker_value> = 5): TLP data Here is an example of SysMontlp_report.html report: Here is an example of SysMontlp_rawdata.csv report containing detailed thread-level profiling data: Metric descriptions Here are some of the common metrics found in the reports: Metric Unit Description MPPS Million packets per second Number of DSP packets. Average MPPS of a real-time use case is constant and independent of the core clock. An increase in MPPS for a non-real-time use case for a given clock indicates effective utilization of L1 and L2 cache. HVX Thread MPPS Million packets per second Number of packets executed by the HVX coprocessor. MPPS metrics captures both scalar core and HVX core packets. The MPPS executed on the scalar DSP threads can be calculated: DSP scalar Thread MPPS = (MPPS - HVX Thread MPPS) . Effective Q6 frequency MHz Actual load on the processor. The ratio of the effective DSP frequency and NPA core clock frequency can be used to get DSP usage: DSP usage = (Effective DSP frequency / NPA core clock) . A DSP usage percentage approaching 1 indicates that the DSP core must run at a higher frequency to avoid any glitches or frame drops. MPPS and pCPP metrics together can be used to decide if the DSP core clock vote or bus clock vote must be adjusted in this case. pCPP Processor cycles per packet Average processor cycles taken per packet. The lower the pCPP factor, the more the work is done for a given core clock frequency. Core stalls due to bus accesses can result in a higher pCPP factor. Increasing the bus clock vote or prefetching data memory before actual usage can help in lowering this factor and increasing the work done for a given core clock frequency. IU stall frequency MHz Derived from measured cycles that the core has stalled on instruction unit cache accesses due to misses. The higher the IU stall frequency, the higher the pCPP factor can be. DU stall frequency MHz Derived from measured cycles that the core has stalled on accessing L1 data cache lines due to misses. The higher the DU stall frequency, the higher the pCPP factor can be. DMT (Dynamic Multi Threading) uses DU stalls of stalled thread and schedules other threads for efficient utilization of the core clock. AXI cached read/write bandwidth MBps AXI bus bandwidth (DDR accesses) generated by read/write access from the core due to L2 cache line misses. Misses include both demand and prefetch misses in L2 cache. L2 fetch bandwidth MBps Bus bandwidth generated by the L2fetch instruction to prefetch data into L2 cache. Clock votes MHz Core clock vote for the frequency the DSP is running at. A bus clock vote captures the overall DSP vote for bus clock. Also, the final bus clock frequency (done outside of the DSP) will be based on votes from other subsystems (application processor, modem, and so on). Static clock votes MHz Aggregated static votes from all clients for core and bus clocks. DCVS clock votes MHz DCVS vote for core and bus clocks.","title":"sysMon Parser"},{"location":"tools/sysmon_parser.html#sysmon-parser","text":"The sysMon Parser consumes profiling data generated by the sysMonApp profiler service or sysMon DSP profiler to generate profiling reports. The sysMon Parser executable located in the following folders: Windows version: $HEXAGON_SDK_ROOT/tools/utils/sysmon/parser_win_v2/HTML_Parser Linux version: $HEXAGON_SDK_ROOT/tools/utils/sysmon/parser_linux_v2/HTML_Parser The parser generates an HTML report along with some CSV files.","title":"sysMon Parser"},{"location":"tools/sysmon_parser.html#usage","text":"HTML sysMon Parser usage: sysmon_parser <input_file> [--tlp <input_tlp_file>] [--outdir <output_path>] [--summary] Argument Description input_file Input binary generated by the sysMonApp profile service or the sysMon profiler . --tlp <input_tlp_file> Input TLP binary generated by the sysMonApp TLP service . --outdir <output_path> Folder in which to generate the profiling reports. Default is current directory. --summary Generate only a sysMon summary report HTML file and skip the generation of all CSV files. Non-HTML sysMon Parser usage: SysmonParser <input_file> <output_path> <mode_type> Argument Description input_file Input binary generated by the sysMonApp profile service or the sysMon profiler . output_path Folder in which to generate the profiling reports. mode_type Identify which mode was used during the collection of the profiling data. Valid values are default and user .","title":"Usage"},{"location":"tools/sysmon_parser.html#files-generated","text":"The sysMon Parser generates the following files from the given input_file : File Description sysmon_report.html Detailed summary report with plots, pie charts, and an HVX analysis (as applicable). post_processed_metrics.csv Includes all metrics computed for all the samples read from the input sysMon raw binary file. This file is useful for plotting metrics against time, and correlating these metrics with other logs. raw_pmu.csv Includes all collected PMU raw values for all samples in the BIN file. This file is useful for computing any metrics relevant to the end user. pmuStats.csv Includes all PMU raw values accumulated for the entire duration of the profiling. (Because not all PMU data are sampled in any given time, missing data is interpolated from the last known values.) Following files are generated when STID/MarkerId enabled: File Description SysMonMarkerID_STID.html Detailed summary report for STID filtered profiling (if enabled) and Markers (if enabled) data. STID_<STID_value>_Metrics.csv Detailed postprocessed metrics of the software thread bearing <STID_value> as its STID. MarkerID_<Marker_value>_PP_Metrics.csv Detailed postprocessed metrics of the marked region bearing <Marker_value> as it's marker identifier. Following files are generated when TLP binary is provided using --tlp option: File Description SysMontlp_report.html Detailed summary report with thread level profiling summary and plots. SysMontlp_rawdata.csv Detailed postprocessed metrics for all the samples read from the input TLP data. SysMontlp_summary_report.csv Summary containing averages per thread. Here are some examples of sysmon_report.html summary report: Here is a sample from a post_processed_metrics.csv file: Sample Index Sampling period(ms) SysClock Time(ms) TimeStamp Effec Q6 Freq(MHz) QDSP6 clk(MHz) Bus Clk vote(MHz) Measured MEMNOC(MHz) MPPS pCPP AXI_RD_BW_128B AXI_WR_BW_128B IU stalls 1 1 16:9:30:657 13.12 1171.2 806 1555.21 2.96 4.43 0.8 0.48 2.1 1 13.8 16:9:30:671 13.12 1171.2 806 1555.21 2.96 4.43 0.8 0.48 2.1 Starting Lahaina, profiling data can capture samples from aDSP/sDSP while in Island mode as well. The parser generates following additional files for samples collected in Island mode: File Description post_processed_metrics_island.csv Includes all Island mode metrics computed from raw samples read from the input sysMon raw binary file. raw_pmu_island.csv Includes raw PMU values corresponding to Island mode operation Here is an example of sysmon_report.html with Island report button under 'Detailed report' column to view a summary of Island mode execution.","title":"Files generated"},{"location":"tools/sysmon_parser.html#stid-and-markers-data","text":"Here are some examples of SysMonMarkerID_STID.html report generated with STID filtering/Markers enabled: Here is an example of MarkerID_5_PP_Metrics.csv containing postprocessed metrics associated with the marker ID as 5 (<Marker_value> = 5):","title":"STID and Markers data"},{"location":"tools/sysmon_parser.html#tlp-data","text":"Here is an example of SysMontlp_report.html report: Here is an example of SysMontlp_rawdata.csv report containing detailed thread-level profiling data:","title":"TLP data"},{"location":"tools/sysmon_parser.html#metric-descriptions","text":"Here are some of the common metrics found in the reports: Metric Unit Description MPPS Million packets per second Number of DSP packets. Average MPPS of a real-time use case is constant and independent of the core clock. An increase in MPPS for a non-real-time use case for a given clock indicates effective utilization of L1 and L2 cache. HVX Thread MPPS Million packets per second Number of packets executed by the HVX coprocessor. MPPS metrics captures both scalar core and HVX core packets. The MPPS executed on the scalar DSP threads can be calculated: DSP scalar Thread MPPS = (MPPS - HVX Thread MPPS) . Effective Q6 frequency MHz Actual load on the processor. The ratio of the effective DSP frequency and NPA core clock frequency can be used to get DSP usage: DSP usage = (Effective DSP frequency / NPA core clock) . A DSP usage percentage approaching 1 indicates that the DSP core must run at a higher frequency to avoid any glitches or frame drops. MPPS and pCPP metrics together can be used to decide if the DSP core clock vote or bus clock vote must be adjusted in this case. pCPP Processor cycles per packet Average processor cycles taken per packet. The lower the pCPP factor, the more the work is done for a given core clock frequency. Core stalls due to bus accesses can result in a higher pCPP factor. Increasing the bus clock vote or prefetching data memory before actual usage can help in lowering this factor and increasing the work done for a given core clock frequency. IU stall frequency MHz Derived from measured cycles that the core has stalled on instruction unit cache accesses due to misses. The higher the IU stall frequency, the higher the pCPP factor can be. DU stall frequency MHz Derived from measured cycles that the core has stalled on accessing L1 data cache lines due to misses. The higher the DU stall frequency, the higher the pCPP factor can be. DMT (Dynamic Multi Threading) uses DU stalls of stalled thread and schedules other threads for efficient utilization of the core clock. AXI cached read/write bandwidth MBps AXI bus bandwidth (DDR accesses) generated by read/write access from the core due to L2 cache line misses. Misses include both demand and prefetch misses in L2 cache. L2 fetch bandwidth MBps Bus bandwidth generated by the L2fetch instruction to prefetch data into L2 cache. Clock votes MHz Core clock vote for the frequency the DSP is running at. A bus clock vote captures the overall DSP vote for bus clock. Also, the final bus clock frequency (done outside of the DSP) will be based on votes from other subsystems (application processor, modem, and so on). Static clock votes MHz Aggregated static votes from all clients for core and bus clocks. DCVS clock votes MHz DCVS vote for core and bus clocks.","title":"Metric descriptions"},{"location":"tools/sysmon_profiler.html","text":"sysMon DSP Profiler The sysMon DSP Profiler is an Android UI application used to profile the DSP workload. This UI application uses FastRPC to communicate with the SysMon profiler service on the targeted DSP. This application can assist with measuring performance, debugging performance related issues, and identifying possible optimizations by collecting the following metrics: Clocks voted for Resource usage Load distribution across available hardware threads Load on processor Bus bandwidth metrics Other profiling metrics This page captures the steps to set up and use the sysMon DSP Profiler and ways to analyze the captured profiling data. Please refer feature matrix for targets supporting the sysMon profiler. Setup Install $HEXAGON_SDK_ROOT/tools/utils/sysmon/sysMon_DSP_Profiler_V2.apk on the device connected to the host machine by running following ADB command: adb install -g sysMon_DSP_Profiler_V2.apk Interface The home page of the sysMon profiler displays as follows: You can choose from different modes of profiling. All these options map to the options supported and described for the sysMonApp profiler service . DCVS Mode option Select this checkbox to enable the DSP DCVS mode, which can adjust DSP core and bus clocks dynamically for the profiling duration. Default Mode option In this mode, a fixed set of performance metrics (eight PMU events) will be monitored. By default, the sampling period is either 1 or 50 milliseconds. The profiler generates a packet at the end of each sampling period with the performance metrics captured in the window. You can also specify a sampling period in multiples of one millisecond to override the sampling period for a fixed set of performance metrics (eight PMU events). If DCVS mode is selected, specifying a sampling period smaller than 50 milliseconds will result in a 1 millisecond sampling interval; specifying a sampling period greater or equal to 50 milliseconds will result in a 1 or 50 millisecond sampling interval as dictated by the DCVS algorithm. For more details about the Default mode, see the sysMonApp profiler service documentation . User Mode option If the Default mode is not selected, User mode is enabled instead. In User mode, specify the PMU events to be captured by clicking the Configuration button: The following table provides a brief description of all supported PMU events: ID PMU event 0x03 MPPS 0x03 pCPP 0x04 DMTV2 0x07 DMTV3 0x08 SMT 0x2A Instructions executed Per Second(IPS) 0x03 Packets executed Per Second(PPS) 0x7F L2 Fetch Miss 0x11 IU Stall 0xE9 DU Cache Stall 0x41 AXI 32-Byte Line Read 0x43 AXI 32-Byte Line Write 0xCE AXI 64-Byte Line Read 0xCF AXI 64-Byte Line Write 0x3F AXI 128-Byte Line Read ( V66 and beyond only) 0x46 AXI 128-Byte Line Write ( V66 and beyond only) 0x44 AHB Read BW 0x45 AHB Write BW 0x3B 1-Thread Active(%) 0x3C 2-Thread Active(%) 0x3D 3-Thread Active(%) 0x3E 4-Thread Active(%) 0x25 1-Thread CPP 0x26 2-Thread CPP 0x27 3-Thread CPP 0x2F 4-Thread CPP 0x118(v66), 0x112(v68) HVX Thread MPPS 0x11D(v66), 0x11A(v68) HVX L2 Load Miss 0x106(v66), 0x103(v68) HVX Stall Cycles Captured performance metrics are postprocessed and displayed on the UI continuously with a refresh rate of one second. For more details about the User mode, see the sysMonApp profiler service documentation . 8-PMU Mode option This option allows you to configure four PMU events or eight PMU events. If the checkbox is not selected, the option is set for four PMU events (default). Select the checkbox for eight PMU events. Also, the 8-PMU Mode checkbox is selected automatically if both the Default and DCVS modes are not selected. Data collection As explained in the sysMonApp profiler service documentation , the sysMonApp profiler stores raw profiling data in the /sdcard or /data folder on the device: For the aDSP, the file name is `sysmon.bin For the cDSP, the file name is sysmon_cdsp.bin For the sDSP, the file name sysmon_sdsp.bin You can retrieve these files with an adb pull command. For example, the following command retrieves aDSP profiling data: adb pull /sdcard/sysmon.bin <destination directory> Data postprocessing To postprocess the profiling data, see the instructions on how to use the sysmon parser . Command line usage The sysMon DSP Profiler can be invoked from the command line: adb shell am start -n com.qualcomm.qti.sysmonappExternal/com.qualcomm.qti.sysmonappExternal.AdspProfiler [-e <option> <value>] Option Description q6 Select the DSP to profile. 0 (default) for aDSP, 2 for sDSP, 3 for cDSP. defaultMode Select profiling mode. 0 for User mode, 1 (default) for Default mode. samplingPeriod Specified sampling period in milliseconds. By default, 1 ms in User mode, and a combination of 1 ms and 50 ms in Default mode. dcvsMode Set DCVS mode. 0 to disable DCVS during profiling, and 1 to enable DCVS. By default, DCVS is enabled when Default mode is selected; otherwise, it is disabled. pmuMode Set PMU mode. 0 to run in 4-PMU mode, and 1 to run in 8-PMU mode. For example, the following command will open the sysMon DSP Profiler application with the Profiler UI displayed on the screen: adb shell am start -n com.qualcomm.qti.sysmonappExternal/com.qualcomm.qti.sysmonappExternal.AdspProfiler -e q6 2 Once the sysMon DSP Profiler application is opened, run the pair of start and stop commands any number of times on the given DSP. The following command will start the profiler and also allow you to set options: adb shell am broadcast -a com.qualcomm.qti.sysmonapp.RUN_EXT_FROM_ADB -e startProfiler 1 -e q6 2 -e defaultMode 1 -e samplingPeriod 10 -e dcvsMode 0 -n com.qualcomm.qti.sysmonappExternal/com.qualcomm.qti.sysmonappExternal.BootCompleteReceiverExternal` The following command will stop the profiler: adb shell am broadcast -a com.qualcomm.qti.sysmonapp.RUN_EXT_FROM_ADB -e stopProfiler 1 -e q6 2 -n com.qualcomm.qti.sysmonappExternal/com.qualcomm.qti.sysmonappExternal.BootCompleteReceiverExternal Finally, you can close the sysMon DSP Profiler application with the following command: adb shell am force-stop com.qualcomm.qti.sysmonappExternal NOTE: To run the profiler on a different DSP, close the application and re-open it with a different DSP value. The start and stop commands also must use the same new DSP value.","title":"sysMon DSP Profiler"},{"location":"tools/sysmon_profiler.html#sysmon-dsp-profiler","text":"The sysMon DSP Profiler is an Android UI application used to profile the DSP workload. This UI application uses FastRPC to communicate with the SysMon profiler service on the targeted DSP. This application can assist with measuring performance, debugging performance related issues, and identifying possible optimizations by collecting the following metrics: Clocks voted for Resource usage Load distribution across available hardware threads Load on processor Bus bandwidth metrics Other profiling metrics This page captures the steps to set up and use the sysMon DSP Profiler and ways to analyze the captured profiling data. Please refer feature matrix for targets supporting the sysMon profiler.","title":"sysMon DSP Profiler"},{"location":"tools/sysmon_profiler.html#setup","text":"Install $HEXAGON_SDK_ROOT/tools/utils/sysmon/sysMon_DSP_Profiler_V2.apk on the device connected to the host machine by running following ADB command: adb install -g sysMon_DSP_Profiler_V2.apk","title":"Setup"},{"location":"tools/sysmon_profiler.html#interface","text":"The home page of the sysMon profiler displays as follows: You can choose from different modes of profiling. All these options map to the options supported and described for the sysMonApp profiler service .","title":"Interface"},{"location":"tools/sysmon_profiler.html#dcvs-mode-option","text":"Select this checkbox to enable the DSP DCVS mode, which can adjust DSP core and bus clocks dynamically for the profiling duration.","title":"DCVS Mode option"},{"location":"tools/sysmon_profiler.html#default-mode-option","text":"In this mode, a fixed set of performance metrics (eight PMU events) will be monitored. By default, the sampling period is either 1 or 50 milliseconds. The profiler generates a packet at the end of each sampling period with the performance metrics captured in the window. You can also specify a sampling period in multiples of one millisecond to override the sampling period for a fixed set of performance metrics (eight PMU events). If DCVS mode is selected, specifying a sampling period smaller than 50 milliseconds will result in a 1 millisecond sampling interval; specifying a sampling period greater or equal to 50 milliseconds will result in a 1 or 50 millisecond sampling interval as dictated by the DCVS algorithm. For more details about the Default mode, see the sysMonApp profiler service documentation .","title":"Default Mode option"},{"location":"tools/sysmon_profiler.html#user-mode-option","text":"If the Default mode is not selected, User mode is enabled instead. In User mode, specify the PMU events to be captured by clicking the Configuration button: The following table provides a brief description of all supported PMU events: ID PMU event 0x03 MPPS 0x03 pCPP 0x04 DMTV2 0x07 DMTV3 0x08 SMT 0x2A Instructions executed Per Second(IPS) 0x03 Packets executed Per Second(PPS) 0x7F L2 Fetch Miss 0x11 IU Stall 0xE9 DU Cache Stall 0x41 AXI 32-Byte Line Read 0x43 AXI 32-Byte Line Write 0xCE AXI 64-Byte Line Read 0xCF AXI 64-Byte Line Write 0x3F AXI 128-Byte Line Read ( V66 and beyond only) 0x46 AXI 128-Byte Line Write ( V66 and beyond only) 0x44 AHB Read BW 0x45 AHB Write BW 0x3B 1-Thread Active(%) 0x3C 2-Thread Active(%) 0x3D 3-Thread Active(%) 0x3E 4-Thread Active(%) 0x25 1-Thread CPP 0x26 2-Thread CPP 0x27 3-Thread CPP 0x2F 4-Thread CPP 0x118(v66), 0x112(v68) HVX Thread MPPS 0x11D(v66), 0x11A(v68) HVX L2 Load Miss 0x106(v66), 0x103(v68) HVX Stall Cycles Captured performance metrics are postprocessed and displayed on the UI continuously with a refresh rate of one second. For more details about the User mode, see the sysMonApp profiler service documentation .","title":"User Mode option"},{"location":"tools/sysmon_profiler.html#8-pmu-mode-option","text":"This option allows you to configure four PMU events or eight PMU events. If the checkbox is not selected, the option is set for four PMU events (default). Select the checkbox for eight PMU events. Also, the 8-PMU Mode checkbox is selected automatically if both the Default and DCVS modes are not selected.","title":"8-PMU Mode option"},{"location":"tools/sysmon_profiler.html#data-collection","text":"As explained in the sysMonApp profiler service documentation , the sysMonApp profiler stores raw profiling data in the /sdcard or /data folder on the device: For the aDSP, the file name is `sysmon.bin For the cDSP, the file name is sysmon_cdsp.bin For the sDSP, the file name sysmon_sdsp.bin You can retrieve these files with an adb pull command. For example, the following command retrieves aDSP profiling data: adb pull /sdcard/sysmon.bin <destination directory>","title":"Data collection"},{"location":"tools/sysmon_profiler.html#data-postprocessing","text":"To postprocess the profiling data, see the instructions on how to use the sysmon parser .","title":"Data postprocessing"},{"location":"tools/sysmon_profiler.html#command-line-usage","text":"The sysMon DSP Profiler can be invoked from the command line: adb shell am start -n com.qualcomm.qti.sysmonappExternal/com.qualcomm.qti.sysmonappExternal.AdspProfiler [-e <option> <value>] Option Description q6 Select the DSP to profile. 0 (default) for aDSP, 2 for sDSP, 3 for cDSP. defaultMode Select profiling mode. 0 for User mode, 1 (default) for Default mode. samplingPeriod Specified sampling period in milliseconds. By default, 1 ms in User mode, and a combination of 1 ms and 50 ms in Default mode. dcvsMode Set DCVS mode. 0 to disable DCVS during profiling, and 1 to enable DCVS. By default, DCVS is enabled when Default mode is selected; otherwise, it is disabled. pmuMode Set PMU mode. 0 to run in 4-PMU mode, and 1 to run in 8-PMU mode. For example, the following command will open the sysMon DSP Profiler application with the Profiler UI displayed on the screen: adb shell am start -n com.qualcomm.qti.sysmonappExternal/com.qualcomm.qti.sysmonappExternal.AdspProfiler -e q6 2 Once the sysMon DSP Profiler application is opened, run the pair of start and stop commands any number of times on the given DSP. The following command will start the profiler and also allow you to set options: adb shell am broadcast -a com.qualcomm.qti.sysmonapp.RUN_EXT_FROM_ADB -e startProfiler 1 -e q6 2 -e defaultMode 1 -e samplingPeriod 10 -e dcvsMode 0 -n com.qualcomm.qti.sysmonappExternal/com.qualcomm.qti.sysmonappExternal.BootCompleteReceiverExternal` The following command will stop the profiler: adb shell am broadcast -a com.qualcomm.qti.sysmonapp.RUN_EXT_FROM_ADB -e stopProfiler 1 -e q6 2 -n com.qualcomm.qti.sysmonappExternal/com.qualcomm.qti.sysmonappExternal.BootCompleteReceiverExternal Finally, you can close the sysMon DSP Profiler application with the following command: adb shell am force-stop com.qualcomm.qti.sysmonappExternal NOTE: To run the profiler on a different DSP, close the application and re-open it with a different DSP value. The start and stop commands also must use the same new DSP value.","title":"Command line usage"}]}; var search = { index: new Promise(resolve => setTimeout(() => resolve(local_index), 0)) }